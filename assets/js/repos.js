var data =
[
  {
    "data_format": 2,
    "description": "definition files for containers used in Hanlab",
    "filenames": [
      "singularity.R.3.6.3.phylo/R.3.6.3.phylo.def",
      "singularity.phylo/phylo.def",
      "singularity.mkl/mkl.ubuntu.def",
      "singularity.mkl/mkl.def",
      "singularity.SAD/SAD.def",
      "singularity.rnaseq/rnaseq.def",
      "singularity.py37.ml.mkl/py37.ml.mkl.def",
      "singularity.Rconda/R.3.6.3.def",
      "singularity.py37.ml.openblas/py37.ml.openblas.def",
      "singularity.R.4.0.2.Bioc/R.4.0.2.Bioc.def",
      "singularity.R.3.6.3.Bioc/R.3.6.3.Bioc.def"
    ],
    "full_name": "HanLabUNLV/hanlab_singularity_defs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc_mpi_cuda_singu_def_file\" class=\"anchor\" href=\"#hpc_mpi_cuda_singu_def_file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehpc_mpi_cuda_singu_def_file\u003c/h1\u003e\n\u003cp\u003eA collect of definition files to build images for singularity containers, which includes hpc benchmarks and mpis with cuda support.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4181\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1617130184.0
  },
  {
    "data_format": 2,
    "description": "Singularity definition files for building various software to run on HPC systems",
    "filenames": [
      "octopus.def",
      "instrain.def",
      "checkm.def",
      "torstyverse.def"
    ],
    "full_name": "slhogle/singularity_def_files",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-definition-files\" class=\"anchor\" href=\"#singularity-definition-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity definition files\u003c/h1\u003e\n\u003cp\u003eCollection of def files for building some bioinformatics software I commonly use.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-instrain-v1214\" class=\"anchor\" href=\"#instrain-v1214\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003einStrain v1.2.14\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/MrOlm/inStrain\"\u003ehttps://github.com/MrOlm/inStrain\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAlso contains these functioning binaries:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/samtools/samtools\"\u003esamtools v1.10\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hyattpd/Prodigal\"\u003eprodigal v2.6.3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lh3/bwa\"\u003ebwa v0.7.17-r1198-dirty\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lh3/minimap2\"\u003eminimap2 v2.17 (r941)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dnbaker/dashing\"\u003eDashing v0.4.8-1-g47e6\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ParBLiSS/FastANI\"\u003eFastANI v1.3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/wwood/CoverM\"\u003eCoverM v0.4.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/slhogle/base/instrain\" rel=\"nofollow\"\u003eImage at Sylabs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload with:\u003cbr\u003e\n\u003ccode\u003esingularity pull library://slhogle/base/instrain\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-octopus-development-branch-version-v070-develop-2bde0433\" class=\"anchor\" href=\"#octopus-development-branch-version-v070-develop-2bde0433\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOctopus development branch version v0.7.0 (develop 2bde0433)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/luntergroup/octopus\"\u003ehttps://github.com/luntergroup/octopus\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBuilt with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epatchelf v0.10\u003c/li\u003e\n\u003cli\u003eopenssl v1.1.1g\u003c/li\u003e\n\u003cli\u003epkg-config v0.29.2\u003c/li\u003e\n\u003cli\u003egpatch v2.7.6\u003c/li\u003e\n\u003cli\u003encurses v6.2\u003c/li\u003e\n\u003cli\u003ecmake v3.17.3\u003c/li\u003e\n\u003cli\u003ehtslib v1.10\u003c/li\u003e\n\u003cli\u003eboost v1.72.0\u003c/li\u003e\n\u003cli\u003eGNU C/C++ compiler v9.3.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTarget: x86_64 Linux 5.3.0-7642-generic\u003cbr\u003e\nSIMD extension: AVX2\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/slhogle/base/octopus\" rel=\"nofollow\"\u003eImage at Sylabs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload with:\u003cbr\u003e\n\u003ccode\u003esingularity pull library://slhogle/base/octopus\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-torstyverse\" class=\"anchor\" href=\"#torstyverse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTorstyverse\u003c/h2\u003e\n\u003cp\u003eBundle of useful packages from \u003ca href=\"https://github.com/tseemann\"\u003eTorsten Seeman\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/samclip\"\u003esampclip v0.4.0\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/any2fasta\"\u003eany2fasta v0.4.2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/barrnap\"\u003ebarrnap v0.9\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/prokka\"\u003eprokka v1.14.6\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/shovill\"\u003eshovill v1.1.0\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/abricate\"\u003eabricate v1.0.1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tseemann/snippy\"\u003esnippy v4.6.0\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/slhogle/base/torstyverse\" rel=\"nofollow\"\u003eImage at Sylabs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload with:\u003cbr\u003e\n\u003ccode\u003esingularity pull library://slhogle/base/torstyverse\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1614942669.0
  },
  {
    "data_format": 2,
    "description": "singularity def file for flair(fluka)",
    "filenames": [
      "flair-cern.def",
      "flair.def"
    ],
    "full_name": "ifurther/flair-def",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-files\" class=\"anchor\" href=\"#singularity-recipe-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe files\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e containers I use the most on HPC clusters.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619686613.0
  },
  {
    "data_format": 2,
    "description": "Singularity definition files for various projects",
    "filenames": [
      "miniconda/Singularity",
      "hauntedhouse/Singularity",
      "hauntedhouse_freesurfer/Singularity"
    ],
    "full_name": "mvdoc/singularity-def",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-files\" class=\"anchor\" href=\"#singularity-recipe-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe files\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e containers I use the most on HPC clusters.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1495630055.0
  },
  {
    "data_format": 2,
    "description": "My Singularity recipe files",
    "filenames": [
      "asciinema/Singularity.def",
      "julia/Singularity.def",
      "root-cern/Singularity.def",
      "gerda-tgsend/Singularity.def",
      "lilypond/Singularity.def",
      "arch-base/Singularity.def",
      "bat/Singularity.def",
      "centos-base/Singularity.def",
      "texlive/Singularity.def",
      "itunes/Singularity.def"
    ],
    "full_name": "gipert/Singularity.def",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-files\" class=\"anchor\" href=\"#singularity-recipe-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity recipe files\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e containers I use the most on HPC clusters.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "containers"
    ],
    "updated_at": 1587858477.0
  },
  {
    "data_format": 2,
    "description": "Def File of Singularity",
    "filenames": [
      "def/edge-connect.def",
      "def/contextual-attention.def",
      "def/wav2pix.def",
      "def/sc-fegan.def",
      "def/vae-mnist.def",
      "def/singan.def",
      "def/stargan.def",
      "def/lafin.def"
    ],
    "full_name": "Nahuel-Mk2/def-space",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-def-space\" class=\"anchor\" href=\"#def-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edef-space\u003c/h1\u003e\n\u003cp\u003eThis repository is def-space for Singularity\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1606189900.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "timo-singularity/rivet",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-recipes\" class=\"anchor\" href=\"#recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erecipes\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622810530.0
  },
  {
    "data_format": 2,
    "description": "Biobb_structure_utils is the Biobb module collection to modify or extract information from a PDB structure file, such as pulling out a particular model or chain, removing water molecules or ligands, or renumbering or sorting atoms or residues.",
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "bioexcel/biobb_structure_utils",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://biobb-structure-utils.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d7c1b5de86a2921c1f759b175820fb443eba3f18bbf45e56e42f2cee72844627/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62696f62622d7374727563747572652d7574696c732f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"\" data-canonical-src=\"https://readthedocs.org/projects/biobb-structure-utils/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/biobb_structure_utils\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a477fdb1fd9bc9eb7ffa6cae6a019c6d4c3902fd468b3126f1b78e56c7dcff83/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e7376673f7374796c653d666c6174\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://quay.io/repository/biocontainers/biobb_structure_utils\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca418e4db0b3de91a09a5df4a59446da015b6164598a8bc255918e911484f84f/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f636b65722d517561792e696f2d626c7565\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/docker-Quay.io-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3836\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/Apache-2.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2a2157c971b7ae1deb8eb095799440551c33dcf61ea3d965d86b496a5a65df55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d417061636865253230322e302d626c75652e737667\" alt=\"\" data-canonical-src=\"https://img.shields.io/badge/License-Apache%202.0-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-biobb_structure_utils\" class=\"anchor\" href=\"#biobb_structure_utils\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebiobb_structure_utils\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eBiobb_structure_utils is the Biobb module collection to modify or extract information from a PDB structure file, such as pulling out a particular model or chain, removing water molecules or ligands, or renumbering or sorting atoms or residues. Biobb (BioExcel building blocks) packages are Python building blocks that create new layer of compatibility and interoperability over popular bioinformatics tools. The latest documentation of this package can be found in our readthedocs site:\n\u003ca href=\"https://biobb-structure-utils.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003elatest API documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version\" class=\"anchor\" href=\"#version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion\u003c/h3\u003e\n\u003cp\u003ev3.6.1 2021.2\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eUsing PIP:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eImportant:\u003c/strong\u003e PIP only installs the package. All the dependencies must be installed separately. To perform a complete installation, please use ANACONDA, DOCKER or SINGULARITY.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  pip install \"biobb_structure_utils\u0026gt;=3.6.1\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage: \u003ca href=\"https://biobb-structure-utils.readthedocs.io/en/latest/modules.html\" rel=\"nofollow\"\u003ePython API documentation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing ANACONDA:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  conda install -c bioconda \"biobb_structure_utils\u0026gt;=3.6.1\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage: With conda installation BioBBs can be used with the \u003ca href=\"https://biobb-structure-utils.readthedocs.io/en/latest/modules.html\" rel=\"nofollow\"\u003ePython API documentation\u003c/a\u003e and the \u003ca href=\"https://biobb-structure-utils.readthedocs.io/en/latest/command_line.html\" rel=\"nofollow\"\u003eCommand Line documentation\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing DOCKER:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker pull quay.io/biocontainers/biobb_structure_utils:3.6.1--pyhdfd78af_0\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  docker run quay.io/biocontainers/biobb_structure_utils:3.6.1--pyhdfd78af_0 \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eUsing SINGULARITY:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMacOS users\u003c/strong\u003e: it\u0027s strongly recommended to avoid Singularity and use \u003cstrong\u003eDocker\u003c/strong\u003e as containerization system.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull --name biobb_structure_utils.sif shub://bioexcel/biobb_structure_utils\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity exec biobb_structure_utils.sif \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe command list and specification can be found at the \u003ca href=\"https://biobb-structure-utils.readthedocs.io/en/latest/command_line.html\" rel=\"nofollow\"\u003eCommand Line documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-copyright--licensing\" class=\"anchor\" href=\"#copyright--licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopyright \u0026amp; Licensing\u003c/h3\u003e\n\u003cp\u003eThis software has been developed in the \u003ca href=\"http://mmb.irbbarcelona.org\" rel=\"nofollow\"\u003eMMB group\u003c/a\u003e at the \u003ca href=\"http://www.bsc.es/\" rel=\"nofollow\"\u003eBSC\u003c/a\u003e \u0026amp; \u003ca href=\"https://www.irbbarcelona.org/\" rel=\"nofollow\"\u003eIRB\u003c/a\u003e for the \u003ca href=\"http://bioexcel.eu/\" rel=\"nofollow\"\u003eEuropean BioExcel\u003c/a\u003e, funded by the European Commission (EU H2020 \u003ca href=\"http://cordis.europa.eu/projects/823830\" rel=\"nofollow\"\u003e823830\u003c/a\u003e, EU H2020 \u003ca href=\"http://cordis.europa.eu/projects/675728\" rel=\"nofollow\"\u003e675728\u003c/a\u003e).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e(c) 2015-2021 \u003ca href=\"https://www.bsc.es/\" rel=\"nofollow\"\u003eBarcelona Supercomputing Center\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e(c) 2015-2021 \u003ca href=\"https://www.irbbarcelona.org/\" rel=\"nofollow\"\u003eInstitute for Research in Biomedicine\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLicensed under the\n\u003ca href=\"https://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\"\u003eApache License 2.0\u003c/a\u003e, see the file LICENSE for details.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-acknolegements\" class=\"anchor\" href=\"#acknolegements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknolegements\u003c/h3\u003e\n\u003cp\u003eThis software uses functions to read and modify GRO files based in the \u003ca href=\"https://github.com/caizkun/gropy\"\u003eGROPY\u003c/a\u003e library created by Zhikun Cai (\u003ca href=\"mailto:caizkun@gmail.com\"\u003ecaizkun@gmail.com\u003c/a\u003e) under the \u003ca href=\"https://github.com/caizkun/gropy/blob/master/LICENSE\"\u003eMIT\u003c/a\u003e. In this project \u003ca href=\"https://github.com/caizkun/gropy\"\u003eGROPY\u003c/a\u003e has been adapted to Python 3 and our own needs.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/39c04282e694d49ea0d56c716a27845cd25b9931f791484540f625dcecf68af2/68747470733a2f2f62696f657863656c2e65752f77702d636f6e74656e742f75706c6f6164732f323031392f30342f42696f657863656c6c5f6c6f676f5f3130383070785f7472616e73702e706e67\" alt=\"\" title=\"Bioexcel\" data-canonical-src=\"https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 8,
    "topics": [],
    "updated_at": 1625224033.0
  },
  {
    "data_format": 2,
    "description": "A Docker/Singularity container for packaging pulsar searching software",
    "filenames": [
      "Singularity"
    ],
    "full_name": "federatedcloud/pulsar-pipeline-container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-docker-pulsar-pipeline\" class=\"anchor\" href=\"#docker-pulsar-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edocker-pulsar-pipeline\u003c/h1\u003e\n\u003cp\u003eA Docker/Singularity container for packaging pulsar searching software\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4541\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622819898.0
  },
  {
    "data_format": 2,
    "description": "A base Singularity container for PRESTO, including dependencies and environment, converted from docker-PRESTO",
    "filenames": [
      "Singularity"
    ],
    "full_name": "federatedcloud/singularity-PRESTO",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-presto\" class=\"anchor\" href=\"#singularity-presto\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-PRESTO\u003c/h1\u003e\n\u003cp\u003eA base Singularity container for PRESTO, including dependencies and environment, converted from docker-PRESTO\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4510\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1622819998.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for singularity-term-img-cli",
    "filenames": [
      "4.1.0/Singularity"
    ],
    "full_name": "icaoberg/singularity-term-img-cli",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-term-img-cli\" class=\"anchor\" href=\"#singularity-term-img-cli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-term-img-cli\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/sindresorhus/term-img-cli/raw/main/screenshot.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/sindresorhus/term-img-cli/raw/main/screenshot.jpg\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sindresorhus/term-img-cli\"\u003eterm-img\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003eterm-img\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/term-img/3.0.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/term-img\u003c/code\u003e as \u003ccode\u003e3.0.0\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622923859.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for chalk-cli.",
    "filenames": [
      "4.1.0/Singularity"
    ],
    "full_name": "icaoberg/singularity-chalk-cli",
    "latest_release": "v4.1.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-chalk-cli/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-chalk-cli/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/41b71df381c58acd22a2f008355de6880684d6fae2cf7bf65fe0a838346e984e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/41b71df381c58acd22a2f008355de6880684d6fae2cf7bf65fe0a838346e984e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-chalk-cli\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ec759ebf35710187fc88f31b68ceb6932b976079e159288cffbbad8dee77d527/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ec759ebf35710187fc88f31b68ceb6932b976079e159288cffbbad8dee77d527/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-chalk-cli\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/27518b30fab8201fa0ff8f34fb9beab4d4c1fa9ca921199f8cadb070e8236575/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/27518b30fab8201fa0ff8f34fb9beab4d4c1fa9ca921199f8cadb070e8236575/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-chalk-cli\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/818fb7a40695878fd336b1ae1e1a55e90b2b70ed4dd5acb0e69e51ed019535e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/818fb7a40695878fd336b1ae1e1a55e90b2b70ed4dd5acb0e69e51ed019535e3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6368616c6b2d636c69\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-chalk-cli\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-chalk-cli\" class=\"anchor\" href=\"#singularity-chalk-cli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-chalk-cli\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/chalk/chalk-cli\"\u003echalk-cli\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-or-similar\" class=\"anchor\" href=\"#installing-the-container-on-bridges-or-similar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges (or similar)\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003echalk-cli\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/chalk-cli/4.1.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/chalk-cli\u003c/code\u003e as \u003ccode\u003e4.1.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec singularity-chalk-cli-4.1.0.sif chalk -t \u0027{red.bold Dungeons and Dragons {~bold.blue (with added fairies)}}\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"images/screenshot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/screenshot.png\" alt=\"Screenshot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-alternative-installation\" class=\"anchor\" href=\"#alternative-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlternative Installation\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003espack install npm\nspack load npm\nnpm install -g chalk-cli\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "cli-utilities",
      "utilities"
    ],
    "updated_at": 1624059802.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity_fastqc",
      "Singularity_multiqc",
      "Singularity_trimmomatic"
    ],
    "full_name": "uf-icbr-bioinformatics/biocontainers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-biocontainers\" class=\"anchor\" href=\"#biocontainers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebiocontainers\u003c/h1\u003e\n\u003cp\u003eThis repository contains recipes for containers used to perform QC, summary statistics, and pre-processing on NGS datasets.\u003c/p\u003e\n\u003cp\u003eIn the future, we may provide the containers themselves. Stay tuned. Work in progress.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623019187.0
  },
  {
    "data_format": 2,
    "description": "Singularity images and recipes",
    "filenames": [
      "Atom/Singularity.atom",
      "rstudio-server/Singularity.rstudio-server",
      "acroread/Singularity.acroread",
      "ubuntu/Singularity.1804",
      "ubuntu/Singularity.2004",
      "VESTA/Singularity.vesta",
      "emacs/Singularity.emacs",
      "xcrysden/Singularity.xcrysden",
      "xcrysden/Singularity.xcrysden_1.5.60",
      "lammps/Singularity.lammps_prophet",
      "lammps/Singularity.lammps_ase",
      "lammps/Singularity.lammps",
      "lammps/Singularity.lammps_ase_kim",
      "AMPE/Singularity.ampe",
      "kmos/Singularity.kmos3_9",
      "kmos/Singularity.kmos",
      "pp/Singularity.pp2",
      "gdis/Singularity.gdis",
      "deal.II/Singularity.deal",
      "MD2-lab/Singularity.md2-lab",
      "mongodb/Singularity.mongodb",
      "jmol/Singularity.jmol",
      "jupyter/Singularity.jupyter",
      "gromacs/Singularity.gromacs",
      "obabel/Singularity.obabel",
      "graphics/Singularity.gnuplot_alpine",
      "graphics/Singularity.graphics",
      "graphics/Singularity.gnuplot_5.4",
      "graphics/Singularity.gnuplot_4.6",
      "graphics/Singularity.gnuplot_4.6a",
      "graphics/Singularity.gnuplot_5.4a",
      "clease/Singularity.clease",
      "tools/Singularity.mc",
      "tools/Singularity.gawk",
      "tools/Singularity.vim",
      "tools/Singularity.meld",
      "tools/Singularity.gnuplot",
      "cuda/Singularity.u18.04_cuda9.2",
      "tesseract/Singularity.tesseract",
      "texlive/Singularity.texlive",
      "ase-twistd/Singularity.ase-twistd",
      "mkdocs-serve/Singularity.mkdocs-serve"
    ],
    "full_name": "pmitev/Teoroo-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2338\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-teoroo-singularity\" class=\"anchor\" href=\"#teoroo-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTeoroo-singularity\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "singularity-containers"
    ],
    "updated_at": 1623074518.0
  },
  {
    "data_format": 2,
    "description": "Definition (recipe) files for singularity containers.",
    "filenames": [
      "cite-seq/Singularity_AJM_COVID.def",
      "cite-seq/Singularity_3.def",
      "cite-seq/Singularity_xenial.def",
      "cite-seq/Singularity_rocker.def",
      "cite-seq/Singularity_publish.def",
      "cytof-workflow-v3/Singularity.def",
      "cytof-workflow-v3/Singularity_SCS.def",
      "cytof-deep-cnn/Singularity.def",
      "H5N1/Singularity.def",
      "H5N1/Singularity_R_3.6.def",
      "comet/Singularity.def",
      "bittersweet/Singularity.def",
      "tf-gpu-chemistry/Singularity",
      "generic/Singularity.def"
    ],
    "full_name": "rohitfarmer/singularity-defs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-definitionrecipe-files-for-singularity-containers\" class=\"anchor\" href=\"#definitionrecipe-files-for-singularity-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDefinition/Recipe Files for Singularity Containers\u003c/h1\u003e\n\u003cp\u003eSome of the containers are available to download from \u003ca href=\"https://cloud.sylabs.io/library/rohitfarmer\" rel=\"nofollow\"\u003ehttps://cloud.sylabs.io/library/rohitfarmer\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor feedback and collaboration write to me at \u003ca href=\"mailto:rohit.farmer@gmail.com\"\u003erohit.farmer@gmail.com\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install-singularity-on-linux\" class=\"anchor\" href=\"#install-singularity-on-linux\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Singularity on Linux\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-version-34\" class=\"anchor\" href=\"#singularity-version-34\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Version 3.4\u003c/h2\u003e\n\u003cp\u003eFollow the instructions on \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.4/user-guide/quick_start.html#quick-installation-steps\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-building-a-singularity-container\" class=\"anchor\" href=\"#building-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a Singularity Container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-readonly-container\" class=\"anchor\" href=\"#readonly-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReadonly Container\u003c/h2\u003e\n\u003cp\u003eTo build a read-only SquashFS Singularity container on a local machine using a recipe/definition file.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build \u0026lt;container-name.sif\u0026gt; \u0026lt;Singularity.def\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTo set a different temporary directory than the default \u003ccode\u003e/tmp\u003c/code\u003e.\u003cbr\u003e\n\u003ccode\u003esudo -E SINGULARITY_TMPDIR=/home/rohit/tmp singularity build \u0026lt;container.sif\u0026gt; \u0026lt;container.def\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-writable-sandbox\" class=\"anchor\" href=\"#writable-sandbox\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWritable Sandbox.\u003c/h2\u003e\n\u003cp\u003eTo build a writable sandbox (essentially a folder) on a local machine using a recipe/definition file.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build --sandbox  \u0026lt;sandbox-folder-name/\u0026gt; \u0026lt;Singularity.def\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: The advantage of building a writable sandbox is that it can be used to install and configure packages as you go, and once you are satisfied with the requirements, the sandbox can be converted into a read-only SquashFS container. To build a sandbox quickly, it\u0027s better to install a minimal set of packages via the definition file.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installconfigure-packages-in-a-writable-sandbox\" class=\"anchor\" href=\"#installconfigure-packages-in-a-writable-sandbox\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall/Configure Packages in a Writable Sandbox\u003c/h3\u003e\n\u003cp\u003eOnce a writable sandbox is created to execute it to invoke the shell of the operating installed in the container in the \"writable\" mode. If the shell is not invoked in the \"writable\" mode, all the changes will be lost once you exit from the container environment.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity shell --writable \u0026lt;sandbox-folder-name/\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eInstall packages as you would, for example, in Ubuntu from the command line.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-convert-a-writable-sandbox-to-a-readonly-container\" class=\"anchor\" href=\"#convert-a-writable-sandbox-to-a-readonly-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConvert a Writable Sandbox to a Readonly Container\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003esudo singularity build \u0026lt;container-name.sif\u0026gt; \u0026lt;sandbox-folder-name/\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-execute-a-container\" class=\"anchor\" href=\"#execute-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecute a Container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-invoke-a-shell\" class=\"anchor\" href=\"#invoke-a-shell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInvoke a shell\u003c/h2\u003e\n\u003cp\u003eThe command below can be used for both read-only/writable containers/sandbox.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity shell \u0026lt;container-name.sif\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote: By default, Singularity binds to your current working and home directory. Therefore, you do not need to do anything else to execute a script that is in your current working directory. It can also pull, for example, Vim settings from the .vimrc file in your home directory. Therefore, if Vim installed in the container, it can be used with the same settings from inside the container as it would from outside.\u003c/em\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-execute-a-command-via-container\" class=\"anchor\" href=\"#execute-a-command-via-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecute a Command via Container\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec \u0026lt;container-name.sif\u0026gt; \u0026lt;command\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eFor example: \u003ccode\u003esingularity exec \u0026lt;container-name.sif\u0026gt; Rscript --vanilla hello.R\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-jupyter-notebooks-from-within-a-container\" class=\"anchor\" href=\"#running-jupyter-notebooks-from-within-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Jupyter Notebooks from Within a Container\u003c/h1\u003e\n\u003cp\u003eThis section is for containers that have Jupyter notebook installed (e.g. cite-seq).\u003c/p\u003e\n\u003cp\u003eA generic command that should work on a personal computer. \u003ccode\u003esingularity exec container-name.sif jupyter notebook --no-browser --ip=127.0.0.1 --port=8888\u003c/code\u003e\u003cbr\u003e\n\u003cem\u003eNote: The IP address and the port number mentioned in the command are the jupyter defaults. They can be changed as per need.\u003c/em\u003e\u003cbr\u003e\nCopy the URL generated by jupyter daemon and paste it in your browser; this should open Jupyter with the list of the files in your current working directory on the host computer.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-with-r-as-a-kernel\" class=\"anchor\" href=\"#running-with-r-as-a-kernel\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning with R as a Kernel\u003c/h2\u003e\n\u003cp\u003eSometimes if you already have an R kernel installed in your home directory, it conflicts with what you have inside the container. Therefore, it would require you to re-install the kernel specs in your home directory via the container.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec container-name.sif R --quiet --slave -e \u0027IRkernel::installspec()\u0027\n\n# Screen log.\n# [InstallKernelSpec] Removing existing kernelspec in /home/user/.local/share/jupyter/kernels/ir\n# [InstallKernelSpec] Installed kernelspec ir in /home/user/.local/share/jupyter/kernels/ir\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-an-hpc\" class=\"anchor\" href=\"#running-on-an-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on an HPC\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSSH to the HPC.\u003c/li\u003e\n\u003cli\u003eClaim an interactive node.\u003c/li\u003e\n\u003cli\u003eNavigate to your project directory. Singularity container should be in your project directory.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esingularity exec container-name.sif jupyter notebook --no-browser --ip=\u00270.0.0.0\u0027\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eKeep the SSH session and Jupyter notebook session running. Copy the URL on your local browser.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cem\u003eNote: On some HPCs, you may have to initiate an additional SSH tunnel connecting your local machine to the interactive node on the HPC. In that case, follow some generic instructions here \u003ca href=\"https://rohitfarmer.github.io/docs/docs/HPC/jupyter/\" rel=\"nofollow\"\u003ehttps://rohitfarmer.github.io/docs/docs/HPC/jupyter/\u003c/a\u003e or ask your system administrator.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1623100431.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "BlueprintPipeline/Resource/gemBS-2.1.1/Singularity"
    ],
    "full_name": "Irfanwustl/Research_code",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-research_code\" class=\"anchor\" href=\"#research_code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResearch_code\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627339320.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for Bismark (https://github.com/FelixKrueger/Bismark)",
    "filenames": [
      "Singularity",
      "Singularity.0.20.0",
      "Singularity.0.23.0",
      "Singularity.0.19.1"
    ],
    "full_name": "powerPlant/bismark-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2263\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the Bismark bisulfite mapping and methylation calling program\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623122485.0
  },
  {
    "data_format": 2,
    "description": "Singularity base container with Nix to be used in XSEDE compute environment (currently in development)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "XSEDE/singularity-nix-base",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-nix-base\" class=\"anchor\" href=\"#singularity-nix-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-nix-base\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4462\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity base container with Nix to be used in XSEDE compute environment (currently in development)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 12,
    "topics": [
      "nix",
      "singularity",
      "singularity-nix"
    ],
    "updated_at": 1623176963.0
  },
  {
    "data_format": 2,
    "description": "Singularity container with Nix and OpenMPI to be used in XSEDE compute environment (currently in development)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "XSEDE/singularity-nix-openmpi",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-nix-openmpi\" class=\"anchor\" href=\"#singularity-nix-openmpi\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-nix-openmpi\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4462\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity container with Nix and OpenMPI to be used in XSEDE compute environment (currently in development)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 12,
    "topics": [],
    "updated_at": 1623177060.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity_environment/Singularity"
    ],
    "full_name": "cpezzato/discrete_active_inference",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-discrete_active_inference-for-robotics\" class=\"anchor\" href=\"#discrete_active_inference-for-robotics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ediscrete_active_inference for robotics\u003c/h1\u003e\n\u003cp\u003eRepository for active inference and behavior trees for discrete decision making. This repository relies on a TIAGo simulation in a simplified retail store. Please read the associated paper for more theorethical considerations about the algorithms.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\"Active Inference and Behavior Trees for Reactive Action Planning and Execution in Robotics\"\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eCorrado Pezzato, Carlos Hernandez, Stefan Bonhof, Martijn Wisse, \u003ca href=\"https://arxiv.org/abs/2011.09756\" rel=\"nofollow\"\u003ehttps://arxiv.org/abs/2011.09756\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-content\" class=\"anchor\" href=\"#content\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContent\u003c/h2\u003e\n\u003cp\u003eThis repositiry contains a Matlab examples and a ROS package for active inference for task planning and execution.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-main-files\" class=\"anchor\" href=\"#main-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMain files\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eMatlab:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eaip.m\u003c/em\u003e the active inference algorithm for decision making is illustrated in the case of heterogeneous states and actions.\u003c/li\u003e\n\u003cli\u003e\n\u003cem\u003eexample.m\u003c/em\u003e example of use of active inference for discrete decision making in a robotic case where conflicts and preconditions checks are required. A robot is assumed to be able to navigate to a point (MoveBase), reach a location with its end effector (Move MPC), and pick and place things. Actions have preconditions and are assumed not instantaneous\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eROS:\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe other folders are related to the ROS package containing a Python implementation of active inference and behavior trees. You can run an example use case with TIAGo in a simplified retail store after installation of the package ad dependancies.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSimulation Environment\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eA singularity image can be downloaded from \u003ca href=\"https://drive.google.com/drive/folders/1DYuRWgCiiHCG4ck_7Pf_Kw4Kn-ZpZ-Oy?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAlternatively, you can build the singularity yourself:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003ecreate a sub directory called \u0027pkgs\u0027 (in the \u003ccode\u003esingularity_environment\u003c/code\u003e directory)\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e   mkdir pkgs\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003euse \u003ccode\u003evcstool\u003c/code\u003e (or \u003ccode\u003ewstool\u003c/code\u003e) to clone/download the dependencies (as specified in \u003ccode\u003eretail_store_lightweight_sim.repos\u003c/code\u003e).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e   vcs import \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e retail_store_lightweight_sim.repos pkgs\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAdding packages to \u003ccode\u003epkg\u003c/code\u003e will allow \u003ccode\u003erosdep\u003c/code\u003e to install all required build and run dependencies into the image, so students can then proceed to build those packages in their own workspaces (otherwise builds would fail due to missing dependencies).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e  Packages in \u003ccode\u003epkg\u003c/code\u003e will be installed on the image, their source will \u003cstrong\u003enot\u003c/strong\u003e be included in the image itself, so there may be some elements that are not installed. So far I\u0027ve only noticed one required change.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eModify the \u003ccode\u003eCMakeList.txt\u003c/code\u003e file from the \u003ccode\u003epal_navigation_sm\u003c/code\u003e inside the \u003ccode\u003epkgs\u003c/code\u003e folder.\u003c/p\u003e\n\u003cp\u003eChange the \u003ccode\u003einstall\u003c/code\u003e instruction (starts at line 10) by adding some scripts as follows.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003einstall(\nPROGRAMS\n   scripts/map_setup.py\n   scripts/pal_navigation_main_sm.py\n   scripts/navigation.sh\n   scripts/base_maps_symlink.sh\n   scripts/cp_maps_to_home.sh\n   scripts/cp_pose_to_home.sh\n   DESTINATION \u003cspan class=\"pl-smi\"\u003e${CATKIN_PACKAGE_BIN_DESTINATION}\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003echeck the \u003ccode\u003eVERSION\u003c/code\u003e variable inside the \u003ccode\u003edocker_build.sh\u003c/code\u003e, \u003ccode\u003ebuild.sh\u003c/code\u003e and \u003ccode\u003eSingularity\u003c/code\u003e files. This version should match the version of your singularity install (\u003ccode\u003esingularity -v\u003c/code\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erun \u003ccode\u003edocker_build.sh\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e   ./docker_build.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfter some time and a successful build, a new docker image will be created. This requires Docker to be installed and configured.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erun \u003ccode\u003ebuild.sh\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e   ./build.sh\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAfter some time and a successful build, a new \u003ccode\u003e.simg\u003c/code\u003e should be generated by \u003ccode\u003esingularity\u003c/code\u003e in the \u003ccode\u003ecwd\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eBehavior trees library\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eInstall the BT library to use this package (tested in Ubuntu 18.04 with ROS Melodic). Before proceeding, it is recommended to to install the following dependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get install libzmq3-dev libboost-dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also easily install the \u003ca href=\"https://github.com/BehaviorTree/BehaviorTree.CPP\"\u003eBehavior Tree library\u003c/a\u003e with the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get install ros-$ROS_DISTRO-behaviortree-cpp-v3\nsudo apt-get update   \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-code\" class=\"anchor\" href=\"#running-the-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the code\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eUsing the virtual environment\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAccess the simngularity image by using the regular Singularity \u003ccode\u003eshell\u003c/code\u003e action:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity shell /path/to/discrete_ai_tiago.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eUse the flag for nvidia drivers if applicable to your machine:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity shell --nv /path/to/discrete_ai_tiago.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen source \u003ccode\u003e/opt/ros/melodic/setup.bash\u003c/code\u003e to access all the TIAGo dependencies installed on the image.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e /opt/ros/melodic/setup.bash\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eHow to run a simple example with TIAGo\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eCreate a new workspace and clone this repository in the \u003ccode\u003esrc\u003c/code\u003e folder. Build the package using \u003ccode\u003ecatkin build\u003c/code\u003e. Run the three commands below from within the singularity image after sourcing \u003ccode\u003esource/devel/setup.bash\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eroslaunch retail_store_simulation tiago_simulation.launch\nrosrun discrete_ai tiago_perception.py\nrosrun discrete_ai active_inference_server.py\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFrom a terminal outside the singularity image run the behavior tree:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003erosrun discrete_ai demo_executeBT\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe expected outcome is the following:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"tiago_sim.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"tiago_sim.gif\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: The sills used in this simulation are based on standard moveBase and moveIt actions, thus robustness (especially of IK solutions) might make TIAGo fail the grasp. Aruco detection can also imprecise and will be improved over time.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623232591.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.5",
      "Singularity.3",
      "Singularity.8",
      "Singularity.14",
      "Singularity.10",
      "Singularity.7",
      "Singularity.11",
      "Singularity.121",
      "Singularity.12",
      "Singularity.4",
      "Singularity.15",
      "Singularity.9",
      "Singularity.111",
      "Singularity.6",
      "Singularity.13"
    ],
    "full_name": "masoudrezai/Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-octopussingularity-container\" class=\"anchor\" href=\"#octopussingularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/luntergroup/octopus\"\u003eOctopus\u003c/a\u003e\n\u003ca href=\"https://github.com/hpcng/singularity\"\u003eSingularity\u003c/a\u003e container\u003c/h1\u003e\n\u003cp\u003eSylvain Schmitt\nApril 28, 2021\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBionformatics software Octopus\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOctopus is a mapping-based variant caller that implements several\ncalling models within a unified haplotype-aware framework. Octopus takes\ninspiration from particle filtering by constructing a tree of haplotypes\nand dynamically pruning and extending the tree based on haplotype\nposterior probabilities in a sequential manner. This allows octopus to\nimplicitly consider all possible haplotypes at a given loci in\nreasonable time.\u003c/p\u003e\n\u003cp\u003eOctopus Version: 0.7.4\u003c/p\u003e\n\u003cp\u003e[\u003ca href=\"https://github.com/luntergroup/octopus\"\u003ehttps://github.com/luntergroup/octopus\u003c/a\u003e]\u003c/p\u003e\n\u003cp\u003eSingularity container based on the recipe: Singularity.template.def\u003c/p\u003e\n\u003cp\u003ePackage installation using Miniconda3 V4.7.12\u003c/p\u003e\n\u003cp\u003eImage singularity (V\u0026gt;=3.3) is automatically test and built and pushed\non the registry using\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\u003etest.yml\u003c/a\u003e\n\u0026amp;\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\u003ebuilder.yml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ebuild\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build octopus.sif Singularity\nsingularity run octopus.sif\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e octopus.sif octopus -h\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003epull\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003esnakemake\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003esingularity\u003c/span\u003e: \n        \u003cspan class=\"pl-s\"\u003e\"https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif\"\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623238419.0
  },
  {
    "data_format": 2,
    "description": "octopus Singularity container ",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sylvainschmitt/singularity-octopus",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-octopussingularity-container\" class=\"anchor\" href=\"#octopussingularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/luntergroup/octopus\"\u003eOctopus\u003c/a\u003e\n\u003ca href=\"https://github.com/hpcng/singularity\"\u003eSingularity\u003c/a\u003e container\u003c/h1\u003e\n\u003cp\u003eSylvain Schmitt\nApril 28, 2021\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBionformatics software Octopus\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOctopus is a mapping-based variant caller that implements several\ncalling models within a unified haplotype-aware framework. Octopus takes\ninspiration from particle filtering by constructing a tree of haplotypes\nand dynamically pruning and extending the tree based on haplotype\nposterior probabilities in a sequential manner. This allows octopus to\nimplicitly consider all possible haplotypes at a given loci in\nreasonable time.\u003c/p\u003e\n\u003cp\u003eOctopus Version: 0.7.4\u003c/p\u003e\n\u003cp\u003e[\u003ca href=\"https://github.com/luntergroup/octopus\"\u003ehttps://github.com/luntergroup/octopus\u003c/a\u003e]\u003c/p\u003e\n\u003cp\u003eSingularity container based on the recipe: Singularity.template.def\u003c/p\u003e\n\u003cp\u003ePackage installation using Miniconda3 V4.7.12\u003c/p\u003e\n\u003cp\u003eImage singularity (V\u0026gt;=3.3) is automatically test and built and pushed\non the registry using\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/test.yml\"\u003etest.yml\u003c/a\u003e\n\u0026amp;\n\u003ca href=\"https://github.com/sylvainschmitt/singularity-template/blob/main/.github/workflows/builder.yml\"\u003ebuilder.yml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ebuild\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build octopus.sif Singularity\nsingularity run octopus.sif\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e octopus.sif octopus -h\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003epull\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003esnakemake\u003c/strong\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003esingularity\u003c/span\u003e: \n        \u003cspan class=\"pl-s\"\u003e\"https://github.com/sylvainschmitt/singularity-octopus/releases/download/0.0.1/sylvainschmitt-singularity-octopus.latest.sif\"\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623243296.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity",
      "container/Singularity_madeline2",
      "container/stranger/Singularity",
      "container/reviewer/Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/nextflow_wgs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-spring-mass-h--02\" class=\"anchor\" href=\"#spring-mass-h--02\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpring mass h = 0.2\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hypereuler\" class=\"anchor\" href=\"#hypereuler\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-heun\" class=\"anchor\" href=\"#heun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperheun\" class=\"anchor\" href=\"#hyperheun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-velocity-verlet\" class=\"anchor\" href=\"#velocity-verlet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVelocity Verlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperverlet\" class=\"anchor\" href=\"#hyperverlet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperVerlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fr4\" class=\"anchor\" href=\"#fr4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFR4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rk4\" class=\"anchor\" href=\"#rk4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRK4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pendulum-h--008\" class=\"anchor\" href=\"#pendulum-h--008\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePendulum h = 0.08\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-euler\" class=\"anchor\" href=\"#euler\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hypereuler-1\" class=\"anchor\" href=\"#hypereuler-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-heun-1\" class=\"anchor\" href=\"#heun-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperheun-1\" class=\"anchor\" href=\"#hyperheun-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-velocity-verlet-1\" class=\"anchor\" href=\"#velocity-verlet-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVelocity Verlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperverlet-1\" class=\"anchor\" href=\"#hyperverlet-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperVerlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fr4-1\" class=\"anchor\" href=\"#fr4-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFR4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rk4-1\" class=\"anchor\" href=\"#rk4-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRK4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-three-body-spring-mass-h--006\" class=\"anchor\" href=\"#three-body-spring-mass-h--006\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThree body spring mass h = 0.06\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-euler-1\" class=\"anchor\" href=\"#euler-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hypereuler-2\" class=\"anchor\" href=\"#hypereuler-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-heun-2\" class=\"anchor\" href=\"#heun-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperheun-2\" class=\"anchor\" href=\"#hyperheun-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-velocity-verlet-2\" class=\"anchor\" href=\"#velocity-verlet-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVelocity Verlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperverlet-2\" class=\"anchor\" href=\"#hyperverlet-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperVerlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fr4-2\" class=\"anchor\" href=\"#fr4-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFR4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rk4-2\" class=\"anchor\" href=\"#rk4-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRK4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626935069.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "Zinoex/hyperverlet",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-spring-mass-h--02\" class=\"anchor\" href=\"#spring-mass-h--02\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpring mass h = 0.2\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hypereuler\" class=\"anchor\" href=\"#hypereuler\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316489-c72b5980-c2dd-11eb-818c-528eaa8b43b1.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-heun\" class=\"anchor\" href=\"#heun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120315967-33598d80-c2dd-11eb-9bb4-5399b5b14980.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperheun\" class=\"anchor\" href=\"#hyperheun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120315979-3785ab00-c2dd-11eb-8f9a-0de0a8b65a7c.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-velocity-verlet\" class=\"anchor\" href=\"#velocity-verlet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVelocity Verlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120315992-39e80500-c2dd-11eb-9394-57053434b952.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperverlet\" class=\"anchor\" href=\"#hyperverlet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperVerlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316009-3e142280-c2dd-11eb-898e-da7c78cd48b5.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fr4\" class=\"anchor\" href=\"#fr4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFR4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316018-410f1300-c2dd-11eb-9ba2-5d951637ffcd.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rk4\" class=\"anchor\" href=\"#rk4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRK4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316032-453b3080-c2dd-11eb-9aa2-c93ff7cdedf6.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pendulum-h--008\" class=\"anchor\" href=\"#pendulum-h--008\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePendulum h = 0.08\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-euler\" class=\"anchor\" href=\"#euler\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316077-5126f280-c2dd-11eb-854e-daad28dea3c7.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hypereuler-1\" class=\"anchor\" href=\"#hypereuler-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316086-53894c80-c2dd-11eb-82d7-7c8aea623b71.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-heun-1\" class=\"anchor\" href=\"#heun-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316104-58e69700-c2dd-11eb-82a7-7a7ad903499e.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperheun-1\" class=\"anchor\" href=\"#hyperheun-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316130-60a63b80-c2dd-11eb-8708-2edd5358c8fc.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-velocity-verlet-1\" class=\"anchor\" href=\"#velocity-verlet-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVelocity Verlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316146-63a12c00-c2dd-11eb-8743-7fbabf5e9f34.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperverlet-1\" class=\"anchor\" href=\"#hyperverlet-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperVerlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316161-669c1c80-c2dd-11eb-94de-2c7bf2cc6962.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fr4-1\" class=\"anchor\" href=\"#fr4-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFR4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316169-68fe7680-c2dd-11eb-8d27-5c4a7fd36f33.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rk4-1\" class=\"anchor\" href=\"#rk4-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRK4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316179-6bf96700-c2dd-11eb-90f7-a4fa2649d625.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-three-body-spring-mass-h--006\" class=\"anchor\" href=\"#three-body-spring-mass-h--006\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThree body spring mass h = 0.06\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-euler-1\" class=\"anchor\" href=\"#euler-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316209-7451a200-c2dd-11eb-9178-09e209da874a.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hypereuler-2\" class=\"anchor\" href=\"#hypereuler-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperEuler\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316218-77e52900-c2dd-11eb-8c49-fe6dfb6d7045.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-heun-2\" class=\"anchor\" href=\"#heun-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316228-7ae01980-c2dd-11eb-8792-5ca5b3d06460.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperheun-2\" class=\"anchor\" href=\"#hyperheun-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperHeun\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316247-7fa4cd80-c2dd-11eb-9ff6-d9607b45cfda.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-velocity-verlet-2\" class=\"anchor\" href=\"#velocity-verlet-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVelocity Verlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316255-82072780-c2dd-11eb-91ec-6b1c41b94751.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hyperverlet-2\" class=\"anchor\" href=\"#hyperverlet-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHyperVerlet\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316264-83d0eb00-c2dd-11eb-914f-69dcff6094a9.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fr4-2\" class=\"anchor\" href=\"#fr4-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFR4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316324-9814e800-c2dd-11eb-93e1-778f062e4778.mp4\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rk4-2\" class=\"anchor\" href=\"#rk4-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRK4\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4\" rel=\"nofollow\"\u003ehttps://user-images.githubusercontent.com/22764100/120316340-9ba86f00-c2dd-11eb-88d9-315dcf53e107.mp4\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627210046.0
  },
  {
    "data_format": 2,
    "description": "container for gatk tools",
    "filenames": [
      "Singularity"
    ],
    "full_name": "aseetharam/gatk",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4700\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-container-for-the-gatk\" class=\"anchor\" href=\"#container-for-the-gatk\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer for the GATK\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tools-included\" class=\"anchor\" href=\"#tools-included\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTools included\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"http://www.htslib.org/\" rel=\"nofollow\"\u003eSamTools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://bio-bwa.sourceforge.net/\" rel=\"nofollow\"\u003eBWA\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.gnu.org/software/datamash/\" rel=\"nofollow\"\u003eDatamash\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://gatk.broadinstitute.org/hc/en-us\" rel=\"nofollow\"\u003eGATK\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://broadinstitute.github.io/picard/\" rel=\"nofollow\"\u003ePicard Tools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/lh3/bioawk\"\u003eBioAWK\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://bedtools.readthedocs.io\" rel=\"nofollow\"\u003eBedTools\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePlease be sure to cite all the programs if you use this container.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h2\u003e\n\u003cp\u003eto pull the image\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name gatk.sif shub://aseetharam/gatk:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto use the image\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec gatk.sif samtools\nsingularity exec gatk.sif bwa\nsingularity exec gatk.sif datamash\nsingularity exec gatk.sif java -jar /gatk/gatk-package-4.1.8.1-local.jar\nsingularity exec gatk.sif java -jar /picard/picard.jar\nsingularity exec gatk.sif bioawk\nsingularity exec gatk.sif bedtools\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623344768.0
  },
  {
    "data_format": 2,
    "description": "A template project to provide software to ESCAPE.",
    "filenames": [
      "Singularity/Singularity"
    ],
    "full_name": "garciagenrique/template_project_escape",
    "latest_release": "v0.0.3-dev",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-template_project_escape\" class=\"anchor\" href=\"#template_project_escape\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etemplate_project_escape\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.5281/zenodo.4923992\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/869b462bf3a319fe4fc2ffa52fb6b0f7c8e42eb4e0ed4bc2482306b9fd5aafab/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343932333939322e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4923992.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape/-/commits/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a25ea71f12537b69d5aca8b409685333243d4e65ceb91c5a401dadb6eacea20e/68747470733a2f2f6769746c61622e696e3270332e66722f657363617065323032302f7770332f74656d706c6174655f70726f6a6563745f6573636170652f6261646765732f6d61737465722f706970656c696e652e737667\" alt=\"pipeline status\" data-canonical-src=\"https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape/badges/master/pipeline.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fd551ba4b042d89480347a0e74e31af63b356b2cac1116c7b80038f41b04a581/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d677265656e2e737667\" alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n   \u003ca href=\"https://camo.githubusercontent.com/64f9054d866a78f16e1451647c19b22fc159a58191e004612257ce5277bd6db9/68747470733a2f2f63646e2e65736f2e6f72672f696d616765732f6c617267652f616e6e3138303834612e6a7067\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/64f9054d866a78f16e1451647c19b22fc159a58191e004612257ce5277bd6db9/68747470733a2f2f63646e2e65736f2e6f72672f696d616765732f6c617267652f616e6e3138303834612e6a7067\" width=\"640\" height=\"453\" data-canonical-src=\"https://cdn.eso.org/images/large/ann18084a.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp\u003eA simple template project to provide software to ESCAPE.\u003c/p\u003e\n\u003cp\u003eThis repository shows the \u003cstrong\u003ebasic content\u003c/strong\u003e that should be included in a project (following the\n\u003ca href=\"https://opensource.guide/starting-a-project/\" rel=\"nofollow\"\u003eopensource guide\u003c/a\u003e):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn \u003ca href=\"https://help.github.com/en/github/creating-cloning-and-archiving-repositories/licensing-a-repository#where-does-the-license-live-on-my-repository\"\u003eopen source\u003c/a\u003e\n\u003cstrong\u003elicense\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eA \u003ca href=\"https://help.github.com/en/github/getting-started-with-github/create-a-repo#commit-your-first-change\"\u003e\u003cstrong\u003eREADME\u003c/strong\u003e file\u003c/a\u003e,\nsimilar to this one.\u003c/li\u003e\n\u003cli\u003eContributing guidelines.\n\u003cul\u003e\n\u003cli\u003eSee below the general guidelines for the ESCAPE repository.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eA \u003ca href=\"https://opensource.guide/code-of-conduct/\" rel=\"nofollow\"\u003ecode of conduct\u003c/a\u003e.\n\u003cul\u003e\n\u003cli\u003eCheck why is a good idea to add one.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eThe repository itself.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt would be highly suitable to include too:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA setup file as well as the basic commands to install the library (see below).\u003c/li\u003e\n\u003cli\u003eA \u003ccode\u003e.gitignore\u003c/code\u003e file.\u003c/li\u003e\n\u003cli\u003eUnitary and integration tests, and ideally a CI pipeline.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ePlease feel free to clone / fork / template this project!\u003c/strong\u003e (For example, look to left of the\n\u003ccode\u003eClone or download\u003c/code\u003e button in the \u003ca href=\"https://github.com/garciagenrique/template_project_escape\"\u003eGitHub\u003c/a\u003e site).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor a detailed explanation of how to submit a contribution to a project / repository (Fork, create a branch, make\na pull request...), you can have a look to the \u003ca href=\"https://opensource.guide/how-to-contribute/#how-to-submit-a-contribution\" rel=\"nofollow\"\u003eopensource guide\u003c/a\u003e\nand/or the \u003ca href=\"https://git-scm.com/doc\" rel=\"nofollow\"\u003egit\u0027s documentation\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eNot that if you have login GitLab by using the \u003ccode\u003e[Shibbolenth]\u003c/code\u003e service (eduGAIN, F\u00e9d\u00e9ration d\u0027Identit\u00e9s\nRENATER), you will need to \u003ca href=\"https://gitlab.in2p3.fr/help/ssh/README#generating-a-new-ssh-key-pair\" rel=\"nofollow\"\u003eadd a SSH key\u003c/a\u003e to\nyour GitLab profile if you want to \u0027push\u0027 your changes to the server.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contribute-to-the-escape-ossr\" class=\"anchor\" href=\"#contribute-to-the-escape-ossr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContribute to the ESCAPE OSSR\u003c/h1\u003e\n\u003cp\u003eIf you want to provide software to the ESCAPE repository:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the \u003ca href=\"https://escape2020.pages.in2p3.fr/wp3/ossr-pages/page/contribute/contribute_ossr/\" rel=\"nofollow\"\u003eESCAPE OSSR guidelines\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor ESCAPE members, follow the steps detailed in \u003ca href=\"https://gitlab.in2p3.fr/escape2020/wp3/onboarding\" rel=\"nofollow\"\u003ethe onboarding project\u003c/a\u003e\nto finalise your contribution and the same onboarding process.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAll the code provided should be uploaded to the \u003ca href=\"https://zenodo.org/communities/escape2020/\" rel=\"nofollow\"\u003eZenodo ESCAPE community\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the following \u003ca href=\"https://escape2020.pages.in2p3.fr/wp3/ossr-pages/page/contribute/publish_tutorial/\" rel=\"nofollow\"\u003etutorial on how to publish content in Zenodo\u003c/a\u003e,\nand how to automatise the upload of each new release of your project.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-this-project-also-includes\" class=\"anchor\" href=\"#this-project-also-includes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThis project also includes\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-how-to-automatise-the-building-of-a-singularity-image-and-upload-it-to-zenodo-using-the-gitlab-ci\" class=\"anchor\" href=\"#1-how-to-automatise-the-building-of-a-singularity-image-and-upload-it-to-zenodo-using-the-gitlab-ci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. How to automatise the building of a Singularity image and upload it to Zenodo using the GitLab-CI\u003c/h2\u003e\n\u003cp\u003eA working example of how to automatise the GitLab-CI to;\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ecreate a Singularity image / container of your code,\u003c/li\u003e\n\u003cli\u003emake it available as a downloadable artifact within your project and\u003c/li\u003e\n\u003cli\u003eupload it to the \u003ca href=\"https://zenodo.org/communities/escape2020\" rel=\"nofollow\"\u003eESCAPE OSSR\u003c/a\u003e,\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ecan be found in the \u003ccode\u003e.singularityci\u003c/code\u003e, and \u003ccode\u003eSingularity\u003c/code\u003e directories and in the \u003ccode\u003e.gitlab-ci.yml\u003c/code\u003e file - the\n\u003ccode\u003ebuild_singularity_image\u003c/code\u003e stage. Please read carefully all the README files.\u003c/p\u003e\n\u003cp\u003eFor an easy example of how to create a Singularity receipt from scratch (and its corresponding container when executed),\nplease have a look to the \u003ccode\u003esingularity_utils\u003c/code\u003e directory.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-how-to-automatise-the-building-of-a-docker-container-and-upload-it-to-the-gitlab-container-registry\" class=\"anchor\" href=\"#2-how-to-automatise-the-building-of-a-docker-container-and-upload-it-to-the-gitlab-container-registry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. How to automatise the building of a Docker container and upload it to the GitLab Container Registry\u003c/h2\u003e\n\u003cp\u003eAn example can be found in the \u003ccode\u003eDocker\u003c/code\u003e directory and in the \u003ccode\u003e.gitlab-ci.yml\u003c/code\u003e file -  the\n\u003ccode\u003ebuild_docker_image\u003c/code\u003e stage.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h1\u003e\n\u003cp\u003eExample of how to show installing instructions (and indeed the way to install this project).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  $ git clone https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape.git\n  $ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e template_project_escape\n  $ pip install \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-citing\" class=\"anchor\" href=\"#citing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting\u003c/h1\u003e\n\u003cp\u003eExample of citing (as well as the DOI to cite this project),\u003c/p\u003e\n\u003cp\u003eIn case of citing this repository, use the following DOI:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ev2.2 \u003ca href=\"https://doi.org/10.5281/zenodo.4923992\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/869b462bf3a319fe4fc2ffa52fb6b0f7c8e42eb4e0ed4bc2482306b9fd5aafab/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343932333939322e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4923992.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ev2.1 \u003ca href=\"https://doi.org/10.5281/zenodo.4790629\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c8f54e2f50cdf0c4d1bd5183d6472cae8b708efacd6a1319b443d8e456f41b7f/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e343739303632392e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.4790629.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ev2.0 \u003ca href=\"https://doi.org/10.5281/zenodo.3884963\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c27201272a77fc3ab3029c8c2c452e02a71736b7adaa0926bc45d3ac825598ed/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333838343936332e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3884963.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ev1.1 \u003ca href=\"https://doi.org/10.5281/zenodo.3743490\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cbfe31d3a9bd48ef4554b414c3c2325276269a476a79ec0217aa9feccc87cecd/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333734333439302e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3743490.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ev1.0 \u003ca href=\"https://doi.org/10.5281/zenodo.3572655\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5ec734ee4c1c793eaa8f9c2b189a6eae6d7d2ccb5f2a92adeb8af479409cc7bb/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333537323635352e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3572655.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDo not forget to include your code / container into the \u003ca href=\"https://zenodo.org/communities/escape2020/\" rel=\"nofollow\"\u003eZenodo ESCAPE community\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003e\u003cstrong\u003eNote that\u003c/strong\u003e\u003c/em\u003e a DOI will be assigned in the moment create a new record/entry in Zenodo.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h1\u003e\n\u003cp\u003ePlease check the licenses of the code within the \u003ccode\u003e.singularityci\u003c/code\u003e directory before adding this template\nto your project.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-report-an-issue--ask-a-question\" class=\"anchor\" href=\"#report-an-issue--ask-a-question\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReport an issue / Ask a question\u003c/h1\u003e\n\u003cp\u003eUse the \u003ca href=\"https://gitlab.in2p3.fr/escape2020/wp3/template_project_escape/-/issues\" rel=\"nofollow\"\u003eGitLab repository Issues\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h1\u003e\n\u003cp\u003eEmail to vuillaume [at] lapp.in2p3.fr / garcia [at] lapp.in2p3.fr.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623346169.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "experiments/ashvin/icml2020/singularity/Singularity",
      "docker/Singularity",
      "docker/railrl_v7/Singularity",
      "docker/railrl_v5/singularity/Singularity",
      "docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity",
      "docker/railrl_v12_cuda10-1_mj2-0-2-2_torch1-1-0_gym0-12-5_py3-6-5/Singularity_cpu",
      "docker/railrl_gpu_mujoco1-5-v4/singularity/Singularity",
      "docker/railrl_v11_cuda10-1_mj2-0-2-2_torch0-3-1_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_hand_tf_v1/Singularity",
      "docker/railrl_hand_tf_v1/Singularity_cpu",
      "docker/railrl_v8_cuda10-1/Singularity",
      "docker/railrl_v9_cuda10-1_mj1-50-1-59_torch0-4-1_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_hand_v1/Singularity",
      "docker/railrl_hand_v1/Singularity_cpu",
      "docker/railrl_ray/Singularity",
      "docker/railrl_v7_cuda8/Singularity",
      "docker/railrl_ray_gym-0-12-0/Singularity_from_scratch_cuda8",
      "docker/railrl_ray_gym-0-12-0/Singularity_from_scratch",
      "docker/railrl_v6_cuda9/Singularity",
      "docker/railrl_hand_v3/Singularity",
      "docker/railrl_hand_v3/Singularity_cpu",
      "docker/railrl_v10_cuda10-1_mj2-0-2-2_torch0-4-1_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_hand_v2/Singularity",
      "docker/railrl_hand_v2/Singularity_cpu",
      "docker/railrl_v9-5_cuda10-1_mj1-50-1-59_torch1-1-0_gym0-10-5_py3-5-2/Singularity",
      "docker/railrl_v6_cuda8/Singularity"
    ],
    "full_name": "Asap7772/railrl_evalsawyer",
    "latest_release": null,
    "readme": "\u003cp\u003eREADME last updated on: 01/24/2018\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-railrl\" class=\"anchor\" href=\"#railrl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erailrl\u003c/h1\u003e\n\u003cp\u003eReinforcement learning framework.\nSome implemented algorithms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"examples/ddpg.py\"\u003eDeep Deterministic Policy Gradient (DDPG)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/sac.py\"\u003eSoft Actor Critic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/dqn_and_double_dqn.py\"\u003e(Double) Deep Q-Network (DQN)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/her.py\"\u003eHindsight Experience Replay (HER)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/model_based_dagger.py\"\u003eMPC with Neural Network Model\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/naf.py\"\u003eNormalized Advantage Function (NAF)\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eWARNING: I haven\u0027t tested this NAF implementation much, so it may not match the paper\u0027s performance. I\u0027m pretty confident about the other two implementations though.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo get started, checkout the example scripts, linked above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-some-dependancies\" class=\"anchor\" href=\"#some-dependancies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSome dependancies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esudo apt-get install swig\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-create-conda-env\" class=\"anchor\" href=\"#create-conda-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Conda Env\u003c/h3\u003e\n\u003cp\u003eInstall and use the included ananconda environment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda env create -f docker/railrl/railrl-env.yml\n$ source activate railrl-env\n(railrl-env) $ # Ready to run examples/ddpg_cheetah_no_doodad.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you want you can use the docker image included.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-simulation-env-code\" class=\"anchor\" href=\"#download-simulation-env-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload Simulation Env Code\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/vitchyr/multiworld\"\u003emultiworld\u003c/a\u003e (contains environments):\u003ccode\u003egit clone https://github.com/vitchyr/multiworld\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-optional-install-doodad\" class=\"anchor\" href=\"#optional-install-doodad\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e(Optional) Install doodad\u003c/h3\u003e\n\u003cp\u003eI recommend installing \u003ca href=\"https://github.com/justinjfu/doodad\"\u003edoodad\u003c/a\u003e to\nlaunch jobs. Some of its nice features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEasily switch between running code locally, on a remote compute with\nDocker, on EC2 with Docker\u003c/li\u003e\n\u003cli\u003eEasily add your dependencies that can\u0027t be installed via pip (e.g. you\nborrowed someone\u0027s code)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you install doodad, also modify \u003ccode\u003eCODE_DIRS_TO_MOUNT\u003c/code\u003e in \u003ccode\u003econfig.py\u003c/code\u003e to\ninclude:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePath to rllab directory\u003c/li\u003e\n\u003cli\u003ePath to railrl directory\u003c/li\u003e\n\u003cli\u003ePath to other code you want to juse\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou\u0027ll probably also need to update the other variables besides the docker\nimages/instance stuff.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup-config-file\" class=\"anchor\" href=\"#setup-config-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup Config File\u003c/h3\u003e\n\u003cp\u003eYou must setup the config file for launching experiments, providing paths to your code and data directories. Inside \u003ccode\u003erailrl/config/launcher_config.py\u003c/code\u003e, fill in the appropriate paths. You can use \u003ccode\u003erailrl/config/launcher_config_template.py\u003c/code\u003e as an example reference.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecp railrl/launchers/config-template.py railrl/launchers/config.py\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-visualizing-a-policy-and-seeing-results\" class=\"anchor\" href=\"#visualizing-a-policy-and-seeing-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVisualizing a policy and seeing results\u003c/h2\u003e\n\u003cp\u003eDuring training, the results will be saved to a file called under\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eLOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLOCAL_LOG_DIR\u003c/code\u003e is the directory set by \u003ccode\u003erailrl.launchers.config.LOCAL_LOG_DIR\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;exp_prefix\u0026gt;\u003c/code\u003e is given either to \u003ccode\u003esetup_logger\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\u0026lt;foldername\u0026gt;\u003c/code\u003e is auto-generated and based off of \u003ccode\u003eexp_prefix\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003einside this folder, you should see a file called \u003ccode\u003eparams.pkl\u003c/code\u003e. To visualize a policy, run\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e(railrl) $ python scripts/sim_policy LOCAL_LOG_DIR/\u0026lt;exp_prefix\u0026gt;/\u0026lt;foldername\u0026gt;/params.pkl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have rllab installed, you can also visualize the results\nusing \u003ccode\u003erllab\u003c/code\u003e\u0027s viskit, described at\nthe bottom of \u003ca href=\"http://rllab.readthedocs.io/en/latest/user/cluster.html\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003etl;dr run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython rllab/viskit/frontend.py LOCAL_LOG_DIR/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eexp_prefix\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-add-paths\" class=\"anchor\" href=\"#add-paths\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdd paths\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=$PYTHONPATH:/path/to/multiworld/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/doodad/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/viskit/repo\nexport PYTHONPATH=$PYTHONPATH:/path/to/railrl-private/repo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credit\" class=\"anchor\" href=\"#credit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredit\u003c/h2\u003e\n\u003cp\u003eA lot of the coding infrastructure is based on \u003ca href=\"https://github.com/rll/rllab\"\u003erllab\u003c/a\u003e.\nAlso, the serialization and logger code are basically a carbon copy.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623985699.0
  },
  {
    "data_format": 2,
    "description": "Massively Parallel, Portable, and Reproducible Tractography",
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "LLNL/MaPPeRTrac",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mappertrac\" class=\"anchor\" href=\"#mappertrac\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaPPeRTrac\u003c/h1\u003e\n\u003cp\u003eMassively Parallel, Portable, and Reproducible Tractography (MaPPeRTrac) is a brain tractography workflow for high performance computing. It incorporates novel technologies to simplify and accelerate neuroimaging research.\n\u003cbr\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h3\u003e\n\u003cp\u003eRequirements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython 3.5+\u003c/li\u003e\n\u003cli\u003eSLURM job scheduling on a multi-node system\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003e1. Install NumPy and Parsl\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epip3 install parsl numpy scipy\u003c/code\u003e\u003cbr\u003e\n(\u003ccode\u003epip3 install parsl numpy scipy --user\u003c/code\u003e for non-root systems)\u003c/p\u003e\n\u003cp\u003e\u003cb\u003e2. Clone repository\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egit clone git@github.com:LLNL/MaPPeRTrac.git\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003ecd MaPPeRTrac/\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cb\u003e3. Load a Singularity container\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003eRequirements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity 3.0+ (\u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/\" rel=\"nofollow\"\u003ehttps://www.sylabs.io/guides/3.0/user-guide/\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBuilding the container:\u003cbr\u003e\ni. Obtain root access (you can copy and run the image in a non-root system afterwards).\u003cbr\u003e\nii. Place a Freesurfer \u003ccode\u003elicense.txt\u003c/code\u003e in the repo directory (\u003ca href=\"https://surfer.nmr.mgh.harvard.edu/fswiki/License\" rel=\"nofollow\"\u003ehttps://surfer.nmr.mgh.harvard.edu/fswiki/License\u003c/a\u003e).\u003cbr\u003e\niii. \u003ccode\u003e./container/build.sh\u003c/code\u003e\n\u003cbr\u003e\nNotes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMake sure to set \u003ccode\u003econtainer_path\u003c/code\u003e to the Singularity container\u0027s location.\u003c/li\u003e\n\u003cli\u003eIf you are having trouble building the container, try branch \u003ccode\u003eno_viz\u003c/code\u003e. This will disable render functionality.\u003c/li\u003e\n\u003cli\u003eAlternatively, download the image \u003ca href=\"https://drive.google.com/file/d/1lh0_5GO6-7qIznjvIcSMY-Ua8iBpZ4DJ/view?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\n\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cb\u003e4. Specify your DICOM or NIfTI data\u003c/b\u003e\u003c/p\u003e\n\u003cp\u003ePlace your data in the same filesystem as the repository.\u003c/p\u003e\n\u003cp\u003eYou can download the example data \u003ca href=\"https://drive.google.com/file/d/1YC0QzWNohq173_zJaqZfnI5d6EPb9On2/view?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-launch\" class=\"anchor\" href=\"#launch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLaunch\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003e./s_run_all.py \u0026lt;config_json\u0026gt;\u003c/code\u003e\n\u003cbr\u003e\nSee \u003ccode\u003eexamples/dummy_config.json\u003c/code\u003e for example parameters.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-file-overview\" class=\"anchor\" href=\"#file-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFile Overview\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eTracktographyScripts/\n+- container/\n|  +- build.sh\n|  +- Singularity               # Singularity build recipe\n|\n+- examples\n|  +- dataset_description.json  # Example of the BIDS dataset description\n|  +- dummy_config.json         # Example of the config JSON\n|  +- dummy_dicom/\n|  +- dummy_nifti/\n|  +- dummy_subjects.json       # Example of the subjects JSON\n|\n+- license.txt                  # Freesurfer license. NOTE: not included, required to build Singularity container\n+- LICENSE                      # MaPPeRTrac license.\n|\n+- lists/\n|  +- connectome_idxs.txt       # Brain region indices for .mat connectome files\n|  +- list_edges_reduced.txt    # Default edges to compute with Probtrackx and EDI (930 edges)\n|  +- list_edges_all.txt        # All possible edges (6643 edges)\n|  +- render_targets.txt        # NiFTI files to visualize with s4_render\n|\n+- README.md\n|\n+- s_run_all.py                 # Main script\n|\n+- subscripts/\n   +- __init__.py\n   +- maskseeds.py              # Helper functions for s2b_freesurfer.py\n   +- run_vtk.py                # Helper script for s4_render.py\n   +- s_debug.py                # For debugging\n   +- s1_dti_preproc.py\n   +- s2a_bedpostx.py\n   +- s2b_freesurfer.py\n   +- s3_probtrackx.py\n   +- s4_render.py\n   +- utilities.py              # General utility functions\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cbr\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output-overview\" class=\"anchor\" href=\"#output-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput Overview\u003c/h3\u003e\n\u003cp\u003eThe following are the most important output files. This list is not comprehensive.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;OUTPUT DIRECTORY\u0026gt;/\n+- sourcedata/                                              # DICOM preprocessing data\n+- rawdata/                                                 # BIDS-compliant NiFTI imaging data\n+- derivatives/\n   +- sub-\u0026lt;SUBJECT NAME\u0026gt;\n      +- [ses-\u0026lt;SESSION NAME\u0026gt;]                               # If session name specified, outputs will be in a session directory\n         +- connectome_idxs.txt                             # Brain region indices for .mat connectome files\n         +- connectome_#samples_oneway.txt                  # Oneway connectome in list form. Each edge has four columns:\n                                                                  Column 1 is the source region\n                                                                  Column 2 is the destination region\n                                                                  Column 3 is number of fibers (NOF): the total count of successful streamlines between the two regions\n                                                                  Column 4 is normalized NOF: the average density of successful streamlines the target region.\n         +- connectome_#samples_twoway.txt                  # Twoway connectome in list form\n         +- connectome_#samples_oneway_nof.mat              # Oneway NOF connectome in matrix form\n         +- connectome_#samples_twoway_nof.mat              # Twoway NOF connectome in matrix form (should be symmetric)\n         +- connectome_#samples_oneway_nof_normalized.mat   # Oneway normalized NOF connectome in matrix form\n         +- connectome_#samples_twoway_nof_normalized.mat   # Twoway normalized NOF connectome in matrix form (should be symmetric)\n         |\n         +- EDI/\n         |  +- EDImaps/\n         |     +- FAtractsumsRaw.nii.gz                     # NiFTI image of total streamline density\n         |     +- FAtractsumsTwoway.nii.gz                  # NiFTI image of edge density (EDI). See Payabvash et al. (2019) for details.\n         |\n         +- log/                                            # Directory containing stdout and performance logs\n         |\n         +- render/                                         # Directory containing NiFTI image renders from step s4_render\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cbr\u003e\n\u003cbr\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-config-parameterscommand-line-arguments\" class=\"anchor\" href=\"#config-parameterscommand-line-arguments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfig Parameters/Command Line Arguments\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eRequired Parameter\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003esubjects_json\u003c/td\u003e\n\u003ctd\u003eJSON file with input directories for each subject\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eoutput_dir\u003c/td\u003e\n\u003ctd\u003eThe super-directory that will contain output directories for each subject.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003escheduler_name\u003c/td\u003e\n\u003ctd\u003eScheduler to be used for running jobs. Value is \"slurm\" for LLNL, \"cobalt\" for ANL, and \"grid_engine\" for UCSF.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cbr\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOptional Parameter\u003c/th\u003e\n\u003cth\u003eDefault\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003esteps\u003c/td\u003e\n\u003ctd\u003es1 s2a s2b s3 s4\u003c/td\u003e\n\u003ctd\u003eSteps to run\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egpu_steps\u003c/td\u003e\n\u003ctd\u003es2a\u003c/td\u003e\n\u003ctd\u003eSteps to enable CUDA-enabled binaries\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003escheduler_bank\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eScheduler bank to charge for jobs. Required for slurm and cobalt.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003escheduler_partition\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eScheduler partition to assign jobs. Required for slurm and cobalt.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003escheduler_options\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eString to prepend to the submit script to the scheduler\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egpu_options\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eString to prepend to the submit blocks for GPU-enabled steps, such as \u0027module load cuda/8.0;\u0027\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eworker_init\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eString to run before starting a worker, such as \u2018module load Anaconda; source activate env;\u2019\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003econtainer_path\u003c/td\u003e\n\u003ctd\u003econtainer/image.simg\u003c/td\u003e\n\u003ctd\u003ePath to Singularity container image\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eunix_username\u003c/td\u003e\n\u003ctd\u003e[[current user]]\u003c/td\u003e\n\u003ctd\u003eUnix username for Parsl job requests\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eunix_group\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eUnix group to assign file permissions\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eforce\u003c/td\u003e\n\u003ctd\u003eFalse\u003c/td\u003e\n\u003ctd\u003eForce re-compute if checkpoints already exist\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003egssapi\u003c/td\u003e\n\u003ctd\u003eFalse\u003c/td\u003e\n\u003ctd\u003eUse Kerberos GSS-API authentication\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003elocal_host_only\u003c/td\u003e\n\u003ctd\u003eTrue\u003c/td\u003e\n\u003ctd\u003eRequest all jobs on local machine, ignoring other hostnames\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eparsl_path\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003ePath to Parsl binaries, if not installed in /usr/bin or /usr/sbin\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003erender_list\u003c/td\u003e\n\u003ctd\u003elists/render_targets.txt\u003c/td\u003e\n\u003ctd\u003eText file list of NIfTI outputs for s4_render (relative to each subject output directory)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epbtx_sample_count\u003c/td\u003e\n\u003ctd\u003e1000\u003c/td\u003e\n\u003ctd\u003eNumber of streamlines per seed voxel in s3_probtrackx\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epbtx_random_seed\u003c/td\u003e\n\u003ctd\u003e[[random number]]\u003c/td\u003e\n\u003ctd\u003eRandom seed in s3_probtrackx\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epbtx_max_memory\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003eMaximum memory per node (in GB) for s3_probtrackx. Default value of 0 indicates unlimited memory bound\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003econnectome_idx_list\u003c/td\u003e\n\u003ctd\u003elists/connectome_idxs.txt\u003c/td\u003e\n\u003ctd\u003eText file with pairs of volumes and connectome indices\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ehistogram_bin_count\u003c/td\u003e\n\u003ctd\u003e256\u003c/td\u003e\n\u003ctd\u003eNumber of bins in NiFTI image histograms\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epbtx_edge_list\u003c/td\u003e\n\u003ctd\u003elists/list_edges_reduced.txt\u003c/td\u003e\n\u003ctd\u003eText file list of edges for steps s3_probtrackx\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ecompress_pbtx_results\u003c/td\u003e\n\u003ctd\u003eTrue\u003c/td\u003e\n\u003ctd\u003eCompress probtrackx outputs to reduce inode and disk space usage\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003edynamic_walltime\u003c/td\u003e\n\u003ctd\u003eFalse\u003c/td\u003e\n\u003ctd\u003eRequest dynamically shortened walltimes, to gain priority on job queue\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es1_job_time\u003c/td\u003e\n\u003ctd\u003e00:15:00\u003c/td\u003e\n\u003ctd\u003eMax time to finish s1 on 1 subject with 1 node, if dynamic_walltime is true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2a_job_time\u003c/td\u003e\n\u003ctd\u003e00:45:00\u003c/td\u003e\n\u003ctd\u003eMax time to finish s2a on 1 subject with 1 node, if dynamic_walltime is true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2b_job_time\u003c/td\u003e\n\u003ctd\u003e10:00:00\u003c/td\u003e\n\u003ctd\u003eMax time to finish s2b on 1 subject with 1 node, if dynamic_walltime is true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es3_job_time\u003c/td\u003e\n\u003ctd\u003e23:59:00\u003c/td\u003e\n\u003ctd\u003eMax time to finish s3 on 1 subject with 1 node, if dynamic_walltime is true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es4_job_time\u003c/td\u003e\n\u003ctd\u003e00:15:00\u003c/td\u003e\n\u003ctd\u003eMax time to finish s4 on 1 subject with 1 node, if dynamic_walltime is true\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es1_cores_per_task\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003eNumber of cores to assign each task for step s1_dti_preproc\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2a_cores_per_task\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eNumber of cores to assign each task for step s2a_bedpostx\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2b_cores_per_task\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eNumber of cores to assign each task for step s2b_freesurfer\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es3_cores_per_task\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003eNumber of cores to assign each task for step s3_probtrackx\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es4_cores_per_task\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003ctd\u003eNumber of cores to assign each task for step s4_render\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es1_hostname\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eHostname of machine to run step s1_dti_preproc, if local_host_only is false\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2a_hostname\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eHostname of machine to run step s2a_bedpostx, if local_host_only is false\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2b_hostname\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eHostname of machine to run step s2b_freesurfer, if local_host_only is false\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es3_hostname\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eHostname of machine to run step s3_probtrackx, if local_host_only is false\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es4_hostname\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eHostname of machine to run step s4_render, if local_host_only is false\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es1_walltime\u003c/td\u003e\n\u003ctd\u003e23:59:00\u003c/td\u003e\n\u003ctd\u003eWalltime for step s1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2a_walltime\u003c/td\u003e\n\u003ctd\u003e23:59:00\u003c/td\u003e\n\u003ctd\u003eWalltime for step s2a\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2b_walltime\u003c/td\u003e\n\u003ctd\u003e23:59:00\u003c/td\u003e\n\u003ctd\u003eWalltime for step s2b\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es3_walltime\u003c/td\u003e\n\u003ctd\u003e23:59:00\u003c/td\u003e\n\u003ctd\u003eWalltime for step s3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es4_walltime\u003c/td\u003e\n\u003ctd\u003e23:59:00\u003c/td\u003e\n\u003ctd\u003eWalltime for step s4\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es1_nodes\u003c/td\u003e\n\u003ctd\u003e[[floor(0.2 * num_subjects)]]\u003c/td\u003e\n\u003ctd\u003eNode count for step s1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2a_nodes\u003c/td\u003e\n\u003ctd\u003e[[floor(1.0 * num_subjects)]]\u003c/td\u003e\n\u003ctd\u003eNode count for step s2a\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2b_nodes\u003c/td\u003e\n\u003ctd\u003e[[floor(1.0 * num_subjects)]]\u003c/td\u003e\n\u003ctd\u003eNode count for step s2b\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es3_nodes\u003c/td\u003e\n\u003ctd\u003e[[floor(1.0 * num_subjects)]]\u003c/td\u003e\n\u003ctd\u003eNode count for step s3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es4_nodes\u003c/td\u003e\n\u003ctd\u003e[[floor(0.1 * num_subjects)]]\u003c/td\u003e\n\u003ctd\u003eNode count for step s4\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es1_cores\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eCores per node for step s1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2a_cores\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eCores per node for step s2a\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es2b_cores\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eCores per node for step s2b\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es3_cores\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eCores per node for step s3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003es4_cores\u003c/td\u003e\n\u003ctd\u003e[[core count on head node]]\u003c/td\u003e\n\u003ctd\u003eCores per node for step s4\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ebids_json\u003c/td\u003e\n\u003ctd\u003eexamples/dummy_bids_desc.json\u003c/td\u003e\n\u003ctd\u003eDescription file dataset_description.json, as specified at \u003ca href=\"https://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html\" rel=\"nofollow\"\u003ehttps://bids-specification.readthedocs.io/en/stable/03-modality-agnostic-files.html\u003c/a\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ebids_readme\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eFree form text file describing the dataset in more detail\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ebids_session_name\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eName for the session timepoint (e.g. 2weeks)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-mri-images-from-openneuro\" class=\"anchor\" href=\"#download-mri-images-from-openneuro\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload MRI Images from OpenNeuro\u003c/h3\u003e\n\u003cp\u003eDownload MRI images from OpenNeuro repository by providing path to install data and accession ID of the MRI image.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eusage: subscripts/download_openneuro.py [-h] [--install-directory INSTALL_DIR] [-a ACC_NUM]\n\narguments:\n  -h, --help            show this help message and exit\n  --install-directory INSTALL_DIR\n                        Path where data will be installed\n  -a ACC_NUM, --accession ACC_NUM\n                        MRI Accession ID from OpenNeuro\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRequirements:\npython package datalad, git-annex\nInstallation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda install -c conda-forge datalad\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eon mac:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebrew install git-annex\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eon linux:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda install -c conda-forge git-annex\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h3\u003e\n\u003cp\u003eMaPPeRTrac is distributed under the terms of the BSD-3 License.\u003c/p\u003e\n\u003cp\u003eLLNL-CODE-811655\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1627333260.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "ploi/planning/FD/misc/releases/19.12/Singularity.19.12",
      "ploi/planning/FD/misc/releases/20.06/Singularity.20.06",
      "ploi/planning/FD/misc/releases/19.06/Singularity.19.06",
      "ploi/planning/FD/misc/releases/latest/Singularity"
    ],
    "full_name": "alestarbucks/ofappdl",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-object-filtering-in-automatic-planning-problems-using-deep-learning\" class=\"anchor\" href=\"#object-filtering-in-automatic-planning-problems-using-deep-learning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eObject Filtering in Automatic Planning Problems using Deep Learning\u003c/h1\u003e\n\u003cp\u003eThis README file is explicitly dedicated to serve as the guide of use of the source code associated to Alejandro \u00c1lvarez Conejo\u0027s Final Bachelor Thesis in order to run the project in any local computer. Note that these instructions are described to be applicable to Linux-based systems.\u003c/p\u003e\n\u003cp\u003eThis repository contains three main folders, which are referred to in this annex as \u003ccode\u003emodules\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ccode\u003eploi\u003c/code\u003e folder contains all the code related to the execution of the main algorithm for PLOI. It includes the code related to the guiders, the planners (including Fast-Downward) and the GNN implementation, as well as the main scripts that allow the whole project to work as discussed in the main body of the thesis. Note that inside the \u003ccode\u003emodel\u003c/code\u003e folder the model and data set files for the conducted tests can be found.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003egenerators\u003c/code\u003e folder contains the scripts that were used to generate the training and test problems. Inside, there is a folder dedicated to each of the domains of study and all of their versions, including the scripts that were used for the first approach described in chapter 5.3 in the \u003ccode\u003eunconnectednoise\u003c/code\u003e subfolder.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003epddlgym\u003c/code\u003e folder, which contains all the code related to the PDDLGym module. It has to be modified in order to include the domains of study inside its existing library of domains and example problems. Note that the original code for this module was also modified in order to make it more flexible to several valid syntaxes in PDDL. These modifications are not related to the core algorithm and thus have not been thoroughly detailed but the code inside the \u003ccode\u003eparser\u003c/code\u003e file of this module can be compared to the original parser in PDDLGym\u2019s original repository in order to examine the specifics of these changes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-projects-source-code-and-dependencies\" class=\"anchor\" href=\"#installing-the-projects-source-code-and-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the project\u2019s source code and dependencies\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eInstall basic dependencies: cmake, g++, make, git, Python 3.6 or higher and pip, if these are not already installed.\u003c/li\u003e\n\u003cli\u003eClone the thesis\u2019 repository using the following command:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/alestarbucks/ofappdl\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eNavigate to the \u003ccode\u003eploi\u003c/code\u003e folder and install the requirements for that module:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip install -r requirements.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eRepeat the same operation for the PDDLGym module.\n4.\tAdditionally, install wandb to avoid missing dependencies:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip install wandb\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003eCreate a symbolic link called \u003ccode\u003evalidate\u003c/code\u003e on the machine\u2019s path, pointing to the VAL validator\u2019s binary:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo ln -s \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003epath_to_ofappdl\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/ofappdl/val/bin/Validate /usr/local/bin/validate\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn order to check that the symbolic link is working as intended, try to enter the command \u003ccode\u003evalidate\u003c/code\u003e in the command line and expect an output showing the usage of the command.\n6.\tBuild the Fast-Downward planner by navigating to ploi/planning/fd and running the following command (it may take a few minutes):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./build.py\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"7\"\u003e\n\u003cli\u003eBefore the first run and every time that a new domain is added to the PDDLGym module, re-install it using the version that exists in the repository. From the root folder of the repository, run:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip install -e ./pddlgym\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis command is automatically included in the provided shell script that runs the project, so it is not explicitly needed to execute this step if such script is used.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-including-a-new-domain\" class=\"anchor\" href=\"#including-a-new-domain\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIncluding a new domain\u003c/h2\u003e\n\u003cp\u003eIn order to use PLOI for the purpose of applying it to other domains, a few changes must be made inside both the \u003ccode\u003epddlgym\u003c/code\u003e module and the \u003ccode\u003eploi\u003c/code\u003e module:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFirst, add the domain. Navigate to \u003ccode\u003epddlgym/pddlgym/pddl\u003c/code\u003e and copy the domain file inside that folder.\u003c/li\u003e\n\u003cli\u003eLikewise, add the training and test problems in two separate folders called \u003ccode\u003e\u0026lt;domain name\u0026gt;\u003c/code\u003e and \u003ccode\u003e\u0026lt;domain name\u0026gt;_test\u003c/code\u003e, respectively, inside the aforementioned folder.\u003c/li\u003e\n\u003cli\u003eOpen the \u003ccode\u003e__init__.py\u003c/code\u003e file inside pddlgym/pddlgym. Locate the list of environments after line 34 (\u003ccode\u003efor env_name, kwargs in [\u003c/code\u003e) and add the following lines, completing with the same name as the domain that was added in 1:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e(\u003cspan class=\"pl-s\"\u003e\"\u0026lt;domain name\u0026gt;\"\u003c/span\u003e,\n    {\u003cspan class=\"pl-s\"\u003e\"operators_as_actions\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e,\n    \u003cspan class=\"pl-s\"\u003e\"dynamic_action_space\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e}\n)\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eThe domain has now been added to the PDDLGym module and now it must be included in the PLOI module. For this, open the \u003ccode\u003emain.py\u003c/code\u003e file inside the ploi module and locate the \u003ccode\u003epddlgym_env_names\u003c/code\u003e dictionary. Add an entry in which the key is the name to which the domain will be referred in the invoking command inside the PLOI module, and the value is the name of the domain inside the PDDLGym module that was used for steps 1 to 3. For clarity, using the same name for both is recommended.\u003c/li\u003e\n\u003cli\u003eIn case of using the provided shell script to run the project, set the \u003ccode\u003eDOMAIN_NAME\u003c/code\u003e variable to match the key of the previously added entry in the dictionary mentioned in step 4.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-project\" class=\"anchor\" href=\"#running-the-project\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the project\u003c/h2\u003e\n\u003cp\u003eThe main command that triggers the start of the project\u2019s execution takes the following parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e--domain_name\u003c/code\u003e (required): The name of the domain of study to which the selected method is intended to be applied. It must be consistent and match the name chosen in the process detailed in the previous section.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--train_planner_name\u003c/code\u003e: The name of the planner used for training. In the experiments detailed in this report, this planner was fd-opt-lmcut (the optimal variant of FD).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--test_planner_name\u003c/code\u003e (required): The name of the planner used for testing. In the experiments detailed in this report, this planner was fd-lama-first (the satisficing variant of FD).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--guider_name\u003c/code\u003e (required): The name of the guider to be used, between gnn-bce-10 (GNN guider) or no-guidance (for standard planning or random score).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--num_seeds\u003c/code\u003e (required): The number of seeds which will be used to randomly initialize the model\u2019s weights before training. The learning phase will be repeated as many times as seeds are specified, and only the best model will be selected. Only one seed was used for the experiments in this thesis.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--num_train_problems\u003c/code\u003e (default to 0): The number of training problems to be considered.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--num_test_problems\u003c/code\u003e (required): The number of testing problems to be considered.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--do_incremental_planning\u003c/code\u003e (required): 1 or 0. Whether or not to use incremental planning, i.e., for PLOI or random scoring, whether it implements random score guidance or GNN-based guidance. For standard planning this flag must be set to 0.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--greedy_search\u003c/code\u003e (default to 0): 1 or 0. Indicates whether the greedy search algorithm is implemented in the phase of training data collection.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--timeout\u003c/code\u003e (required): Time in seconds that each test problem is dedicated before time running out and the problem being skipped. For this thesis, this time span was of 120 seconds.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--num_epochs\u003c/code\u003e (default 1001): Number of epochs that will constitute the learning phase.\nThe command is then executed as:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython3 main.py [flags]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe provided shell script called \u003ccode\u003emyrun.sh\u003c/code\u003e inside the PLOI module serves as an easy way to control the experimental process. The selected domain and method must be uncommented from the file and the script will run the appropriate command to execute the required experimental run.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624570598.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "MontrealSergiy/deformation",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deformation-field\" class=\"anchor\" href=\"#deformation-field\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeformation field\u003c/h1\u003e\n\u003cp\u003eThis PERL script is a wrapper that is calling sequence of commands for generating deformation fields scrips\n\u003ca href=\"https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields\" rel=\"nofollow\"\u003ehttps://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields\u003c/a\u003e\nSource code for deformation pipeline and dependencies (MINC):\n\u003ca href=\"https://github.com/Mouse-Imaging-Centre/generate_deformation_fields\"\u003ehttps://github.com/Mouse-Imaging-Centre/generate_deformation_fields\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsage\u003c/p\u003e\n\u003cp\u003edeformation_2.pl -input ICBM_00100_t1_final.mnc \u0026lt;\u0026lt;this could be any anatomical minc file, for a collection of minc files\u0026gt;\u0026gt; -output dummy_hoho -deformation_ratio 0.6 -coordinate 70 100 70 10 10 10 -tolerance_space 4 \u0026lt;\u0026gt; -blur_determinant 0.25 \u0026lt;\u0026gt; -error 0.00001 \u0026lt;\u0026gt; -iteration 100\u003c/p\u003e\n\u003cp\u003eThe output of running this command looks like this:\nICBM_00100_t1_final_deformed_by_0.4atROIx70-y100-z70dimx10.dimy10.dimz10.mnc. \u003c/p\u003e\n\u003cp\u003eWe will also have a directory dummy_hoho/TMP that will contain the in-between-files.\u003c/p\u003e\n\u003cp\u003e$:/dummy_hoho/TMP$ ls\u003c/p\u003e\n\u003cp\u003eblock.mnc\u003c/p\u003e\n\u003cp\u003eblurred0.25determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc\u003c/p\u003e\n\u003cp\u003eDDDDdilated.mnc\u003c/p\u003e\n\u003cp\u003eDDDDring.mnc\u003c/p\u003e\n\u003cp\u003edeterminant_r_0.4_grid.mnc\u003c/p\u003e\n\u003cp\u003edeterminant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc\u003c/p\u003e\n\u003cp\u003edeterminant_r_0.4.xfm\u003c/p\u003e\n\u003cp\u003emask.mnc\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623632255.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for honggfuzz (https://github.com/google/honggfuzz)",
    "filenames": [
      "Singularity.1804",
      "Singularity.1604",
      "Singularity.i386",
      "v21/Singularity.v21"
    ],
    "full_name": "shub-fuzz/honggfuzz",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/honggfuzz/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3641\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSingularity image for honggfuzz (\u003ca href=\"https://github.com/google/honggfuzz\"\u003ehttps://github.com/google/honggfuzz\u003c/a\u003e)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name honggfuzz.sif https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.2/shub-fuzz-honggfuzz.1604.sif\n\nsingularity shell honggfuzz.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 18.04 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name honggfuzz.1804.sif https://github.com/shub-fuzz/honggfuzz/releases/download/0.0.2/shub-fuzz-honggfuzz.1804.sif\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682711.0
  },
  {
    "data_format": 2,
    "description": "Singularity Image for AFL (https://github.com/google/AFL)",
    "filenames": [
      "Singularity.1804",
      "Singularity.1604",
      "Singularity.i386"
    ],
    "full_name": "shub-fuzz/afl",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003eSingularity Image for AFL (\u003ca href=\"https://github.com/google/AFL\"\u003ehttps://github.com/google/AFL\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/afl/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/afl/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eusage:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name afl.sif https://github.com/shub-fuzz/afl/releases/download/0.0.2/shub-fuzz-afl.1604.sif\n\nsingularity shell afl.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 18.04 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name afl.1804.sif https://github.com/shub-fuzz/afl/releases/download/0.0.2/shub-fuzz-afl.1804.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 16.04 i386 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name afl_i386.sif https://github.com/shub-fuzz/afl/releases/download/0.0.2/shub-fuzz-afl.i386.sif\n\nsingularity pull --name afl_i386.sif shub://shub-fuzz/afl:i386\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682579.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for afl++ (https://github.com/AFLplusplus/AFLplusplus)",
    "filenames": [
      "Singularity.1804",
      "Singularity.1604",
      "Singularity.i386",
      "Singularity.2004"
    ],
    "full_name": "shub-fuzz/aflpp",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003eSingularity image for AFL++ (\u003ca href=\"https://github.com/AFLplusplus/AFLplusplus\"\u003ehttps://github.com/AFLplusplus/AFLplusplus\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/aflpp/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eusage:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name aflpp.sif https://github.com/shub-fuzz/aflpp/releases/download/0.0.2/shub-fuzz-aflpp.1604.sif\n\nsingularity shell aflpp.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003epull Ubuntu 18.04 container\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name aflpp.1804.sif https://github.com/shub-fuzz/aflpp/releases/download/0.0.2/shub-fuzz-aflpp.1804.sif\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682681.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Ankou (https://github.com/SoftSec-KAIST/Ankou)",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/ankou",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003eSingularity image for Ankou (\u003ca href=\"https://github.com/SoftSec-KAIST/Ankou\"\u003ehttps://github.com/SoftSec-KAIST/Ankou\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/ankou/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4173\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eusage:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name ankou.sif https://github.com/shub-fuzz/ankou/releases/download/0.0.2/shub-fuzz-ankou.1604.sif\n\nsingularity shell ankou.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682696.0
  },
  {
    "data_format": 2,
    "description": "QSYM  - Concolic Execution Engine (https://github.com/sslab-gatech/qsym)",
    "filenames": [
      "Singularity.1804",
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/qsym",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003eSingularity Image for QSYM (\u003ca href=\"https://github.com/sslab-gatech/qsym\"\u003ehttps://github.com/sslab-gatech/qsym\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/qsym/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3625\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eQSYM  - Concolic Execution Engine (\u003ca href=\"https://github.com/sslab-gatech/qsym\"\u003ehttps://github.com/sslab-gatech/qsym\u003c/a\u003e)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eusage:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name qsym.sif https://github.com/shub-fuzz/qsym/releases/download/0.0.2/shub-fuzz-qsym.1604.sif\n\nsingularity shell qsym.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682731.0
  },
  {
    "data_format": 2,
    "description": "FLAC (/fl\u00e6k/; Free Lossless Audio Codec) is an audio coding format for lossless compression of digital audio.",
    "filenames": [
      "1.3.3/Singularity"
    ],
    "full_name": "pscedu/singularity-flac",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-flac/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-flac/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/37fadb899e2280d332672dd4ff8c55c77b6d1da4314ad56fb36c15142413fbda/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d666c6163\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/37fadb899e2280d332672dd4ff8c55c77b6d1da4314ad56fb36c15142413fbda/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d666c6163\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-flac\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5ccca52861f2fb4e7486645f35b923f2cbc790f1e7ed709d7422b0c9f2dc19d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d666c6163\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5ccca52861f2fb4e7486645f35b923f2cbc790f1e7ed709d7422b0c9f2dc19d7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d666c6163\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-flac\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e8f82abbc9bacec399c48512a0390d8b4d29091355a9ce463d889ecb16cd4775/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d666c6163\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e8f82abbc9bacec399c48512a0390d8b4d29091355a9ce463d889ecb16cd4775/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d666c6163\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-flac\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5325f2a609a18c54057940e4962347ba4f1beeef88a23a92c7149e8016cf4e13/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d666c6163\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5325f2a609a18c54057940e4962347ba4f1beeef88a23a92c7149e8016cf4e13/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d666c6163\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-flac\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-flac\" class=\"anchor\" href=\"#singularity-flac\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-flac\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/flac\"\u003eflac\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-this-repository\" class=\"anchor\" href=\"#about-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout this repository\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/4d9e61b063851d778ce9938e3f7bb848d158ebf589dbcd68bf21c5d5eeef6d40/68747470733a2f2f6d65646961322e67697068792e636f6d2f6d656469612f31334867774773584630616947592f67697068792e6769663f6369643d6563663035653437396d61316e736b74386d786278726c323076377375656868343931687532306b6973786878636265267269643d67697068792e6769662663743d67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4d9e61b063851d778ce9938e3f7bb848d158ebf589dbcd68bf21c5d5eeef6d40/68747470733a2f2f6d65646961322e67697068792e636f6d2f6d656469612f31334867774773584630616947592f67697068792e6769663f6369643d6563663035653437396d61316e736b74386d786278726c323076377375656868343931687532306b6973786878636265267269643d67697068792e6769662663743d67\" alt=\"DANGER\" data-canonical-src=\"https://media2.giphy.com/media/13HgwGsXF0aiGY/giphy.gif?cid=ecf05e479ma1nskt8mxbxrl20v7suehh491hu20kisxhxcbe\u0026amp;rid=giphy.gif\u0026amp;ct=g\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe purpose of this repository is to highlight how to deploy a Singularity and Spack together.\u003c/li\u003e\n\u003cli\u003eAt this moment, the workflow is expected to fail as we have not found a good solution to deploying the images (yet).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the Perl scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/flac/1.3.3\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/flac\u003c/code\u003e as \u003ccode\u003e1.3.3.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "utilities",
      "singularity"
    ],
    "updated_at": 1624547157.0
  },
  {
    "data_format": 2,
    "description": "UPPMAX Singularity builds",
    "filenames": [
      "gapseq/Singularity.gapseq",
      "UniteM/Singularity.UniteM",
      "MitoZ/Singularity.v2.3-pm",
      "gromacs/Singularity.gromacs",
      "metaWRAP/Singularity.metaWRAP-deps-only",
      "metaWRAP/Singularity.metaWRAP",
      "metaWRAP/Singularity.metaWRAP-deps-only-ubuntu",
      "bonito/Singularity.bonito"
    ],
    "full_name": "pmitev/UPPMAX-Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-uppmax-singularity\" class=\"anchor\" href=\"#uppmax-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUPPMAX-Singularity\u003c/h1\u003e\n\u003cp\u003eUPPMAX Singularity builds\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623764674.0
  },
  {
    "data_format": 2,
    "description": "C++ API \u0026 command-line toolkit for working with BAM data",
    "filenames": [
      "2.5.1/Singularity"
    ],
    "full_name": "pscedu/singularity-bamtools",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bamtools/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bamtools/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/43cc1554b9e51a28dfa82673f6a9629d4b3f4151419378b68631040a1a5f52a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/43cc1554b9e51a28dfa82673f6a9629d4b3f4151419378b68631040a1a5f52a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/bb25827e818656bc2a93d95c923b954c29792611568e2828cee62c6501555455/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bb25827e818656bc2a93d95c923b954c29792611568e2828cee62c6501555455/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/30cf5022df93f7c848ab5284b476648b2a424ac87e287144bfdbc9460bb75256/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/30cf5022df93f7c848ab5284b476648b2a424ac87e287144bfdbc9460bb75256/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/72066080adaa298a3f65d9ad3d27681498685739defe4d4061e06d30f8bf5277/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/72066080adaa298a3f65d9ad3d27681498685739defe4d4061e06d30f8bf5277/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bamtools\" class=\"anchor\" href=\"#singularity-bamtools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bamtools\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/pezmaster31/bamtools\"\u003ebamtools\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ebamtools\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bamtools/2.5.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/bamtools\u003c/code\u003e as \u003ccode\u003e2.5.1\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1625149894.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.3.0"
    ],
    "full_name": "onuryukselen/singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity\u003c/h1\u003e\n\u003cp\u003eDevelopment Branch\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1623903591.0
  },
  {
    "data_format": 2,
    "description": "metarepo for tidying up container recipes, currently Singularity",
    "filenames": [
      "ubuntu/Singularity.ubuntu2004",
      "tensorflow/Singularity.tensorflow-v1.15.4-compiled-partial",
      "tensorflow/Singularity.tensorflow-v2.5.0-compiled",
      "tensorflow/Singularity.tensorflow-v2.2.0-compiled",
      "tensorflow/Singularity.tensorflow-v2.4.0-rc4-compiled",
      "tensorflow/Singularity.tensorflow-v2.0.3-compiled",
      "pacbio/Singularity.pacbio",
      "bioinfmunger/Singularity.bioinfmunger",
      "bioconda/Singularity.bioconda",
      "r/Singularity.r",
      "r/Singularity.r-plus",
      "jupyter/Singularity.jupyter-plus-tensorflow-v2.5.0-compiled",
      "jupyter/Singularity.jupyter-plus",
      "jupyter/Singularity.jupyter-plus-tensorflow-v2.4.0-rc4-compiled",
      "jupyter/Singularity.jupyter-plus-tensorflow-v2.2.0-compiled",
      "jupyter/Singularity.jupyter-plus-bioconda",
      "jupyter/Singularity.jupyter",
      "jupyter/Singularity.jupyter-plus-alignparse",
      "lh3-aligners/Singularity.lh3-aligners",
      "shell/Singularity.shell-plus",
      "starcode/Singularity.starcode-v0.1.1",
      "base/Singularity.base"
    ],
    "full_name": "darachm/containers",
    "latest_release": null,
    "readme": "\u003cp\u003eThis is for tracking, hosting recipes for Singularity containers, such that\nit can get mirrored on Github and singularity-hub can get it.\u003c/p\u003e\n\u003cp\u003eOrganzation copied from \u003ca href=\"https://github.com/jlboat/BioinfoContainers\"\u003ejlboat\u003c/a\u003e.\n(Of course, makes total sense to just use tags to organize things!)\u003c/p\u003e\n\u003cp\u003eSome recipes are for individual tools, some are for workflows and so are\ncombos.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624074888.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.pophuman",
      "Singularity.abcmk",
      "Singularity.breakseq",
      "Singularity.isafe"
    ],
    "full_name": "jmurga/bgd-pic",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast Downward\u003c/h1\u003e\n\u003cp\u003eFast Downward is a domain-independent classical planning system.\u003c/p\u003e\n\u003cp\u003eCopyright 2003-2020 Fast Downward contributors (see below).\u003c/p\u003e\n\u003cp\u003eFor further information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Downward website: \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReport a bug or file an issue: \u003ca href=\"http://issues.fast-downward.org\" rel=\"nofollow\"\u003ehttp://issues.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward mailing list: \u003ca href=\"https://groups.google.com/forum/#!forum/fast-downward\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/fast-downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward main repository: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tested-software-versions\" class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTested software versions\u003c/h2\u003e\n\u003cp\u003eThis version of Fast Downward has been tested with the following software versions:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOS\u003c/th\u003e\n\u003cth\u003ePython\u003c/th\u003e\n\u003cth\u003eC++ compiler\u003c/th\u003e\n\u003cth\u003eCMake\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 20.04\u003c/td\u003e\n\u003ctd\u003e3.8\u003c/td\u003e\n\u003ctd\u003eGCC 9, GCC 10, Clang 10, Clang 11\u003c/td\u003e\n\u003ctd\u003e3.16\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 18.04\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eGCC 7, Clang 6\u003c/td\u003e\n\u003ctd\u003e3.10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emacOS 10.15\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eAppleClang 12\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWindows 10\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eVisual Studio Enterprise 2017 (MSVC 19.16) and 2019 (MSVC 19.28)\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu, we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and on macOS, we do not test LP solvers (yet).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003eThe following list includes all people that actively contributed to\nFast Downward, i.e. all people that appear in some commits in Fast\nDownward\u0027s history (see below for a history on how Fast Downward\nemerged) or people that influenced the development of such commits.\nCurrently, this list is sorted by the last year the person has been\nactive, and in case of ties, by the earliest year the person started\ncontributing, and finally by last name.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2003-2020 Malte Helmert\u003c/li\u003e\n\u003cli\u003e2008-2016, 2018-2020 Gabriele Roeger\u003c/li\u003e\n\u003cli\u003e2010-2020 Jendrik Seipp\u003c/li\u003e\n\u003cli\u003e2010-2011, 2013-2020 Silvan Sievers\u003c/li\u003e\n\u003cli\u003e2012-2020 Florian Pommerening\u003c/li\u003e\n\u003cli\u003e2013, 2015-2020 Salome Eriksson\u003c/li\u003e\n\u003cli\u003e2016-2020 Cedric Geissmann\u003c/li\u003e\n\u003cli\u003e2017-2020 Guillem Franc\u00e8s\u003c/li\u003e\n\u003cli\u003e2018-2020 Augusto B. Corr\u00eaa\u003c/li\u003e\n\u003cli\u003e2018-2020 Patrick Ferber\u003c/li\u003e\n\u003cli\u003e2015-2019 Manuel Heusner\u003c/li\u003e\n\u003cli\u003e2017 Daniel Killenberger\u003c/li\u003e\n\u003cli\u003e2016 Yusra Alkhazraji\u003c/li\u003e\n\u003cli\u003e2016 Martin Wehrle\u003c/li\u003e\n\u003cli\u003e2014-2015 Patrick von Reth\u003c/li\u003e\n\u003cli\u003e2015 Thomas Keller\u003c/li\u003e\n\u003cli\u003e2009-2014 Erez Karpas\u003c/li\u003e\n\u003cli\u003e2014 Robert P. Goldman\u003c/li\u003e\n\u003cli\u003e2010-2012 Andrew Coles\u003c/li\u003e\n\u003cli\u003e2010, 2012 Patrik Haslum\u003c/li\u003e\n\u003cli\u003e2003-2011 Silvia Richter\u003c/li\u003e\n\u003cli\u003e2009-2011 Emil Keyder\u003c/li\u003e\n\u003cli\u003e2010-2011 Moritz Gronbach\u003c/li\u003e\n\u003cli\u003e2010-2011 Manuela Ortlieb\u003c/li\u003e\n\u003cli\u003e2011 Vidal Alc\u00e1zar Saiz\u003c/li\u003e\n\u003cli\u003e2011 Michael Katz\u003c/li\u003e\n\u003cli\u003e2011 Raz Nissim\u003c/li\u003e\n\u003cli\u003e2010 Moritz Goebelbecker\u003c/li\u003e\n\u003cli\u003e2007-2009 Matthias Westphal\u003c/li\u003e\n\u003cli\u003e2009 Christian Muise\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cp\u003eThe current version of Fast Downward is the merger of three different\nprojects:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe original version of Fast Downward developed by Malte Helmert\nand Silvia Richter\u003c/li\u003e\n\u003cli\u003eLAMA, developed by Silvia Richter and Matthias Westphal based on\nthe original Fast Downward\u003c/li\u003e\n\u003cli\u003eFD-Tech, a modified version of Fast Downward developed by Erez\nKarpas and Michael Katz based on the original code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition to these three main sources, the codebase incorporates\ncode and features from numerous branches of the Fast Downward codebase\ndeveloped for various research papers. The main contributors to these\nbranches are Malte Helmert, Gabi R\u00f6ger and Silvia Richter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe following directory is not part of Fast Downward as covered by\nthis license:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or (at\nyour option) any later version.\n\nFast Downward is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see \u0026lt;https://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624197047.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "misc/releases/19.12/Singularity.19.12",
      "misc/releases/20.06/Singularity.20.06",
      "misc/releases/19.06/Singularity.19.06",
      "misc/releases/latest/Singularity"
    ],
    "full_name": "salome-eriksson/downward-unsolvability",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast Downward\u003c/h1\u003e\n\u003cp\u003eFast Downward is a domain-independent classical planning system.\u003c/p\u003e\n\u003cp\u003eCopyright 2003-2020 Fast Downward contributors (see below).\u003c/p\u003e\n\u003cp\u003eFor further information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Downward website: \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReport a bug or file an issue: \u003ca href=\"http://issues.fast-downward.org\" rel=\"nofollow\"\u003ehttp://issues.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward mailing list: \u003ca href=\"https://groups.google.com/forum/#!forum/fast-downward\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/fast-downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward main repository: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tested-software-versions\" class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTested software versions\u003c/h2\u003e\n\u003cp\u003eThis version of Fast Downward has been tested with the following software versions:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOS\u003c/th\u003e\n\u003cth\u003ePython\u003c/th\u003e\n\u003cth\u003eC++ compiler\u003c/th\u003e\n\u003cth\u003eCMake\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 20.04\u003c/td\u003e\n\u003ctd\u003e3.8\u003c/td\u003e\n\u003ctd\u003eGCC 9, GCC 10, Clang 10, Clang 11\u003c/td\u003e\n\u003ctd\u003e3.16\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 18.04\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eGCC 7, Clang 6\u003c/td\u003e\n\u003ctd\u003e3.10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emacOS 10.15\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eAppleClang 12\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWindows 10\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eVisual Studio Enterprise 2017 (MSVC 19.16) and 2019 (MSVC 19.28)\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu, we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and on macOS, we do not test LP solvers (yet).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003eThe following list includes all people that actively contributed to\nFast Downward, i.e. all people that appear in some commits in Fast\nDownward\u0027s history (see below for a history on how Fast Downward\nemerged) or people that influenced the development of such commits.\nCurrently, this list is sorted by the last year the person has been\nactive, and in case of ties, by the earliest year the person started\ncontributing, and finally by last name.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2003-2020 Malte Helmert\u003c/li\u003e\n\u003cli\u003e2008-2016, 2018-2020 Gabriele Roeger\u003c/li\u003e\n\u003cli\u003e2010-2020 Jendrik Seipp\u003c/li\u003e\n\u003cli\u003e2010-2011, 2013-2020 Silvan Sievers\u003c/li\u003e\n\u003cli\u003e2012-2020 Florian Pommerening\u003c/li\u003e\n\u003cli\u003e2013, 2015-2020 Salome Eriksson\u003c/li\u003e\n\u003cli\u003e2016-2020 Cedric Geissmann\u003c/li\u003e\n\u003cli\u003e2017-2020 Guillem Franc\u00e8s\u003c/li\u003e\n\u003cli\u003e2018-2020 Augusto B. Corr\u00eaa\u003c/li\u003e\n\u003cli\u003e2018-2020 Patrick Ferber\u003c/li\u003e\n\u003cli\u003e2015-2019 Manuel Heusner\u003c/li\u003e\n\u003cli\u003e2017 Daniel Killenberger\u003c/li\u003e\n\u003cli\u003e2016 Yusra Alkhazraji\u003c/li\u003e\n\u003cli\u003e2016 Martin Wehrle\u003c/li\u003e\n\u003cli\u003e2014-2015 Patrick von Reth\u003c/li\u003e\n\u003cli\u003e2015 Thomas Keller\u003c/li\u003e\n\u003cli\u003e2009-2014 Erez Karpas\u003c/li\u003e\n\u003cli\u003e2014 Robert P. Goldman\u003c/li\u003e\n\u003cli\u003e2010-2012 Andrew Coles\u003c/li\u003e\n\u003cli\u003e2010, 2012 Patrik Haslum\u003c/li\u003e\n\u003cli\u003e2003-2011 Silvia Richter\u003c/li\u003e\n\u003cli\u003e2009-2011 Emil Keyder\u003c/li\u003e\n\u003cli\u003e2010-2011 Moritz Gronbach\u003c/li\u003e\n\u003cli\u003e2010-2011 Manuela Ortlieb\u003c/li\u003e\n\u003cli\u003e2011 Vidal Alc\u00e1zar Saiz\u003c/li\u003e\n\u003cli\u003e2011 Michael Katz\u003c/li\u003e\n\u003cli\u003e2011 Raz Nissim\u003c/li\u003e\n\u003cli\u003e2010 Moritz Goebelbecker\u003c/li\u003e\n\u003cli\u003e2007-2009 Matthias Westphal\u003c/li\u003e\n\u003cli\u003e2009 Christian Muise\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cp\u003eThe current version of Fast Downward is the merger of three different\nprojects:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe original version of Fast Downward developed by Malte Helmert\nand Silvia Richter\u003c/li\u003e\n\u003cli\u003eLAMA, developed by Silvia Richter and Matthias Westphal based on\nthe original Fast Downward\u003c/li\u003e\n\u003cli\u003eFD-Tech, a modified version of Fast Downward developed by Erez\nKarpas and Michael Katz based on the original code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition to these three main sources, the codebase incorporates\ncode and features from numerous branches of the Fast Downward codebase\ndeveloped for various research papers. The main contributors to these\nbranches are Malte Helmert, Gabi R\u00f6ger and Silvia Richter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe following directory is not part of Fast Downward as covered by\nthis license:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or (at\nyour option) any later version.\n\nFast Downward is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see \u0026lt;https://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624374031.0
  },
  {
    "data_format": 2,
    "description": "Definition files for singularity container",
    "filenames": [
      "Singularity.reach",
      "Singularity.one-point-stats"
    ],
    "full_name": "piyanatk/singularity-containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-containers\" class=\"anchor\" href=\"#singularity-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-containers\u003c/h1\u003e\n\u003cp\u003eDefinition files for singularity container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624371927.0
  },
  {
    "data_format": 2,
    "description": "Talking to Hinkskalle",
    "filenames": [
      "Singularity"
    ],
    "full_name": "csf-ngs/hinkskalle-api",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hinkskalle-api\" class=\"anchor\" href=\"#hinkskalle-api\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHinkskalle API\u003c/h1\u003e\n\u003cp\u003eTalking to \u003ca href=\"https://github.com/csf-ngs/hinkskalle\"\u003eHinkskalle\u003c/a\u003e made easy\u003c/p\u003e\n\u003cp\u003eUse me to\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elist available downloads\u003c/li\u003e\n\u003cli\u003edownload data\u003c/li\u003e\n\u003cli\u003eupload data\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003ehinkskalle-api provides\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea small library with a thin wrapper over the JSON API\u003c/li\u003e\n\u003cli\u003ea CLI (\u003ccode\u003ehinkli\u003c/code\u003e: short for hink-cli, get it?)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eYou will need python3 and pip. Then you can run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install git+https://github.com/csf-ngs/hinkskalle-api\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-line-interface\" class=\"anchor\" href=\"#command-line-interface\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand Line Interface\u003c/h3\u003e\n\u003cp\u003eGet a list of available commands and options:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ehinkli --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYour first step should be logging in:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e non-VBCF.NGS users get your own instance!\u003c/span\u003e\nhinkli --base https://singularity.ngs.vbcf.ac.at/ login\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e answer prompt for username and password\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe registry and token should now be stored in \u003ccode\u003e~/.hink_api.yml\u003c/code\u003e and available for further use.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-discovering--downloading-data\" class=\"anchor\" href=\"#discovering--downloading-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDiscovering \u0026amp; Downloading Data\u003c/h4\u003e\n\u003cp\u003eYour most likely use case will be downloading data provided via Hinkskalle.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e shows available collections of containers\u003c/span\u003e\nhinkli list-collections\nhinkli list-containers [collection]\nhinkli list-downloads [collection]/[container]\nhinkli pull [collection]/[container]:[tag]\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e username is optional, but can be provided, too:\u003c/span\u003e\nhinkli list-collections test.hase\nhinkli list-containers test.hase/[collection]\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e etc\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBasic structure:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA Collection holds a bunch of containers (topic, type, ...)\u003c/li\u003e\n\u003cli\u003eContainers hold tagged data\u003c/li\u003e\n\u003cli\u003eEach tag points to some data (some tags point to the same data)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf Hinkskalle shows you these downloads in your container \u003ccode\u003etest.hase/example/FAQ4711\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e- \u003cspan class=\"pl-ent\"\u003efilename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003ebunch_of_reads.fastq.gz\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003esize\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e41.5 MB\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003etags\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003ebasecalled,20210621\u003c/span\u003e\n- \u003cspan class=\"pl-ent\"\u003efilename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003erawdata.tar.gz\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003esize\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e41.5 TB\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003etags\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eraw\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can use these commands to download:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e either one fetches bunch_of_reads.fastq\u003c/span\u003e\nhinkli pull example/FAQ4711:basecalled\nhinkli pull example/FAQ4711:20210621\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e fetches rawdata.tar.gz\u003c/span\u003e\nhinkli pull example/FAQ4711:raw\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHinkli will even check the sha256 checksum for you!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-api\" class=\"anchor\" href=\"#api\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAPI\u003c/h3\u003e\n\u003cp\u003eNot documented - use at your own risk!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehinkskalle_api\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eHinkApi\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eapi\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eHinkApi\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003ecollections\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eapi\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003elist_collections\u003c/span\u003e()\n\u003cspan class=\"pl-c\"\u003e# etc\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003eBy default, hinkli reads its config from \u003ccode\u003e~/.hink_api.yml\u003c/code\u003e. This file should look like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003ehink_api_base\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003ehttps://singularity.ngs.vbcf.ac.at\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003ehink_api_key\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eyour_super_secret_token\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can use these env variables to override:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eHINK_API_BASE\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eHINK_API_KEY\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eHINK_API_CFG\u003c/code\u003e - to look for the config file in a different location\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h1\u003e\n\u003cp\u003eYou can regenerate the models from the Hinkskalle swagger/openapi definition:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install git+https://ngs.vbcf.ac.at/repo/software/swagspotta.git\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e from pkg.ngs.vbcf.ac.at production:\u003c/span\u003e\nshare/create_models.sh\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e from your local hinkskalle dev server:\u003c/span\u003e\nshare/create_models.sh http://localhost:7660/swagger\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626991307.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "shrutir11/lolcow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lolcow\" class=\"anchor\" href=\"#lolcow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elolcow\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624383824.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "QsingularityAi/polar-pfc-master_active-crystel",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-polar-pfc-master_active-crystel\" class=\"anchor\" href=\"#polar-pfc-master_active-crystel\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epolar-pfc-master_active-crystel\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624399268.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "dcgc-bfx/singularity-sc-rhapsody",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singlecell-sc-rhapsody\" class=\"anchor\" href=\"#singlecell-sc-rhapsody\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esinglecell-sc-rhapsody\u003c/h1\u003e\n\u003cp\u003eDCGC singularity recipe for containerized versions of the BD Rhapsody Targeted Analysis and Whole Transcriptome Analysis (WTA) pipelines (available at \u003ca href=\"https://bitbucket.org/CRSwDev/cwl/src/master/\" rel=\"nofollow\"\u003ehttps://bitbucket.org/CRSwDev/cwl/src/master/\u003c/a\u003e).\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1624446752.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity"
    ],
    "full_name": "bananaeat/Cinnamon_assembly",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cinnamon\" class=\"anchor\" href=\"#cinnamon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCinnamon\u003c/h1\u003e\n\u003cp\u003eThis directory contains the code for the Cinnamon language compiler.  This compiler is described in the paper:\u003c/p\u003e\n\u003cp\u003eCinnamon: A Domain-Specific Language for Binary Profiling and Monitoring,\nMahwish Arif, Ruoyu Zhou, Hsi-Ming Ho and Timothy M. Jones,\nCGO 2021\u003c/p\u003e\n\u003cp\u003ePlease cite this paper if you produce any work that builds upon this code and / or data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licence\" class=\"anchor\" href=\"#licence\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicence\u003c/h2\u003e\n\u003cp\u003eCinnamon is released under an Apache licence.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-cinnamon\" class=\"anchor\" href=\"#building-cinnamon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Cinnamon\u003c/h2\u003e\n\u003cp\u003eCinnamon can currently target three different binary frameworks; Janus, Pin and Dyninst.  To build the compiler:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003eexport CINNAMON_ROOT = /path/to/cinnamon-source\ncd $(CINNAMON_ROOT)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the Cinnamon backend for Janus:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003emake TARGET=janus\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the Cinnamon backend for Pin:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003emake TARGET=pin\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo build the Cinnamon backend for Dyninst:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003emake TARGET=dyninst\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-compiling-a-sample-program\" class=\"anchor\" href=\"#compiling-a-sample-program\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompiling a sample program\u003c/h2\u003e\n\u003cp\u003eCinnamon sample programs are available in the  \u003ccode\u003etests\u003c/code\u003e directory.  The following commands will compile the Cinnamon program \u003ccode\u003eins.dsl\u003c/code\u003e and integrate the resulting code into one of the target frameworks. You will need to set the path to your target framework installation in the respective scripts:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003e$(CINNAMON_ROOT)/Scripts/compileToJanus.py $CINNAMON_ROOT/tests/ins.dsl\n$(CINNAMON_ROOT)/Scripts/compileToPin.py $CINNAMON_ROOT/tests/ins.dsl\n$(CINNAMON_ROOT)/Scripts/compileToDyn.py $CINNAMON_ROOT/tests/ins.dsl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter this, the final tool can be built and run using the target framework\u0027s build instructions.\u003c/p\u003e\n\u003cp\u003eIf you just want to compile the Cinnamon DSL code and not yet integrate it into a target framework, run the following command.  This will generate a number of different files containing relevant code for the cinnamon program:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ecd $CINNAMON_ROOT\n./bdc $CINNAMON_ROOT/tests/ins.dsl\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-target-frameworks\" class=\"anchor\" href=\"#target-frameworks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTarget frameworks\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-janus\" class=\"anchor\" href=\"#janus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJanus\u003c/h3\u003e\n\u003cp\u003eYou can get the Janus implementation with placeholders, templates and utility libraries for Cinnamon from the main Janus repository at \u003ca href=\"https://github.com/timothymjones/Janus.git\"\u003ehttps://github.com/timothymjones/Janus.git\u003c/a\u003e, then switch to the \u003ccode\u003ecinnamon\u003c/code\u003e branch.\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003egit clone https://github.com/timothymjones/Janus.git\ncd Janus\ngit checkout -b cinnamon origin/cinnamon\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNext set \u003ccode\u003eJanusPATH\u003c/code\u003e in \u003ccode\u003ecompileToJanus.py\u003c/code\u003e to be the location that you have cloned Janus.\u003c/p\u003e\n\u003cp\u003eOnce the code for Janus has been generated and integrated (after running the \u003ccode\u003ecompileToJanus.py\u003c/code\u003e script from above), you can build the final tool using the following commands:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003e(cd build; cmake ..; make -j8)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the final tool on the target binary:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003e./janus/jdsl_run \u0026lt;target_binary\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pin\" class=\"anchor\" href=\"#pin\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePin\u003c/h3\u003e\n\u003cp\u003eEverything required for Pin is contained within the \u003ccode\u003etargets/Pin\u003c/code\u003e directory.  Copy the \u003ccode\u003eMyDSLTool\u003c/code\u003e directory to \u003ccode\u003epath-to-your-pin-root-dir/source/tools\u003c/code\u003e, where \u003ccode\u003epath-to-your-pin-root\u003c/code\u003e should be self-explanatory.\u003c/p\u003e\n\u003cp\u003eNext set \u003ccode\u003ePinPATH=your-pin-root-dir/source/tools/MyDSLTool\u003c/code\u003e in \u003ccode\u003ecompileToPin.py\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce the code for Pin has been generated and integrated (after running the \u003ccode\u003ecompileToPin.py\u003c/code\u003e script from above), you can build the final tool using the following commands:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ecd your-pin-root-dir/source/tools/MyDSLTool\nmake obj-intel64/MyDSLTool.so\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the final tool on the target binary:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003eyour-pin-root-dir/pin -t obj-intel64/MyDSLTool.so -- \u0026lt;target_binary\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dyninst\" class=\"anchor\" href=\"#dyninst\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDyninst\u003c/h3\u003e\n\u003cp\u003eYou can obtain Dyninst version 10.1.0 as follows:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ewget https://github.com/dyninst/dyninst/archive/v10.1.0.tar.gz``\ntar xzvf v10.1.0.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce extracted, add \u003ccode\u003ec_LoadInsn\u003c/code\u003e and \u003ccode\u003ec_StoreInsn\u003c/code\u003e into \u003ccode\u003eenum InsnCategory\u003c/code\u003e in \u003ccode\u003edyninst-10.1.0/instructionAPI/h/InstructionCategories.h\u003c/code\u003e and then build by following the Dyninst build instructions.\u003c/p\u003e\n\u003cp\u003eEverything else required for Dyninst is contained within the \u003ccode\u003etargets/Dyninst\u003c/code\u003e directory.  Copy the \u003ccode\u003eMyDSLTool\u003c/code\u003e directory to \u003ccode\u003epath-to-your-dyn-root-dir/examples\u003c/code\u003e, where \u003ccode\u003epath-to-your-dyn-root-dir\u003c/code\u003e should be self-explanatory.\u003c/p\u003e\n\u003cp\u003eNext set \u003ccode\u003eDynPATH=path-to-your-dyn-root-dir/examples/MyDSLTool\u003c/code\u003e in \u003ccode\u003ecompileToDyn.py\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce the code for Dyninst has been generated and integrated (after running the \u003ccode\u003ecompileToDyn.py\u003c/code\u003e script from above), you can build the final tool using the following commands:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003ecd path-to-your-dyn-root-dir/examples/MyDSLTool\nmake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run the final tool on the target binary:\u003c/p\u003e\n\u003cpre lang=\"shell-session\"\u003e\u003ccode\u003epath-to-your-dyn-root-dir/examples/MyDSLTool/DSLtool -m static -o \u0026lt;output_binary\u0026gt; \u0026lt;input_binary\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625671641.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "yuma-35/wave-U-guiter",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wave-u-net-pytorch\" class=\"anchor\" href=\"#wave-u-net-pytorch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWave-U-Net (Pytorch)\u003c/h1\u003e\n\u003cp\u003eImproved version of the \u003ca href=\"https://arxiv.org/abs/1806.03185\" rel=\"nofollow\"\u003eWave-U-Net\u003c/a\u003e for audio source separation, implemented in Pytorch.\u003c/p\u003e\n\u003cp\u003eClick \u003ca href=\"www.github.com/f90/Wave-U-Net\"\u003ehere\u003c/a\u003e for the original Wave-U-Net implementation in Tensorflow.\nYou can find more information about the model and results there as well.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-improvements\" class=\"anchor\" href=\"#improvements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImprovements\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eMulti-instrument separation by default, using a separate standard Wave-U-Net for each source (can be set to one model as well)\u003c/li\u003e\n\u003cli\u003eMore scalable to larger data: A depth parameter D can be set that employs D convolutions for each single convolution in the original Wave-U-Net\u003c/li\u003e\n\u003cli\u003eMore configurable: Layer type, resampling factor at each level etc. can be easily changed (different normalization, residual connections...)\u003c/li\u003e\n\u003cli\u003eFast training: Preprocesses the given dataset by saving the audio into HDF files, which can be read very quickly during training, thereby avoiding slowdown due to resampling and decoding\u003c/li\u003e\n\u003cli\u003eModular thanks to Pytorch: Easily replace components of the model with your own variants/layers/losses\u003c/li\u003e\n\u003cli\u003eBetter output handling: Separate output convolution for each source estimate with linear activation so amplitudes near 1 and -1 can be easily predicted, at test time thresholding to valid amplitude range [-1,1]\u003c/li\u003e\n\u003cli\u003eFixed or dynamic resampling: Either use fixed lowpass filter to avoid aliasing during resampling, or use a learnable convolution\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003cp\u003eGPU strongly recommended to avoid very long training times.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-option-1-direct-install-recommended\" class=\"anchor\" href=\"#option-1-direct-install-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOption 1: Direct install (recommended)\u003c/h3\u003e\n\u003cp\u003eSystem requirements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinux-based OS\u003c/li\u003e\n\u003cli\u003ePython 3.6\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://mega-nerd.com/libsndfile/\" rel=\"nofollow\"\u003elibsndfile\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.ffmpeg.org/\" rel=\"nofollow\"\u003effmpeg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eCUDA 10.1 for GPU usage\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eClone the repository:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/f90/Wave-U-Net-Pytorch.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRecommended: Create a new virtual environment to install the required Python packages into, then activate the virtual environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evirtualenv --python /usr/bin/python3.6 waveunet-env\nsource waveunet-env/bin/activate\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eInstall all the required packages listed in the \u003ccode\u003erequirements.txt\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install -r requirements.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-option-2-singularity\" class=\"anchor\" href=\"#option-2-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOption 2: Singularity\u003c/h3\u003e\n\u003cp\u003eWe also provide a Singularity container which allows you to avoid installing the correct Python, CUDA and other system libraries, however we don\u0027t provide specific advice on how to run the container and so only do this if you have to or know what you are doing (since you need to mount dataset paths to the container etc.)\u003c/p\u003e\n\u003cp\u003eTo pull the container, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://f90/Wave-U-Net-Pytorch\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen run the container from the directory where you cloned this repository to, using the commands listed further below in this readme.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-download-datasets\" class=\"anchor\" href=\"#download-datasets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload datasets\u003c/h1\u003e\n\u003cp\u003eTo directly use the pre-trained models we provide for download to separate your own songs, now skip directly to the \u003ca href=\"#test\"\u003elast section\u003c/a\u003e, since the datasets are not needed in that case.\u003c/p\u003e\n\u003cp\u003eTo start training your own models, download the \u003ca href=\"https://sigsep.github.io/datasets/musdb.html\" rel=\"nofollow\"\u003efull MUSDB18HQ dataset\u003c/a\u003e and extract it into a folder of your choice. It should have two subfolders: \"test\" and \"train\" as well as a README.md file.\u003c/p\u003e\n\u003cp\u003eYou can of course use your own datasets for training, but for this you would need to modify the code manually, which will not be discussed here. However, we provide a loading function for the normal MUSDB18 dataset as well.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-training-the-models\" class=\"anchor\" href=\"#training-the-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining the models\u003c/h1\u003e\n\u003cp\u003eTo train a Wave-U-Net, the basic command to use is\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython3.6 train.py --dataset_dir /PATH/TO/MUSDB18HQ \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere the path to MUSDB18HQ dataset needs to be specified, which contains the \u003ccode\u003etrain\u003c/code\u003e and \u003ccode\u003etest\u003c/code\u003e subfolders.\u003c/p\u003e\n\u003cp\u003eAdd more command line parameters as needed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e--cuda\u003c/code\u003e to activate GPU usage\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--hdf_dir PATH\u003c/code\u003e to save the preprocessed data (HDF files) to custom location PATH, instead of the default \u003ccode\u003ehdf\u003c/code\u003e subfolder in this repository\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--checkpoint_dir\u003c/code\u003e and \u003ccode\u003e--log_dir\u003c/code\u003e to specify where checkpoint files and logs are saved/loaded\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--load_model checkpoints/model_name/checkpoint_X\u003c/code\u003e to start training with weights given by a certain checkpoint\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor more config options, see \u003ccode\u003etrain.py\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTraining progress can be monitored by using Tensorboard on the respective \u003ccode\u003elog_dir\u003c/code\u003e.\nAfter training, the model is evaluated on the MUSDB18HQ test set, and SDR/SIR/SAR metrics are reported for all instruments and written into both the Tensorboard, and in more detail also into a \u003ccode\u003eresults.pkl\u003c/code\u003e file in the \u003ccode\u003echeckpoint_dir\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content--test-trained-models-on-songs\" class=\"anchor\" href=\"#-test-trained-models-on-songs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-test\"\u003e\u003c/a\u003e Test trained models on songs!\u003c/h1\u003e\n\u003cp\u003eWe provide the default model in a pre-trained form as download so you can separate your own songs right away.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-downloading-our-pretrained-models\" class=\"anchor\" href=\"#downloading-our-pretrained-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownloading our pretrained models\u003c/h2\u003e\n\u003cp\u003eDownload our pretrained model \u003ca href=\"https://www.dropbox.com/s/r374hce896g4xlj/models.7z?dl=1\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\nExtract the archive into the \u003ccode\u003echeckpoints\u003c/code\u003e subfolder in this repository, so that you have one subfolder for each model (e.g. \u003ccode\u003eREPO/checkpoints/waveunet\u003c/code\u003e)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-pretrained-model\" class=\"anchor\" href=\"#run-pretrained-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun pretrained model\u003c/h2\u003e\n\u003cp\u003eTo apply our pretrained model to any of your own songs, simply point to its audio file path using the \u003ccode\u003einput_path\u003c/code\u003e parameter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython3.6 predict.py --load_model checkpoints/waveunet/model --input \"audio_examples/Cristina Vane - So Easy/mix.mp3\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eAdd \u003ccode\u003e--cuda \u003c/code\u003e when using a GPU, it should be much quicker\u003c/li\u003e\n\u003cli\u003ePoint \u003ccode\u003e--input\u003c/code\u003e to the music file you want to separate\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy default, output is written where the input music file is located, using the original file name plus the instrument name as output file name. Use \u003ccode\u003e--output\u003c/code\u003e to customise the output directory.\u003c/p\u003e\n\u003cp\u003eTo run your own model:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePoint \u003ccode\u003e--load_model\u003c/code\u003e to the checkpoint file of the model you are using. If you used non-default hyper-parameters to train your own model, you must specify them here again so the correct model is set up and can receive the weights!\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624842105.0
  },
  {
    "data_format": 2,
    "description": "Prokka: rapid prokaryotic genome annotation.",
    "filenames": [
      "1.14.5/Singularity"
    ],
    "full_name": "pscedu/singularity-prokka",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-two-photon\" class=\"anchor\" href=\"#two-photon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etwo-photon\u003c/h1\u003e\n\u003cp\u003eThis repository contains utilities for analyzing 2p data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#analysis-pipeline\"\u003eAnalysis Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ripping-containers\"\u003eRipping Containers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-analysis-pipeline\" class=\"anchor\" href=\"#analysis-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnalysis Pipeline\u003c/h2\u003e\n\u003cp\u003eThe analysis pipeline consists of the following stages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eraw2tiff: converts Bruker proprietary output format to a TIFF stack\u003c/li\u003e\n\u003cli\u003econvert: converts tiff and csv/text files to hdf5.\u003c/li\u003e\n\u003cli\u003epreprocess: detect and remove stim artefacts\u003c/li\u003e\n\u003cli\u003eqa: make QA plots to check stim artefact removal\u003c/li\u003e\n\u003cli\u003eanalyze: run suite2p, optionally combining multiple preprocessed datasets\u003c/li\u003e\n\u003cli\u003ebackup: back up input/intermediate/output data to a safe place\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h3\u003e\n\u003cp\u003eFirst, install the code. You can use \u003ca href=\"https://desktop.github.com/\"\u003eGitHub desktop\u003c/a\u003e, or use git on the command line. This only has to be done once.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/deisseroth-lab/two-photon.git\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNext, install the environment. You will need to install \u003ca href=\"https://docs.conda.io/en/latest/\" rel=\"nofollow\"\u003econda\u003c/a\u003e first. Then\nuse the following command from within the directory where you installed the repo above. This also only has\nto be done once.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda env create -f environment.yml -n two-photon\nconda activate two-photon\npip install -e \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e installs the 2p script (in editable mode, so you can update the code)\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-executing\" class=\"anchor\" href=\"#executing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecuting\u003c/h3\u003e\n\u003cp\u003eTo run the processing script, the environment needs to be activated. This needs to be done each time you start a\nnew terminal.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda activate two-photon\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe executable is called \u003ccode\u003e2p\u003c/code\u003e, and each stage is a different subcommand\nthat can be run. It is possible to run multiple stages by specifying\nmultiple subcommands.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-data-layout-and-global-flags\" class=\"anchor\" href=\"#data-layout-and-global-flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData Layout and Global Flags\u003c/h4\u003e\n\u003cp\u003eThe scripts required a strict layout of data, and assume the input data\nfollows a directory structure and filenaming that the Bruker scopes\ncreate. The data is setup in subdirectories of a \u003ccode\u003ebase-path\u003c/code\u003e, named\nby the stage and the \u003ccode\u003eacquisition\u003c/code\u003e name.\u003c/p\u003e\n\u003cp\u003eTo point the script to the correct location of of dataset,\nuse the following flags:\u003c/p\u003e\n\u003cpre lang=\"txt\"\u003e\u003ccode\u003e  --base-path PATH    Top-level storage for local data.  [required]\n  --acquisition TEXT  Acquisition sub-directory to process.  [required]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUsing the following global flags (meaning after \u003ccode\u003e2p\u003c/code\u003e but before other commands or flags):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewill use the following locations for the data. Note the expected location of the raw data.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003edata type\u003c/th\u003e\n\u003cth\u003elocation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eRAWDATA, csv, xml, and env files from scope\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/raw/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etiff stacks\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/tiff/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003econverted hdf5 data\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/convert/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epreprocess\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/preprocess/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eqa\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/qa/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eanalyze - suite2p output\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/analyze/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-command-raw2tiff\" class=\"anchor\" href=\"#command-raw2tiff\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: raw2tiff\u003c/h4\u003e\n\u003cp\u003eThe raw2tiff command runs the Bruker software to rip the RAWDATA into a tiff stack.\nThis is a Windows-only command, until the kinks of running on Linux are ironed out.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001\n    raw2tiff\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-convert\" class=\"anchor\" href=\"#command-convert\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: convert\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003econvert\u003c/code\u003e command converts the tiff stacks and voltage data to hdf5.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001 \\\n    convert --channel 3\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-preprocess\" class=\"anchor\" href=\"#command-preprocess\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: preprocess\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003epreprocess\u003c/code\u003e command performs processing like stim removal on the data. It should be\nrun even if there are no stim artefacts (in which case, no actual computation is done),\nso that downstream stages find the data in the correct place.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001 \\\n    preprocess --frame-channel-name=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eframe starts\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --stim-channel-name=respir\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample based on piezeo period:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/minoue2/2p_CNC/ \\\n    --acquisition Chris_210429/10263_920nm_PC250-300-001  \\\n    preprocess \\\n    --frame-channel-name=StartFrameResonant \\\n    --stim-channel-name=LEDSyncSignal \\\n    --piezo-period-frames=7 \\\n    --piezo-skip-frames=3\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-qa\" class=\"anchor\" href=\"#command-qa\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: qa\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eqa\u003c/code\u003e command makes some QA plots to understand if the stim effects are\nbeing correctly removed during preprocessing. It plots a number of frames\n(given by --max-frames) containing stims, showing the data before and after\nstim removal.\u003c/p\u003e\n\u003cp\u003eThis is an optional step.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001 \\\n    qa\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-analyze\" class=\"anchor\" href=\"#command-analyze\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: analyze\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eanalyze\u003c/code\u003e command runs Suite2p on the preprocessed dataset.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    analyze\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample of analyzing multiple acquisitions together:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    analyze --extra-acquisitions 20210428M198/slm-000\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample of using non-default Suite2p options file (json format):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    analyze --suite2p-params-file two_photon/ops_files/drinnedb.json\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-backup\" class=\"anchor\" href=\"#command-backup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: backup\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ebackup\u003c/code\u003e command copies the output of one or more stages to backup directory.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    backup \\\n    --backup-path /media/hdd1/oak/mount/two-photon/backup \\\n    --backup-stage raw,tiff\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e--backup_path\"\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-multiple-commands-at-once\" class=\"anchor\" href=\"#using-multiple-commands-at-once\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing multiple commands at once\u003c/h2\u003e\n\u003cp\u003eSeveral commands can be run in succession by adding each one to your command line with its\nnecessary flags.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    raw2tiff \\\n    convert --channel 3 \\\n    preprocess --stim-channel-name=respir \\\n    analyze --extra-acquisitions 20210428M198/slm-000\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ripping-containers\" class=\"anchor\" href=\"#ripping-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRipping Containers\u003c/h2\u003e\n\u003cp\u003eRipping is the process for converting a Bruker RAWDATA file into a set of TIFF files.\u003c/p\u003e\n\u003cp\u003eContainers exist to help run the ripping on any platform, but it has been found they\nperform sub-optimally and are 10-100x slower than ripping on a Windows machine using\nthe native ripper. It is advised NOT to use this yet.\u003c/p\u003e\n\u003cp\u003eThe lab has created \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e and\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e containers with the Bruker Prairie View software,\nwhich can be used to rip raw data computers with either set of container software installed.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ripping-via-singularity\" class=\"anchor\" href=\"#ripping-via-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRipping via Singularity\u003c/h3\u003e\n\u003cp\u003eIf you would like to run from a container on \u003ca href=\"https://www.sherlock.stanford.edu/\" rel=\"nofollow\"\u003eSherlock\u003c/a\u003e,\nthe lab keeps a copy available in $OAK/pipeline/bruker-rip/containers.\u003c/p\u003e\n\u003cp\u003eHere\u0027s a quick demo:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ mkdir -p \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test\n$ cp -r \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/pipeline/bruker-rip/sampledata/overview-023 \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test\n$ chmod -R u+w \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test/overview-023  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Write permissions needed to convert files.\u003c/span\u003e\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test/overview-023\n$ singularity run --bind=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:/data \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/pipeline/bruker-rip/containers/bruker-rip.sif\n\nCopying wine environment.\n\nExecuting rip. One err and four fixme statements are OK.\n\n2020-11-16 17:25:43.859 rip:50 INFO Data created with Prairie version 5.4, using ripper: /apps/Prairie View 5.5/Utilities/Image-Block Ripping Utility.exe\n2020-11-16 17:25:43.861 rip:77 INFO Ripping from:\n /data/Cycle00001_Filelist.txt\n /data/CYCLE_000001_RAWDATA_000025\n2020-11-16 17:25:43.883 rip:123 INFO Watching \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e 3600 more seconds\n000d:err:menubuilder:init_xdg error looking up the desktop directory\n0031:fixme:ntdll:EtwEventRegister ({5eec90ab-c022-44b2-a5dd-fd716a222a15}, 0x5571000, 0x5582030, 0x5582050) stub.\n0031:fixme:ntdll:EtwEventSetInformation (deadbeef, 2, 0x557fd70, 43) stub\n0031:fixme:nls:GetThreadPreferredUILanguages 00000038, 0x4fccdb4, 0x4fccdd0 0x4fccdb0\n0031:fixme:nls:get_dummy_preferred_ui_language (0x38 0x4fccdb4 0x4fccdd0 0x4fccdb0) returning a dummy value (current locale)\n2020-11-16 17:25:53.889 rip:134 INFO   Found filelist files: None\n2020-11-16 17:25:53.889 rip:135 INFO   Found rawdata files: None\n2020-11-16 17:25:53.889 rip:136 INFO   Found this many tiff files: 1\n2020-11-16 17:25:53.889 rip:123 INFO Watching \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e 3590 more seconds\n2020-11-16 17:26:03.899 rip:134 INFO   Found filelist files: None\n2020-11-16 17:26:03.899 rip:135 INFO   Found rawdata files: None\n2020-11-16 17:26:03.899 rip:136 INFO   Found this many tiff files: 1\n2020-11-16 17:26:03.899 rip:139 INFO Detected ripping is \u003cspan class=\"pl-c1\"\u003ecomplete\u003c/span\u003e\n2020-11-16 17:26:13.909 rip:141 INFO Killing ripper\n2020-11-16 17:26:13.910 rip:143 INFO Ripper has been killed\n2020-11-16 17:26:14.912 rip:115 INFO cleaned up\u003cspan class=\"pl-k\"\u003e!\u003c/span\u003e\nX connection to :99 broken (explicit \u003cspan class=\"pl-c1\"\u003ekill\u003c/span\u003e or server shutdown).\nX connection to :99 broken (explicit \u003cspan class=\"pl-c1\"\u003ekill\u003c/span\u003e or server shutdown).\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHere\u0027s how to run on your own data. We request a node allocation using \u003ccode\u003esdev\u003c/code\u003e as\nlong-running jobs should not use login nodes.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e my/data/path\n$ sdev  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e May take some time to get a machine for development use\u003c/span\u003e\n$ singularity run --bind=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:/data \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/pipeline/bruker-rip/containers/bruker-rip.sif\n\n[Similar output as above]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd here\u0027s how to run a batch job, using the \u003ccode\u003erip.sbatch\u003c/code\u003e script from this\nrepository.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e my/data/path\n$ sbatch path/to/two-photon/rip.sbatch \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\nSubmitted batch job ABCDEFGH\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ripping-via-docker\" class=\"anchor\" href=\"#ripping-via-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRipping via Docker\u003c/h3\u003e\n\u003cp\u003eYou can run on a device with Docker installed using the command below. The image\nwill be available locally if you\u0027ve build from source (see below), or it will be\nfetched from the the \u003ca href=\"https://code.stanford.edu/deisseroth-lab/bruker-rip\" rel=\"nofollow\"\u003eStanford GitLab\u003c/a\u003e. Contact \u003ca href=\"mailto:croat@stanford.edu\"\u003ecroat@stanford.edu\u003c/a\u003e if you need access.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ./rip_docker.sh \\\n    scr.svc.stanford.edu/deisseroth-lab/bruker-rip:20200903 \\\n    /path/to/data/with/filelist/and/rawdata/\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ./rip_docker.sh \\\n    scr.svc.stanford.edu/deisseroth-lab/bruker-rip:20200903 \\\n    /media/hdd0/two-photon/sample/overview-023\nSetting up wine environment\n\nExecuting rip.  It is OK to see 1 err and 4 fixme statements \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e what follows\n\n2020-09-03 14:41:33.936 rip:50 INFO Ripping from:\n /data/Cycle00001_Filelist.txt\n /data/CYCLE_000001_RAWDATA_000025\n2020-09-03 14:41:33.940 rip:96 INFO Waiting \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish: 3600 seconds remaining\n000d:err:menubuilder:init_xdg error looking up the desktop directory\n0031:fixme:ntdll:EtwEventRegister ({5eec90ab-c022-44b2-a5dd-fd716a222a15}, 0xd441000, 0xd452030, 0xd452050) stub.\n0031:fixme:ntdll:EtwEventSetInformation (deadbeef, 2, 0xd44fd70, 43) stub\n0031:fixme:nls:GetThreadPreferredUILanguages 00000038, 0xdaacdb4, 0xdaacdd0 0xdaacdb0\n0031:fixme:nls:get_dummy_preferred_ui_language (0x38 0xdaacdb4 0xdaacdd0 0xdaacdb0) returning a dummy value (current locale)\n2020-09-03 14:41:43.951 rip:107 INFO   Found filelist files: None\n2020-09-03 14:41:43.951 rip:108 INFO   Found rawdata files: None\n2020-09-03 14:41:43.951 rip:109 INFO   Found this many tiff files: 1\n2020-09-03 14:41:43.951 rip:96 INFO Waiting \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish: 3590 seconds remaining\n2020-09-03 14:41:53.962 rip:107 INFO   Found filelist files: None\n2020-09-03 14:41:53.962 rip:108 INFO   Found rawdata files: None\n2020-09-03 14:41:53.962 rip:109 INFO   Found this many tiff files: 1\n2020-09-03 14:41:53.963 rip:112 INFO Detected ripping is \u003cspan class=\"pl-c1\"\u003ecomplete\u003c/span\u003e\n2020-09-03 14:42:03.973 rip:114 INFO Killing ripper\n2020-09-03 14:42:03.973 rip:116 INFO Ripper has been killed\n2020-09-03 14:42:04.975 rip:88 INFO cleaned up\u003cspan class=\"pl-k\"\u003e!\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-containers\" class=\"anchor\" href=\"#building-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Containers\u003c/h3\u003e\n\u003cp\u003eTo build all available containers, which will first build the Docker container, and then convert it\nto a Singularity container:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake build\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo build just the docker containers:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake build_docker\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eView the \u003ca href=\"Makefile\"\u003eMakefile\u003c/a\u003e for additional targets, including targets to build just build specific containers.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1624982164.0
  },
  {
    "data_format": 2,
    "description": "Docker image",
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "AdamWilsonLab/docker_geospatial_plus",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-geospatial-plus\" class=\"anchor\" href=\"#geospatial-plus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeospatial Plus\u003c/h1\u003e\n\u003cp\u003eBuilding on the versioned geospatial Rocker image.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-github-actions\" class=\"anchor\" href=\"#github-actions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGithub Actions\u003c/h1\u003e\n\u003cp\u003eThis repository uses GitHub Actions to test the docker image prior to making it available as a GitHub package.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624971946.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/mniconn",
    "latest_release": "v3.3.0-beta2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mniconn\" class=\"anchor\" href=\"#mniconn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emniconn\u003c/h1\u003e\n\u003cp\u003eComputes functional connectivity maps and matrices for a specified set of ROIs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ewremovegm_niigz\u003c/code\u003e, \u003ccode\u003ewkeepgm_niigz\u003c/code\u003e, \u003ccode\u003ewmeanfmri_niigz\u003c/code\u003e. Preprocessed fMRI data from \u003ca href=\"https://github.com/baxpr/connprep\"\u003econnprep\u003c/a\u003e. This may be supplied in atlas space or subject native space, as long as the ROI image is in the same space.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ewroi_niigz\u003c/code\u003e.  ROI image. This may be an image existing within the container (e.g. the MNI space \u0027AABHHIP_LR.nii.gz\u0027). Or, it may be any supplied image. In the latter case, \u003ccode\u003ewroilabel_csv\u003c/code\u003e must also be supplied; this file must contain Label and Region columns, or may be the STATS output of a slant assessor.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ewt1_niigz\u003c/code\u003e. T1 image for the PDF report.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eResample the ROI image to match the fMRI. It\u0027s assumed both are already aligned and in the same space as the ROI image.\u003c/li\u003e\n\u003cli\u003eExtract mean time series from the supplied fMRI for each ROI in the ROI image.\u003c/li\u003e\n\u003cli\u003eCompute functional connectivity: \u003ccode\u003eR\u003c/code\u003e, the correlation coefficient; and \u003ccode\u003eZ\u003c/code\u003e, the Fisher transformed correlation, \u003ccode\u003eatanh(R) * sqrt(N-3)\u003c/code\u003e where \u003ccode\u003eN\u003c/code\u003e is number of time points. The ROI-to_ROI matrix is computed, and also voxelwise connectivity maps.\u003c/li\u003e\n\u003cli\u003eGenerate a PDF report and organize outputs for XNAT.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1625437966.0
  },
  {
    "data_format": 2,
    "description": "RAxML - Randomized Axelerated Maximum Likelihood.",
    "filenames": [
      "8.2.9/Singularity"
    ],
    "full_name": "pscedu/singularity-raxml",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h1\u003e\n\u003cp\u003eThis repository contains tools for building Singularity and Charliecloud containers.\u003c/p\u003e\n\u003cp\u003eFurthermore, since Intel Docker containers cannot be distributed through Docker Hub, they are also handled here.\u003c/p\u003e\n\u003cp\u003eThe instructions below are intended for the JEDI core team, who are responsible for maintaining JEDI containers and distributing them publicly or privately.\u003c/p\u003e\n\u003cp\u003eHowever, since the JEDI core team cannot legally distribute intel containers for licensing reasons, JEDI users and developers are encouraged to build their own intel development container.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"myIntel/Intel.md\"\u003eSee here for instructions on how to build your own JEDI Intel development container: Docker, Singularity, or Charliecloud\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-organization-of-repository\" class=\"anchor\" href=\"#organization-of-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOrganization of Repository\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003etop-level directory: tools for building Singularity, Docker, and Charliecloud containers\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evagrant\u003c/code\u003e: tools for building Vagrant virtual machines that are provisioned to run JEDI containers\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emodulefiles\u003c/code\u003e, \u003ccode\u003erunscripts\u003c/code\u003e: These directories contain sample modulefiles and batch scripts for running JEDI \"Supercontainers\" across nodes on HPC systems\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emyIntel\u003c/code\u003e is intended to help users from the general JEDI community build their own JEDI intel development containers.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eintel19\u003c/code\u003e contains deprecated build tools for intel Parallel Studio.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eexamples\u003c/code\u003e is a sandbox, containing instructive examples of how to implement features that may not be used now but might be used in the future.  An example is how to build writable singularity containers.   These scripts are not maintained; there is no guarantee that they will run as is.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eIn order to build Docker, Singularity, or Charliecloud containers, you will of course need to have the appropriate software installed, namely \u003ccode\u003edocker\u003c/code\u003e, \u003ccode\u003esingularity\u003c/code\u003e, or \u003ccode\u003echarliecloud\u003c/code\u003e.  Members of the JEDI core team can launch an AWS node with all of these pre-installed.  Or, you can install them yourself as described in the JEDI documentation.\u003c/p\u003e\n\u003cp\u003eThe scripts in this directory also assume that you have root privileges.\u003c/p\u003e\n\u003cp\u003eAlso, core developers often find it necessary to access feature or bugfix branches of the jedi stack for testing purposes.  So, the \u003ccode\u003ebuild_container.sh\u003c/code\u003e script uses the JCSDA-internal (private) jedi-stack repo.  For this reason, you need to provide an ssh key for access.  This script uses a generic academy ssh key to ensure that it has read-only access to selected JCSDA repositories.  If you do not have access to this key, you can replace it with another by changing the \u003ccode\u003eKEY\u003c/code\u003e variable in \u003ccode\u003ebuild_containers.sh\u003c/code\u003e.  But it is recommended to retain the read-only access.  You can build the \u003ccode\u003emyIntel\u003c/code\u003e container without an ssh key.\u003c/p\u003e\n\u003cp\u003eNote: to build the tutorial container you have to copy the ssh key into the directory \u003ccode\u003essh-key\u003c/code\u003e and modify the singularity recipe file accordingly if it has a different name.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-a-dev-container\" class=\"anchor\" href=\"#build-a-dev-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild a dev container\u003c/h2\u003e\n\u003cp\u003eTo build a Singularity, Charliecloud, and/or a Docker container, enter this and respond to the prompts to build the containers of your choice.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./build_containers.sh \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhere \u003ccode\u003e\u0026lt;name\u0026gt;\u003c/code\u003e matches one of the available Dockerfile extensions, e.g. \u003ccode\u003egnu-openmpi-dev\u003c/code\u003e.  It also accepts an optional second argument to specify a tag.  The default tag is \u003ccode\u003ebeta\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor the the gnu and clang containers, the Singularity containers are built directly from the images on Docker Hub.  A Docker container will only be created if you choose to build a Charliecloud container, which is then built from the Docker container.\u003c/p\u003e\n\u003cp\u003eFor the \u003ccode\u003eintel-impi-dev\u003c/code\u003e container, a Docker file is always created and then the Singularity and Charliecloud containers are created from that.\u003c/p\u003e\n\u003cp\u003eThe intel Docker container is the one used for CI so it is kept relatively compact.  If you wish to add additional components such as Vtune, it is recommended you use the companion scripts in the \u003ccode\u003emyIntel\u003c/code\u003e directory.  These scripts are simplified in the root directory but they are intended for use by the general JEDI user and developer community.  The main simplification is that there is no need to supply an ssh key because those scripts only access the public jedi-stack repo.\u003c/p\u003e\n\u003cp\u003eTo build the tutorial container, just specify \u003ccode\u003etutorial\u003c/code\u003e for the \u003ccode\u003e\u0026lt;name\u0026gt;\u003c/code\u003e.  The tutorial container is exclusively a Singularity container and uses GNU-OpenMPI: There are no clang or intel options and there are no Docker or Charliecloud containers created.\u003c/p\u003e\n\u003cp\u003eThe Singularity and Charliecloud container files will be placed in a subdirectory called \u003ccode\u003econtainers\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eNote: building the Mellanox-enabled HPC container isn\u0027t yet automated.  For this or other non-standard cases, you can edit the Dockerfiles, Singularity files, and scripts manually as needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-test-the-container\" class=\"anchor\" href=\"#test-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest the container\u003c/h2\u003e\n\u003cp\u003eBefore distributing a container, it\u0027s always important to test it.  A good test is usually to enter the container and then build and test fv3-bundle.\u003c/p\u003e\n\u003cp\u003eTo enter the Singularity container, enter:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity shell -e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd, for CharlieCloud, you can do this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir -p \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/ch-jedi\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/ch-jedi\nch-tar2dir \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003epath-to-tarfile\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/ch-jedi-gnu-openmpi-dev.tar.gz \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\nch-run ch-jedi-gnu-openmpi-dev -- bash\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-distribute-the-latest-container\" class=\"anchor\" href=\"#distribute-the-latest-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDistribute the latest container\u003c/h2\u003e\n\u003cp\u003eThe latest Singularity containers are made available on Sylabs Cloud, the latest Charliecloud containers are made available on a public AWS S3 bucket, and the latest intel containers are made available on a private AWS S3 bucket.  The purpose of the \u003ccode\u003epush_containers.sh\u003c/code\u003e script is to push the new container to these distribution sites.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ebeta\u003c/code\u003e tag is a special case.  If the tag is \u003ccode\u003ebeta\u003c/code\u003e, it is assumed that, after it passes tests, this container is ready to be deployed as \u003ccode\u003elatest\u003c/code\u003e.  In this case, a copy of the current \u003ccode\u003elatest\u003c/code\u003e container is saved with the tag \u003ccode\u003erevert\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSo, the typical workflow would be to enter\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./push_containers.sh \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAs with \u003ccode\u003ebuild_containers.sh\u003c/code\u003e, \u003ccode\u003epush_containers.sh\u003c/code\u003e accepts an optional second argument which is a tag.  This is sometimes useful for experimental cases but is not part of the normal workflow.\u003c/p\u003e\n\u003cp\u003eFor instructions on how to download these containers, see \u003ca href=\"https://jointcenterforsatellitedataassimilation-jedi-docs.readthedocs-hosted.com/en/latest/using/jedi_environment/containers.html#available-containers\" rel=\"nofollow\"\u003ethe JEDI Documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tagged-releases\" class=\"anchor\" href=\"#tagged-releases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTagged Releases\u003c/h2\u003e\n\u003cp\u003eMost developers use the latest development containers but it\u0027s also useful to have tagged containers that accompany JEDI releases.  This is particularly relevant for scientific users (as opposed to developers) who may wish to use tagged releases and containers for reproducibility in research.  Tagged containers can also be used to provide stability for operational or Near-Real-Time (NRT) workflows.\u003c/p\u003e\n\u003cp\u003eSylabs cloud has a storage quota (currently 11 GB) that would be quickly overwhelmed if we were to store many release containers there.  So, this is reserved for \"latest\" and \"revert\".\u003c/p\u003e\n\u003cp\u003eTagged singularity containers are distributed on the \u003ca href=\"http://data.jcsda.org/pages/containers.html\" rel=\"nofollow\"\u003eJCSDA Public Container Repository\u003c/a\u003e along with the latest and tagged Charliecloud containers.  Tagged docker release containers can be obtained from Docker Hub.  For example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker pull jcsda/docker-gnu-openmpi-dev:v1.0.0\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1624981389.0
  },
  {
    "data_format": 2,
    "description": "Custom implementation of neurodocker (https://github.com/ReproNim/neurodocker)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "achennings/neurodocker",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-neurodocker\" class=\"anchor\" href=\"#neurodocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eneurodocker\u003c/h1\u003e\n\u003cp\u003eCustom implementation of neurodocker (\u003ca href=\"https://github.com/ReproNim/neurodocker\"\u003ehttps://github.com/ReproNim/neurodocker\u003c/a\u003e)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625007486.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "Clinical-Genomics-Lund/SomaticPanelPipeline",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-singularity-veins\" class=\"anchor\" href=\"#singularity-veins\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-veins\u003c/h2\u003e\n\u003cp\u003eScripts for building a \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container for quickly building and running Veins simulations anywhere.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e 3.5.2\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://wiki.debian.org/Debootstrap\" rel=\"nofollow\"\u003eDebootstrap\u003c/a\u003e 1.0.114\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDebian Buster\u003c/li\u003e\n\u003cli\u003eVeins 5.0\u003c/li\u003e\n\u003cli\u003eOMNeT++ 5.6\u003c/li\u003e\n\u003cli\u003eSUMO 1.4.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003ePATH=$PATH:/usr/sbin singularity build --fakeroot singularity-veins.sif singularity-veins.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-help\" class=\"anchor\" href=\"#help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHelp\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help singularity-veins.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-buildingrunning-simulations\" class=\"anchor\" href=\"#buildingrunning-simulations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding/running simulations\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p work/src\ncd work/src\ngit clone --branch veins-5.0 https://github.com/sommer/veins veins\nsudo singularity run -H work:/work -C singularity-veins.sif --chdir src/veins -- ./configure\nsudo singularity run -H work:/work -C singularity-veins.sif --chdir src/veins -- make\nsudo singularity run -H work:/work -C singularity-veins.sif --chdir src/veins/examples/veins --launchd -- ./run -u Cmdenv\nhead work/src/veins/examples/veins/results/General-\\#0.sca\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eVeins is composed of many parts. See the version control log for a full list of\ncontributors and modifications. Each part is protected by its own, individual\ncopyright(s), but can be redistributed and/or modified under an open source\nlicense. License terms are available at the top of each file. Parts that do not\nexplicitly include license text shall be assumed to be governed by the \"GNU\nGeneral Public License\" as published by the Free Software Foundation -- either\nversion 2 of the License, or (at your option) any later version\n(SPDX-License-Identifier: GPL-2.0-or-later). Parts that are not source code and\ndo not include license text shall be assumed to allow the Creative Commons\n\"Attribution-ShareAlike 4.0 International License\" as an additional option\n(SPDX-License-Identifier: GPL-2.0-or-later OR CC-BY-SA-4.0). Full license texts\nare available with the source distribution.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1625292950.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "mherkazandjian/ismcpak",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/_container/5f9bd736bccfe9cf4578f166\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h1\u003e\n\u003cp\u003eTo run a quick example, the following container can be used:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ git clone -b alpha-master https://github.com/mherkazandjian/ismcpak.git ~/ismcpak\n$ cd ~/ismcpak/tests\n$ singularity exec library://mher/default/ismcpak:latest mpiexec python run_singleMesh.py \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is a package which implements some utilities useful for modelling and\nanalyzing simulation output of PDRs.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ejupyter notebooks\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo run a jupyter server inside the container with the full ismcpak environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec --scratch /run/user library://mher/default/ismcpak:latest jupyter-lab\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-the-container-locally\" class=\"anchor\" href=\"#build-the-container-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild the container locally\u003c/h2\u003e\n\u003cp\u003eThe following command build the singularity container on a local machine. The\nonly prerequisite is to have singularity installed and to have sudo access.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ git clone -b alpha-master https://github.com/mherkazandjian/ismcpak.git ~/ismcpak\n$ cd ~/ismcpak\n$ sudo make singularity\n$ cd tests\n$ singularity exec ../container.sif mpiexec python run_singleMesh.py \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h1\u003e\n\u003cp\u003eamuse  - mpich\nPyQt4\nipython\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installing-the-pdr-code\" class=\"anchor\" href=\"#installing-the-pdr-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the PDR code\u003c/h1\u003e\n\u003cp\u003eThe PDR code should be copied into:\namuse/src/amuse/community/pdr\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-compiling-the-pdr-code\" class=\"anchor\" href=\"#compiling-the-pdr-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompiling the PDR code\u003c/h1\u003e\n\u003cp\u003eThe PDR code can be compiled using:\n~\u0026gt; cd amuse/src/amuse/community/pdr\n~\u0026gt; make all\nThe generates the libpdr.a library\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setting-up-the-working-environment\" class=\"anchor\" href=\"#setting-up-the-working-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetting up the working environment\u003c/h1\u003e\n\u003cp\u003eThe path to ismcpak should be added to the PYTHONPATH environment variable. For\nbash, the following line should be added:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport PYTHONPATH=/PATH/TO/ismcpak:$PYTHONPATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto tcsh :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esetenv PYTHONPATH /PATH/TO/ismcpak:$PYTHONPATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-the-code\" class=\"anchor\" href=\"#running-the-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the code\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-basic-test---single-model\" class=\"anchor\" href=\"#basic-test---single-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBasic test - single model\u003c/h2\u003e\n\u003cp\u003eThe PDR code can only be run through the AMUSE ( \u003ca href=\"http://amusecode.org\" rel=\"nofollow\"\u003ehttp://amusecode.org\u003c/a\u003e ).\nDepending on the mpi environment installed with AMUSE, it might be\nnecessary to launch the mpd deamon before executing either:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e~\u0026gt; mpirun -np 1 python run_singleMesh.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor via\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e~\u0026gt; python run_singleMesh.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-a-grid-of-models\" class=\"anchor\" href=\"#running-a-grid-of-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning a Grid of models\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-the-working-environment-variables\" class=\"anchor\" href=\"#setup-the-working-environment-variables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esetup the working environment variables\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003esource setdev\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install-the-pdr-code-into-amuse-make-sure-the-correct\" class=\"anchor\" href=\"#install-the-pdr-code-into-amuse-make-sure-the-correct\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003einstall the pdr code into amuse (make sure the correct\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-path-of-amuse-is-set-in-setenv\" class=\"anchor\" href=\"#path-of-amuse-is-set-in-setenv\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epath of amuse is set in setenv\u003c/h1\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003emake pdr_install\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-after-these-two-steps-the-tests\" class=\"anchor\" href=\"#after-these-two-steps-the-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eafter these two steps, the tests\u003c/h1\u003e\n\u003cp\u003erun_singleMesh.py\nchemical_network_pdr_code.py\nshould run without errors\u003c/p\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003e\n\u003cp\u003eto run a grid, use the following under ismcpak:\n~\u0026gt;  ipython --pylab=qt tests/run_oneSidedGrid.py\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eafter the model data is written to\ntests/oneSidedGrid/meshes\nwe need to construct the database files .db using constructReadArchive.py\n~\u0026gt; ipython --pylab=qt constructReadArchive.py\u003c/p\u003e\n\u003cp\u003eafter the database is constructed we must have the file\nmeshes.db  meshes.db.info\nin the output directory and a message\narchive integrity test passed\nmust be displayed\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eafter creating the database, a reference file must be generated which\nstores information about the parameters which have been used in\ngenerating the data. A template of this file is located under\nruns/tests/templateDir/used_params.py\nwhere the parameters used by run_oneSidedGrid.py should be filled in\nby hand. Once the values are changed :\n~\u0026gt; python used_parms.py\ngenerates the pickle file\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eset the desired display parameters in analyzeArchive.py and invoke :\n~\u0026gt; ipython --pylab=qt analyzeArchive.py\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eto generate the radex databases, the bottom part of analyzeArchive.py should be enabled to\nallow radex databases to be computed and written do disk. Set the desired values of\nAv to compute and the species whose emission will be computed and re-run:\n~\u0026gt; ipython --pylab=qt analyzeArchive.py\nAs a check, the data in\ntests/oneSidedGrid/radexDbs\nshould have directories with the Avs we have set and each directory should\nhave files for each species we have specified.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eafter producing the radex database files, we can convert that data to ascii data using :\n~\u0026gt; ipython ismcpak2Ascii.py\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDisclaimer\u003c/h1\u003e\n\u003cp\u003eTHIS SOFTWARE IS PROVIDED UNDER THE GPL LICENSE BY THE COPYRIGHT HOLDERS AND\nCONTRIBUTORS \u201cAS IS\u201d AND DOES NOT EXPRESS OR PROVIDE IMPLIED WARRANTIES,\nINCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND F\nITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT\nOF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT\n, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY\nWAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\u003c/p\u003e\n\u003cp\u003eSee LICENSE.txt for more information about the GPL license.\u003c/p\u003e\n\u003cp\u003ePlease cite the following papers if any part of this package is used in your\nresearch.\u003c/p\u003e\n\u003cp\u003eKazandjian, M. V., Meijerink, R., Pelupessy, I., Israel, F. P., Spaans, M.,\narXiv:1403.7000\u003c/p\u003e\n\u003cp\u003eKazandjian, M. V., Meijerink, R., Pelupessy, I., Israel, F. P., Spaans, M.,\n2012, A\u0026amp;A, 542, A65, 26\u003c/p\u003e\n\u003cp\u003eMeijerink, R., Spaans, M., \u0026amp; Israel, F. P. 2007, A\u0026amp;A, 461, 793\u003c/p\u003e\n\u003cp\u003eMeijerink, R. \u0026amp; Spaans, M. 2005, A\u0026amp;A, 436, 397\u003c/p\u003e\n\u003cp\u003eIsmpak makes makes use of \"Radex\" internally to compute the line emissions. Please\nreference the RADEX paper as well:\u003c/p\u003e\n\u003cp\u003eVan der Tak, F.F.S., Black, J.H., Sch\u00f6ier, F.L., Jansen, D.J., van Dishoeck, E.F. 2007, A\u0026amp;A 468, 627\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625261030.0
  },
  {
    "data_format": 2,
    "description": "A place to keep my Singularity recipes",
    "filenames": [
      "Singularity.gapfiller",
      "Singularity.igv",
      "Singularity.stacks",
      "Singularity.ipyrad",
      "Singularity.unicycler",
      "Singularity.quast",
      "Singularity.trinity",
      "Singularity.faststructure",
      "Singularity.kat"
    ],
    "full_name": "bmichanderson/singularity-containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-containers\" class=\"anchor\" href=\"#singularity-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-containers\u003c/h1\u003e\n\u003cp\u003eA place to keep my Singularity recipes.\nThis repository contains recipes in the format \"Singularity.[program]\" and is linked to Singularity Hub so that all commits trigger builds there.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1625130034.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "misc/releases/19.12/Singularity.19.12",
      "misc/releases/20.06/Singularity.20.06",
      "misc/releases/19.06/Singularity.19.06",
      "misc/releases/latest/Singularity"
    ],
    "full_name": "salome-eriksson/downward-issue751-prototype",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast Downward\u003c/h1\u003e\n\u003cp\u003eFast Downward is a domain-independent classical planning system.\u003c/p\u003e\n\u003cp\u003eCopyright 2003-2020 Fast Downward contributors (see below).\u003c/p\u003e\n\u003cp\u003eFor further information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Downward website: \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReport a bug or file an issue: \u003ca href=\"http://issues.fast-downward.org\" rel=\"nofollow\"\u003ehttp://issues.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward mailing list: \u003ca href=\"https://groups.google.com/forum/#!forum/fast-downward\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/fast-downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward main repository: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tested-software-versions\" class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTested software versions\u003c/h2\u003e\n\u003cp\u003eThis version of Fast Downward has been tested with the following software versions:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOS\u003c/th\u003e\n\u003cth\u003ePython\u003c/th\u003e\n\u003cth\u003eC++ compiler\u003c/th\u003e\n\u003cth\u003eCMake\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 20.04\u003c/td\u003e\n\u003ctd\u003e3.8\u003c/td\u003e\n\u003ctd\u003eGCC 9, GCC 10, Clang 10, Clang 11\u003c/td\u003e\n\u003ctd\u003e3.16\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 18.04\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eGCC 7, Clang 6\u003c/td\u003e\n\u003ctd\u003e3.10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emacOS 10.15\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eAppleClang 12\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWindows 10\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eVisual Studio Enterprise 2017 (MSVC 19.16) and 2019 (MSVC 19.28)\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu, we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and on macOS, we do not test LP solvers (yet).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003eThe following list includes all people that actively contributed to\nFast Downward, i.e. all people that appear in some commits in Fast\nDownward\u0027s history (see below for a history on how Fast Downward\nemerged) or people that influenced the development of such commits.\nCurrently, this list is sorted by the last year the person has been\nactive, and in case of ties, by the earliest year the person started\ncontributing, and finally by last name.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2003-2020 Malte Helmert\u003c/li\u003e\n\u003cli\u003e2008-2016, 2018-2020 Gabriele Roeger\u003c/li\u003e\n\u003cli\u003e2010-2020 Jendrik Seipp\u003c/li\u003e\n\u003cli\u003e2010-2011, 2013-2020 Silvan Sievers\u003c/li\u003e\n\u003cli\u003e2012-2020 Florian Pommerening\u003c/li\u003e\n\u003cli\u003e2013, 2015-2020 Salome Eriksson\u003c/li\u003e\n\u003cli\u003e2016-2020 Cedric Geissmann\u003c/li\u003e\n\u003cli\u003e2017-2020 Guillem Franc\u00e8s\u003c/li\u003e\n\u003cli\u003e2018-2020 Augusto B. Corr\u00eaa\u003c/li\u003e\n\u003cli\u003e2018-2020 Patrick Ferber\u003c/li\u003e\n\u003cli\u003e2015-2019 Manuel Heusner\u003c/li\u003e\n\u003cli\u003e2017 Daniel Killenberger\u003c/li\u003e\n\u003cli\u003e2016 Yusra Alkhazraji\u003c/li\u003e\n\u003cli\u003e2016 Martin Wehrle\u003c/li\u003e\n\u003cli\u003e2014-2015 Patrick von Reth\u003c/li\u003e\n\u003cli\u003e2015 Thomas Keller\u003c/li\u003e\n\u003cli\u003e2009-2014 Erez Karpas\u003c/li\u003e\n\u003cli\u003e2014 Robert P. Goldman\u003c/li\u003e\n\u003cli\u003e2010-2012 Andrew Coles\u003c/li\u003e\n\u003cli\u003e2010, 2012 Patrik Haslum\u003c/li\u003e\n\u003cli\u003e2003-2011 Silvia Richter\u003c/li\u003e\n\u003cli\u003e2009-2011 Emil Keyder\u003c/li\u003e\n\u003cli\u003e2010-2011 Moritz Gronbach\u003c/li\u003e\n\u003cli\u003e2010-2011 Manuela Ortlieb\u003c/li\u003e\n\u003cli\u003e2011 Vidal Alc\u00e1zar Saiz\u003c/li\u003e\n\u003cli\u003e2011 Michael Katz\u003c/li\u003e\n\u003cli\u003e2011 Raz Nissim\u003c/li\u003e\n\u003cli\u003e2010 Moritz Goebelbecker\u003c/li\u003e\n\u003cli\u003e2007-2009 Matthias Westphal\u003c/li\u003e\n\u003cli\u003e2009 Christian Muise\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cp\u003eThe current version of Fast Downward is the merger of three different\nprojects:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe original version of Fast Downward developed by Malte Helmert\nand Silvia Richter\u003c/li\u003e\n\u003cli\u003eLAMA, developed by Silvia Richter and Matthias Westphal based on\nthe original Fast Downward\u003c/li\u003e\n\u003cli\u003eFD-Tech, a modified version of Fast Downward developed by Erez\nKarpas and Michael Katz based on the original code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition to these three main sources, the codebase incorporates\ncode and features from numerous branches of the Fast Downward codebase\ndeveloped for various research papers. The main contributors to these\nbranches are Malte Helmert, Gabi R\u00f6ger and Silvia Richter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe following directory is not part of Fast Downward as covered by\nthis license:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or (at\nyour option) any later version.\n\nFast Downward is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see \u0026lt;https://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625214736.0
  },
  {
    "data_format": 2,
    "description": "Docker recipe for building Interproscan",
    "filenames": [
      "Singularity",
      "Singularity.open"
    ],
    "full_name": "biocorecrg/interproscan_docker",
    "latest_release": "5.48-83.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-interproscan_docker\" class=\"anchor\" href=\"#interproscan_docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003einterproscan_docker\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://zenodo.org/badge/latestdoi/150708687\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5867fa2b54b675356b6c4b17144ce558f6902bee46de35012c7bdafc38d90f88/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3135303730383638372e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/150708687.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eContainer recipes for building \u003ca href=\"https://interproscan-docs.readthedocs.io\" rel=\"nofollow\"\u003eInterproscan\u003c/a\u003e. Both \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e and \u003ca href=\"https://singularity.hpcng.org/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e versions are provided (the latter recomended).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you want to use Interproscan external privative software, these programs must be obtained first with granted academic permissions.\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.cbs.dtu.dk/services/SignalP/\" rel=\"nofollow\"\u003eSignalP\u003c/a\u003e \u003ccode\u003esignalp-4.1b.Linux.tar.Z\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.cbs.dtu.dk/services/TMHMM/\" rel=\"nofollow\"\u003eTMHMM\u003c/a\u003e \u003ccode\u003etmhmm-2.0c.Linux.tar.gz\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://phobius.sbc.su.se/\" rel=\"nofollow\"\u003ePhobious\u003c/a\u003e \u003ccode\u003ephobius101_linux.tar.gz\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eRegarding phobius: \u003ca href=\"https://www.biostars.org/p/238642/\" rel=\"nofollow\"\u003ehttps://www.biostars.org/p/238642/\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eKeep in mind that some other modifications are also needed in those programs above in advance, e. g., replacing \u003ccode\u003e/usr/bin/perl\u003c/code\u003e for \u003ccode\u003e/usr/bin/env perl\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLast software package versions of Interproscan include the whole data by default. For container performance and distribution, we don\u0027t keep Interproscan data directory in the container and we replace it for a symbolic link / volume pointing to a defined location in our HPC storage by using build argument \u003ccode\u003eIPSCAN_DATA\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIt is important to ensure that program and data versions match and that this is adequately reflected in \u003ccode\u003einterproscan.properties\u003c/code\u003e or \u003ccode\u003einterproscan.open.properties\u003c/code\u003e files. Otherwise Interproscan is not likely to work.\u003c/p\u003e\n\u003cp\u003eBuilding with Singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# With privative software\nsudo singularity build iprscan-5.48-83.0.sif Singularity\n# Without privative software\nsudo singularity build iprscan-5.48-83.0.open.sif Singularity.open\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can avoid using \u003ccode\u003esudo\u003c/code\u003e with \u003ccode\u003e--fakeroot\u003c/code\u003e Singularity build option.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://biocore.crg.eu/iprscan/\" rel=\"nofollow\"\u003ePregenerated open images\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNOTES\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eIPSCAN_DATA\u003c/code\u003e build argument in the recipes must be adapted to fit the configuration of your HPC storage setup. Moreover, keep into account that a user with suitable permissions may need to run it for first time to index \u003ccode\u003eIPSCAN_DATA\u003c/code\u003e dataset (e.g., with \u003ccode\u003epython3 /usr/local/interproscan/initial_setup.py\u003c/code\u003e). You can use the very container images. Details here: \u003ca href=\"https://interproscan-docs.readthedocs.io/en/5.48-83.0/HowToRun.html\" rel=\"nofollow\"\u003ehttps://interproscan-docs.readthedocs.io/en/5.48-83.0/HowToRun.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eDepending on your setup, you may need to change \u003ccode\u003eSINGULARITY_TMPDIR\u003c/code\u003e (and \u003ccode\u003eSINGULARITY_CACHEDIR\u003c/code\u003e) environment variables for pointing to a location with enough space. More details at: \u003ca href=\"https://singularity.hpcng.org/admin-docs/master/installation.html\" rel=\"nofollow\"\u003ehttps://singularity.hpcng.org/admin-docs/master/installation.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eWhen using Singularity ensure \u003ccode\u003eIPSCAN_DATA\u003c/code\u003e is in a \u003ccode\u003ebind path\u003c/code\u003e directory. More details at: \u003ca href=\"https://singularity.hpcng.org/admin-docs/master/configfiles.html\" rel=\"nofollow\"\u003ehttps://singularity.hpcng.org/admin-docs/master/configfiles.html\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1626105383.0
  },
  {
    "data_format": 2,
    "description": "METHYLPY, is an analysis pipeline for DNA methylation data.",
    "filenames": [
      "1.4.3/Singularity"
    ],
    "full_name": "pscedu/singularity-methylpy",
    "latest_release": "v1.4.3",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-methylpy/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-methylpy/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/68dce811b911761f8ba92aae500e0e1400720248c31724fc8f3215bead82ab11/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6d657468796c7079\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/68dce811b911761f8ba92aae500e0e1400720248c31724fc8f3215bead82ab11/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6d657468796c7079\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-methylpy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/fe10607a3df63b8791aff64a9cb4897318e187e28b12c3a563a6a0a76bbb8f73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6d657468796c7079\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fe10607a3df63b8791aff64a9cb4897318e187e28b12c3a563a6a0a76bbb8f73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6d657468796c7079\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-methylpy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ca28e9dbb4ca3fb891088deca8c37945a3dd96b32bfc0e01a8eef88efa3fe02e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6d657468796c7079\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca28e9dbb4ca3fb891088deca8c37945a3dd96b32bfc0e01a8eef88efa3fe02e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6d657468796c7079\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-methylpy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/c59e0399bb825884a9fa4f45b91bdba428f86d1decc49df207e011187efa29b1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6d657468796c7079\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c59e0399bb825884a9fa4f45b91bdba428f86d1decc49df207e011187efa29b1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6d657468796c7079\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-methylpy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-methylpy\" class=\"anchor\" href=\"#singularity-methylpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-methylpy\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/METHYLPY\"\u003eMETHYLPY\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the Perl scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/methylpy/1.4.3\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/methylpy\u003c/code\u003e as \u003ccode\u003e1.4.3.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1625256690.0
  },
  {
    "data_format": 2,
    "description": "A powerful toolset for genome arithmetic",
    "filenames": [
      "Singularity"
    ],
    "full_name": "pscedu/singularity-bedtools",
    "latest_release": "v2.29.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bedtools\" class=\"anchor\" href=\"#singularity-bedtools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bedtools\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.sylabs.io/library/icaoberg/default/bedtools\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2747e70595bc577024d908f158c1c8b1d458085960e3bdd70770858769cdf396/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d73796c6162732e696f2d677265656e2e737667\" alt=\"Hosted\" data-canonical-src=\"https://img.shields.io/badge/hosted-sylabs.io-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/8025f5254ec80610d1aa2daad722b5f63139d443fc969f0e566b87230d2519c5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d76322e32392e322d677265656e2e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8025f5254ec80610d1aa2daad722b5f63139d443fc969f0e566b87230d2519c5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f72656c656173652d76322e32392e322d677265656e2e737667\" alt=\"Release\" data-canonical-src=\"https://img.shields.io/badge/release-v2.29.2-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.org/icaoberg/singularity-bedtools\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4bd5a5797e0eea5576a22f0d6438bb219e7e9d1165583a395f84114bcdf8d56e/68747470733a2f2f7472617669732d63692e6f72672f6963616f626572672f73696e67756c61726974792d626564746f6f6c732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/icaoberg/singularity-bedtools.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/icaoberg/singularity-bedtools/issues\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8daa37ea52a8b910d7804b0dea5e36dc1ea51550f4d4d7e83fa044b7785964a3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d626564746f6f6c732e737667\" alt=\"GitHub issues\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-bedtools.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/icaoberg/singularity-bedtools/network\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ea5237be05f56a481118a0905c976d88658d70a95b16af6c060101aea4e73a98/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d626564746f6f6c732e737667\" alt=\"GitHub forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-bedtools.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/icaoberg/singularity-bedtools/stargazers\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/192b2d7fbd67aa10f3c8f5535205f7be9d07653cb29eb16d34176d9dc6b541a0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d626564746f6f6c732e737667\" alt=\"GitHub stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-bedtools.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.gnu.org/licenses/quick-guide-gplv3.en.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0b6758422f85bc2599288b346c7de30c6b7b217112c0a877ae4b25a7009722e4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c76332d626c75652e737667\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/badge/license-GPLv3-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"images/logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/logo.png\" alt=\"Logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCollectively, the \u003ca href=\"https://bedtools.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ebedtools\u003c/a\u003e utilities are a swiss-army knife of tools for a wide-range of genomics analysis tasks. The most widely-used tools enable genome arithmetic: that is, set theory on the genome. For example, bedtools allows one to intersect, merge, count, complement, and shuffle genomic intervals from multiple files in widely-used genomic file formats such as BAM, BED, GFF/GTF, VCF. While each individual tool is designed to do a relatively simple task (e.g., intersect two interval files), quite sophisticated analyses can be conducted by combining multiple bedtools operations on the UNIX command line.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pre-requisites\" class=\"anchor\" href=\"#pre-requisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePre-requisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity v3.5.+\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou will need to edit the script above to match your account on \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSyLabs.io\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pulling-from-the-repository\" class=\"anchor\" href=\"#pulling-from-the-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePulling from the repository\u003c/h3\u003e\n\u003cp\u003eIf you have the client installed and cannot build the image locally nor remotely, simply run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --arch amd64 library://icaoberg/default/bedtools:v2.29.2\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDisclaimer\u003c/h2\u003e\n\u003cp\u003eI am nothing but a humble programmer creating the container for this wonderful app. \u003ca href=\"https://bedtools.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ebedtools\u003c/a\u003e is developed in the \u003ca href=\"http://quinlanlab.org/\" rel=\"nofollow\"\u003eQuinlan laboratory\u003c/a\u003e at the \u003ca href=\"https://www.utah.edu/\" rel=\"nofollow\"\u003eUniversity of Utah\u003c/a\u003e and benefits from fantastic contributions made by scientists worldwide.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"http://www.cbd.cmu.edu\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/47fafd631a30ef553735a724705fea74cb02d1bb0b9eab22c64aca8c41885f3d/687474703a2f2f7777772e6362642e636d752e6564752f77702d636f6e74656e742f75706c6f6164732f323031372f30372f776f726470726573732d64656661756c742e706e67\" alt=\"CBD\" data-canonical-src=\"http://www.cbd.cmu.edu/wp-content/uploads/2017/07/wordpress-default.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCopyleft \u00a9 2019-2020 \u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.cbd.cmu.edu\" rel=\"nofollow\"\u003eComputational Biology Department\u003c/a\u003e in \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "singularity-container",
      "bedtools"
    ],
    "updated_at": 1625243546.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for shellcheck",
    "filenames": [
      "0.5.0/Singularity"
    ],
    "full_name": "pscedu/singularity-shellcheck",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-pandoc\" class=\"anchor\" href=\"#singularity-pandoc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-pandoc\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://pandoc.org/\" rel=\"nofollow\"\u003epandoc\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625243633.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for pandoc.",
    "filenames": [
      "2.2.1/Singularity"
    ],
    "full_name": "pscedu/singularity-pandoc",
    "latest_release": "2.2.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-pandoc\" class=\"anchor\" href=\"#singularity-pandoc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-pandoc\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://pandoc.org/\" rel=\"nofollow\"\u003epandoc\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities",
      "pandoc"
    ],
    "updated_at": 1625255888.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for browsh",
    "filenames": [
      "Singularity"
    ],
    "full_name": "pscedu/singularity-browsh",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-browsh\" class=\"anchor\" href=\"#singularity-browsh\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-browsh\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/1c8b7e6a5bc8e1f4e40f52ab2153b6dd39453b9b244c19612a99142ab2fa5cf0/68747470733a2f2f7777772e62726f772e73682f6173736574732f696d616765732f62726f7773682d6865616465722e6a7067\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1c8b7e6a5bc8e1f4e40f52ab2153b6dd39453b9b244c19612a99142ab2fa5cf0/68747470733a2f2f7777772e62726f772e73682f6173736574732f696d616765732f62726f7773682d6865616465722e6a7067\" alt=\"Logo\" data-canonical-src=\"https://www.brow.sh/assets/images/browsh-header.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ee7cb50ef5a31fe25a8e149b0a09b50a8523d5e11a27311af8e63f22a8c915b2/687474703a2f2f7777772e616e647265772e636d752e6564752f757365722f6963616f626572672f696d616765732f6c6f676f732f7073632e706e67\" alt=\"PSC\" data-canonical-src=\"http://www.andrew.cmu.edu/user/icaoberg/images/logos/psc.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "browsh"
    ],
    "updated_at": 1625255979.0
  },
  {
    "data_format": 2,
    "description": "Simple terminal UI for git commands.",
    "filenames": [
      "0.28.2/Singularity",
      "0.23.1/Singularity"
    ],
    "full_name": "icaoberg/singularity-lazygit",
    "latest_release": "v0.28.2",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-lazygit/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-lazygit/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ce43f9f05edc280dcb72fb4ca8be46c0dab1ad9b88f48b7c2d9b8273288d266f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6c617a79676974\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ce43f9f05edc280dcb72fb4ca8be46c0dab1ad9b88f48b7c2d9b8273288d266f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6c617a79676974\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-lazygit\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/7fdf306f4f7e4fcc9fc77bc1030ff82b19deed57ab2965e952d30b70fb7b674a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6c617a79676974\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7fdf306f4f7e4fcc9fc77bc1030ff82b19deed57ab2965e952d30b70fb7b674a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6c617a79676974\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-lazygit\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e8240aa02d92e2a9a5ee84af2261f39ed2c8fc86a0c2f54f6e1f6bab629e0fe5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6c617a79676974\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e8240aa02d92e2a9a5ee84af2261f39ed2c8fc86a0c2f54f6e1f6bab629e0fe5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6c617a79676974\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-lazygit\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/cef5da0642ce33fb4fb6a644a1c76721ffe638963e959e6812c008aa8c6ce693/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6c617a79676974\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cef5da0642ce33fb4fb6a644a1c76721ffe638963e959e6812c008aa8c6ce693/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6c617a79676974\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-lazygit\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-lazygit\" class=\"anchor\" href=\"#singularity-lazygit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-lazygit\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"images/screenshot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/screenshot.png\" alt=\"Screenshot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\nSingularity recipe for \u003ca href=\"https://github.com/jesseduffield/lazygit\"\u003elazygit\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003elazygit\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/lazygit/4.8.25\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/lazygit\u003c/code\u003e as \u003ccode\u003e4.8.25.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/lazygits/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1625268998.0
  },
  {
    "data_format": 2,
    "description": "xxHash is an extremely fast non-cryptographic hash algorithm, working at RAM speed limit.",
    "filenames": [
      "0.8.0/Singularity"
    ],
    "full_name": "pscedu/singularity-xxhash",
    "latest_release": "v0.8.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-xxhash/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-xxhash/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ad181fa59c33840ef53b20079ac66d4da64ed5e3a7c55bffc35110217f7a8315/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d787868617368\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ad181fa59c33840ef53b20079ac66d4da64ed5e3a7c55bffc35110217f7a8315/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d787868617368\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-xxhash\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/98b27c52ede8d4973c79b45954b3a0491bad8d8599f9799224c5b4cea3200ab6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d787868617368\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/98b27c52ede8d4973c79b45954b3a0491bad8d8599f9799224c5b4cea3200ab6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d787868617368\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-xxhash\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/392e70c79e1a5e216763b429054179518d620fe741e6bc6869abaae633d699bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d787868617368\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/392e70c79e1a5e216763b429054179518d620fe741e6bc6869abaae633d699bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d787868617368\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-xxhash\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/8f852375d11fe413454675da713f74280931dc12e04c03367bcbd9a0245f3f2b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d787868617368\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8f852375d11fe413454675da713f74280931dc12e04c03367bcbd9a0245f3f2b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d787868617368\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-xxhash\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-xxhash\" class=\"anchor\" href=\"#singularity-xxhash\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-xxhash\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/mc\"\u003exxhash\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003exxh128sum\u003c/code\u003e, \u003ccode\u003exxh32sum\u003c/code\u003e, \u003ccode\u003exxh64sum\u003c/code\u003e and \u003ccode\u003exxhsum\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/xxhash/4.8.25\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/xxhash\u003c/code\u003e as \u003ccode\u003e4.8.25.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1625277714.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "volsung-cudnn8-runtime-ubuntu18.04/Singularity",
      "vdt_base/Singularity"
    ],
    "full_name": "AvciRecep/chaste_nesi",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4539\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsing conventions described here.\n\u003ca href=\"https://singularityhub.github.io/singularityhub-docs/docs/getting-started/naming\" rel=\"nofollow\"\u003ehttps://singularityhub.github.io/singularityhub-docs/docs/getting-started/naming\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625528992.0
  },
  {
    "data_format": 2,
    "description": "Heuristic Algorithms for Quantum Computers",
    "filenames": [
      "SingularityFile.def"
    ],
    "full_name": "vivekkatial/HAQC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-haqc----heuristic-algorithms-for-quantum-computing-research-group-\" class=\"anchor\" href=\"#haqc----heuristic-algorithms-for-quantum-computing-research-group-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHAQC -- Heuristic Algorithms for Quantum Computing Research Group \u003ca href=\"https://travis-ci.com/vivekkatial/HAQC\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/52d2ffbcfa48e5c6383ca1dc80ae58c3bd5a5b55cfdd65a8f3a2dd02d5368160/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f484151432e7376673f746f6b656e3d7679627a68504d65666e56714a42667079344570266272616e63683d6d61696e\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/vivekkatial/HAQC.svg?token=vybzhPMefnVqJBfpy4Ep\u0026amp;branch=main\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h1\u003e\n\u003cp\u003eResearch group to run optimisation algorithms on Quantum Computers at the University of Melbourne\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eBefore getting started, ensure you have Python 3.7+. We use \u003ca href=\"https://pipenv-fork.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003epipenv\u003c/a\u003e to manage the python environment (the .gitignore file should already ignore it).\u003c/p\u003e\n\u003cpre lang=\"{shell}\"\u003e\u003ccode\u003e$ pipenv install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo add a package to your new project:\u003c/p\u003e\n\u003cpre lang=\"{shell}\"\u003e\u003ccode\u003e$ pipenv install \u0026lt;package\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will automatically edit your pipfile with the new package you provided.\u003c/p\u003e\n\u003cp\u003eNext, activate the Pipenv shell:\u003c/p\u003e\n\u003cpre lang=\"{shell}\"\u003e\u003ccode\u003e$ pipenv shell\n$ python --version\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will spawn a new shell subprocess, which can be deactivated by using exit.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h3\u003e\n\u003cp\u003eFor testing, we use \u003ccode\u003epytest\u003c/code\u003e. To run the tests, just type the command \u003ccode\u003epytest\u003c/code\u003e, or you can specify a file e.g. \u003ccode\u003epytest tests/test_graph_generator.py\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eWe will use \u003ccode\u003eblack\u003c/code\u003e as our code formatter. Simply run \u003ccode\u003eblack -S .\u003c/code\u003e to run black over all the files before committing. The \u003ccode\u003e-S\u003c/code\u003e is to skip string normalisation, because we prefer single quotes/don\u0027t really care (\u003ca href=\"https://github.com/psf/black/issues/118\"\u003eflame war, I know\u003c/a\u003e).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-before-making-a-pr\" class=\"anchor\" href=\"#before-making-a-pr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBefore making a PR\u003c/h3\u003e\n\u003cp\u003eIn summary, before merging a PR, you should:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Make sure all tests pass\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e src\npipenv run python -m pytest tests/\u003cspan class=\"pl-k\"\u003e*\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Format with black\u003c/span\u003e\npipenv run python -m black -S \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mlflow-tracking\" class=\"anchor\" href=\"#mlflow-tracking\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMLFlow Tracking\u003c/h2\u003e\n\u003cp\u003eTo get the MLFlow tracking functionality to work you will need to setup \u003ccode\u003eawscli\u003c/code\u003e credentials, so MLFlow can properly log artifacts.\u003c/p\u003e\n\u003cp\u003eIf you\u0027re keen to do this then please follow the instructions \u003ca href=\"https://wiki-rcs.unimelb.edu.au/display/RCS/AWS+CLI\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eYou can request the credentials for this experiment from Vivek at \u003ca href=\"mailto:vkatial@student.unimelb.edu.au\"\u003evkatial@student.unimelb.edu.au\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-a-test-instance\" class=\"anchor\" href=\"#running-a-test-instance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning a test instance\u003c/h2\u003e\n\u003cp\u003eTo run a test instance try out the steps below:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython qaoa_vrp/main.py -f \u003cspan class=\"pl-c1\"\u003etest\u003c/span\u003e -T False \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e -T tracking for MLFlow\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter-notebooks\" class=\"anchor\" href=\"#jupyter-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter Notebooks\u003c/h3\u003e\n\u003cp\u003eFirst ensure that your Python is \u003cem\u003enot\u003c/em\u003e aliased in your \u003ccode\u003e.bashrc\u003c/code\u003e or \u003ccode\u003e.zshrc\u003c/code\u003e file.\u003c/p\u003e\n\u003cp\u003eAfter this launch your \u003ccode\u003epipenv\u003c/code\u003e by\u003c/p\u003e\n\u003cpre lang=\"{shell}\"\u003e\u003ccode\u003epipenv shell\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen do:\u003c/p\u003e\n\u003cpre lang=\"{shell}\"\u003e\u003ccode\u003epython -m ipykernel install --user --name=qaoa-vrp\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen launch the notebook\u003c/p\u003e\n\u003cpre lang=\"{shell}\"\u003e\u003ccode\u003ejupyter notebook\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn your notebook, Kernel -\u0026gt; Change Kernel. Your kernel should now be an option.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"images/jupyter-install.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/jupyter-install.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors\" class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eVivek Katial\u003c/li\u003e\n\u003cli\u003eFloyd Creevey\u003c/li\u003e\n\u003cli\u003eJedwin Villareal Villanueva\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626934977.0
  },
  {
    "data_format": 2,
    "description": "Container image with signalp and targetp programs for functional analysis pipelines",
    "filenames": [
      "Singularity"
    ],
    "full_name": "biocorecrg/sigtarp_docker",
    "latest_release": "4.1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sigtarp_docker\" class=\"anchor\" href=\"#sigtarp_docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esigtarp_docker\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://zenodo.org/badge/latestdoi/152766566\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a13c48f1c8cd76a173ca24646a80c645c4e34bb76466d0f7b12e355f471ede0e/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3135323736363536362e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/152766566.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eContainer image with \u003ca href=\"http://www.cbs.dtu.dk/services/SignalP/\" rel=\"nofollow\"\u003esignalP\u003c/a\u003e, \u003ca href=\"http://www.cbs.dtu.dk/services/TargetP/\" rel=\"nofollow\"\u003etargetP\u003c/a\u003e and \u003ca href=\"http://www.cbs.dtu.dk/services/ChloroP/\" rel=\"nofollow\"\u003echloroP\u003c/a\u003e programs for functional analysis pipelines.\u003c/p\u003e\n\u003cp\u003eCreate a directory named \u003ccode\u003eexternal\u003c/code\u003e and place 3 directories with its associated files and binaries as downloaded from the links above. You need to be granted an academic license permission first.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esignalp (4.1)\u003c/li\u003e\n\u003cli\u003etargetp (1.1)\u003c/li\u003e\n\u003cli\u003echlorop (1.1)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eContainer recipes will grab the necessary files from these directories.\u003c/p\u003e\n\u003cp\u003eBuilding with \u003ca href=\"https://singularity.hpcng.org/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build sigtarp.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can avoid using sudo with \u003ccode\u003e--fakeroot\u003c/code\u003e Singularity build option.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1625498174.0
  },
  {
    "data_format": 2,
    "description": "Command Line Interface and Python API for Forskalle",
    "filenames": [
      "Singularity"
    ],
    "full_name": "csf-ngs/forskalle-api",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fsk-api--cli\" class=\"anchor\" href=\"#fsk-api--cli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFSK API + cli\u003c/h1\u003e\n\u003cp\u003ePython library for Fsk3 API. Will add functionality as needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eInstall from the VBCF.NGS repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install git+https://ngs.vbcf.ac.at/repo/software/forskalle-api.git\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or github\u003c/span\u003e\npip3 install git+https://github.com/csf-ngs/forskalle-api.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-cli\" class=\"anchor\" href=\"#cli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCLI\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003efsk-cli [command] [options] etc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePoint it at your favorite Forskalle instance either by\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esetting environment variables: \u003ccode\u003eFSK_API_BASE\u003c/code\u003e and \u003ccode\u003eFSK_API_KEY\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eusing a config file in \u003ccode\u003e~/.fsk_api.yml\u003c/code\u003e, please see \u003ca href=\"doc/\"\u003edoc/\u003c/a\u003e for an example\u003c/li\u003e\n\u003cli\u003eproviding \u003ccode\u003e--base\u003c/code\u003e and \u003ccode\u003e--key\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTry \u003ccode\u003efsk-cli --help\u003c/code\u003e for some hints!\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h4\u003e\n\u003cp\u003eSet all sequenced samples of a multiplex to Ok:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003efsk-cli multi get M4711 \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e jq \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e.multiplex_samples[].sample_id\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e \\\n  \u003cspan class=\"pl-k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003eread\u003c/span\u003e sample_id\u003cspan class=\"pl-k\"\u003e;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003edo\u003c/span\u003e \n    fsk-cli set-sequencing-status \u003cspan class=\"pl-smi\"\u003e$sample_id\u003c/span\u003e --status Ok\n  \u003cspan class=\"pl-k\"\u003edone\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn place editing with jq and updating:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e update all request lanes to status Ready\u003c/span\u003e\nfsk-cli request get R4711 \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e \\\n  jq \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e.request_lanes[].status=\"Ready\"\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e \\\n  fsk-cli request update R4711\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-library\" class=\"anchor\" href=\"#library\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLibrary\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003efrom forskalle_api import FskApi\n\nfsk_api = FskApi()\nsample_json = fsk_api.get_sample(54321)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003efrom forskalle_api import FskApi\nfrom forskalle_api.auto.queryparams import IlluminaRunFilters\nfrom forskalle_api.fsk_query import FskQuery\n\nfsk_api = FskApi()\nirf = IlluminaRunFilters(sequenced_after=\"2020-05-01\")\nq = FskQuery(filters=irf)\nruns = fsk_api.get_runs_illumina(q)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere is no API-doc or similar, but we all love reading python source code!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-models\" class=\"anchor\" href=\"#models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eModels\u003c/h3\u003e\n\u003cp\u003eModels and Query Parameters are autogenerated from forskalle. Return values of most api calls are thin class layers with type hints, e.g. forskalle_api.auto.models.Sample with all properties and relationships to allow easy navigation in your source code editor.\u003c/p\u003e\n\u003cp\u003eYou can also find de/serialization helpers (serializeSample from Class to dict, plainToSample from dict to Class).\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1625599328.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "ashokdahal/FrameFieldLearning_Anaconda_Windows",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-polygonal-building-segmentation-by-frame-field-learning-modified-from-lydorn-to-work-on-anaconda-and-windows-device-without-docker-installation\" class=\"anchor\" href=\"#polygonal-building-segmentation-by-frame-field-learning-modified-from-lydorn-to-work-on-anaconda-and-windows-device-without-docker-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePolygonal Building Segmentation by Frame Field Learning Modified from LYDORN to work on anaconda and windows device without docker installation.\u003c/h1\u003e\n\u003cp\u003eWe add a frame field output to an image segmentation neural network to improve segmentation quality\nand provide structural information for the subsequent polygonization step.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n    \u003ca href=\"images/frame_field_sample.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/frame_field_sample.png\" width=\"512\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n    \u003cbr\u003e\n    Figure 1: Close-up of our additional frame field output on a test image.\n    \u003cbr\u003e\n    \u003cbr\u003e\n    \u003cbr\u003e\n    \u003ca href=\"images/model_training.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/model_training.png\" width=\"768\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n    \u003cbr\u003e\n    Figure 2: Given an overhead image, the model outputs an edge mask, an interior mask,\n    and a frame field for buildings. The total loss includes terms that align the masks and\n    frame field to ground truth data as well as regularizers to enforce smoothness of the\n    frame field and consistency between the outputs.\n    \u003cbr\u003e\n    \u003cbr\u003e\n    \u003cbr\u003e\n    \u003ca href=\"images/schematic_polygonization.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/schematic_polygonization.png\" width=\"768\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n    \u003cbr\u003e\n    Figure 3: Given classification maps and a frame field as input, we optimize skeleton polylines to\n    align to the frame field using an Active Skeleton Model (ASM) and detect corners using\n    the frame field, simplifying non-corner vertices.\n\u003c/p\u003e\n\u003cp\u003eThis repository contains the official code for the paper:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePolygonal Building Segmentation by Frame Field Learning\u003c/strong\u003e\u003cbr\u003e\n\u003ca href=\"https://www-sop.inria.fr/members/Nicolas.Girard/\" rel=\"nofollow\"\u003eNicolas Girard\u003c/a\u003e,\n\u003ca href=\"https://people.csail.mit.edu/smirnov/\" rel=\"nofollow\"\u003eDmitriy Smirnov\u003c/a\u003e,\n\u003ca href=\"https://people.csail.mit.edu/jsolomon/\" rel=\"nofollow\"\u003eJustin Solomon\u003c/a\u003e,\n\u003ca href=\"https://www-sop.inria.fr/members/Yuliya.Tarabalka/\" rel=\"nofollow\"\u003eYuliya Tarabalka\u003c/a\u003e\u003cbr\u003e\nPre-print\u003cbr\u003e\n\u003cstrong\u003e[\u003ca href=\"https://arxiv.org/pdf/2004.14875.pdf\" rel=\"nofollow\"\u003epaper\u003c/a\u003e, \u003ca href=\"https://www.youtube.com/watch?v=XdQMD3HTYCU\u0026amp;t=5s\" rel=\"nofollow\"\u003evideo\u003c/a\u003e]\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhose short version has been published as:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRegularized Building Segmentation by Frame Field Learning\u003c/strong\u003e\u003cbr\u003e\n\u003ca href=\"https://www-sop.inria.fr/members/Nicolas.Girard/\" rel=\"nofollow\"\u003eNicolas Girard\u003c/a\u003e,\n\u003ca href=\"https://people.csail.mit.edu/smirnov/\" rel=\"nofollow\"\u003eDmitriy Smirnov\u003c/a\u003e,\n\u003ca href=\"https://people.csail.mit.edu/jsolomon/\" rel=\"nofollow\"\u003eJustin Solomon\u003c/a\u003e,\n\u003ca href=\"https://www-sop.inria.fr/members/Yuliya.Tarabalka/\" rel=\"nofollow\"\u003eYuliya Tarabalka\u003c/a\u003e\u003cbr\u003e\nIGARSS 2020\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-git-submodules\" class=\"anchor\" href=\"#git-submodules\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGit submodules\u003c/h2\u003e\n\u003cp\u003eThis project uses various git submodules that should be cloned too.\u003c/p\u003e\n\u003cp\u003eTo clone a repository including its submodules execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive --jobs 8 \u0026lt;URL to Git repo\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you already have cloned the repository and now want to load it\u2019s submodules execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit submodule update --init --recursive --jobs 8\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit submodule update --recursive\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more about explanations about using submodules and git, see \u003ca href=\"SUBMODULES.md\"\u003eSUBMODULES.md\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h2\u003e\n\u003cp\u003eThe easiest way to setup environment is to use the Docker image provided in the \u003ca href=\"docker\"\u003edocker\u003c/a\u003e (see README inside the folder).\u003c/p\u003e\n\u003cp\u003eOnce the docker container is built and launched, execute the \u003ca href=\"setup.sh\"\u003esetup.sh\u003c/a\u003e script inside to install required packages.\u003c/p\u003e\n\u003cp\u003eThe environment in the container is now ready for use.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-conda-environment\" class=\"anchor\" href=\"#conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConda environment\u003c/h2\u003e\n\u003cp\u003eAlternatively you can install all dependencies in a conda environment.\nI provide my environment specifications in the  \u003ca href=\"environment.yml\"\u003eenvironment.yml\u003c/a\u003e which you can use to create your environment own with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f environment.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-data\" class=\"anchor\" href=\"#data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData\u003c/h1\u003e\n\u003cp\u003eSeveral datasets are used in this work.\nWe typically put all datasets in a \"data\" folder which we link to the \"/data\" folder in the container (with the \u003ccode\u003e-v\u003c/code\u003e argument when running the container).\nEach dataset has it\u0027s own sub-folder, usually named with a short version of that dataset\u0027s name.\nEach dataset sub-folder should have a \"raw\" folder inside containing all the original folders and files fo the datset.\nWhen pre-processing data, \"processed\" folders will be created alongside the \"raw\" folder.\u003c/p\u003e\n\u003cp\u003eFor example, here is an example working file structure inside the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/data \n|-- AerialImageDataset\n     |-- raw\n         |-- train\n         |   |-- aligned_gt_polygons_2\n         |   |-- gt\n         |   |-- gt_polygonized\n         |   |-- images\n         `-- test\n             |-- aligned_gt_polygons_2\n             |-- images\n`-- mapping_challenge_dataset\n     |-- raw\n         |-- train\n         |   |-- images\n         |   |-- annotation.json\n         |   `-- annotation-small.json\n         `-- val\n              `-- ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf however you would like to use a different folder for the datasets (for example while not using Docker),\nyou can change the path to datasets in config files.\nYou can modify the \"data_dir_candidates\" list in the config to only include your path.\nThe training script checks this list of paths one at a time and picks the first one that exists.\nIt then appends the \"data_root_partial_dirpath\" directory to get to the dataset.\u003c/p\u003e\n\u003cp\u003eYou can find some of the data we used in this shared \"data\" folder: \u003ca href=\"https://drive.google.com/drive/folders/19yqseUsggPEwLFTBl04CmGmzCZAIOYhy?usp=sharing\" rel=\"nofollow\"\u003ehttps://drive.google.com/drive/folders/19yqseUsggPEwLFTBl04CmGmzCZAIOYhy?usp=sharing\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inria-aerial-image-labeling-dataset\" class=\"anchor\" href=\"#inria-aerial-image-labeling-dataset\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInria Aerial Image Labeling Dataset\u003c/h2\u003e\n\u003cp\u003eLink to the dataset: \u003ca href=\"https://project.inria.fr/aerialimagelabeling/\" rel=\"nofollow\"\u003ehttps://project.inria.fr/aerialimagelabeling/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor the Inria dataset, the original ground truth is just a collection of raster masks.\nAs our method requires annotations to be polygons in order to compute the ground truth angle for the frame field, we made 2 versions of the dataset:\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eInria OSM dataset\u003c/em\u003e has aligned annotations pulled from OpenStreetMap.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003eInria Polygonized dataset\u003c/em\u003e has polygon annotations obtained from using our frame field polygonization algorithm on the original raster masks.\nThis was done by running the \u003ccode\u003epolygonize_mask.py\u003c/code\u003e script like so:\n\u003ccode\u003epython polygonize_mask.py --run_name inria_dataset_osm_mask_only.unet16 --filepath ~/data/AerialImageDataset/raw/train/gt/*.tif\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eYou can find this new ground truth for both cases in the shared \"data\" folder (\u003ca href=\"https://drive.google.com/drive/folders/19yqseUsggPEwLFTBl04CmGmzCZAIOYhy?usp=sharing\" rel=\"nofollow\"\u003ehttps://drive.google.com/drive/folders/19yqseUsggPEwLFTBl04CmGmzCZAIOYhy?usp=sharing\u003c/a\u003e.).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-the-mainpy-script\" class=\"anchor\" href=\"#running-the-mainpy-script\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the main.py script\u003c/h1\u003e\n\u003cp\u003eExecute \u003ca href=\"main.py\"\u003emain.py\u003c/a\u003e script to train a model, test a model or use a model on your own image.\nSee the help of the main script with:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython main.py --help\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe script can be launched on multiple GPUs for multi-GPU training and evaluation.\nSimply set the \u003ccode\u003e--gpus\u003c/code\u003e argument to the number of gpus you want to use.\nHowever, for the first launch of the script on a particular dataset (when it will pre-process the data),\nit is best to leave it at 1 as I did not implement multi-GPU synchronization when pre-processing datasets.\u003c/p\u003e\n\u003cp\u003eAn example use is for training a model with a certain config file, like so:\n\u003ccode\u003epython main.py --config configs/config.mapping_dataset.unet_resnet101_pretrained\u003c/code\u003e\nwhich will train the Unet-Resnet101 on the CrowdAI Mapping Challenge dataset.\nThe batch size can be adjusted like so:\n\u003ccode\u003epython main.py --config configs/config.mapping_dataset.unet_resnet101_pretrained -b \u0026lt;new batch size\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWhen training is done, the script can be launched in eval mode, to evaluate the trained model:\n\u003ccode\u003epython main.py --config configs/config.mapping_dataset.unet_resnet101_pretrained --mode eval\u003c/code\u003e.\nDepending on the eval parameters of the config file, running this will output results on the test dataset.\u003c/p\u003e\n\u003cp\u003eFinally, if you wish to compute AP and AR metrics with the COCO API, you can run:\n\u003ccode\u003epython main.py --config configs/config.mapping_dataset.unet_resnet101_pretrained --mode eval_coco\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-launch-inference-on-one-image\" class=\"anchor\" href=\"#launch-inference-on-one-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLaunch inference on one image\u003c/h2\u003e\n\u003cp\u003eMake sure the run folder has the correct structure:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePolygonization-by-Frame-Field-Learning\n|-- frame_field_learning\n|   |-- runs\n|   |   |-- \u0026lt;run_name\u0026gt; | \u0026lt;yyyy-mm-dd hh:mm:ss\u0026gt;\n|   |   `-- ...\n|   |-- inference.py\n|   `-- ...\n|-- main.py\n|-- README.md (this file)\n`-- ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExecute the [main.py] script like so (filling values for arguments run_name and in_filepath):\n\u003ccode\u003epython main.py --run_name \u0026lt;run_name\u0026gt; --in_filepath \u0026lt;your_image_filepath\u0026gt;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe outputs will be saved next to the input image\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download-trained-models\" class=\"anchor\" href=\"#download-trained-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload trained models\u003c/h2\u003e\n\u003cp\u003eWe provide already-trained models so you can run inference right away.\nDownload here: \u003ca href=\"https://drive.google.com/drive/folders/1poTQbpCz12ra22CsucF_hd_8dSQ1T3eT?usp=sharing\" rel=\"nofollow\"\u003ehttps://drive.google.com/drive/folders/1poTQbpCz12ra22CsucF_hd_8dSQ1T3eT?usp=sharing\u003c/a\u003e.\nEach model was trained in a \"run\", whose folder (named with the format \u003ccode\u003e\u0026lt;run_name\u0026gt; | \u0026lt;yyyy-mm-dd hh:mm:ss\u0026gt;\u003c/code\u003e) you can download at the provided link.\nYou should then place those runs in a folder named \"runs\" inside the \"frame_field_learning\" folder like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePolygonization-by-Frame-Field-Learning\n|-- frame_field_learning\n|   |-- runs\n|   |   |-- inria_dataset_polygonized.unet_resnet101_pretrained.leaderboard | 2020-06-02 07:57:31\n|   |   |-- mapping_dataset.unet_resnet101_pretrained.field_off.train_val | 2020-09-07 11:54:48\n|   |   |-- mapping_dataset.unet_resnet101_pretrained.train_val | 2020-09-07 11:28:51\n|   |   `-- ...\n|   |-- inference.py\n|   `-- ...\n|-- main.py\n|-- README.md (this file)\n`-- ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause Google Drive reformats folder names, you have to rename the run folders as above.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-cite\" class=\"anchor\" href=\"#cite\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCite:\u003c/h1\u003e\n\u003cp\u003eIf you use this code for your own research, please cite\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@InProceedings{Girard_2020_IGARSS,\n  title = {{Regularized Building Segmentation by Frame Field Learning}},\n  author = {Girard, Nicolas and Smirnov, Dmitriy and Solomon, Justin and Tarabalka, Yuliya},\n  booktitle = {IEEE International Geoscience and Remote Sensing Symposium (IGARSS)},\n  ADDRESS = {Waikoloa, Hawaii},\n  year = {2020},\n  month = Jul,\n}\n\n@misc{girard2020polygonal,\n    title={Polygonal Building Segmentation by Frame Field Learning},\n    author={Nicolas Girard and Dmitriy Smirnov and Justin Solomon and Yuliya Tarabalka},\n    year={2020},\n    eprint={2004.14875},\n    archivePrefix={arXiv},\n    primaryClass={cs.CV}\n}\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625572221.0
  },
  {
    "data_format": 2,
    "description": "Validate and submit reads using Webin-CLI in batch.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "enasequence/ena-bulk-webincli",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ena-webin-cli-bulk-submission-tool\" class=\"anchor\" href=\"#ena-webin-cli-bulk-submission-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eENA Webin-CLI Bulk Submission Tool\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis tool is a wrapper to bulk submit read, un-annotated genome, targeted sequence or taxonomic reference data to the ENA using Webin-CLI.\u003c/p\u003e\n\u003cp\u003eThe tool requires an appropriate metadata spreadsheet which it uses to generate manifest files for the user and validate or submit their submission. The tool does not handle study and sample registration, therefore visit \u003ca href=\"https://ena-docs.readthedocs.io/en/latest/submit/general-guide.html\" rel=\"nofollow\"\u003eENA Submissions Documentation\u003c/a\u003e for more information on this. The documentation also provides information on manifest file fields for your type of submission (which correlate to the headers in the spreadsheet file).\u003c/p\u003e\n\u003cp\u003eAn example template spreadsheet has been provided (example_template_input.txt). This file is a tab-delimited text file, however the script also consumes spreadsheets in native MS Excel formats (e.g. .xslx) or comma-separated (.csv).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h4\u003e\n\u003cp\u003eTo ease in usage, the tool has been containerised using \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e. The only requirement is to have Docker \u003ca href=\"https://docs.docker.com/get-docker/\" rel=\"nofollow\"\u003einstalled\u003c/a\u003e. Once installed, run the following commands to setup:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClone the repository:\n\u003ccode\u003egit clone https://github.com/nadimm-rahman/ena-bulk-webincli.git \u0026amp;\u0026amp; cd ena-bulk-webincli\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eBuild the docker image:\n\u003ccode\u003edocker build --tag ena-bulk-webincli .\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eReady to go! Run the tool using docker using the following command:\n\u003ccode\u003edocker run --rm -v \u0026lt;LOCAL_DATA_DIRECTORY\u0026gt;:/data ena-bulk-webincli -h\u003c/code\u003e (for help)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u0026lt;LOCAL_DATA_DIRECTORY\u0026gt; is recommended to be the directory or parent directory on your machine containing your data files to submit. Below is an example command which would submit read data to the test server:\n\u003ccode\u003edocker run --rm -v pathto/data:/data ena-bulk-webincli -u Webin-XXXX -p XXXX -g reads -s example_template_read.txt -d /data -m submit -t\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eNote: For data files to be submitted, relative file paths in accordance to \u003ccode\u003e\u0026lt;LOCAL_DATA_DIRECTORY\u0026gt;\u003c/code\u003e must be provided within the input spreadsheet.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-other\" class=\"anchor\" href=\"#other\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOther\u003c/h4\u003e\n\u003cp\u003eTo use the tool without Docker:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClone the repository:\n\u003ccode\u003egit clone https://github.com/nadimm-rahman/ena-bulk-webincli.git \u0026amp;\u0026amp; cd ena-bulk-webincli\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eDownload the latest version of \u003ca href=\"https://github.com/enasequence/webin-cli/releases\"\u003eWebin-CLI\u003c/a\u003e installed.\u003c/li\u003e\n\u003cli\u003eDownload tool dependencies listed below.\u003c/li\u003e\n\u003cli\u003eEdit the \u0027Configuration\u0027 section at the top of bulk_webincli.py to include the full path to the Webin-CLI jar file and whether parallel processing should be carried out.\u003c/li\u003e\n\u003cli\u003eRun the tool using \u003ccode\u003epython bulk_webincli.py --help\u003c/code\u003e(for help)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe script accepts full paths to files (to be submitted e.g. fastq/fasta) within the input spreadsheet. To control location of outputs, a specific directory can be provided using the \u003ccode\u003e--directory/-d\u003c/code\u003e parameter, where the folders listed below will be generated.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cp\u003eMandatory arguments include Webin submission account username and password, genetic context and metadata spreadsheet. Note that the \u003ccode\u003e--test/-t\u003c/code\u003e flag can be specified to use Webin test submission services.\u003c/p\u003e\n\u003cp\u003eBy default, the script utilises two additional directories:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u0027manifests\u0027 - which houses all generated manifest files and report files.\u003c/li\u003e\n\u003cli\u003e\u0027submissions\u0027 - housing all validation and submission related reports and files, includes analysis and receipt XMLs of submissions.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h3\u003e\n\u003cp\u003eThe tool runs using \u003ca href=\"https://www.python.org/downloads/\" rel=\"nofollow\"\u003ePython3.6+\u003c/a\u003e and requires installation of \u003ca href=\"https://pandas.pydata.org/\" rel=\"nofollow\"\u003ePython Pandas\u003c/a\u003e and \u003ca href=\"https://joblib.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ejoblib\u003c/a\u003e. This can be installed in a \u003ca href=\"https://docs.python.org/3/tutorial/venv.html\" rel=\"nofollow\"\u003evirtual environment\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1625847484.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "VUIIS/demo-singularity-matlab-fsl",
    "latest_release": "v1.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demo-singularity-container-for-matlab-plus-fsl\" class=\"anchor\" href=\"#demo-singularity-container-for-matlab-plus-fsl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo singularity container for Matlab plus FSL\u003c/h1\u003e\n\u003cp\u003eThis example container takes a Nifti image as input, zeroes out a hole in it of\nthe specified diameter, and saves the result to a new Nifti file. Quick,\npointless, and easy to tell whether it worked right.\u003c/p\u003e\n\u003cp\u003eThis is one way to organize a Matlab-based Singularity container -\nperhaps most easily conceived of as a series of wrappers around the main\ncodebase. Done this way, it\u0027s fairly easy to work on each piece in isolation,\nproblem-solving from the inside out.\u003c/p\u003e\n\u003cp\u003eThis container also includes an installation of FSL, which has a lot of handy\ntools including fsleyes to make the QA PDF. The FSL parts could be removed from\nthe Singularity file if FSL isn\u0027t used, to end up with a smaller container.\nContrariwise, all the Matlab parts could be removed to end up with an FSL-only\ncontainer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSingularity container\n|   Primary entrypoint (shell script)\n|   |   X11 wrapper\n|   |   |   Shell script preprocessing\n|   |   |   Matlab processing (compiled)\n|   |   |   |   Matlab entrypoint\n|   |   |   |       Matlab main function\n|   |   |   \\           Matlab sub-functions / codebase\n\\   \\   \\   Shell script postprocessing\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDependencies in terms of the actual files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSingularity\n    src/pipeline_entrypoint.sh\n        src/pipeline_main.sh\n            src/copy_inputs.sh\n            src/preprocessing.sh\n            matlab/bin/run_matlab_entrypoint.sh\n                matlab/bin/matlab_entrypoint\n                    / matlab/src/matlab_entrypoint.m \\  Used for compilation,\n                    |     matlab/src/matlab_main.m   |  but not at container\n                    \\         matlab/src/*           /  runtime\n            src/postprocessing.sh\n            src/make_pdf.sh\n            src/finalize.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe process of putting it together is described below. The scripts and code in\nthis repository are extensively commented, so if something isn\u0027t clear here,\nit\u0027s probably explained in the Singularity file or the example code.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-this-container-before-editing-anything\" class=\"anchor\" href=\"#building-this-container-before-editing-anything\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding this container before editing anything\u003c/h2\u003e\n\u003cp\u003eTry building this from scratch, to find any immediate issues:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eGet the installers for Matlab Compiled Runtime and FSL and place them in the\n\u003ccode\u003eexternal\u003c/code\u003e directory. URLs for these are in the \u003ccode\u003eSingularity\u003c/code\u003e file. Alternatively,\ncomment out the installer files in the \u0027%files\u0027 section and uncomment the download\nlines (\u0027wget\u0027) later - this way they will be downloaded as part of the build.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the container, following the instructions below\n\u003ca href=\"https://github.com/baxpr/demo-singularity-matlab-fsl#building-the-container\"\u003ehttps://github.com/baxpr/demo-singularity-matlab-fsl#building-the-container\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-matlab-part\" class=\"anchor\" href=\"#matlab-part\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMatlab part\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-write-the-basic-matlab-code\" class=\"anchor\" href=\"#write-the-basic-matlab-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWrite the basic Matlab code\u003c/h3\u003e\n\u003cp\u003eWrite Matlab code that does what\u0027s needed. Put it in \u003ccode\u003ematlab/src\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eA popular toolbox for reading and writing Nifti files that\u0027s available on Matlab\nCentral has a lot of insidious bugs and is not being maintained. Matlab\u0027s own\nfunctions for Nifti files are quite limited. Here is an alternative, which is\nused in this example:\n\u003ca href=\"https://github.com/VUIIS/spm_readwrite_nii\"\u003ehttps://github.com/VUIIS/spm_readwrite_nii\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-write-the-matlab-entrypoint\" class=\"anchor\" href=\"#write-the-matlab-entrypoint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWrite the Matlab entrypoint\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003ematlab/src/matlab_entrypoint.m\u003c/code\u003e exists to take command line arguments, parse\nthem, and call the main code. A convenient way to set things up is to write a\nmain function that takes a structure as its sole input, with the structure\ncontaining whatever inputs are needed. See \u003ccode\u003ematlab/src/matlab_main.m\u003c/code\u003e for an\nexample of this.\u003c/p\u003e\n\u003cp\u003eCouple of things to note in the entrypoint code are the quit/exit sections at\nbeginning and end. The bit at the beginning allows the executable to run during\nthe container build, without actually doing anything - this is needed to extract\nthe CTF archive into the container at the only time the container is writeable\n(h/t \u003ca href=\"https://twitter.com/annash128\" rel=\"nofollow\"\u003ehttps://twitter.com/annash128\u003c/a\u003e for figuring that one out). The bit at the\nend exits matlab when the function is finished. Without it, the running Matlab\nprocess won\u0027t release execution back to the calling script when it\u0027s done.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-test-the-matlab-entrypoint\" class=\"anchor\" href=\"#test-the-matlab-entrypoint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest the Matlab entrypoint\u003c/h3\u003e\n\u003cp\u003eThe script \u003ccode\u003ematlab/src/test_matlab_entrypoint.m\u003c/code\u003e is an example of how to do\nthis. The appropriate Matlab must be installed on the testing computer.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-compile-the-matlab-code\" class=\"anchor\" href=\"#compile-the-matlab-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompile the Matlab code\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003ematlab/compile_matlab.sh\u003c/code\u003e shows how. Many compiled executables are likely to be\ntoo large to store on github. Git LFS may be a solution.\n\u003ca href=\"https://docs.github.com/en/github/managing-large-files/working-with-large-files\"\u003ehttps://docs.github.com/en/github/managing-large-files/working-with-large-files\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-test-the-compiled-matlab-code\" class=\"anchor\" href=\"#test-the-compiled-matlab-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest the compiled Matlab code\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003ematlab/test_compiled_matlab.sh\u003c/code\u003e. The appropriate Matlab Runtime must be\ninstalled on the testing computer.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-shell-script-part\" class=\"anchor\" href=\"#shell-script-part\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eShell script part\u003c/h2\u003e\n\u003cp\u003eAll of the below procedures could be done in the matlab part, if desired,\ninstead of in shell script. If so, parsing inputs should be done following the\nexample in \u003ccode\u003ematlab/src/matlab_entrypoint.m\u003c/code\u003e. But it\u0027s often easier to move\nfiles, create the QA PDF, etc using shell script and FSL. So that\u0027s what we are\ndoing in this example. All this code is in the \u003ccode\u003esrc\u003c/code\u003e directory.\u003c/p\u003e\n\u003cp\u003eAll the shell scripts called from \u003ccode\u003esrc/pipeline_entrypoint.sh\u003c/code\u003e \"know\" the\nenvironment variables that are exported there. This is a very convenient way to\npass along the input arguments, although it isn\u0027t entirely transparent, because\nthere\u0027s no hint in the shell scripts where the variables\u0027 values are coming from\nunless we explain it in the comments.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-main-entrypoint\" class=\"anchor\" href=\"#main-entrypoint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMain entrypoint\u003c/h3\u003e\n\u003cp\u003eThis is \u003ccode\u003esrc/pipeline_entrypoint.sh\u003c/code\u003e. It uses bash to parse the command line\ninputs and export them to environment variables so they\u0027re accessible. Then it\ncalls the primary shell script \u003ccode\u003esrc/pipeline_main.sh\u003c/code\u003e which in turn calls\neverything else. The main script is run in xvfb to provide a virtual display,\noften needed by matlab and required for fsleyes.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-copy-inputs\" class=\"anchor\" href=\"#copy-inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopy inputs\u003c/h3\u003e\n\u003cp\u003eWe copy input files to the output/working directory so we don\u0027t mess them up.\nThis also is an opportunity to rename them to something consistent. It\u0027s very\nconvenient to hard-code the filenames so we don\u0027t have to store and manipulate\nfilenames in environment variables or the like. Also, this makes it easy to\nproduce output files with consistent names - outputs of one pipeline may serve\nas inputs to another, and it\u0027s much easier to manage this if filenames are the\nsame for every run, or at least consistent.\u003c/p\u003e\n\u003cp\u003eWe generally assume the output directory starts out empty and will not be\ninterfered with by any other processes - this is true for XNAT/DAX, but\nimportant to be aware of in other contexts.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-preprocessing\" class=\"anchor\" href=\"#preprocessing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreprocessing\u003c/h3\u003e\n\u003cp\u003eFor this example, there is no preprocessing before the matlab part. But initial\nFSL steps or similar could be put here: \u003ccode\u003esrc/preprocessing.sh\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-postprocessing\" class=\"anchor\" href=\"#postprocessing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePostprocessing\u003c/h3\u003e\n\u003cp\u003eThere isn\u0027t any postprocessing for this example either, but there could be:\n\u003ccode\u003esrc/postprocessing.sh\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pdf-creation\" class=\"anchor\" href=\"#pdf-creation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePDF creation\u003c/h3\u003e\n\u003cp\u003eAll assessors on VUIIS XNAT require a PDF QA report of some sort. For this\nexample, a display of the segmented ROIs overlaid on the T1 is created using\nfsleyes and ImageMagick, \u003ccode\u003esrc/make_pdf.sh\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003ePDF creation can be done in Matlab instead. It\u0027s hard to make these look good.\nAn example with some tricks, including a \u003ccode\u003e.fig\u003c/code\u003e file painstakingly made with\nMatlab\u0027s GUIDE, is\n\u003ca href=\"https://github.com/baxpr/connprep/blob/855dadc/src/connectivity_filter.m#L271\"\u003ehttps://github.com/baxpr/connprep/blob/855dadc/src/connectivity_filter.m#L271\u003c/a\u003e\nA way to show slices of functional images with a nice red/blue colormap is\n\u003ca href=\"https://github.com/baxpr/connprep/blob/855dadc/src/make_network_maps.m\"\u003ehttps://github.com/baxpr/connprep/blob/855dadc/src/make_network_maps.m\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-finalizing-the-output\" class=\"anchor\" href=\"#finalizing-the-output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFinalizing the output\u003c/h3\u003e\n\u003cp\u003eAll Niftis must be compressed for storage on XNAT, and outputs can be organized\nin an easily understandable way: \u003ccode\u003esrc/finalize.sh\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eWrite an informative README - so tedious, yet so helpful. Include the\nappropriate citations for all the methods and software you have used. Even\nessentially write the methods section for a paper that uses the pipeline. Here\u0027s\nan excellent example: \u003ca href=\"https://github.com/MASILab/PreQual\"\u003ehttps://github.com/MASILab/PreQual\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAlternatively, git-ify some documentation like this:\n\u003ca href=\"https://github.com/VUIIS/dax/tree/main/docs\"\u003ehttps://github.com/VUIIS/dax/tree/main/docs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eto get something like this:\n\u003ca href=\"https://dax.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttps://dax.readthedocs.io/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-container\" class=\"anchor\" href=\"#building-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the container\u003c/h2\u003e\n\u003cp\u003eBe sure the Matlab code is newly compiled, see above. You can run\n\u003ccode\u003ematlab/check_for_compilation.sh\u003c/code\u003e first to make sure there\u0027s no source code\nnewer than the compiled executable.\u003c/p\u003e\n\u003cp\u003eThen from the root directory of the working copy of the repo, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build \u0026lt;container_name\u0026gt;.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGood practice: before you build, create a release on github (if using github) or\nat least tag the commit you are about to build. Give the container a versioned\nname like \u003ccode\u003edemo-singularity-matlab-fsl_v1.0.0.simg\u003c/code\u003e that matches the repository\nname and release version/tag.\u003c/p\u003e\n\u003cp\u003eExternal binaries such as Matlab Runtime and FSL can be included by copying\nlocal copies into the container in the Singularity file\u0027s \u003ccode\u003e%files\u003c/code\u003e section. This\ntends to be a little faster when multiple builds are needed during debugging,\nor necessary for files that are not available to download, and this is what\u0027s\nbeing done in the example Singularity file. Alternatively, binaries or install\nfiles can be downloaded from their source at build time - there are some\ncommented-out sections in the Singularity file showing how that is done. (Thanks\n\u003ca href=\"https://github.com/praitayini\"\u003ehttps://github.com/praitayini\u003c/a\u003e for exploring this in detail)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-container\" class=\"anchor\" href=\"#running-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the container\u003c/h2\u003e\n\u003cp\u003eSee \u003ccode\u003etest_singularity_container.sh\u003c/code\u003e for an example run command and some\nimportant info.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h3\u003e\n\u003cp\u003ePaths to files are relative to the container.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--t1_niigz        A T1 image\n--seg_niigz       Its corresponding segmentation from e.g. slant pipeline\n--diameter_mm     Diameter of the hole to zero out, in mm (default 30)\n\n--label_info      A label to annotate the QA PDF, e.g. info from XNAT\n\n--out_dir         Where outputs will be stored (default /OUTPUTS)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003ePDF/holed_image.pdf           QA report\nHOLED_T1/holed_t1.nii.gz      T1 image with a hole in it\nHOLED_SEG/holed_seg.nii.gz    Segmentation with a hole in it\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-container-with-dax\" class=\"anchor\" href=\"#running-the-container-with-dax\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the container with DAX\u003c/h2\u003e\n\u003cp\u003eWith a suitable configuration file, DAX (\u003ca href=\"https://github.com/VUIIS/dax\"\u003ehttps://github.com/VUIIS/dax\u003c/a\u003e) can run this on a cluster.\u003c/p\u003e\n\u003cp\u003eInstructions are here: \u003ca href=\"https://dax.readthedocs.io/en/latest/processors.html\" rel=\"nofollow\"\u003ehttps://dax.readthedocs.io/en/latest/processors.html\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn example is here:\n\u003ca href=\"https://github.com/VUIIS/dax_yaml_processor_examples/blob/master/demo-matfsl_v1.0.0_processor.yaml\"\u003ehttps://github.com/VUIIS/dax_yaml_processor_examples/blob/master/demo-matfsl_v1.0.0_processor.yaml\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626126045.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/conncalc",
    "latest_release": "v1.0.4",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-conncalc\" class=\"anchor\" href=\"#conncalc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econncalc\u003c/h1\u003e\n\u003cp\u003eComputes functional connectivity maps and matrices for a specified set of ROIs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eremovegm_niigz\u003c/code\u003e, \u003ccode\u003ekeepgm_niigz\u003c/code\u003e, \u003ccode\u003emeanfmri_niigz\u003c/code\u003e. Preprocessed fMRI data from\n\u003ca href=\"https://github.com/baxpr/connprep\"\u003econnprep\u003c/a\u003e. This may be supplied in atlas space or\nsubject native space. The first two are 4D time series, the last a single 3D image.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eroi_niigz\u003c/code\u003e.  ROI image. This may be an image existing within the container (e.g. the\nMNI space \u0027AABHHIP_LR.nii.gz\u0027, see src/rois/README.md). Or, it may be any supplied\nimage. In the latter case, \u003ccode\u003eroilabel_csv\u003c/code\u003e must also be supplied; this file must contain\nLabel and Region columns, or may be the STATS output of a slant assessor. The ROI\nimage must be already be aligned with the T1 and the fMRI (though needn\u0027t be sampled to\nthe same voxel grid or field of view) - no coregistration or warp is performed on any\nof the images.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003et1_niigz\u003c/code\u003e. T1 image for the PDF report.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003emask_niigz\u003c/code\u003e. Brain mask - will be binarized and dilated and used to exclude any clearly\nex-brain voxels in the stored connectivity maps. Supply \u0027none\u0027 to mask to the entire\nvolume (i.e. no masking).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003econnmaps_out\u003c/code\u003e. \u0027yes\u0027 or \u0027no\u0027 to choose whether to additionally store voxelwise\nconnectivity images for each ROI in the ROI image.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eResample the ROI image to match the fMRI voxel sampling. It\u0027s assumed both are already\naligned.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExtract mean time series from the supplied fMRI for each ROI in the ROI image.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCompute functional connectivity. The ROI-to-ROI connectivity matrix is computed, and also\nvoxelwise connectivity Z maps if requested.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eR\u003c/code\u003e, the correlation coefficient\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eZ\u003c/code\u003e, the Fisher transformed correlation, \u003ccode\u003eatanh(R) * sqrt(N-3)\u003c/code\u003e where \u003ccode\u003eN\u003c/code\u003e is number of time points\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eVdf\u003c/code\u003e, \u003ccode\u003ePdf\u003c/code\u003e, \u003ccode\u003eZdf\u003c/code\u003e autocorrelation-adjusted connectivity metrics from \u003ca href=\"https://github.com/asoroosh/xDF\"\u003ehttps://github.com/asoroosh/xDF\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGenerate a PDF report and organize outputs for XNAT.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625759471.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for multiqc (https://github.com/ewels/MultiQC)",
    "filenames": [
      "Singularity",
      "Singularity.1.11",
      "Singularity.1.6",
      "Singularity.1.7",
      "Singularity.1.8",
      "Singularity.1.9",
      "Singularity.1.5"
    ],
    "full_name": "powerPlant/multiqc-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the MultiQC tool to aggregate results from bioinformatics analyses across many samples into a single report.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625703839.0
  },
  {
    "data_format": 2,
    "description": "Notebook template using Fink API for the LSST broker workshop",
    "filenames": [
      "Singularity"
    ],
    "full_name": "astrolabsoftware/fink-notebook-template",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fink-broker-tutorials\" class=\"anchor\" href=\"#fink-broker-tutorials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFink broker tutorials\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://colab.research.google.com/github/astrolabsoftware/fink-notebook-template/blob/main\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository contains materials (notebooks \u0026amp; presentation) to explore the \u003ca href=\"https://fink-broker.org\" rel=\"nofollow\"\u003eFink broker\u003c/a\u003e alert data. As of April 2021, Fink has collected more than 80 million alerts from the ZTF public stream, and processed more than 30 millions (after quality cuts). Among these, you will find extragalatic sources (supernovae, AGN, ...), galactic sources (many classes of transients incl. variables stars from our galaxy or gravitational microlensing events, ...) and moving objects from our Solar System (asteroids, comets, and made-man objects like space-debris!). Some sources are already confirmed, many are candidates!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-materials\" class=\"anchor\" href=\"#materials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaterials\u003c/h2\u003e\n\u003cp\u003eThe repository contains a number of notebooks focusing on the use of the Fink REST API. We shortly present different science cases:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExtragalactic science: AGN \u0026amp; supernovae (\u003ca href=\"extragalactic/extragalactic.ipynb\"\u003esee notebook\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eGalactic science: variable stars \u0026amp; microlensing (\u003ca href=\"galactic/galactic.ipynb\"\u003esee notebook\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSolar system science: asteroids, comets \u0026amp; space debris (\u003ca href=\"sso/sso.ipynb\"\u003esee notebook\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eMulti-messenger astronomy: searching for kilonovae (\u003ca href=\"MMA/MMA.ipynb\"\u003esee notebook\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eBroker interfaces: presentation on the livestream service, the Science Portal and its API, and the Fink TOM module (\u003ca href=\"interfaces/README.md\"\u003esee the presentation\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese sciences are not exhaustive and we welcome new collaborations to expand them!\u003c/p\u003e\n\u003cp\u003eYou can try the notebooks using Google Colab (follow the link above). You can also clone the repo, and try it locally (very little external libraries are required).\u003c/p\u003e\n\u003cp\u003eWe also provide a Singularity script to work in a contained environment (thanks @bregeon):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBuild with \u003ccode\u003esingularity build --fakeroot fink.sif Singularity\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun with \u003ccode\u003esingularity run fink.sif\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eOpen the link in your browser (from the host)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-contribute\" class=\"anchor\" href=\"#how-to-contribute\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to contribute\u003c/h2\u003e\n\u003cp\u003eHow to contribute:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClone (or fork) this repo, and open a new branch.\u003c/li\u003e\n\u003cli\u003eCreate a new folder with a meaningful name (e.g. \u003ccode\u003esupernovae\u003c/code\u003e, \u003ccode\u003egrb\u003c/code\u003e, ...)\u003c/li\u003e\n\u003cli\u003eRead and copy an existing notebook to get an idea of the structure of a tutorial.\u003c/li\u003e\n\u003cli\u003eOnce your notebook is finished, open a Pull Request such that we review the tutorial and merge it!\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1625729812.0
  },
  {
    "data_format": 2,
    "description": "A repository for vibration analyses of Allen Human Brain Atlas data",
    "filenames": [
      "container/Singularity"
    ],
    "full_name": "netneurolab/markello_transcriptome",
    "latest_release": "1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-standardizing-workflows-in-imaging-transcriptomics-with-the-abagen-toolbox\" class=\"anchor\" href=\"#standardizing-workflows-in-imaging-transcriptomics-with-the-abagen-toolbox\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStandardizing workflows in imaging transcriptomics with the \u003ccode\u003eabagen\u003c/code\u003e toolbox\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-whats-in-this-repository\" class=\"anchor\" href=\"#whats-in-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"What\u0027s in this repository?\"\u003c/h2\u003e\n\u003cp\u003eThis repository contains data, code, and results for the manuscript \"Standardizing workflows in imaging transcriptomics with the \u003ccode\u003eabagen\u003c/code\u003e toolbox\" by Markello et al. \u003cem\u003eBiorxiv\u003c/em\u003e, 2021.\nWe investigate how variability in processing of the Allen Human Brain Atlas impacts analyses relating gene expression to neuroimaging data and highlight how functionality from the \u003ca href=\"https://github.com/rmarkello/abagen\"\u003e\u003ccode\u003eabagen\u003c/code\u003e\u003c/a\u003e toolbox can help to standardize these workflows.\u003c/p\u003e\n\u003cp\u003eWe\u0027ve tried to document the various aspects of this repository with a whole bunch of README files, so feel free to jump around and check things out.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-just-let-me-run-the-things\" class=\"anchor\" href=\"#just-let-me-run-the-things\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"Just let me run the things!\"\u003c/h2\u003e\n\u003cp\u003eItching to just run the analyses?\nYou\u0027ll need to make sure you have installed the appropriate software packages, have access to the HCP, and have downloaded the appropriate data files (check out our \u003ca href=\"https://netneurolab.github.io/markello_transcriptome\" rel=\"nofollow\"\u003ewalkthrough\u003c/a\u003e for more details!).\nOnce you\u0027ve done that, you can get going with the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/netneurolab/markello_transcriptome\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e markello_transcriptome\nconda env create -f environment.yml\nconda activate markello_transcriptome\npip install vibecheck/\nmake all\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you don\u0027t want to deal with the hassle of creating a new Python environment you can create a Singularity image run things in there:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/netneurolab/markello_transcriptome\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e markello_transcriptome\nbash container/gen_simg.sh\nsingularity run container/markello_transcriptome.simg make all\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNote, however, that \u003cstrong\u003ewe don\u0027t recommend re-running our analyses in this manner\u003c/strong\u003e as it will take a \u003cem\u003every\u003c/em\u003e long time to do so!\nInstead, we refer to our \u003ca href=\"https://netneurolab.github.io/markello_transcriptome\" rel=\"nofollow\"\u003ewalkthrough\u003c/a\u003e for more information on the optimal way to reproduce our results.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-id-like-more-information\" class=\"anchor\" href=\"#id-like-more-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"I\u0027d like more information.\"\u003c/h2\u003e\n\u003cp\u003eIf you want a step-by-step through all the methods + analyses take a look at our \u003ca href=\"https://netneurolab.github.io/markello_transcriptome\" rel=\"nofollow\"\u003ewalkthrough\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-i-have-some-questions\" class=\"anchor\" href=\"#i-have-some-questions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"I have some questions...\"\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/netneurolab/markello_transcriptome/issues\"\u003eOpen an issue\u003c/a\u003e on this repository and someone will try and get back to you as soon as possible!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625757987.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "3.14.1/Singularity"
    ],
    "full_name": "pscedu/singularity-spades",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demo-singularity-container-with-spm12-and-freeview\" class=\"anchor\" href=\"#demo-singularity-container-with-spm12-and-freeview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo singularity container with SPM12 and Freeview\u003c/h1\u003e\n\u003cp\u003eSPM12-based pipelines require a little extra work to get them compiled and working in a\ncontainer. Freesurfer\u0027s Freeview is also included here, as it\u0027s very handy for creating\nthe PDF QA report. This example shows three different ways of creating image displays for\nthe QA PDF.\u003c/p\u003e\n\u003cp\u003eSee \u003ca href=\"https://github.com/baxpr/demo-singularity-matlab-fsl\"\u003ehttps://github.com/baxpr/demo-singularity-matlab-fsl\u003c/a\u003e for a lot of detailed info about\nputting Matlab code into singularity containers. This example uses the same structure.\u003c/p\u003e\n\u003cp\u003eA licensed Matlab installation is required to compile the Matlab code, but is not needed\nto run the compiled executable in the container.\u003c/p\u003e\n\u003cp\u003eSPM12 (\u003ca href=\"https://www.fil.ion.ucl.ac.uk/spm/software/spm12/\" rel=\"nofollow\"\u003ehttps://www.fil.ion.ucl.ac.uk/spm/software/spm12/\u003c/a\u003e) is not in this repository and must\nbe installed separately on the compilation host. Edit \u003ccode\u003ematlab/compile_matlab.sh\u003c/code\u003e to point\nto it.\u003c/p\u003e\n\u003cp\u003eSPM requires jumping an extra hurdle at the compilation step - we use a modified version\nof SPM\u0027s own compiler function \u003ccode\u003espm_make_standalone.m\u003c/code\u003e, found at\n\u003ccode\u003ematlab/spm_make_standalone_local.m\u003c/code\u003e. This process captures a lot of dependencies that\ncould otherwise easily be left out of the executable, with the resulting symptom that it\ncompiles just fine but fails at run time with various cryptic error messages. In addition\nto SPM12, everything in the \u003ccode\u003ematlab/src\u003c/code\u003e directory is included in the path at compile time.\nIf Matlab toolboxes are used, they will need to be added to the list of included toolboxes\nin \u003ccode\u003ematlab/spm_make_standalone_local.m\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe compiled Matlab executable is stored on github using git LFS. A regular git clone will\ndownload a pointer text file instead of the executable binary. The result of building a\ncontainer from that will be a cryptic error message - so, compile it yourself. Or, if\nstoring on github, download it manually and replace the pointer text file, or include this\nstep in the Singularity file if helpful - example here:\n\u003ca href=\"https://github.com/baxpr/gf-fmri/blob/47b0552/Singularity.v1.3.4#L65\"\u003ehttps://github.com/baxpr/gf-fmri/blob/47b0552/Singularity.v1.3.4#L65\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFreesurfer requires a license to run:\n\u003ca href=\"https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License\" rel=\"nofollow\"\u003ehttps://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License\u003c/a\u003e\nBest practice is to store your license file on the host that will run the container, and\nbind it to the container at runtime - NOT to include your own license file in the\ncontainer itself.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1625771764.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "VUIIS/demo-singularity-spm-freeview",
    "latest_release": "v1.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demo-singularity-container-with-spm12-and-freeview\" class=\"anchor\" href=\"#demo-singularity-container-with-spm12-and-freeview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo singularity container with SPM12 and Freeview\u003c/h1\u003e\n\u003cp\u003eSPM12-based pipelines require a little extra work to get them compiled and working in a\ncontainer. Freesurfer\u0027s Freeview is also included here, as it\u0027s very handy for creating\nthe PDF QA report. This example shows three different ways of creating image displays for\nthe QA PDF.\u003c/p\u003e\n\u003cp\u003eSee \u003ca href=\"https://github.com/baxpr/demo-singularity-matlab-fsl\"\u003ehttps://github.com/baxpr/demo-singularity-matlab-fsl\u003c/a\u003e for a lot of detailed info about\nputting Matlab code into singularity containers. This example uses the same structure.\u003c/p\u003e\n\u003cp\u003eA licensed Matlab installation is required to compile the Matlab code, but is not needed\nto run the compiled executable in the container.\u003c/p\u003e\n\u003cp\u003eSPM12 (\u003ca href=\"https://www.fil.ion.ucl.ac.uk/spm/software/spm12/\" rel=\"nofollow\"\u003ehttps://www.fil.ion.ucl.ac.uk/spm/software/spm12/\u003c/a\u003e) is not in this repository and must\nbe installed separately on the compilation host. Edit \u003ccode\u003ematlab/compile_matlab.sh\u003c/code\u003e to point\nto it.\u003c/p\u003e\n\u003cp\u003eSPM requires jumping an extra hurdle at the compilation step - we use a modified version\nof SPM\u0027s own compiler function \u003ccode\u003espm_make_standalone.m\u003c/code\u003e, found at\n\u003ccode\u003ematlab/spm_make_standalone_local.m\u003c/code\u003e. This process captures a lot of dependencies that\ncould otherwise easily be left out of the executable, with the resulting symptom that it\ncompiles just fine but fails at run time with various cryptic error messages. In addition\nto SPM12, everything in the \u003ccode\u003ematlab/src\u003c/code\u003e directory is included in the path at compile time.\nIf Matlab toolboxes are used, they will need to be added to the list of included toolboxes\nin \u003ccode\u003ematlab/spm_make_standalone_local.m\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe compiled Matlab executable is stored on github using git LFS. A regular git clone will\ndownload a pointer text file instead of the executable binary. The result of building a\ncontainer from that will be a cryptic error message - so, compile it yourself. Or, if\nstoring on github, download it manually and replace the pointer text file, or include this\nstep in the Singularity file if helpful - example here:\n\u003ca href=\"https://github.com/baxpr/gf-fmri/blob/47b0552/Singularity.v1.3.4#L65\"\u003ehttps://github.com/baxpr/gf-fmri/blob/47b0552/Singularity.v1.3.4#L65\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFreesurfer requires a license to run:\n\u003ca href=\"https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License\" rel=\"nofollow\"\u003ehttps://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License\u003c/a\u003e\nBest practice is to store your license file on the host that will run the container, and\nbind it to the container at runtime - NOT to include your own license file in the\ncontainer itself.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625837739.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ddbj/singularity-apache2-igvwebapp",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-apache2-igvwebapp\" class=\"anchor\" href=\"#singularity-apache2-igvwebapp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-apache2-igvwebapp\u003c/h1\u003e\n\u003cp\u003eigv-webapp\u3068apache2\u3092\u5b9f\u884c\u3059\u308bsingularity instance\u3092\u8d77\u52d5\u3059\u308b\u305f\u3081\u306e\u30d5\u30a1\u30a4\u30eb\u4e00\u5f0f\u3067\u3059\u3002\nsingularity image\u306f\u521d\u56de\u8d77\u52d5\u6642\u306bSylabs Cloud\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u521d\u671f\u8a2d\u5b9a\" class=\"anchor\" href=\"#%E5%88%9D%E6%9C%9F%E8%A8%AD%E5%AE%9A\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u521d\u671f\u8a2d\u5b9a\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-httpdconf\" class=\"anchor\" href=\"#httpdconf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehttpd.conf\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003eServerRoot \"/usr/local/apache2\"\n\nListen 38080\nUser user1\nGroup group1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003euser1\u3092\u81ea\u5206\u306e\u30a2\u30ab\u30a6\u30f3\u30c8\u540d\u3001group1\u3092\u81ea\u5206\u306e\u30b0\u30eb\u30fc\u30d7\u540d\u300138080\u3092apache2\u304c\u4f7f\u7528\u3059\u308b\u30dd\u30fc\u30c8\u756a\u53f7\u306b\u4fee\u6b63\u3057\u307e\u3059\u3002\nnetstat\u30b3\u30de\u30f3\u30c9\u306738080\u304c\u672a\u4f7f\u7528\u306a\u3089\u5909\u66f4\u4e0d\u8981\u3067\u3059\u3002\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-packagejson\" class=\"anchor\" href=\"#packagejson\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epackage.json\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e{\n  \"name\": \"igv-webapp\",\n  \"version\": \"1.5.5\",\n  \"description\": \"igv web app\",\n  \"keywords\": [],\n  \"author\": \"Douglass Turner and Jim Robinson\",\n  \"license\": \"MIT\",\n  \"scripts\": {\n    \"start\": \"npx http-server -p 38081 dist\",\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e38081\u3092igv-webapp\u304c\u4f7f\u7528\u3059\u308b\u30dd\u30fc\u30c8\u756a\u53f7\u306b\u4fee\u6b63\u3057\u307e\u3059\u3002\nnetstat\u30b3\u30de\u30f3\u30c9\u306738081\u304c\u672a\u4f7f\u7528\u306a\u3089\u5909\u66f4\u4e0d\u8981\u3067\u3059\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-instance\u306e\u8d77\u52d5\" class=\"anchor\" href=\"#singularity-instance%E3%81%AE%E8%B5%B7%E5%8B%95\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity instance\u306e\u8d77\u52d5\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e$ bash start_container.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u521d\u56de\u5b9f\u884c\u6642\u306b\u3001ubuntu-18.04-apache-2.4.48-igv-webapp-1.5.5_latest.sif \u304c\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u307e\u3059\u3002\n\u307e\u305f\u3001cgi-bin, htdocs, logs\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u304c\u4f5c\u6210\u3055\u308c\u307e\u3059\u3002\nhtdocs\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u3001igv-webapp\u3067\u8868\u793a\u3057\u305f\u3044bam\u30d5\u30a1\u30a4\u30eb\u3068\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30d5\u30a1\u30a4\u30eb\u3092\u914d\u7f6e\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-igv-webapp\u3078\u306e\u30a2\u30af\u30bb\u30b9\" class=\"anchor\" href=\"#igv-webapp%E3%81%B8%E3%81%AE%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eigv-webapp\u3078\u306e\u30a2\u30af\u30bb\u30b9\u003c/h2\u003e\n\u003cp\u003e\u30a6\u30a7\u30d6\u30d6\u30e9\u30a6\u30b6\u3067 http://\u0026lt;\u30db\u30b9\u30c8\u306eIP\u30a2\u30c9\u30ec\u30b9\u0026gt;:\u0026lt;package.json\u306b\u8a2d\u5b9a\u3057\u305f\u30dd\u30fc\u30c8\u756a\u53f7\u0026gt; \u306b\u30a2\u30af\u30bb\u30b9\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\u30c8\u30e9\u30c3\u30af\u306e\u8ffd\u52a0\u306f\u3001Tracks\u30e1\u30cb\u30e5\u30fc\u304b\u3089URL\u3092\u9078\u3073\u3001\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ehttp://\u0026lt;\u30db\u30b9\u30c8\u306eID\u30a2\u30c9\u30ec\u30b9\u0026gt;:\u0026lt;httpd.conf\u306b\u8a2d\u5b9a\u3057\u305f\u30dd\u30fc\u30c8\u756a\u53f7\u0026gt;/\u0026lt;htdocs\u306b\u914d\u7f6e\u3057\u305fbam\u30d5\u30a1\u30a4\u30eb\u0026gt;\u003c/li\u003e\n\u003cli\u003ehttp://\u0026lt;\u30db\u30b9\u30c8\u306eID\u30a2\u30c9\u30ec\u30b9\u0026gt;:\u0026lt;httpd.conf\u306b\u8a2d\u5b9a\u3057\u305f\u30dd\u30fc\u30c8\u756a\u53f7\u0026gt;/\u0026lt;htdocs\u306b\u914d\u7f6e\u3057\u305f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u30d5\u30a1\u30a4\u30eb\u0026gt;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u3092\u958b\u3044\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1625812484.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "misc/releases/19.12/Singularity.19.12",
      "misc/releases/20.06/Singularity.20.06",
      "misc/releases/19.06/Singularity.19.06",
      "misc/releases/latest/Singularity"
    ],
    "full_name": "No-Diehl/FD-SAT",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast Downward\u003c/h1\u003e\n\u003cp\u003eFast Downward is a domain-independent classical planning system.\u003c/p\u003e\n\u003cp\u003eCopyright 2003-2020 Fast Downward contributors (see below).\u003c/p\u003e\n\u003cp\u003eFor further information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Downward website: \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReport a bug or file an issue: \u003ca href=\"http://issues.fast-downward.org\" rel=\"nofollow\"\u003ehttp://issues.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward mailing list: \u003ca href=\"https://groups.google.com/forum/#!forum/fast-downward\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/fast-downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward main repository: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003eThe following list includes all people that actively contributed to\nFast Downward, i.e. all people that appear in some commits in Fast\nDownward\u0027s history (see below for a history on how Fast Downward\nemerged) or people that influenced the development of such commits.\nCurrently, this list is sorted by the last year the person has been\nactive, and in case of ties, by the earliest year the person started\ncontributing, and finally by last name.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2003-2020 Malte Helmert\u003c/li\u003e\n\u003cli\u003e2008-2016, 2018-2020 Gabriele Roeger\u003c/li\u003e\n\u003cli\u003e2010-2020 Jendrik Seipp\u003c/li\u003e\n\u003cli\u003e2010-2011, 2013-2020 Silvan Sievers\u003c/li\u003e\n\u003cli\u003e2012-2020 Florian Pommerening\u003c/li\u003e\n\u003cli\u003e2013, 2015-2020 Salome Eriksson\u003c/li\u003e\n\u003cli\u003e2016-2020 Cedric Geissmann\u003c/li\u003e\n\u003cli\u003e2017-2020 Guillem Franc\u00e8s\u003c/li\u003e\n\u003cli\u003e2018-2020 Augusto B. Corr\u00eaa\u003c/li\u003e\n\u003cli\u003e2018-2020 Patrick Ferber\u003c/li\u003e\n\u003cli\u003e2015-2019 Manuel Heusner\u003c/li\u003e\n\u003cli\u003e2017 Daniel Killenberger\u003c/li\u003e\n\u003cli\u003e2016 Yusra Alkhazraji\u003c/li\u003e\n\u003cli\u003e2016 Martin Wehrle\u003c/li\u003e\n\u003cli\u003e2014-2015 Patrick von Reth\u003c/li\u003e\n\u003cli\u003e2015 Thomas Keller\u003c/li\u003e\n\u003cli\u003e2009-2014 Erez Karpas\u003c/li\u003e\n\u003cli\u003e2014 Robert P. Goldman\u003c/li\u003e\n\u003cli\u003e2010-2012 Andrew Coles\u003c/li\u003e\n\u003cli\u003e2010, 2012 Patrik Haslum\u003c/li\u003e\n\u003cli\u003e2003-2011 Silvia Richter\u003c/li\u003e\n\u003cli\u003e2009-2011 Emil Keyder\u003c/li\u003e\n\u003cli\u003e2010-2011 Moritz Gronbach\u003c/li\u003e\n\u003cli\u003e2010-2011 Manuela Ortlieb\u003c/li\u003e\n\u003cli\u003e2011 Vidal Alc\u00e1zar Saiz\u003c/li\u003e\n\u003cli\u003e2011 Michael Katz\u003c/li\u003e\n\u003cli\u003e2011 Raz Nissim\u003c/li\u003e\n\u003cli\u003e2010 Moritz Goebelbecker\u003c/li\u003e\n\u003cli\u003e2007-2009 Matthias Westphal\u003c/li\u003e\n\u003cli\u003e2009 Christian Muise\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cp\u003eThe current version of Fast Downward is the merger of three different\nprojects:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe original version of Fast Downward developed by Malte Helmert\nand Silvia Richter\u003c/li\u003e\n\u003cli\u003eLAMA, developed by Silvia Richter and Matthias Westphal based on\nthe original Fast Downward\u003c/li\u003e\n\u003cli\u003eFD-Tech, a modified version of Fast Downward developed by Erez\nKarpas and Michael Katz based on the original code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition to these three main sources, the codebase incorporates\ncode and features from numerous branches of the Fast Downward codebase\ndeveloped for various research papers. The main contributors to these\nbranches are Malte Helmert, Gabi R\u00f6ger and Silvia Richter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe following directory is not part of Fast Downward as covered by\nthis license:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or (at\nyour option) any later version.\n\nFast Downward is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see \u0026lt;https://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625821718.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.0.2.1",
      "Singularity.0.1-alpha",
      "Singularity.0.2.2",
      "Singularity.0.2.0"
    ],
    "full_name": "dcgc-bfx/singularity-single-cell",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/dcgc-bfx/dcgc-single-cell/workflows/Build/badge.svg?branch=main\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/dcgc-bfx/dcgc-single-cell/workflows/Build/badge.svg?branch=main\" alt=\"Build\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5095\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dcgc-single-cell\" class=\"anchor\" href=\"#dcgc-single-cell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edcgc-single-cell\u003c/h1\u003e\n\u003cp\u003eDCGC singularity recipe for single cell analysis\u003c/p\u003e\n\u003cp\u003ePull it from the singularity hub: \u003ca href=\"https://singularity-hub.org/collections/5095\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/5095\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStart jupyter lab:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --writable-tmpfs --app jupyter shub://dcgc-bfx/dcgc-single-cell\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eStart rstudio server listening on port 8787:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --writable-tmpfs --app rserver shub://dcgc-bfx/dcgc-single-cell 8787\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625833022.0
  },
  {
    "data_format": 2,
    "description": "VisiData is an interactive multitool for tabular data. ",
    "filenames": [
      "2.4/Singularity"
    ],
    "full_name": "pscedu/singularity-visidata",
    "latest_release": "v2.4",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-visidata/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-visidata/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/6ba46b521c105dbb136921a99a18f1022bf5604dd8ef9b53db8c5398035fb61e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7669736964617461\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6ba46b521c105dbb136921a99a18f1022bf5604dd8ef9b53db8c5398035fb61e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7669736964617461\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-visidata\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/645b79536f621cdb897b966961372b4220c39325d20d980e50e60fd35d1f8c55/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7669736964617461\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/645b79536f621cdb897b966961372b4220c39325d20d980e50e60fd35d1f8c55/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7669736964617461\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-visidata\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f9018f245b1a0ba49029b6900d246ef55f947fa6eac7ba46beb911cf59177fc7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7669736964617461\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f9018f245b1a0ba49029b6900d246ef55f947fa6eac7ba46beb911cf59177fc7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7669736964617461\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-visidata\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/680760b923c357ae8d468be7c27aa97b243c97d0624fa391a2f1135db4dc0214/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7669736964617461\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/680760b923c357ae8d468be7c27aa97b243c97d0624fa391a2f1135db4dc0214/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7669736964617461\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-visidata\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-visidata\" class=\"anchor\" href=\"#singularity-visidata\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-visidata\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://www.visidata.org/\" rel=\"nofollow\"\u003evisidata\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003evd\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/visidata/2.4\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/visidata\u003c/code\u003e as \u003ccode\u003e2.4.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626982895.0
  },
  {
    "data_format": 2,
    "description": "FastANI is developed for fast alignment-free computation of whole-genome Average Nucleotide Identity (ANI)",
    "filenames": [
      "1.33/Singularity"
    ],
    "full_name": "pscedu/singularity-fastani",
    "latest_release": "v1.3.3",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-fastani/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-fastani/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/8dc34ff6e6f588edb478bbdd040070223d6309ed57ff692ec669a99a4ce3c044/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d66617374616e69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8dc34ff6e6f588edb478bbdd040070223d6309ed57ff692ec669a99a4ce3c044/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d66617374616e69\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-fastani\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/eee7b443f78bf774c4336377ac5dee69d66644752fb8ba1f4c38931628d44fb4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d66617374616e69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/eee7b443f78bf774c4336377ac5dee69d66644752fb8ba1f4c38931628d44fb4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d66617374616e69\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-fastani\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/27c0f180ef8bc2d2094b529129fd31169bf0e10b3a965f25fb6ef0d8b8311868/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d66617374616e69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/27c0f180ef8bc2d2094b529129fd31169bf0e10b3a965f25fb6ef0d8b8311868/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d66617374616e69\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-fastani\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/86bc49a261c15fbea5000ea905a561248a21175c8d281a5949cf47e66c9eae71/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d66617374616e69\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/86bc49a261c15fbea5000ea905a561248a21175c8d281a5949cf47e66c9eae71/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d66617374616e69\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-fastani\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-fastani\" class=\"anchor\" href=\"#singularity-fastani\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-fastani\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"github.com/parbliss/fastani\"\u003efastANI\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003efastANI\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/fastANI/1.33\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/fastANI\u003c/code\u003e as \u003ccode\u003e1.33.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626981460.0
  },
  {
    "data_format": 2,
    "description": "PHYLIP is a free package of programs for inferring phylogenies.",
    "filenames": [
      "3.697/Singularity"
    ],
    "full_name": "pscedu/singularity-phylip-suite",
    "latest_release": "v3.697",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-phylip-suite/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-phylip-suite/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/c170f91fccaa5cf129589585cae304316c06a6c4926ef489d6ae6cec8639c69d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c170f91fccaa5cf129589585cae304316c06a6c4926ef489d6ae6cec8639c69d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-phylip-suite\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5e02750234489a3b6681aab63cc8c7dee07d7b579324265bc6a45b0d56dad6d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5e02750234489a3b6681aab63cc8c7dee07d7b579324265bc6a45b0d56dad6d6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-phylip-suite\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a77ca9fdcc58325c1ddfe94710d8ad36e21b307dbe40570153f1e80be6cac559/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a77ca9fdcc58325c1ddfe94710d8ad36e21b307dbe40570153f1e80be6cac559/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-phylip-suite\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/50a0a1f8e6d29153411b82f9caaa4f006a72cf5b55af95c617d808d70a1588b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/50a0a1f8e6d29153411b82f9caaa4f006a72cf5b55af95c617d808d70a1588b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7068796c69702d7375697465\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-phylip-suite\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-phylip-suite\" class=\"anchor\" href=\"#singularity-phylip-suite\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-phylip-suite\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/b55efde725f14299bbb17335a6c6f17cde58bd9f451a128b97c0cfb8fe5b4edc/68747470733a2f2f65766f6c7574696f6e2e67656e65746963732e77617368696e67746f6e2e6564752f7068796c69702e676966\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b55efde725f14299bbb17335a6c6f17cde58bd9f451a128b97c0cfb8fe5b4edc/68747470733a2f2f65766f6c7574696f6e2e67656e65746963732e77617368696e67746f6e2e6564752f7068796c69702e676966\" alt=\"Logo\" data-canonical-src=\"https://evolution.genetics.washington.edu/phylip.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://evolution.genetics.washington.edu/phylip.html\" rel=\"nofollow\"\u003ePHYLIP\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/phylip-suite/3.697\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/phylip-suite\u003c/code\u003e as \u003ccode\u003e3.697.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "bioinformatics",
      "singularity"
    ],
    "updated_at": 1625845341.0
  },
  {
    "data_format": 2,
    "description": "BLAST finds regions of similarity between biological sequences.",
    "filenames": [
      "2.11.0/Singularity",
      "2.9.0/Singularity"
    ],
    "full_name": "pscedu/singularity-blast",
    "latest_release": "v2.11.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-blast/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-blast/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/34775a544d028af1a76f53887556e0b47d8a40289aa78e79056c5e13dcbc48e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626c617374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/34775a544d028af1a76f53887556e0b47d8a40289aa78e79056c5e13dcbc48e4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626c617374\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-blast\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e33eb8d62dc7df0e7a911833138fefaed18e36044c13f7a3aa53c104c1ffc719/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626c617374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e33eb8d62dc7df0e7a911833138fefaed18e36044c13f7a3aa53c104c1ffc719/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626c617374\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-blast\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/c141231c49671d4a45e13d6d79f61b30ec62be1462b950fac124cfb21cc2d206/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626c617374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c141231c49671d4a45e13d6d79f61b30ec62be1462b950fac124cfb21cc2d206/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626c617374\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-blast\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0ec9871992e72eb86a829ac86878026892749bddbb4623030eb745093184def6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626c617374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0ec9871992e72eb86a829ac86878026892749bddbb4623030eb745093184def6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626c617374\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-blast\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-blast\" class=\"anchor\" href=\"#singularity-blast\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-blast\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web\u0026amp;PAGE_TYPE=BlastDocs\u0026amp;DOC_TYPE=Download\" rel=\"nofollow\"\u003eBLAST\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the other scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/blast/2.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/blast\u003c/code\u003e as \u003ccode\u003e2.11.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "bioinformatics",
      "singularity"
    ],
    "updated_at": 1626286759.0
  },
  {
    "data_format": 2,
    "description": "bowtie2 is an ultrafast and memory-efficient tool for aligning sequencing reads to long reference sequences.",
    "filenames": [
      "2.4.2/Singularity",
      "2.2.5/Singularity",
      "2.4.4/Singularity",
      "2.4.1/Singularity"
    ],
    "full_name": "pscedu/singularity-bowtie2",
    "latest_release": "v2.4.4",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bowtie2/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bowtie2/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/78e679d7d4d533686856a3aabf05836e6e4c4332ede5b3be04e448bcb0af367d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626f7774696532\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/78e679d7d4d533686856a3aabf05836e6e4c4332ede5b3be04e448bcb0af367d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626f7774696532\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bowtie2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/1de77fb074acfa00a23f3b57ec7ee2899825e96179ba08e9726d43d479a8d305/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626f7774696532\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1de77fb074acfa00a23f3b57ec7ee2899825e96179ba08e9726d43d479a8d305/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626f7774696532\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bowtie2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/022a36667f17e7d4a64d83341d6f9b3ef4e8e2ca7bbb26a5e59ea5d45b8bd4ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626f7774696532\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/022a36667f17e7d4a64d83341d6f9b3ef4e8e2ca7bbb26a5e59ea5d45b8bd4ca/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626f7774696532\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bowtie2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/6a118105cb6b5b857541ef5a71ff99144f58396c20ec65334f65aef06d79ae43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626f7774696532\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6a118105cb6b5b857541ef5a71ff99144f58396c20ec65334f65aef06d79ae43/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626f7774696532\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bowtie2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bowtie2\" class=\"anchor\" href=\"#singularity-bowtie2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bowtie2\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/bowtie2\"\u003ebowtie2\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the Perl scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bowtie2/2.4.4\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/bowtie2\u003c/code\u003e as \u003ccode\u003e2.4.4.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "bioinformatics",
      "singularity"
    ],
    "updated_at": 1625845488.0
  },
  {
    "data_format": 2,
    "description": "Picard is a set of command line tools for manipulating high-throughput sequencing (HTS) data and formats such as SAM/BAM/CRAM and VCF. ",
    "filenames": [
      "2.23.2/Singularity"
    ],
    "full_name": "pscedu/singularity-picard",
    "latest_release": "v2.23.2",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-picard/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-picard/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f2ee028725767bd1588c30ee90365dddfc357c08ce7b5a43ed492ec5987e19f9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d706963617264\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f2ee028725767bd1588c30ee90365dddfc357c08ce7b5a43ed492ec5987e19f9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d706963617264\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-picard\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0011e731d3015546848fd1f5982cbad69352604dc45cd908e9ff27c2773e8107/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d706963617264\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0011e731d3015546848fd1f5982cbad69352604dc45cd908e9ff27c2773e8107/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d706963617264\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-picard\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a19ffb10b86582f80f0dd57f7696abb065ebaaab0d440703423ea0f2441278ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d706963617264\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a19ffb10b86582f80f0dd57f7696abb065ebaaab0d440703423ea0f2441278ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d706963617264\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-picard\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/b6afcdc9a6fce9707e6a449e49036b6136dd0335325684f4c8605d441b992e1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d706963617264\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b6afcdc9a6fce9707e6a449e49036b6136dd0335325684f4c8605d441b992e1f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d706963617264\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-picard\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-picard\" class=\"anchor\" href=\"#singularity-picard\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-picard\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/PIGER\"\u003ePicard\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the Perl scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/picard/2.23.2\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/picard\u003c/code\u003e as \u003ccode\u003e2.23.2.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "bioinformatics",
      "singularity"
    ],
    "updated_at": 1625845531.0
  },
  {
    "data_format": 2,
    "description": "BWA is a program for aligning sequencing reads against a large reference genome (e.g. human genome). ",
    "filenames": [
      "0.7.3a/Singularity"
    ],
    "full_name": "pscedu/singularity-bwa",
    "latest_release": "v0.7.3a",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bwa/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bwa/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/dbf644fd78a6a349b822d2b6db36a3f1ab2e4155f5d67671d27202073ac6cb6a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d627761\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/dbf644fd78a6a349b822d2b6db36a3f1ab2e4155f5d67671d27202073ac6cb6a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d627761\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bwa\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0f4fa8203031531a1e542e55475a3668dbdc3d10e36a3ce505f5f098b465b1ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d627761\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0f4fa8203031531a1e542e55475a3668dbdc3d10e36a3ce505f5f098b465b1ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d627761\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bwa\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5c10a09139792e339dfd18dbbab5079f2bf2027194d2dcfc6796e85d1bab3b88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d627761\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c10a09139792e339dfd18dbbab5079f2bf2027194d2dcfc6796e85d1bab3b88/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d627761\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bwa\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a1977c9c08b068aecc0940ff8e8665b3e38fd423b0e217273d28b8ada424e70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d627761\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1977c9c08b068aecc0940ff8e8665b3e38fd423b0e217273d28b8ada424e70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d627761\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bwa\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bwa\" class=\"anchor\" href=\"#singularity-bwa\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bwa\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/Bwa\"\u003ebwa\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the Perl scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bwa/0.7.3a\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/bwa\u003c/code\u003e as \u003ccode\u003e0.7.3a.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "bioinformatics",
      "singularity"
    ],
    "updated_at": 1625845512.0
  },
  {
    "data_format": 2,
    "description": "The GATK is the industry standard for identifying SNPs and indels in germline DNA and RNAseq data. ",
    "filenames": [
      "4.1.9.0/Singularity"
    ],
    "full_name": "pscedu/singularity-gatk",
    "latest_release": "v4.1.9.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-gatk/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-gatk/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/93c4f349e621f15ffd8933b4c7d4ea8eea7b6f9156528a6b483b1b47d8064a91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6761746b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/93c4f349e621f15ffd8933b4c7d4ea8eea7b6f9156528a6b483b1b47d8064a91/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6761746b\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-gatk\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/744613aa898037bbfe237a7943e118e4fe72355964b8726f785c33e44de131e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6761746b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/744613aa898037bbfe237a7943e118e4fe72355964b8726f785c33e44de131e0/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6761746b\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-gatk\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5d9f7edad1535dfc343a82ee05a1cee751f4185de5e88e9959f1e306baf3af56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6761746b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5d9f7edad1535dfc343a82ee05a1cee751f4185de5e88e9959f1e306baf3af56/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6761746b\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-gatk\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/936434bf1d55721ba57e95e4bb99cfb8b78d330106b7ffebafb8911c75556071/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6761746b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/936434bf1d55721ba57e95e4bb99cfb8b78d330106b7ffebafb8911c75556071/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6761746b\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-gatk\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-gatk\" class=\"anchor\" href=\"#singularity-gatk\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-gatk\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/f0a6e98fde6f4e7dd338612de8a154b1169c4572acc8ca3ffed117a81e28d4be/68747470733a2f2f7468656d652e7a646173736574732e636f6d2f7468656d655f6173736574732f323337383336302f646630383566313534333231666161633931353964646135376635303130336238376134663734332e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f0a6e98fde6f4e7dd338612de8a154b1169c4572acc8ca3ffed117a81e28d4be/68747470733a2f2f7468656d652e7a646173736574732e636f6d2f7468656d655f6173736574732f323337383336302f646630383566313534333231666161633931353964646135376635303130336238376134663734332e706e67\" alt=\"Logo\" data-canonical-src=\"https://theme.zdassets.com/theme_assets/2378360/df085f154321faac9159dda57f50103b87a4f743.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\nSingularity recipe for \u003ca href=\"https://gatk.broadinstitute.org/hc/en-us\" rel=\"nofollow\"\u003eGATK\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003egatk\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/gatk/4.1.9.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/gatk\u003c/code\u003e as \u003ccode\u003e4.1.9.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1625845594.0
  },
  {
    "data_format": 2,
    "description": "bcftools \u2014 utilities for variant calling and manipulating VCFs and BCFs.",
    "filenames": [
      "1.10.2/Singularity"
    ],
    "full_name": "pscedu/singularity-bcftools",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bcftools/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bcftools/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/21cb9e2660fdd990c04bf4bc7515a991b3ddb05aebecd953555f0ead24b1d3c2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626366746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/21cb9e2660fdd990c04bf4bc7515a991b3ddb05aebecd953555f0ead24b1d3c2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626366746f6f6c73\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bcftools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/9d929bef348ba4b606ab1d9cc38c8c06bc88f112c406dec178565e73bd80da90/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626366746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9d929bef348ba4b606ab1d9cc38c8c06bc88f112c406dec178565e73bd80da90/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626366746f6f6c73\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bcftools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/da32c09b58451cf778e8808fe0db30e8d76285ff1271dd0e6ca98b832dec3d75/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626366746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/da32c09b58451cf778e8808fe0db30e8d76285ff1271dd0e6ca98b832dec3d75/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626366746f6f6c73\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bcftools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e89b2465f4665a8068dc6582a024bacf55d1efb9d32d6290fbad515ba5ba6b98/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626366746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e89b2465f4665a8068dc6582a024bacf55d1efb9d32d6290fbad515ba5ba6b98/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626366746f6f6c73\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bcftools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bcftools\" class=\"anchor\" href=\"#singularity-bcftools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bcftools\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/icaoberg/bcftools\"\u003ebcftools\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626981806.0
  },
  {
    "data_format": 2,
    "description": "HISAT2 is a fast and sensitive alignment program for mapping next-generation sequencing reads (both DNA and RNA) to a population of human genomes as well as to a single reference genome. ",
    "filenames": [
      "2.2.1/Singularity"
    ],
    "full_name": "pscedu/singularity-hisat2",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hisat\" class=\"anchor\" href=\"#singularity-hisat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hisat\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://daehwankimlab.github.io/hisat2/\" rel=\"nofollow\"\u003ehisat2\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the scripts \u003ccode\u003ehisat2*\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/hisat2/2.2.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/hisat2\u003c/code\u003e as \u003ccode\u003e2.2.1.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-singularity-definition-file\" class=\"anchor\" href=\"#building-the-image-using-the-singularity-definition-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the Singularity definition file\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "bioinformatics",
      "singularity"
    ],
    "updated_at": 1626981896.0
  },
  {
    "data_format": 2,
    "description": "ABySS is a de novo sequence assembler that is designed for very short reads",
    "filenames": [
      "2.1.5/Singularity"
    ],
    "full_name": "pscedu/singularity-abyss",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-abyss/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-abyss/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/61f7c02135e022542644bca33ec90a8b6bdc9cbb8ec4b0e3ec8b5480f035beaa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6162797373\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/61f7c02135e022542644bca33ec90a8b6bdc9cbb8ec4b0e3ec8b5480f035beaa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6162797373\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-abyss\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e1d01eb58e3d5d47f45f00fffe4ab16909d1541fcb3cf25e0c01bf5f9d0c8028/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6162797373\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e1d01eb58e3d5d47f45f00fffe4ab16909d1541fcb3cf25e0c01bf5f9d0c8028/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6162797373\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-abyss\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/b5bcaa1865f5b4c8a6901097150a4b2915c2e0a967f27a5cd175b8cd7c653ce3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6162797373\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b5bcaa1865f5b4c8a6901097150a4b2915c2e0a967f27a5cd175b8cd7c653ce3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6162797373\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-abyss\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/1d51b6d070f927bea877c0ebb5437d326390ba7459fc3a1baa9ecefc8c1f63ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6162797373\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1d51b6d070f927bea877c0ebb5437d326390ba7459fc3a1baa9ecefc8c1f63ea/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6162797373\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-abyss\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-abyss\" class=\"anchor\" href=\"#singularity-abyss\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-abyss\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/ABYSS\"\u003eABYSS\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/ABySS/2.1.5\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/ABySS\u003c/code\u003e as \u003ccode\u003e2.1.5.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626995949.0
  },
  {
    "data_format": 2,
    "description": "Trimmomatic performs a variety of useful trimming tasks for illumina paired-end and single ended data",
    "filenames": [
      "0.39/Singularity"
    ],
    "full_name": "pscedu/singularity-trimmomatic",
    "latest_release": "0.39",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-trimmomatic/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-trimmomatic/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/886b78ddfce96f64ae6be83548b2b219652f30a8ab8e6f78413fcf5c763a5fe3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/886b78ddfce96f64ae6be83548b2b219652f30a8ab8e6f78413fcf5c763a5fe3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-trimmomatic\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f83a71da75047a61c081c63788b045654ea1befe0da1eccfc4e3d46b96c656e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f83a71da75047a61c081c63788b045654ea1befe0da1eccfc4e3d46b96c656e5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-trimmomatic\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/42de55829ff65764902a5aa22015e62165ede512bd9503a795afe4c4815d9e61/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/42de55829ff65764902a5aa22015e62165ede512bd9503a795afe4c4815d9e61/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-trimmomatic\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/77a4e6c187943c941ff1ee8267ebe7f72b3e5d5723a03d3ac4e91e826dc35c98/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/77a4e6c187943c941ff1ee8267ebe7f72b3e5d5723a03d3ac4e91e826dc35c98/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7472696d6d6f6d61746963\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-trimmomatic\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-trimmomatic\" class=\"anchor\" href=\"#singularity-trimmomatic\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-trimmomatic\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/usadellab/Trimmomatic\"\u003eTrimmomatic\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003etrimmomatic\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/trimmomatic/0.39\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/trimmomatic\u003c/code\u003e as \u003ccode\u003e0.39.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626981981.0
  },
  {
    "data_format": 2,
    "description": "Graphviz is a package of open-source tools initiated by AT\u0026T Labs Research for drawing graphs specified in DOT language scripts.",
    "filenames": [
      "2.44.0/Singularity"
    ],
    "full_name": "pscedu/singularity-graphviz",
    "latest_release": "v2.44.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-graphviz/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-graphviz/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4f27d7f7ac7b9a86a1f0f6c45d19b90496b7c8ce89d5004d3fe96d163fe99e73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d677261706876697a\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4f27d7f7ac7b9a86a1f0f6c45d19b90496b7c8ce89d5004d3fe96d163fe99e73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d677261706876697a\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-graphviz\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e6789d316f02fdfe74574853fb2870a7e4b7b6cccca76d201ff81cb1ac2adfbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d677261706876697a\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e6789d316f02fdfe74574853fb2870a7e4b7b6cccca76d201ff81cb1ac2adfbe/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d677261706876697a\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-graphviz\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/b71cbcc295d522b3323d66e9141fdec85461c9d128011383fac4956c54d95d73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d677261706876697a\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b71cbcc295d522b3323d66e9141fdec85461c9d128011383fac4956c54d95d73/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d677261706876697a\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-graphviz\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5227dbb6ba22ff0c43933a4284a416f0f8d311e9972bf97e5383fe334f545102/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d677261706876697a\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5227dbb6ba22ff0c43933a4284a416f0f8d311e9972bf97e5383fe334f545102/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d677261706876697a\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-graphviz\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-graphviz\" class=\"anchor\" href=\"#singularity-graphviz\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-graphviz\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\" alt=\"Logo\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/en/4/48/GraphvizLogo.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://graphviz.org/\" rel=\"nofollow\"\u003egraphviz\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003egraphviz\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/graphviz/2.44.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/graphviz\u003c/code\u003e as \u003ccode\u003e 2.44.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1625845721.0
  },
  {
    "data_format": 2,
    "description": "A command-line benchmarking tool.",
    "filenames": [
      "1.11.0/Singularity"
    ],
    "full_name": "pscedu/singularity-hyperfine",
    "latest_release": "v1.11.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hyperfine\" class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hyperfine\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sharkdp/hyperfine\"\u003ehyperfine\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ehyperfine\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/hyperfine/1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/hyperfine\u003c/code\u003e as \u003ccode\u003e1.11.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1625845763.0
  },
  {
    "data_format": 2,
    "description": "A cat(1) clone with syntax highlighting and Git integration.",
    "filenames": [
      "0.17.1/Singularity"
    ],
    "full_name": "pscedu/singularity-bat",
    "latest_release": "v0.17.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bat/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bat/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/80ce266ea551486e532b8479474ece87011121c1b177e0e067f4d8022ad2f52c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626174\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/80ce266ea551486e532b8479474ece87011121c1b177e0e067f4d8022ad2f52c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d626174\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/b6d53daf33b347fa4d1a74ab0a30fd631965db08d1237e10991c7e7230ed671f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626174\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b6d53daf33b347fa4d1a74ab0a30fd631965db08d1237e10991c7e7230ed671f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d626174\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/d23d5a1f67b897b9eeaf3e805efa8c4c8562272babd67bc8d1da9c8ca70d6259/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626174\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d23d5a1f67b897b9eeaf3e805efa8c4c8562272babd67bc8d1da9c8ca70d6259/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d626174\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/bf0a53aac9fa2abb057c22f8fb00c5d07141c61f15c27983fd073a5445c15421/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626174\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bf0a53aac9fa2abb057c22f8fb00c5d07141c61f15c27983fd073a5445c15421/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d626174\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bat\" class=\"anchor\" href=\"#singularity-bat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bat\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sharkdp/bat\"\u003ebat\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ebat\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bat/0.17.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/bat\u003c/code\u003e as \u003ccode\u003e0.17.1.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1625845844.0
  },
  {
    "data_format": 2,
    "description": "Target/Integrative Genetic Element Retriever",
    "filenames": [
      "5.32.1/Singularity"
    ],
    "full_name": "pscedu/singularity-tiger",
    "latest_release": "v5.32.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-tiger/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-tiger/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/45b51fb9658ca6d66b992d887a9258c238463b14dc9fb58c4ed17ba4e75f2efc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7469676572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b51fb9658ca6d66b992d887a9258c238463b14dc9fb58c4ed17ba4e75f2efc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d7469676572\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-tiger\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/266d33a01bfbc88af8ef713b4ab3f12e4207aea6a693420544c401bcc4687384/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7469676572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/266d33a01bfbc88af8ef713b4ab3f12e4207aea6a693420544c401bcc4687384/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d7469676572\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-tiger\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/cd42640e858ab21849faf7ea1ae7b80b386fea232400bb8666ab226125727f09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7469676572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cd42640e858ab21849faf7ea1ae7b80b386fea232400bb8666ab226125727f09/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d7469676572\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-tiger\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/049f66af13d64dfd0fc9fb6af0dd2b62c6d9f524419d096508747447c1c661ac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7469676572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/049f66af13d64dfd0fc9fb6af0dd2b62c6d9f524419d096508747447c1c661ac/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d7469676572\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-tiger\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-tiger\" class=\"anchor\" href=\"#singularity-tiger\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-tiger\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/tiger\"\u003etiger\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ednaStats.pl\u003c/code\u003e, \u003ccode\u003eislander.pl\u003c/code\u003e, \u003ccode\u003eresolve.pl\u003c/code\u003e, \u003ccode\u003etater.pl\u003c/code\u003e and \u003ccode\u003etiger.pl\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/tiger/4.8.25\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/tiger\u003c/code\u003e as \u003ccode\u003e4.8.25.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/tigers/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1625845871.0
  },
  {
    "data_format": 2,
    "description": "This program computes the cross entropy for groups of sequences  that have been assigned to groups on the basis of biochemical,  physiological, or other biological property. ",
    "filenames": [
      "1.0.0/Singularity"
    ],
    "full_name": "pscedu/singularity-gent",
    "latest_release": "v1.0.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-gent/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-gent/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0bc7e2953fe196a794842a90c0c691e61aae4d46a38b5ea94f97be5354c5563e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d67656e74\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0bc7e2953fe196a794842a90c0c691e61aae4d46a38b5ea94f97be5354c5563e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d67656e74\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-gent\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/736a01217414593b006aba14bc9a1c3d29361075a38dea7b579f6297408854ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d67656e74\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/736a01217414593b006aba14bc9a1c3d29361075a38dea7b579f6297408854ba/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d67656e74\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-gent\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/258cd3bbde4ace9b70ffb87058e2ec74b2af329db10c4be6571e9947e38b92b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d67656e74\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/258cd3bbde4ace9b70ffb87058e2ec74b2af329db10c4be6571e9947e38b92b9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d67656e74\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-gent\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/9bd43b6e0fd124c9db72f1eae2f35f9f1a11833efa211af988ce1d994bf3b481/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d67656e74\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9bd43b6e0fd124c9db72f1eae2f35f9f1a11833efa211af988ce1d994bf3b481/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d67656e74\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-gent\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-gent\" class=\"anchor\" href=\"#singularity-gent\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-gent\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/icaoberg/gent\"\u003eGeNT\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1625845894.0
  },
  {
    "data_format": 2,
    "description": "Use ImageMagick\u00ae to create, edit, compose, or convert digital images.",
    "filenames": [
      "7.1.0-2/Singularity",
      "7.0.10-48/Singularity"
    ],
    "full_name": "pscedu/singularity-imagemagick",
    "latest_release": "v7.1.0-2",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-imagemagick/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-imagemagick/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/bffbdc414c9ed8c423a6d7872563464afb2c8b09a20904e6f94fd5680fa7f35d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bffbdc414c9ed8c423a6d7872563464afb2c8b09a20904e6f94fd5680fa7f35d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-imagemagick\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/35b84b7c162ce5ebe06e76f87b697a035224e0c24abedf842c651c84f2e9b813/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/35b84b7c162ce5ebe06e76f87b697a035224e0c24abedf842c651c84f2e9b813/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-imagemagick\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/fdf01b52f0de8a81d8c325c33baa4ceae4bcc52b77776e85246c13305d72a428/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fdf01b52f0de8a81d8c325c33baa4ceae4bcc52b77776e85246c13305d72a428/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-imagemagick\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/21aa44f17b0246428f16808d167f9cc3d1d229437bf99468ce80bc4fff7dba95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/21aa44f17b0246428f16808d167f9cc3d1d229437bf99468ce80bc4fff7dba95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d696d6167656d616769636b\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-imagemagick\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-imagemagick\" class=\"anchor\" href=\"#imagemagick\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImageMagick\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/9ab0d6d887bab73cdba783d7832cd74843f37520aa8981cede970b01a4a95db1/68747470733a2f2f65787465726e616c2d636f6e74656e742e6475636b6475636b676f2e636f6d2f69752f3f753d6874747025334125324625324679656e7061692e696469732e636f6d2e747725324677702d636f6e74656e7425324675706c6f616473253246323031322532463131253246696d6167656d616769636b5f77697a6172645f7468756d622e6a706726663d31266e6f66623d31\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9ab0d6d887bab73cdba783d7832cd74843f37520aa8981cede970b01a4a95db1/68747470733a2f2f65787465726e616c2d636f6e74656e742e6475636b6475636b676f2e636f6d2f69752f3f753d6874747025334125324625324679656e7061692e696469732e636f6d2e747725324677702d636f6e74656e7425324675706c6f616473253246323031322532463131253246696d6167656d616769636b5f77697a6172645f7468756d622e6a706726663d31266e6f66623d31\" alt=\"Logo\" data-canonical-src=\"https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fyenpai.idis.com.tw%2Fwp-content%2Fuploads%2F2012%2F11%2Fimagemagick_wizard_thumb.jpg\u0026amp;f=1\u0026amp;nofb=1\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://imagemagick.org/index.php\" rel=\"nofollow\"\u003eImageMagick\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUse ImageMagick\u00ae to create, edit, compose, or convert bitmap images. It can read and write images in a variety of formats (over 200) including PNG, JPEG, GIF, HEIC, TIFF, DPX, EXR, WebP, Postscript, PDF, and SVG. Use ImageMagick to resize, flip, mirror, rotate, distort, shear and transform images, adjust image colors, apply various special effects, or draw text, lines, polygons, ellipses and B\u00e9zier curves.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity",
      "utilities",
      "image-processing"
    ],
    "updated_at": 1626982898.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.ExplainAI",
      "Singularity.ubuntu_torch",
      "Singularity.pytorch",
      "Singularity.physio",
      "Singularity.torch_mmf",
      "Singularity.Spektral",
      "Singularity.mac_local",
      "Singularity.centos_torch",
      "Singularity.ubuntu_pre",
      "Singularity.centos_torch2",
      "Singularity.ubuntu_tf",
      "Singularity.centos_tf2",
      "Singularity.centos_tf",
      "Singularity.ExplainAI2",
      "Singularity.jax",
      "Singularity.torch",
      "Singularity.centos_torch3"
    ],
    "full_name": "cyang31/containers",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4054\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-images-supporting-basedmux-workflow\" class=\"anchor\" href=\"#singularity-images-supporting-basedmux-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity images supporting \u003ca href=\"https://github.com/vibaotram/baseDmux.git\"\u003ebaseDmux workflow\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.guppy-cpu-conda\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 CPU, Miniconda3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:guppy-cpu-conda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.cpu-guppy3.4-conda-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 CPU, Miniconda3, ONT_FAST5_API\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:cpu-guppy3.4-conda-api\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.guppy3.4gpu-conda-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 GPU, Miniconda3, ONT_FAST5_API\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:guppy3.4gpu-conda-api\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.deepbinner-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining deepbinner 2.0.0, ONT_FAST5_API, python3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:deepbinner-api\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625949452.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/bedpost-singularity",
    "latest_release": "v3.0.0",
    "readme": "\u003cp\u003eRuns FSL\u0027s bedpostx on the input DWI data set, and creates a PDF report of the results.\nQuite simple - see /opt/src/pipeline.sh for the main script.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626106357.0
  },
  {
    "data_format": 2,
    "description": "Rclone is a command line program to manage files on cloud storage. It is a feature rich alternative to cloud vendors\u0027 web storage interfaces.",
    "filenames": [
      "1.55.1/Singularity"
    ],
    "full_name": "pscedu/singularity-rclone",
    "latest_release": "v1.55.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-rclone/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-rclone/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f31bf43512fe989665052aceb62d92cb2f4e9a9edeb3b1b0d8bc249a77520f6e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d72636c6f6e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f31bf43512fe989665052aceb62d92cb2f4e9a9edeb3b1b0d8bc249a77520f6e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d72636c6f6e65\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-rclone\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4112143cfddf18da1c82c040d25965a3f3c2a344d4b1eff145b510baeb571677/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d72636c6f6e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4112143cfddf18da1c82c040d25965a3f3c2a344d4b1eff145b510baeb571677/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d72636c6f6e65\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-rclone\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/005088e44a1d6ffe07b6d97e456d3deb7878f9bb91560db5ece75b8aff184f32/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d72636c6f6e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/005088e44a1d6ffe07b6d97e456d3deb7878f9bb91560db5ece75b8aff184f32/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d72636c6f6e65\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-rclone\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0d0ce263787353de0284943fc84d23d8906e758dd563e0b8806f8877dc36b02a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d72636c6f6e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0d0ce263787353de0284943fc84d23d8906e758dd563e0b8806f8877dc36b02a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d72636c6f6e65\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-rclone\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rclone\" class=\"anchor\" href=\"#singularity-rclone\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-rclone\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/486e96833b074a96dff53dfa6bd1d2d4eb9d08629ea07e48bdd38a9f3e13e622/68747470733a2f2f72636c6f6e652e6f72672f696d672f6c6f676f5f6f6e5f6c696768745f5f686f72697a6f6e74616c5f636f6c6f722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/486e96833b074a96dff53dfa6bd1d2d4eb9d08629ea07e48bdd38a9f3e13e622/68747470733a2f2f72636c6f6e652e6f72672f696d672f6c6f676f5f6f6e5f6c696768745f5f686f72697a6f6e74616c5f636f6c6f722e737667\" alt=\"Logo\" data-canonical-src=\"https://rclone.org/img/logo_on_light__horizontal_color.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\nSingularity recipe for \u003ca href=\"https://rclone.org/\" rel=\"nofollow\"\u003erclone\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003erclone\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/rclone/1.55.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/rclone\u003c/code\u003e as \u003ccode\u003e1.55.1.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626983153.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "aces/simulation_toolkit_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-simulation-toolkit-for-coticometry-pipeline\" class=\"anchor\" href=\"#simulation-toolkit-for-coticometry-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimulation Toolkit for Coticometry Pipeline\u003c/h1\u003e\n\u003cp\u003eTools in this repository can be used to simulate artificial lesions in the brain in order to estimate the sensitivity and specificity of lesion\ndetection, using different automated corticometry pipelines.\u003c/p\u003e\n\u003cp\u003eTo set up software you need the following:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the software packages needed to run the deformation-2.pl script. Please follow steps in: \u003ca href=\"https://github.com/aces/simulation_toolkit_singularity/blob/main/Singularity\"\u003ehttps://github.com/aces/simulation_toolkit_singularity/blob/main/Singularity\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eObtain data from\n\u003ca href=\"https://ida.loni.usc.edu/collaboration/access/appLicense.jsp;jsessionid=B0278AF5FD413E9AC14512DF841FFCA4/\" rel=\"nofollow\"\u003ehttps://ida.loni.usc.edu/collaboration/access/appLicense.jsp;jsessionid=B0278AF5FD413E9AC14512DF841FFCA4/\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun deformation pipeline\"\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eUsage\nUsage: deformation.pl -input \u0026lt;.mnc\u0026gt; -output  [options]\u003c/p\u003e\n\u003cp\u003eMandatory options:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e-deformation_ratio    provide the ratio of deformation, values must be between 0.1 [shrinkage] to 1.50 [expansion] [e.g. 0.1,1.2,0.6,\u2026]\n\n-mask                 Specify a tolerance map file (.mnc) indicating voxels that have a different amount of error allowed e.g., CSF, background [e.g. your-mask.mnc]\n\n-coordinate           Specify a hyperslab starting at \u0026lt;x\u0026gt; \u0026lt;y\u0026gt; \u0026lt;z\u0026gt; and extending in respective directions by \u0026lt;sizex\u0026gt; \u0026lt;sizey\u0026gt; \u0026lt;sizez\u0026gt; [e.g. 70 100 80 5 5 5]\n\n-tolerance_space      Define the buffer area around the deformation region [default = 4]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOther options:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e-blur_determinant     Blurring kernel size for blurring deformation determinant blurring kernel 0-1\n\n-error                Specify the amount of error that is allowed between the specified determinant and the final determinant (per voxel) [default =0.00001]\n\n-iteration            Specify the maximum number of iterations to update the deformations field (-1 means until convergence) [default 1000]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cp\u003e./deformation.pl -input ICBM_00100_t1_final.mnc -output Debugging_Folder -deformation_ratio 0.6 -coordinate 70 100 70 10 10 10 -tolerance_space 4 -blur_determinant 0.25  -error 0.00001  -iteration 100\u003c/p\u003e\n\u003cp\u003eThe locally-deformed output file name includes input parameters to simplify creating GLM matrices for statistical analysis.\u003c/p\u003e\n\u003cp\u003eICBM_00100_t1_final_deformed_by_0.4atROIx70-y100-z70dimx10.dimy10.dimz10.mnc.\u003c/p\u003e\n\u003cp\u003eThere following intermediate files are generated to help you do quality control and can be deleted:\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/block.mnc\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/blurred0.25determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/DDDDdilated.mnc   \u0026lt;\u0026lt;number of D\u0027s corresponds to the number of times the tolerance space (defined to be 4 in the commandline) is dilated\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/DDDDring.mnc\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/determinant_r_0.4_grid.mnc\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/determinant_r_0.4x70-y100-z70dimx10.dimy10.dimz10.mnc\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMP/determinant_r_0.4.xfm\u003c/p\u003e\n\u003cp\u003e/Debugging_Folder/TMPmask.mnc\u003c/p\u003e\n\u003cp\u003eALTERNATIVELY: If you don\u0027t want to use this Perl wrapper, then follow the instructions for creating your own deformations:\n\u003ca href=\"https://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields\" rel=\"nofollow\"\u003ehttps://wiki.mouseimaging.ca/display/MICePub/Generating+deformation+fields\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSource code for deformation pipeline and dependencies (MINC):\n\u003ca href=\"https://github.com/Mouse-Imaging-Centre/generate_deformation_fields\"\u003ehttps://github.com/Mouse-Imaging-Centre/generate_deformation_fields\u003c/a\u003e\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\n\u003cp\u003eExample Data, Scripts and Statistical analysis used in our Frontier\u0027s Paper can be found here: \u003ca href=\"https://github.com/aces/simulation_toolkit_statistics\"\u003ehttps://github.com/aces/simulation_toolkit_statistics\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAll these tools and data needed will be made available via CBRAIN. To learn more, please contact us at \u003ca href=\"mailto:cbrain-support.mni@mcgill.ca\"\u003ecbrain-support.mni@mcgill.ca\u003c/a\u003e. In the subject line, pleasee be sure to write SIMULATION TOOLKIT.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626116668.0
  },
  {
    "data_format": 2,
    "description": "A complete, cross-platform solution to record, convert and stream audio and video.",
    "filenames": [
      "4.3.1/Singularity"
    ],
    "full_name": "pscedu/singularity-ffmpeg",
    "latest_release": "v4.3.1",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-ffmpeg/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-ffmpeg/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/995f5ae739e4b5e84bed8c94242535f1e6574cc5235830d1dfc03fb2c27382dd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d66666d706567\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/995f5ae739e4b5e84bed8c94242535f1e6574cc5235830d1dfc03fb2c27382dd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d66666d706567\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-ffmpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/751ca05bfc064a3625d33ff3e8c03f3de87f71ec9dddd793595d069d15810c10/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d66666d706567\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/751ca05bfc064a3625d33ff3e8c03f3de87f71ec9dddd793595d069d15810c10/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d66666d706567\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-ffmpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ee0114c2f5583f3eb834c4a96dd51bc36819a88db8bbb7aa824f8fb6d2f0ca80/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d66666d706567\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ee0114c2f5583f3eb834c4a96dd51bc36819a88db8bbb7aa824f8fb6d2f0ca80/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d66666d706567\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-ffmpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ce3c524f22af51a683432f83f9b6327df7ceaf1d06ad3a84afc59e0218eacc46/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d66666d706567\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ce3c524f22af51a683432f83f9b6327df7ceaf1d06ad3a84afc59e0218eacc46/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d66666d706567\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-ffmpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626997205.0
  },
  {
    "data_format": 2,
    "description": "FastTree infers approximately-maximum-likelihood phylogenetic trees from alignments of nucleotide or protein sequences. ",
    "filenames": [
      "2.1.11/Singularity"
    ],
    "full_name": "pscedu/singularity-fasttree",
    "latest_release": "v2.1.11",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-fasttree/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-fasttree/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/776c7e7d72b508b47ce8bd555bc38368be7629ae529461cdfcfaf5af2920c141/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6661737474726565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/776c7e7d72b508b47ce8bd555bc38368be7629ae529461cdfcfaf5af2920c141/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6661737474726565\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-fasttree\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/6ba1353dc0bb816d3bca4dafd9f90d560fb7d5234867b2ac4b80d0557d1bdc90/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6661737474726565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6ba1353dc0bb816d3bca4dafd9f90d560fb7d5234867b2ac4b80d0557d1bdc90/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6661737474726565\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-fasttree\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a6aa9ae28b1371a303884250204eea3a43e273bae8eb19f063f732416f9d6bec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6661737474726565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a6aa9ae28b1371a303884250204eea3a43e273bae8eb19f063f732416f9d6bec/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6661737474726565\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-fasttree\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0cc26b73063b0fb2d5f502b2eeb83e3568bac891ce91b016300d28524b83b564/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6661737474726565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0cc26b73063b0fb2d5f502b2eeb83e3568bac891ce91b016300d28524b83b564/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6661737474726565\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-fasttree\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-fasttree\" class=\"anchor\" href=\"#singularity-fasttree\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-fasttree\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003eFastTree\u003c/code\u003e, \u003ccode\u003eFastTreeMP\u003c/code\u003e and \u003ccode\u003eFastTreeDbl\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/FastTree/2.1.11\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/FastTree\u003c/code\u003e as \u003ccode\u003e2.1.11.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626120833.0
  },
  {
    "data_format": 2,
    "description": "Nextflow pipelines for routine bioinformatics analyses",
    "filenames": [
      "nextstrain/environments/Singularity"
    ],
    "full_name": "matt-sd-watson/nextflow_for_bioinformatics",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow_for_bioinformatics\" class=\"anchor\" href=\"#nextflow_for_bioinformatics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enextflow_for_bioinformatics\u003c/h1\u003e\n\u003cp\u003eNextflow pipelines for routine bioinformatics analyses\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627333212.0
  },
  {
    "data_format": 2,
    "description": "asciinema [as-kee-nuh-muh] is a free and open source solution for recording terminal sessions and sharing them on the web.",
    "filenames": [
      "2.0.2/Singularity"
    ],
    "full_name": "pscedu/singularity-asciinema",
    "latest_release": "v2.0.2-r3",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-asciinema/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-asciinema/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ac969b397cac62ab873b2b28f38187c3275736a2c043406f91165f585f809d33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d61736369696e656d61\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ac969b397cac62ab873b2b28f38187c3275736a2c043406f91165f585f809d33/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d61736369696e656d61\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-asciinema\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/d2444a0f87d191c45cf7d8a7728e1ddf6cdcc6c64b6b3151bad420bbbf0befef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d61736369696e656d61\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d2444a0f87d191c45cf7d8a7728e1ddf6cdcc6c64b6b3151bad420bbbf0befef/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d61736369696e656d61\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-asciinema\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/732781cbc4146c3ac3303175bd32db5eba1c9dace6f303faf1393d2384cedb58/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d61736369696e656d61\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/732781cbc4146c3ac3303175bd32db5eba1c9dace6f303faf1393d2384cedb58/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d61736369696e656d61\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-asciinema\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/1aa805bbf02a6328054cf26eb89500fb5e53456ece2a67bb5f54d15e86e71370/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d61736369696e656d61\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1aa805bbf02a6328054cf26eb89500fb5e53456ece2a67bb5f54d15e86e71370/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d61736369696e656d61\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-asciinema\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-asciinema\" class=\"anchor\" href=\"#singularity-asciinema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-asciinema\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/asciinema\"\u003easciinema\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003easciinema\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/asciinema/2.0.2\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/asciinema\u003c/code\u003e as \u003ccode\u003e2.0.2.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626144723.0
  },
  {
    "data_format": 2,
    "description": "du + rust = dust. Like du but more intuitive.",
    "filenames": [
      "0.5.4/Singularity",
      "0.6.0/Singularity"
    ],
    "full_name": "pscedu/singularity-dust",
    "latest_release": "v0.6.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-dust/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-dust/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5c03a3a0c39c4bcd2fd882586df6b4c94ef095b690cc8918ff9c7f7121b699f5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d64757374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c03a3a0c39c4bcd2fd882586df6b4c94ef095b690cc8918ff9c7f7121b699f5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d64757374\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-dust\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/b64d777f7ab79be22d2c6a3a092aa35845fe1fdd0043d607ace41468a89fcaae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d64757374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b64d777f7ab79be22d2c6a3a092aa35845fe1fdd0043d607ace41468a89fcaae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d64757374\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-dust\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/3896c740167ff8ef43b82e7e352cb3540ecaa2bee029033a9be9bbbd7d91575a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d64757374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3896c740167ff8ef43b82e7e352cb3540ecaa2bee029033a9be9bbbd7d91575a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d64757374\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-dust\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4eb12296596ab94dcb1a488179c5b541b54d2a5559c9e67868f60876e6f1b6e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d64757374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4eb12296596ab94dcb1a488179c5b541b54d2a5559c9e67868f60876e6f1b6e8/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d64757374\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-dust\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-dust\" class=\"anchor\" href=\"#singularity-dust\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-dust\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/bootandy/dust/raw/master/media/snap.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/bootandy/dust/raw/master/media/snap.png\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/bootandy/dust\"\u003edust\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003edust\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/dust/0.5.4\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/dust\u003c/code\u003e as \u003ccode\u003e0.5.4.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626982220.0
  },
  {
    "data_format": 2,
    "description": "Recipes for docker and singularity containers for COHERENT projects",
    "filenames": [
      "geant4/Singularity_geant4",
      "geant4/Singularity"
    ],
    "full_name": "NuTufts/coherent-containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coherent-containers\" class=\"anchor\" href=\"#coherent-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoherent-containers\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626189399.0
  },
  {
    "data_format": 2,
    "description": "msee is a command-line tool to read markdown file.",
    "filenames": [
      "0.3.5/Singularity"
    ],
    "full_name": "icaoberg/singularity-msee",
    "latest_release": "v0.3.5",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-msee/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-msee/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/8f15c74b6b0c9b138f9da5b4933fe28294369dc8e7eb93b731dc2ce072de357e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6d736565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8f15c74b6b0c9b138f9da5b4933fe28294369dc8e7eb93b731dc2ce072de357e/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d6d736565\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-msee\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/dc49ece7c14937e5c2802e8ab4824bc9c4606e21bd4f04eede89efffab599a2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6d736565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/dc49ece7c14937e5c2802e8ab4824bc9c4606e21bd4f04eede89efffab599a2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d6d736565\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-msee\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/bc6a9157f824c9765da6596c0beb4a82c4f71d7a27b46cec8e32781fb8c3faad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6d736565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bc6a9157f824c9765da6596c0beb4a82c4f71d7a27b46cec8e32781fb8c3faad/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d6d736565\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-msee\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5858470ac4cc23a618c46e8f874d611a5c2ba2762846b37cae4b6767e4e8784f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6d736565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5858470ac4cc23a618c46e8f874d611a5c2ba2762846b37cae4b6767e4e8784f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d6d736565\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-msee\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-msee\" class=\"anchor\" href=\"#singularity-msee\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-msee\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://cloud.githubusercontent.com/assets/157338/10902801/531ba216-823d-11e5-87ac-986b8d5ea4cc.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://cloud.githubusercontent.com/assets/157338/10902801/531ba216-823d-11e5-87ac-986b8d5ea4cc.png\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\nSingularity recipe for \u003ca href=\"https://www.npmjs.com/package/msee\" rel=\"nofollow\"\u003emsee\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003emsee\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/msee/0.3.5\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/msee\u003c/code\u003e as \u003ccode\u003e0.3.5.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/msees/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilties"
    ],
    "updated_at": 1626319169.0
  },
  {
    "data_format": 2,
    "description": "R server within singularity container on HPC",
    "filenames": [
      "Singularity_bioc_python"
    ],
    "full_name": "retogerber/singularity_rserver",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-r-server-in-singularity\" class=\"anchor\" href=\"#r-server-in-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR server in singularity\u003c/h1\u003e\n\u003cp\u003eThis workflow together with the script \u003ccode\u003esingRstudio.sh\u003c/code\u003e facilitates setting up an R server running in a singularity container on a HPC and accessing it on a local PC.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-workflow\" class=\"anchor\" href=\"#workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prepare-only-first-time\" class=\"anchor\" href=\"#prepare-only-first-time\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare (only first time)\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-on-local-pc\" class=\"anchor\" href=\"#on-local-pc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOn local PC\u003c/h3\u003e\n\u003cp\u003eSince building a singularity image requires root privilege it is often not possible to directly build on your HPC. A simple workaround is to build in on your local PC and the copy to the server.\nBuild Singularity image file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build singularity_container.sif Singularity_bioc_python\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe given Singularity build file is just an example, to customize for your needs have a look at the \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/build_a_container.html\" rel=\"nofollow\"\u003esingularity documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAfter building the image copy to server, e.g.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003escp singularity_container.sif SERVERNAME:/some/location\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively there is the possibily to build without sudo using the \u003ccode\u003e--remote\u003c/code\u003e flage. \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/cloud_library.html\" rel=\"nofollow\"\u003eSingularity documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-on-server\" class=\"anchor\" href=\"#on-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOn server\u003c/h3\u003e\n\u003cp\u003eMake sure a suitable temporary directory is available, e.g. \u003ccode\u003e~/tmp\u003c/code\u003e (the default).\u003c/p\u003e\n\u003cp\u003eDecide on the port you want to use, the default is 8788.\u003c/p\u003e\n\u003cp\u003eRun rserver with singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash singRstudio.sh -c singularity_container.sif -t ~/tmp -p 8789\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-on-local-pc-1\" class=\"anchor\" href=\"#on-local-pc-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOn local PC\u003c/h3\u003e\n\u003cp\u003eRedirect traffic from port on server to local port via ssh:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -Nf -L LOCALPORT:localhost:SERVERPORT SERVERNAME\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ereplacing \u003ccode\u003eLOCALPORT\u003c/code\u003e with the port you want to use on your local pc, \u003ccode\u003eSERVERPORT\u003c/code\u003e with the above specified port (default 8788) and \u003ccode\u003eSERVERNAME\u003c/code\u003e with the address of the server.\ne.g:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -Nf -L 8787:localhost:8788 user@myserver.com\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen open a browser and go to \u003ccode\u003ehttp://localhost:LOCALPORT\u003c/code\u003e again replacing \u003ccode\u003eLOCALPORT\u003c/code\u003e. Login with your server username and passwort (as specified with the \u003ccode\u003e-a\u003c/code\u003e argument, default: \u003ccode\u003epassword\u003c/code\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-other-options\" class=\"anchor\" href=\"#other-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOther options:\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-bind-local-directories-to-container\" class=\"anchor\" href=\"#bind-local-directories-to-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBind local directories to container\u003c/h3\u003e\n\u003cp\u003eTo connect directories to the container in a specific manner set the \u003ccode\u003e-b\u003c/code\u003e argument:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash singRstudio.sh -c singularity_container.sif -b \"local/dir/1:/absolute/container/dir/1,local/dir/2:/absolute/container/dir/2\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-local-r-library\" class=\"anchor\" href=\"#local-r-library\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elocal R library\u003c/h3\u003e\n\u003cp\u003eSince singularity containers are read-only, installing R packages is not possible. For reproducibility this is great as it is always clear what packages were used,\nbut sometimes it can be a nuissance when testing stuff. A workaround is to specify a local directory in which the packages are installed. This can be done setting\nthe \u003ccode\u003e-r\u003c/code\u003e argument:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash singRstudio.sh -c singularity_container.sif -r ~/my/R/library\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dry-run\" class=\"anchor\" href=\"#dry-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDry run\u003c/h3\u003e\n\u003cp\u003eTo just show the \"built\" singularity command without executing it add \u003ccode\u003e-d\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash singRstudio.sh -c singularity_container.sif -d\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626332421.0
  },
  {
    "data_format": 2,
    "description": "This repository will hold: the build script to install singularity and other dependencies for it, and a definition file for the singularity container for HPC.",
    "filenames": [
      "Singularity",
      "SingularitySC"
    ],
    "full_name": "perminaa/SingularityHPC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularityhpc\" class=\"anchor\" href=\"#singularityhpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularityHPC\u003c/h1\u003e\n\u003cp\u003eThis repository will hold: the build script to install singularity and other dependencies for it, and a definition file for the singularity container for HPC.\u003c/p\u003e\n\u003cp\u003eTo install, run \u003ccode\u003egit clone https://github.com/perminaa/SingularityHPC.git \u0026amp;\u0026amp; cd SingularityHPC \u0026amp;\u0026amp; bash buildscript.sh\u003c/code\u003e. This will install and configure singularity\nand build a container called \u003ccode\u003eContainer.sif\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTo run the container, you can use \u003ccode\u003esingularity shell Container.sif\u003c/code\u003e to run in the singularity shell or \u003ccode\u003esingularity exec Container.sif \u0026lt;command\u0026gt;\u003c/code\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626501688.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for HERA software",
    "filenames": [
      "Singularity.hera1",
      "Singularity.rtp",
      "Singularity.h4c",
      "Singularity.casa6_full",
      "Singularity.casa6_modular"
    ],
    "full_name": "HERA-Team/hera-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hera-singularity\" class=\"anchor\" href=\"#hera-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehera-singularity\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notice\" class=\"anchor\" href=\"#notice\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotice\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eJuly 15, 2021\u003c/strong\u003e:\nWe are currently manually building and uploading the containers to the HERA project directory on Ilifu on an irregular basis. Please check the built dates of the container files and contact @piyanatk if you need the containers to be rebuilt. Scheduled daily re-building is being planned.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eThis repository contains recipe files of the Singularity containers for the HERA software stack.\u003c/p\u003e\n\u003cp\u003eIlifu users, please make sure to read the relevant page on the HERA wiki. A singularity container is required for computing on the Ilifu. If you need specific Python modules to be installed in the containers, please contact @piyanatk.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-container-and-singularity\" class=\"anchor\" href=\"#about-container-and-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout Container and Singularity\u003c/h2\u003e\n\u003cp\u003eContainers are encapsulated software environments and abstract the software and applications from the underlying operating system. This allows users to run workflows in customized environments, switch between environments, and to share these environments with colleagues and research teams.\u003c/p\u003e\n\u003cp\u003eSingularity is a free, cross-platform and open-source computer program that performs operating-system-level virtualization also known as containerization (another widely used one being Docker).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-container-content\" class=\"anchor\" href=\"#container-content\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer Content\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-packages\" class=\"anchor\" href=\"#python-packages\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython Packages\u003c/h3\u003e\n\u003cp\u003eAll containers are built with \u003ccode\u003eUbuntu 20.04\u003c/code\u003e and \u003ccode\u003eminiconda\u003c/code\u003e with \u003ccode\u003epython=3.8\u003c/code\u003e unless otherwise specify \u003ca href=\"###-Different-Between-Containers:\"\u003ebelow\u003c/a\u003e. All variances come standard with the following packages:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eData Analysis\u003c/th\u003e\n\u003cth\u003eAstronomical\u003c/th\u003e\n\u003cth\u003eHERA\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003edask\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eaipy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003elinsolve\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ejupyterlab\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eastropy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003euvtools\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ematplotlib\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eastropy-healpix\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_qm\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003enumpy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eastroquery\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_cal\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003epandas\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ecartopy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_sim\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003escipy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehealpy\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ehera_psepc\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003escikit-learn\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003ccode\u003epyuvdata\u003c/code\u003e\u003csup\u003e\u003ca href=\"#myfootnote1\"\u003e1\u003c/a\u003e\u003c/sup\u003e\n\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003exarray\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003ccode\u003epyuvsim\u003c/code\u003e\u003csup\u003e\u003ca href=\"#myfootnote2\"\u003e2\u003c/a\u003e\u003c/sup\u003e\n\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003ccode\u003eSSINS\u003c/code\u003e\u003csup\u003e\u003ca href=\"#myfootnote3\"\u003e3\u003c/a\u003e\u003c/sup\u003e\n\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003ca name=\"user-content-myfootnote1\"\u003e1\u003c/a\u003e: With CASA measurement sets, HEALPix beam, and CST beam functionalities, see \u003ca href=\"https://github.com/RadioAstronomySoftwareGroup/pyuvdata%5C\"\u003ehttps://github.com/RadioAstronomySoftwareGroup/pyuvdata\\\u003c/a\u003e\n\u003ca name=\"user-content-myfootnote2\"\u003e2\u003c/a\u003e: With profiling and full simulator, see \u003ca href=\"https://github.com/RadioAstronomySoftwareGroup/pyuvsim\"\u003ehttps://github.com/RadioAstronomySoftwareGroup/pyuvsim\u003c/a\u003e\n\u003ca name=\"user-content-myfootnote3\"\u003e3\u003c/a\u003e: See \u003ca href=\"https://github.com/mwilensky768/SSINS\"\u003ehttps://github.com/mwilensky768/SSINS\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-variances\" class=\"anchor\" href=\"#variances\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVariances:\u003c/h3\u003e\n\u003cp\u003eWe are currently building the following variances.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ehera1\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eInclude all packages in the table above. Intended for general-purpose computing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecasa6_full\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eEquivalent to \u003ccode\u003ehera1\u003c/code\u003e with a full installation of \u003ccode\u003ecasa-6\u003c/code\u003e, and \u003ccode\u003eAPLpy\u003c/code\u003e for visualisation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecasa6_modular\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eEquivalent to \u003ccode\u003ehera1\u003c/code\u003e with a pip-wheel installation of \u003ccode\u003ecasa-6\u003c/code\u003e, making \u003ccode\u003ecasatasks\u003c/code\u003e, \u003ccode\u003ecasatools\u003c/code\u003e, and \u003ccode\u003ecasampi\u003c/code\u003e packages (see \u003ca href=\"https://casa-pip.nrao.edu/\" rel=\"nofollow\"\u003ehttps://casa-pip.nrao.edu/\u003c/a\u003e), and \u003ccode\u003eAPLpy\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eBased on \u003ccode\u003ePython 3.6\u003c/code\u003e and \u003ccode\u003eUbuntu 18.04\u003c/code\u003e for casa-pip compatibility.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ertp\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eFor testing the \u003ccode\u003emakeflow\u003c/code\u003e pipeline.\u003c/li\u003e\n\u003cli\u003eEquivalent to \u003ccode\u003ehera1\u003c/code\u003e with an addition of \u003ccode\u003ehera_opm\u003c/code\u003e, \u003ccode\u003ehera_mc\u003c/code\u003e, and  \u003ccode\u003ehera_notebook_templates\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ehera_pipelines\u003c/code\u003e is cloned to \u003ccode\u003e/usr/local\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eh4c\u003c/code\u003e:\n\u003cul\u003e\n\u003cli\u003eAlmost equivalent to \u003ccode\u003ertp\u003c/code\u003e except some specific branches on \u003ccode\u003ehera_cal\u003c/code\u003e and \u003ccode\u003epspec\u003c/code\u003e for H4C analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-environment\" class=\"anchor\" href=\"#python-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython Environment\u003c/h3\u003e\n\u003cp\u003eAll containers use Miniconda3, which are installed at \u003ccode\u003e/usr/local/miniconda3/\u003c/code\u003e inside the containers.\u003c/p\u003e\n\u003cp\u003eThe name of Conda environment in each container is the same as the container name, e.g. \u003ccode\u003ehera1\u003c/code\u003e, \u003ccode\u003ecasa6_full\u003c/code\u003e, and etc, The default conda environment \u003ccode\u003ebase\u003c/code\u003e is not used.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-environment-variables\" class=\"anchor\" href=\"#environment-variables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment Variables\u003c/h3\u003e\n\u003cp\u003eThe following environment variables are also exported in all containers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCONDA_INSTALL_PATH=\"/usr/local/miniconda3\"\nCONDA_INIT_SCRIPT=\"$CONDA_INSTALL_PATH/etc/profile.d/conda.sh\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe latter is especially useful to make the \u003ccode\u003econda\u003c/code\u003e command available inside the container (see the section on \u003ca href=\"####-%60shell%60\"\u003e\u003ccode\u003esingularly shell\u003c/code\u003e usage\u003c/a\u003e below).\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ertp\u003c/code\u003e container has an additional environment variable that point to \u003ccode\u003ehera_pipelines\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHERA_PIPELINES_PATH=\"/usr/local/hera_pipelines\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-commands\" class=\"anchor\" href=\"#singularity-commands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Commands\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-pull\" class=\"anchor\" href=\"#pull\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003epull\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eUse \u003ccode\u003esingularity pull\u003c/code\u003e to download the container from Singularity Hub\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull [name_to_save_the_image_(optional)] shub://HERA-Team/hera-singularity:\u0026lt;recipe\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor example,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity pull rtp.sif shub://HERA-Team/hera-singularity:rtp\nINFO:    Downloading shub image\n 1.98 GiB / 1.98 GiB [=======================================================] 100.00% 13.12 MiB/s 2m34s\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-shell\" class=\"anchor\" href=\"#shell\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003eshell\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003esingularity shell\u003c/code\u003e command allows you to spawn a new shell within your container and interact with it as though it were a small virtual machine.\u003c/p\u003e\n\u003cp\u003eBy default, \u003ccode\u003eshell\u003c/code\u003e invokes \u003ccode\u003e/bin/sh --norc\u003c/code\u003e, which means that \u003ccode\u003e.bashrc\u003c/code\u003e will not be executed (more on this \u003ca href=\"https://github.com/hpcng/singularity/issues/643\"\u003ehere\u003c/a\u003e) and thus Conda will not be initialized. To make the \u003ccode\u003econda\u003c/code\u003e command available, you can do one of the following:\u003c/p\u003e\n\u003cp\u003ea) Run \u003ccode\u003eexec $SHELL\u003c/code\u003e inside the singularity shell. If \u003ccode\u003e$SHELL\u003c/code\u003e is \u003ccode\u003e\\bin\\bash\u003c/code\u003e (as in our Ubuntu build), \u003ccode\u003e.bashrc\u003c/code\u003e will be read.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell rtp.sif\nSingularity\u0026gt; exec $SHELL\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eb) Manually execute the conda initialization script inside singularity shell. The \u003ccode\u003eCONDA_INIT_SCRIPT\u003c/code\u003e environment variable pointing to the absolute path of the script (\u003ccode\u003e/usr/local/miniconda3/etc/profile.d/conda.sh\u003c/code\u003e), is made available for this purpose. Note that \u003ccode\u003e.\u003c/code\u003e must be used as \u003ccode\u003esource\u003c/code\u003e won\u0027t work under \u003ccode\u003esh\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell rtp.sif\nSingularity\u0026gt; . $CONDA_INIT_SCRIPT\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eb) Specify \u003ccode\u003e\\bin\\bash\u003c/code\u003e as a shell to use when executing the \u003ccode\u003eshell\u003c/code\u003e command, either by using the \u003ccode\u003eSINGULARITY_SHELL\u003c/code\u003e environment variable,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ SINGULARITY_SHELL=/bin/bash singularity shell hera-rtp.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor \u003ccode\u003e-s\u003c/code\u003e option,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell -s /bin/bash hera-rtp.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-exec\" class=\"anchor\" href=\"#exec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003eexec\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eThe \u003ccode\u003esingularity exec\u003c/code\u003e command allows you to execute a custom command within a container by specifying the image file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec rtp.sif echo \"Hello World!\"\nHello World!\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e$ cat myscript.sh\nHello World!\n$ singularity exec rtp.sif bash myscript.sh\nHello World!\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-file-permission-and-bind-path\" class=\"anchor\" href=\"#file-permission-and-bind-path\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFile Permission and Bind Path\u003c/h3\u003e\n\u003cp\u003eSingularity containers run as the user and share host services. When Singularity \u2018switch\u2019 from the host operating system to the containerized operating system, the OS-level system files on the host becomes inaccessible. (the root user on the host system is also different from the root in the container!)\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-specific-usages-for-ilifu\" class=\"anchor\" href=\"#specific-usages-for-ilifu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpecific Usages for Ilifu\u003c/h3\u003e\n\u003cp\u003ePlese see the relevant page on the HERA wiki.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 13,
    "topics": [],
    "updated_at": 1626378976.0
  },
  {
    "data_format": 2,
    "description": "GNU Midnight Commander is a visual file manager, licensed under GNU General Public License and therefore qualifies as Free Software.",
    "filenames": [
      "4.8.26/Singularity",
      "4.8.25/Singularity"
    ],
    "full_name": "pscedu/singularity-mc",
    "latest_release": "v4.8.26",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-mc/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-mc/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/20f9b62353e523d3ab71a141bef01e7c6c9b266b5d21ca495fec6bcf5149a691/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6d63\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/20f9b62353e523d3ab71a141bef01e7c6c9b266b5d21ca495fec6bcf5149a691/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6d63\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-mc\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/197b32bc5699c413928a8dc98e7fc7ba78e4232c6d05026a53625842fe00f633/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6d63\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/197b32bc5699c413928a8dc98e7fc7ba78e4232c6d05026a53625842fe00f633/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6d63\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-mc\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/d972290aeff699a37c55f27a4af658b413ceba5d7bc7697189e546d9cc80fa22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6d63\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d972290aeff699a37c55f27a4af658b413ceba5d7bc7697189e546d9cc80fa22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6d63\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-mc\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/3210b070126fcd60c3222ffb78e07d55b8223b3eb1c9f4e7e8cb2b33fb54931c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6d63\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3210b070126fcd60c3222ffb78e07d55b8223b3eb1c9f4e7e8cb2b33fb54931c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6d63\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-mc\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-mc\" class=\"anchor\" href=\"#singularity-mc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-mc\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/07885183f92acdf4c924ec41b91e32bf00dd0b1f3f820c477551a32ec8dfad98/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f392f39622f4d69646e696768745f436f6d6d616e6465725f342e372e302e395f6f6e5f5562756e74755f31312e30342e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/07885183f92acdf4c924ec41b91e32bf00dd0b1f3f820c477551a32ec8dfad98/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f392f39622f4d69646e696768745f436f6d6d616e6465725f342e372e302e395f6f6e5f5562756e74755f31312e30342e706e67\" alt=\"Image\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/9/9b/Midnight_Commander_4.7.0.9_on_Ubuntu_11.04.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/mc\"\u003emc\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003emc\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/mc/4.8.25\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/mc\u003c/code\u003e as \u003ccode\u003e4.8.25.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626981581.0
  },
  {
    "data_format": 2,
    "description": "Containerisation of NEMO Employing Singularity",
    "filenames": [
      "Singularity.nemo"
    ],
    "full_name": "NOC-MSM/CoNES",
    "latest_release": "0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containerisation-of-nemo-employing-singularity-cones\" class=\"anchor\" href=\"#containerisation-of-nemo-employing-singularity-cones\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainerisation of NEMO Employing Singularity (CoNES)\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://cones.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/be5581c539aaf212df6fa28589f126444a52feca29ebc442ea146c24334cc87d/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f636f6e65732f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/cones/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eThese instructions are based on those from \u003ca href=\"https://github.com/singularityhub/singularity-deploy\"\u003eSingularity Deploy\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-write-your-singularity-recipes\" class=\"anchor\" href=\"#1-write-your-singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Write your Singularity Recipe(s)\u003c/h3\u003e\n\u003cp\u003eFirst, you should write your container recipe(s) in the present working directory.\nFor good practice, when you are updating recipes you should checkout a new branch\nand open a pull request, as the repository comes with a workflow to trigger on a PR\nto \u003ca href=\".github/workflows/test.yml\"\u003etest your container build\u003c/a\u003e. Note that in the main workflow\nthat deploys the releases, the current branch is set to be \u003ccode\u003emain-branch\u003c/code\u003e. You should\nupdate this to be your main \"production\" branch that you want to deploy releases on merge.\nYou are also free to choose a different trigger and release strategy. You can add any additional\ntests that that you might need. By default, any Singularity.* file will be automatically detected.\nIf there is no extension (the name Singularity), the name used will be \"latest.\"\nYou can use these tags across multiple releases of your containers. In the case of CoNES,\nwe generate packages with sifs named as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"Singularity.nemo\"\u003eSingularity.nemo\u003c/a\u003e maps to \u003ca href=\"https://github.com/NOC-MSM/CoNES/releases/download/0.0.1/NOC-MSM-CoNES.nemo.sif\"\u003ehttps://github.com/NOC-MSM/CoNES/releases/download/0.0.1/NOC-MSM-CoNES.nemo.sif\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor each name, you can see the direct download URL contains the repository (NOC-MSM/CoNES),\nYou should not use any \u003ccode\u003e:\u003c/code\u003e characters in either your container tag (the GitHub extension) or\nthe GitHub tags (the release tags) as this might interfere with parsing.\nThe GitHub release tag (0.0.1 in the example above) is discussed next.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-update-the-version-file\" class=\"anchor\" href=\"#2-update-the-version-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Update the VERSION file\u003c/h3\u003e\n\u003cp\u003eAny time that you prepare new container recipes, you should update the \u003ca href=\"VERSION\"\u003eVERSION\u003c/a\u003e\nfile. The way that this repository works is to generate a release based on the\nstring in \u003ccode\u003eVERSION\u003c/code\u003e. A version is just a tag, so it could be something like\n\u003ccode\u003e0.0.1\u003c/code\u003e or \u003ccode\u003e0.0.1-slim\u003c/code\u003e. Keep in mind that GitHub releases cannot have duplicated\nnames, so you should not repeat the same tag. Do not use \u003ccode\u003e:\u003c/code\u003e in your tag names.\nIf you do need to re-release a tag (not recommended if a user might be using it and then it\u0027s changed) you can manually delete\nthe release and the tag in the GitHub interface. This is a nice structure because it\nmeans you can have containers with different names under the same tag. In the example\nabove, we have each of \"pokemon,\" \"latest,\" and \"salad\" released under tag 0.0.1.\nThis is how it looks on GitHub:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-how-to-develop\" class=\"anchor\" href=\"#3-how-to-develop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. How to Develop\u003c/h3\u003e\n\u003cp\u003eAs we mentioned previously, the container builds will be tested on a pull request,\nand the release will trigger on merge into your main branch (main). See the \u003ca href=\".github/workflows/builder.yml\"\u003e.github/workflows/builder.yml\u003c/a\u003e)\nto edit this. The idea is that you can:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDevelop your container via a development branch\u003c/li\u003e\n\u003cli\u003eOpen a pull request to test the container (the \u003ca href=\".github/workflows/test.yml\"\u003e.github/workflows/test.yml\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eOn merge, your container will be released!\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-how-to-pull\" class=\"anchor\" href=\"#4-how-to-pull\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. How to pull\u003c/h3\u003e\n\u003cp\u003eTechnically, Singularity can pull just knowing the URL. For example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity pull https://github.com/NOC-MSM/CoNES/releases/download/0.0.1/NOC-MSM-CoNES.nemo.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHowever, the \u003ca href=\"singularity-hpc\"\u003esingularity-hpc\u003c/a\u003e tool (will be) designed to be able to parse and handle\nthese container uris automatically. For the containers here, you could do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:latest\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:salad\n$ shpc pull gh://singularityhub/singularity-deploy/0.0.1:pokemon\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor write the container URI into a registry entry:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egh: sNOC-MSM/CoNES\nlatest:\n  latest: \"0.0.1\"\ntags:\n  \"latest\": \"0.0.1\"\n  \"salad\": \"0.0.1\"\n  \"pokemon\": \"0.0.1\"\nmaintainer: \"@jdha\"\nurl: https://github.com/NOC-MSM/CoNES\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(This part is still under development!)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626700010.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "docker/Singularity.snowflake"
    ],
    "full_name": "pnplab/preprocessing",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-useful-utilities-for-single-cell-processing-with-alevin-fry\" class=\"anchor\" href=\"#useful-utilities-for-single-cell-processing-with-alevin-fry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful utilities for single-cell processing with alevin-fry\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/COMBINE-lab/alevin-fry\"\u003eAlevin-fry\u003c/a\u003e is a fast, accurate and memory-frugal tool for preprocessing single-cell and single-nucleus RNA-seq data.  You can read more about alevin-fry in \u003ca href=\"https://www.biorxiv.org/content/10.1101/2021.06.29.450377v1\" rel=\"nofollow\"\u003eits pre-print\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis respoistory contains scripts, functions and utilities that are useful for preparing data for processing with alevin-fry, as well as for reading alevin-fry data into other packages for downstream analysis.\u003c/p\u003e\n\u003cp\u003eThe different utilities are broken down in this repository by the language in which they are written (right now, Python, R and bash).  A brief listing of\nthe available utilities currently in the repository is:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-r-language\" class=\"anchor\" href=\"#r-language\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR language\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e \u2014 A script to build a spliced + intron (splici) ref for indexing and quantification with \u003ccode\u003ealevin-fry\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esplici.R\u003c/code\u003e \u2014 Contains the \u003ccode\u003emake_splici_txome\u003c/code\u003e function, which is the function called by the \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e wrapper script.  If you want to build a splici reference programatically in R code, you can use this function.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecellRangerLikeEmptyDrops.R\u003c/code\u003e \u2014 An implementation of the hybrid UMI count filtering and \u003ca href=\"https://github.com/MarioniLab/DropletUtils\"\u003e\u003ccode\u003eemptyDrops\u003c/code\u003e\u003c/a\u003e used by CellRanger (and subsequently by \u003ca href=\"https://github.com/alexdobin/STAR\"\u003eSTARsolo\u003c/a\u003e). This R implementation is a translation of the implemntation in STARsolo, which itself was reverse-engineered from CellRanger.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eload_fry.R\u003c/code\u003e \u2014 Contains a function to load \u003ccode\u003ealevin-fry\u003c/code\u003e output (including from USA mode quantification) into a \u003ca href=\"https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html\" rel=\"nofollow\"\u003e\u003ccode\u003eSingleCellExperiment\u003c/code\u003e\u003c/a\u003e object.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-language\" class=\"anchor\" href=\"#python-language\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython language\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eload_fry.py\u003c/code\u003e \u2014 Contains a Python function \u003ccode\u003eload_fry\u003c/code\u003e which is intended to load \u003ccode\u003ealevin-fry\u003c/code\u003e output (including from USA mode quantification) into a \u003ca href=\"https://github.com/theislab/scanpy\"\u003e\u003ccode\u003eScanpy\u003c/code\u003e\u003c/a\u003e object.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-bash\" class=\"anchor\" href=\"#bash\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBash\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eget_10x_permit_lists.sh\u003c/code\u003e \u2014 Provides a script to download the 10x chromium v2 or v3 permit lists.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esimpleaf\u003c/code\u003e \u2014 Provides a script to run the entire \u003ccode\u003esalmon -\u0026gt; alevin-fry (generate-permit-list \u0026gt; collate \u0026gt; quant)\u003c/code\u003e pipeline, though providing only a simplified set of options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-simpleaf\" class=\"anchor\" href=\"#using-simpleaf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing simpleaf\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003esimpleaf\u003c/code\u003e script that resides in the \u003ccode\u003ebash\u003c/code\u003e subdirectory is intended to simply the running of \u003ccode\u003ealevin-fry\u003c/code\u003e in common usage scenarios.  By limiting some of the different options that can be set, it provides a streamlined way to build the splici reference and index in a single command, as well as to process an experiment from raw FASTQ files to a count matrix in a single command.\u003c/p\u003e\n\u003cp\u003eTo work properly, \u003ccode\u003esimpleaf\u003c/code\u003e has a few requirements.  First, it should be run from \u003cem\u003ewithin\u003c/em\u003e the \u003ccode\u003ebash\u003c/code\u003e subdirectory of this repository.  This is because it currently makes assumptions about the relative paths of the scripts \u003ccode\u003eget_10x_permit_lists.sh\u003c/code\u003e and \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e.  Additionally, the following environment variables are used within \u003ccode\u003esimpleaf\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eALEVIN_FRY_HOME\u003c/code\u003e \u003cstrong\u003eREQUIRED\u003c/strong\u003e \u2014 This directory will be used for persistent configuration and small file (\u0026lt;1G) storage between runs.  If you provide a directory and it doesn\u0027t exist, it will be created.  It is easiest to just set this in your enviornment globally so that the same home can be used over many runs without you having to provide the variable explicitly each time.  A good choice for this variable might be something like \u003ccode\u003e~/.alevin_fry_home\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSALMON_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ccode\u003esalmon\u003c/code\u003e executable of version \u0026gt;= 1.5.1.  If not provided, the script will assume it can simply invoke \u003ccode\u003esalmon\u003c/code\u003e in the current enviornment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eFRY_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ccode\u003ealevin-fry\u003c/code\u003e executable of version \u0026gt;= 0.4.0.  If not provided, the script will assume it can simply invoke \u003ccode\u003ealevin-fry\u003c/code\u003e in the current enviornment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eTIME_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ca href=\"https://www.gnu.org/software/time/\" rel=\"nofollow\"\u003eGNU time\u003c/a\u003e executable; this is different from the shell \u003ccode\u003etime\u003c/code\u003e command, and on most linux systems exists at \u003ccode\u003e/usr/bin/time\u003c/code\u003e.  If this variable is not provided, the script will assume it can use \u003ccode\u003e/usr/bin/time\u003c/code\u003e.  On OSX systems, you should install GNU time explicitly.  This can be done with \u003ca href=\"https://anaconda.org/conda-forge/time\" rel=\"nofollow\"\u003econda\u003c/a\u003e or homebrew.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ccode\u003esimpleaf\u003c/code\u003e script has two sub-commands:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eindex\u003c/code\u003e \u2014 The \u003ccode\u003eindex\u003c/code\u003e command will take a reference genome FASTA and GTF as input, build a splici reference using the \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e script, and then build a sparse \u003ccode\u003esalmon\u003c/code\u003e index on the resulting reference. \u003cstrong\u003eNote\u003c/strong\u003e: The \u003ccode\u003eindex\u003c/code\u003e command requires the \u003ccode\u003eRscript\u003c/code\u003e executable to be in the path, as well as all of theR packages that are required by \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e. The relevant options (which you can obtain by running \u003ccode\u003e./simpleaf index -h\u003c/code\u003e) are:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{bash}\"\u003e\u003ccode\u003eUsage: ./simpleaf index [options]\n      options:\n       -f, --fasta REQUIRED genome reference FASTA file\n       -g, --gtf REQUIRED GTF file with gene annotations\n       -l, --rlen REQUIRED the target read length the index will be built for\n       -o, --output REQUIRED path to output directory (will be created if it doesn\u0027t exist)\n       -s, --spliced OPTIONAL path to FASTA file with extra spliced sequence to add to the index\n       -u, --unspliced OPTIONAL path to FASTA file with extra unspliced sequence to add to the index\n       -d, --dedup FLAG OPTIONAL deduplicate identical sequences inside the R script when building the splici reference\n       -t, --threads OPTIONAL number of threads to use when running [default: min(16, num cores)]\n       -h, --help display this help message\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003equant\u003c/code\u003e \u2014 The \u003ccode\u003equant\u003c/code\u003e command takes as input the index, reads, and relevant information about the experiment (e.g. chemistry), and runs all of the steps of the \u003ccode\u003ealevin-fry\u003c/code\u003e pipeline, from mapping with \u003ccode\u003esalmon\u003c/code\u003e through \u003ccode\u003equant\u003c/code\u003e with \u003ccode\u003ealevin-fry\u003c/code\u003e. The relevant options (which you can obtain by running \u003ccode\u003e./simpleaf quant -h\u003c/code\u003e) are:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{bash}\"\u003e\u003ccode\u003eUsage: ./simpleaf quant [options]\n      options:\n       -1, --r1 REQUIRED comma separated list of left reads\n       -2, --r2 REQUIRED comma separated list of right reads\n       -i, --index REQUIRED path to a (sparse or dense) salmon splici index\n       -o, --output REQUIRED path to output directory (will be created if it doesn\u0027t exist)\n       -f, --fmode REQUIRED permit list filter mode, one of {knee, k, unfilt, u}\n       -c, --chem REQUIRED chemistry of experiment, one of {v2, v3}\n       -r, --res REQUIRED resolution strategy for alevin-fry, one of {cr-like, cr-like-em}\n       -m, --t2g REQUIRED three-column txp-to-gene file to pass to alevin-fry quant command\n       -t, --threads OPTIONAL number of threads to use when running [default: min(16, num cores)]\n       -h, --help display this help message\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1626495060.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.0.1.1",
      "Singularity.0.1"
    ],
    "full_name": "dcgc-bfx/singularity-base-conda",
    "latest_release": "v0.1-alpha",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/dcgc-bfx/dcgc-base-conda/workflows/Build/badge.svg?branch=main\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/dcgc-bfx/dcgc-base-conda/workflows/Build/badge.svg?branch=main\" alt=\"Build\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5252\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dcgc-base-conda\" class=\"anchor\" href=\"#dcgc-base-conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edcgc-base-conda\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626686541.0
  },
  {
    "data_format": 2,
    "description": "modified version of nicMSlesions",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jstutters/nicpython36",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ms_cnn\" class=\"anchor\" href=\"#ms_cnn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMS_CNN\u003c/h1\u003e\n\u003cp\u003e[This is a modified version of nicMSlesions (\u003ca href=\"https://github.com/NIC-VICOROB/nicMSlesions\"\u003ehttps://github.com/NIC-VICOROB/nicMSlesions\u003c/a\u003e)]\n\u003cbr\u003e\n\u003ca href=\"CNN.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"300\" src=\"CNN.jpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-this--version-support-additionally-the-following-functionalities\" class=\"anchor\" href=\"#this--version-support-additionally-the-following-functionalities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThis  version support additionally the following functionalities:\u003c/h1\u003e\n\u003cdl\u003e\n  \u003cdt\u003e(1) Runnable on a Mac system/computer\u003c/dt\u003e\n  \u003cdt\u003e(2) Cold start and warm start support:\u003c/dt\u003e\n  \u003cdd\u003e- Allowing to re-create the architecture of the model\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to use the saved weights of the model\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to use  the training configuration and avoiding to run preprocessing again\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to resume training exactly where it left off(interrupting the training is     \n    allowed throughout the training process)\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to use pretrained model\u003c/dd\u003e\n  \u003cdt\u003e(3) Supporting Python 3\u003c/dt\u003e\n  \u003cdt\u003e(4) Integrated Tensorborad [to provide the measurements and visualisations of TensorFlow execution (to understand, debug, and optimisation of  the TensorFlow programs)]\u003c/dt\u003e\n  \u003cdt\u003e(5) Checking whether a file or directory is relevant for Training and Testing\u003c/dt\u003e \n  \u003cdt\u003e(6) Easy HPC (High Performance Computing) support\u003c/dt\u003e \n  \u003cdt\u003e(7) Bias correction of masks using FSL\u003c/dt\u003e\n  \u003cdt\u003e(8) Registration, moving all images to the Flair, T1 or Standard space\u003c/dt\u003e\n\u003c/dl\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"BR.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"500\" src=\"BR.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"note.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"100\" src=\"note.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n# Running the Program!\n\u003cp\u003eThis modified version can be run with or without a GUI (similar to original version)\u003c/p\u003e\n\u003cp\u003eAfter lunching the graphical user interface, user will need to provide necessary information to start training/testing as follows:\u003c/p\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"GUI_NM.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"500\" src=\"GUI_NM.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003ch1\u003e\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\" class=\"anchor\" href=\"#running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the Program on the HPC cluster using NVIDIA GPUs(without any additional library/dependency installation):\u003c/h1\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"hpc.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"200\" src=\"hpc.jpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003cp\u003eFirst, user will need to be sure that \"singularity\"\n\u003ca href=\"https://singularity.lbl.gov/\" rel=\"nofollow\"\u003ehttps://singularity.lbl.gov/\u003c/a\u003e\nis available on local or remote machine.\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity pull docker://kbronik/ms_cnn_ucl:latest  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfter running the above, a singularity image using docker hub (docker://kbronik/ms_cnn_ucl:latest) will be generated:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - path to singularity//..///ms_cnn_ucl_latest.sif  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFinally:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity run --nv   (path to singularity)//..///ms_cnn_ucl_latest.sif  python  (path to nicpython36)/nic_train_network_batch.py (or other nic-python code)\u003c/pre\u003e\u003c/div\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"note_HPC.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"120\" src=\"note_HPC.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003ch1\u003e\n\u003ca id=\"user-content-for-an-interactive-session\" class=\"anchor\" href=\"#for-an-interactive-session\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor an interactive session:\u003c/h1\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity shell   (path to singularity)//..///ms_cnn_ucl_latest.sif \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - \u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e activate idp\n  - python (path to nicpython36)/app.py\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-for-an-interactive-session-tensorflow-on-cpu-only\" class=\"anchor\" href=\"#for-an-interactive-session-tensorflow-on-cpu-only\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor an interactive session (TensorFlow on CPU only):\u003c/h1\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e  docker://kbronik/ms-ucl-cnn-cpu:CPU_Latest  python  (path to nicpython36)/app.py \u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1624933808.0
  },
  {
    "data_format": 2,
    "description": "proof of concept for running singularity in a singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "lkirk/singularity-in-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-in-singularity\" class=\"anchor\" href=\"#singularity-in-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity in singularity\u003c/h1\u003e\n\u003cp\u003eThis is a proof-of-concept to show that it is indeed possible to run nested singularity processes.\nMy purpose for doing this is to create containers that can run applications that are in other other containers, allowing me to decompose the containers into small, purpose-built units.\u003c/p\u003e\n\u003cp\u003eTo test this for yourself, you can do the following:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003esudo singularity build container.sif Singularity\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003esingularity shell container.sif\u003c/span\u003e\n\n# \u003cspan class=\"pl-s1\"\u003ethen, go ahead and try running\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003esingularity shell container.sif\u003c/span\u003e\n# \u003cspan class=\"pl-s1\"\u003eas many \u003cspan class=\"pl-c1\"\u003etimes\u003c/span\u003e as you want\u003cspan class=\"pl-k\"\u003e!\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHere is an example session where I nest 3 containers:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euser@host$ singularity shell container.sif\nSingularity\u0026gt; singularity shell container.sif\nINFO:    Converting SIF file to temporary sandbox...\nSingularity\u0026gt; singularity shell container.sif\nINFO:    Converting SIF file to temporary sandbox...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd the resulting process tree (reported by htop):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSingularity runtime parent\n\u251c\u2500 /bin/bash --norc\n\u2502  \u2514\u2500 Singularity runtime parent\n\u2502     \u251c\u2500 /bin/bash --norc\n\u2502     \u2502  \u2514\u2500 Singularity runtime parent\n\u2502     \u2502     \u251c\u2500 /bin/bash --norc\n\u2502     \u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u2502     \u2514\u2500 Singularity runtime parent\n\u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u251c\u2500 Singularity runtime parent\n\u2502     \u2514\u2500 Singularity runtime parent\n\u251c\u2500 Singularity runtime parent\n\u251c\u2500 Singularity runtime parent\n\u251c\u2500 Singularity runtime parent\n\u251c\u2500 Singularity runtime parent\n\u251c\u2500 Singularity runtime parent\n\u251c\u2500 Singularity runtime parent\n\u2514\u2500 Singularity runtime parent\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you do not want to coerce conversion to a temporary sandbox on every call (it can be time intensive for large images), you can simply create the sandbox upfront:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euser@host$ singularity build --sandbox test container.sif\nWARNING: \u0027nodev\u0027 mount option set on /tmp, it could be a source of failure during build process\nINFO:    Starting build...\nINFO:    Verifying bootstrap image container.sif\nINFO:    Creating sandbox directory...\nINFO:    Build complete: test\nuser@host$ singularity shell container.sif\nSingularity\u0026gt; singularity shell test\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626825365.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.05131402",
      "Singularity.tut0804",
      "Singularity.05221357",
      "Singularity.1908121107",
      "Singularity",
      "Singularity.386",
      "Singularity.cuda10",
      "Singularity.05211526",
      "Singularity.sf",
      "Singularity.05201328"
    ],
    "full_name": "timkphd/Containers",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container-build-scripts\" class=\"anchor\" href=\"#singularity-container-build-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container build Scripts\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-see-httpssingularity-huborgcollections2962\" class=\"anchor\" href=\"#see-httpssingularity-huborgcollections2962\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSee \u003ca href=\"https://singularity-hub.org/collections/2962\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/2962\u003c/a\u003e\n\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:16.04, , R, MPI (intel and openMPI ), python\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.05131402\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:16.04, basic stuff, does not actually install Intel Python\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.05201328\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:16.04, R, MPI, python, tutorial stuff\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.05211526\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:16.04, R, MPI, python, tutorial stuff\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.05221357 (Built)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:16.04, R, MPI, python, tutorial stuff\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.1908121107 (Built)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:latest, R, MPI, python, tutorial stuff\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.386 (Built)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBasic 32 bit with Fortran, c++ make, nano,vim\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.sf (Built)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:18.04, STAR-Fusion\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.tut0804\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eubuntu:16.04, R, MPI, python, tutorial stuff\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1627001875.0
  },
  {
    "data_format": 2,
    "description": "\u5b8f\u57fa\u56e0\u7ec4\u76f8\u5173\u4ee3\u7801",
    "filenames": [
      "scripts/lathe/singularity/Singularity.longread",
      "scripts/lathe/singularity/Singularity.quickmerge",
      "scripts/lathe/singularity/Singularity.htsbox"
    ],
    "full_name": "JiaLonghao1997/Microbiome",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-microbiome\" class=\"anchor\" href=\"#microbiome\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMicrobiome\u003c/h1\u003e\n\u003cp\u003e\u5b8f\u57fa\u56e0\u7ec4\u76f8\u5173\u4ee3\u7801\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626932334.0
  },
  {
    "data_format": 2,
    "description": "RAdiation SEmiconductoR ",
    "filenames": [
      "Singularity"
    ],
    "full_name": "dt-np/raser",
    "latest_release": "v1.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-raser\" class=\"anchor\" href=\"#raser\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRASER\u003c/h1\u003e\n\u003cp\u003eRAdiation SEmiconductoR\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-with-singularity\" class=\"anchor\" href=\"#build-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild with Singularity\u003c/h1\u003e\n\u003cp\u003eBefore running the code, install the Singularity on your OS.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e./sinularity_build.sh\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e./singularity_run.sh\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eraser\u0026gt; geant4_build.sh\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-with-singularity\" class=\"anchor\" href=\"#run-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun with Singularity\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e./singularity_run.sh\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eraser\u0026gt; ./run\u003c/p\u003e\n\u003c/blockquote\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626955371.0
  },
  {
    "data_format": 2,
    "description": "HTSlib A C library for reading/writing high-throughput sequencing data. ",
    "filenames": [
      "1.13/Singularity"
    ],
    "full_name": "pscedu/singularity-htslib",
    "latest_release": "v1.13",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-htslib/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-htslib/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/c19127d592d5b1774f1b776b581a4ee3e088dc8836040a4dcfb0112e233e0272/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6874736c6962\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c19127d592d5b1774f1b776b581a4ee3e088dc8836040a4dcfb0112e233e0272/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6874736c6962\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-htslib\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/7eb94edba6c28c8efe671ccb26366254e3fa67b9ba22e17183a8353c985c30f7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6874736c6962\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7eb94edba6c28c8efe671ccb26366254e3fa67b9ba22e17183a8353c985c30f7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6874736c6962\" alt=\"Forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-htslib\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/0c4e24ae23aaa5d0244c3ba0370b21835d696a720659445acd0879d08953dbf5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6874736c6962\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0c4e24ae23aaa5d0244c3ba0370b21835d696a720659445acd0879d08953dbf5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6874736c6962\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-htslib\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e0b439a0b671ca5e751012f4801e4febe27cb2fa88ea95ed862cca4a347af459/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6874736c6962\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e0b439a0b671ca5e751012f4801e4febe27cb2fa88ea95ed862cca4a347af459/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6874736c6962\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-htslib\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-htslib\" class=\"anchor\" href=\"#singularity-htslib\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-htslib\u003c/h2\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/samtools/htslib\"\u003ehtslib\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ehtsfile\u003c/code\u003e, \u003ccode\u003etabix\u003c/code\u003e and  \u003ccode\u003ebgzip\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/htslib/1.13\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/htslib\u003c/code\u003e as \u003ccode\u003e1.13.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626982203.0
  },
  {
    "data_format": 2,
    "description": "Samtools is a suite of programs for interacting with high-throughput sequencing data.",
    "filenames": [
      "1.13.0/Singularity",
      "1.11.0/Singularity",
      "1.10.0/Singularity"
    ],
    "full_name": "pscedu/singularity-samtools",
    "latest_release": "v1.13",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-samtools/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-samtools/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/b27c44fffb3dfd07d3f2fae366a0624515bb7434a51439a717a4eb270c0646fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b27c44fffb3dfd07d3f2fae366a0624515bb7434a51439a717a4eb270c0646fa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-samtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/47a7d0220a86fa07d316c74e429ee984b0a1f74fade49760c7eb7c9e17b98e99/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/47a7d0220a86fa07d316c74e429ee984b0a1f74fade49760c7eb7c9e17b98e99/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-samtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/27e87cc5b7585d03ee2801f8e12e651ee9bf3df1d7a90f08f8cc2d5b5f5690f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/27e87cc5b7585d03ee2801f8e12e651ee9bf3df1d7a90f08f8cc2d5b5f5690f4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-samtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/8dd1f706a75d5d11aa34dd149a9e9e5bb5ec9681ec8373e56c45f17fbce01fb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8dd1f706a75d5d11aa34dd149a9e9e5bb5ec9681ec8373e56c45f17fbce01fb2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d73616d746f6f6c73\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-samtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-samtools\" class=\"anchor\" href=\"#singularity-samtools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-samtools\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://www.htslib.org/\" rel=\"nofollow\"\u003esamtools\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003eace2sam\u003c/code\u003e, \u003ccode\u003eblast2sam.pl\u003c/code\u003e \u003ccode\u003ebowtie2sam.pl\u003c/code\u003e, \u003ccode\u003eexport2sam.pl\u003c/code\u003e, \u003ccode\u003efasta-sanitize.pl\u003c/code\u003e, \u003ccode\u003egenerate_binaries.sh\u003c/code\u003e, \u003ccode\u003einterpolate_sam.pl\u003c/code\u003e, \u003ccode\u003emaq2sam-long\u003c/code\u003e, \u003ccode\u003emaq2sam-short\u003c/code\u003e, \u003ccode\u003emd5fa\u003c/code\u003e, \u003ccode\u003emd5sum-lite\u003c/code\u003e, \u003ccode\u003enovo2sam.pl\u003c/code\u003e, \u003ccode\u003eplot-ampliconstats\u003c/code\u003e, \u003ccode\u003eplot-bamstats\u003c/code\u003e, \u003ccode\u003epsl2sam.pl\u003c/code\u003e, \u003ccode\u003esam2vcf.pl\u003c/code\u003e, \u003ccode\u003esamtools\u003c/code\u003e, \u003ccode\u003esamtools.pl\u003c/code\u003e, \u003ccode\u003eseq_cache_populate.pl\u003c/code\u003e, \u003ccode\u003esoap2sam.pl\u003c/code\u003e, \u003ccode\u003ewgsim\u003c/code\u003e, \u003ccode\u003ewgsim_eval.pl\u003c/code\u003e and \u003ccode\u003ezoom2sam.pl\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/samtools/1.13.0\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/samtools\u003c/code\u003e as \u003ccode\u003e1.13.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626983164.0
  },
  {
    "data_format": 2,
    "description": "Convert a VCF into a MAF, where each variant is annotated to only one of all possible gene isoforms",
    "filenames": [
      "1.6.21/Singularity"
    ],
    "full_name": "pscedu/singularity-vcf2maf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-vcf2maf/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-vcf2maf/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/01db6c8fcf2eeb01ab319708cd86ccda638c916f6d19010a297a891186ac6b87/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d766366326d6166\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/01db6c8fcf2eeb01ab319708cd86ccda638c916f6d19010a297a891186ac6b87/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d766366326d6166\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-vcf2maf\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/31085d43f70d09cc4aa42ab183a672d482b451babd6adee0e95909856d64a0aa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d766366326d6166\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/31085d43f70d09cc4aa42ab183a672d482b451babd6adee0e95909856d64a0aa/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d766366326d6166\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-vcf2maf\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a9219a5e2629d2415beac1fdf1bc9e27b3f8439f7079d30ea52125fcf50b59b3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d766366326d6166\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a9219a5e2629d2415beac1fdf1bc9e27b3f8439f7079d30ea52125fcf50b59b3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d766366326d6166\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-vcf2maf\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4e821b0428255af47ee55b03ace554634255969dc1869dc763b379a501446202/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d766366326d6166\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4e821b0428255af47ee55b03ace554634255969dc1869dc763b379a501446202/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d766366326d6166\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-vcf2maf\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-vcf2maf\" class=\"anchor\" href=\"#singularity-vcf2maf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-vcf2maf\u003c/h2\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/mskcc/vcf2maf\"\u003evcf2maf\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003evcf2maf\u003c/code\u003e, \u003ccode\u003evcf2vcf\u003c/code\u003e, \u003ccode\u003emaf2maf\u003c/code\u003e and \u003ccode\u003emaf2vcf\u003c/code\u003e scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/vcf2maf/1.6.21\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/vcf2maf\u003c/code\u003e as \u003ccode\u003e1.6.21.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "bioinformatics"
    ],
    "updated_at": 1626985512.0
  },
  {
    "data_format": 2,
    "description": "ncdu is a disk utility for Unix systems",
    "filenames": [
      "1.16/Singularity",
      "1.13/Singularity"
    ],
    "full_name": "pscedu/singularity-ncdu",
    "latest_release": "v1.15",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-ncdu/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-ncdu/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/eaa1a97bcc02fbffef0179891a67cb9d34371fdbf6c61570a97001c1dff2ea72/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6e636475\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/eaa1a97bcc02fbffef0179891a67cb9d34371fdbf6c61570a97001c1dff2ea72/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6e636475\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-ncdu\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/c473372da3ec18d0c9c5900e104c88f5f0f2cee7d198db3d5f8f58680a68c7bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6e636475\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c473372da3ec18d0c9c5900e104c88f5f0f2cee7d198db3d5f8f58680a68c7bc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6e636475\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-ncdu\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/99ed580080c0f8fd01c853788721b23ce51195660c271dae604f0ed589c3396c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6e636475\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/99ed580080c0f8fd01c853788721b23ce51195660c271dae604f0ed589c3396c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6e636475\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-ncdu\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/809a1d2881b7c8af4d47d10ea094ef76bd3497a15ae6b290eea9545b8865f985/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6e636475\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/809a1d2881b7c8af4d47d10ea094ef76bd3497a15ae6b290eea9545b8865f985/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6e636475\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-ncdu\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-ncdu\" class=\"anchor\" href=\"#singularity-ncdu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-ncdu\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/1f9d5b0052b4be66a4d6d7f14c03622a3a6851fd2fef41140de28ebbb4514c46/68747470733a2f2f6465762e796f7268656c2e6e6c2f696d672f6e63647568656c70322d322e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1f9d5b0052b4be66a4d6d7f14c03622a3a6851fd2fef41140de28ebbb4514c46/68747470733a2f2f6465762e796f7268656c2e6e6c2f696d672f6e63647568656c70322d322e706e67\" alt=\"Screenshot\" data-canonical-src=\"https://dev.yorhel.nl/img/ncduhelp2-2.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://dev.yorhel.nl/ncdu\" rel=\"nofollow\"\u003encdu\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003encdu\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/ncdu/1.16\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/ncdu\u003c/code\u003e as \u003ccode\u003e1.16.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1627060161.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "0.22.3/Singularity"
    ],
    "full_name": "pscedu/singularity-bismark",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bismark/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bismark/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ac6bb8f2b406618278034b709c736e772b92723686bd930d0ec0e0caaf278599/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6269736d61726b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ac6bb8f2b406618278034b709c736e772b92723686bd930d0ec0e0caaf278599/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6269736d61726b\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bismark\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5f93c1931fec2c3f8a38122749025c57f09fb930eb75c591ad412dfa911b97ae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6269736d61726b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5f93c1931fec2c3f8a38122749025c57f09fb930eb75c591ad412dfa911b97ae/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6269736d61726b\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bismark\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f20c48b5b2226a8f0a7cff096f8cedfe80b073f0b801009d9684d408ee31ca07/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6269736d61726b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f20c48b5b2226a8f0a7cff096f8cedfe80b073f0b801009d9684d408ee31ca07/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6269736d61726b\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bismark\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/07c4815a6c07dc179ddc97de5e5faa3c7fec941cc2c81a94adaed0547946fd27/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6269736d61726b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/07c4815a6c07dc179ddc97de5e5faa3c7fec941cc2c81a94adaed0547946fd27/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6269736d61726b\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bismark\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bismark\" class=\"anchor\" href=\"#singularity-bismark\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bismark\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/FelixKrueger/Bismark\"\u003ebismark\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ebismark\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bismark/0.22.3\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/bismark\u003c/code\u003e as \u003ccode\u003e0.22.3.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1627059389.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.10/Singularity"
    ],
    "full_name": "pscedu/singularity-cutadapt",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-cutadapt/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-cutadapt/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/1a971ebb81368ce57d4702d1ac0fa0534e1de4e11c2f3a1fc98cb5c5203ae017/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6375746164617074\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1a971ebb81368ce57d4702d1ac0fa0534e1de4e11c2f3a1fc98cb5c5203ae017/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6375746164617074\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-cutadapt\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/460062e41af5fcb53936a892aaf05699f5552d0c369d8c9d05308a15ecb18032/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6375746164617074\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/460062e41af5fcb53936a892aaf05699f5552d0c369d8c9d05308a15ecb18032/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6375746164617074\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-cutadapt\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5403066544c7ca2002d9c3183ace39dbb1aabbed23f96489c23f815b84b3a31f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6375746164617074\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5403066544c7ca2002d9c3183ace39dbb1aabbed23f96489c23f815b84b3a31f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6375746164617074\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-cutadapt\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/d60627004d3b1d480327260e840077df4c84113f1dcd462a39d25dfc08daae2a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6375746164617074\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d60627004d3b1d480327260e840077df4c84113f1dcd462a39d25dfc08daae2a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6375746164617074\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-cutadapt\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-cutadapt\" class=\"anchor\" href=\"#singularity-cutadapt\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-cutadapt\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://cutadapt.readthedocs.io/en/stable\" rel=\"nofollow\"\u003ecutadapt\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ecutadapt\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/cutadapt/2.10\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/cutadapt\u003c/code\u003e as \u003ccode\u003e2.10.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1627059795.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.6.3/Singularity"
    ],
    "full_name": "pscedu/singularity-prodigal",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-prodigal/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-prodigal/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/dbf23f859880f70436ee9de9ee89a1528a5171defb337385614da4eee0ffeb2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d70726f646967616c\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/dbf23f859880f70436ee9de9ee89a1528a5171defb337385614da4eee0ffeb2d/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d70726f646967616c\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-prodigal\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f5f6ed86bbd6c79a6449e24738f02f9844e1fe9972bd1d162eb62316c06bf9dc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d70726f646967616c\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f5f6ed86bbd6c79a6449e24738f02f9844e1fe9972bd1d162eb62316c06bf9dc/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d70726f646967616c\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-prodigal\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4ef2a97630fd3ac7037f653270a9129fe20826c8bf90956c9b995c0eea86da02/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d70726f646967616c\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4ef2a97630fd3ac7037f653270a9129fe20826c8bf90956c9b995c0eea86da02/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d70726f646967616c\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-prodigal\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/e41f95bac858eb80bd1956886531b10a1a335ab73e0c0cbdc7ab720cf22824cd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d70726f646967616c\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e41f95bac858eb80bd1956886531b10a1a335ab73e0c0cbdc7ab720cf22824cd/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d70726f646967616c\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-prodigal\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-prodigal\" class=\"anchor\" href=\"#singularity-prodigal\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-prodigal\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sandialabs/prodigal\"\u003eprodigal\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003eprodigal\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/prodigal/2.6.3\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/prodigal\u003c/code\u003e as \u003ccode\u003e2.6.3.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1627059905.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "lkirk/nb-env",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nb-env\" class=\"anchor\" href=\"#nb-env\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enb-env\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627204676.0
  },
  {
    "data_format": 2,
    "description": "Singularity example 1: Hello World",
    "filenames": [
      "Singularity"
    ],
    "full_name": "richelbilderbeek/singularity_example_1",
    "latest_release": "v2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_example_1\" class=\"anchor\" href=\"#singularity_example_1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_example_1\u003c/h1\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eBranch\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://github.com/richelbilderbeek/singularity_example_1/actions\"\u003e\u003cimg src=\"GitHubActions.png\" alt=\"GitHub Actions logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003emaster\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/richelbilderbeek/singularity_example_1/workflows/build_singularity/badge.svg?branch=master\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/richelbilderbeek/singularity_example_1/workflows/build_singularity/badge.svg?branch=master\" alt=\"build_singularity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003edevelop\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/richelbilderbeek/singularity_example_1/workflows/build_singularity/badge.svg?branch=develop\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/richelbilderbeek/singularity_example_1/workflows/build_singularity/badge.svg?branch=develop\" alt=\"build_singularity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eSingularity example 1: Hello World.\u003c/p\u003e\n\u003cp\u003eSee \u003ca href=\"Singularity\"\u003eSingularity\u003c/a\u003e (a script) to see what the container does.\u003c/p\u003e\n\u003cp\u003eThis repo builds the container, runs it and uploads it.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1627290514.0
  },
  {
    "data_format": 2,
    "description": "ElikoPy is Python library aiming at easing the processing of diffusion imaging for microstructural analysis.",
    "filenames": [
      "Singularity_elikopy"
    ],
    "full_name": "Hyedryn/elikopy",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-elikopy\" class=\"anchor\" href=\"#elikopy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eElikoPy\u003c/h1\u003e\n\u003cp\u003eElikoPy is Python library aiming at easing the processing of diffusion imaging for microstructural analysis.\nThis Python library is based on\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDIPY, a python library for the analysis of MR diffusion imaging.\u003c/li\u003e\n\u003cli\u003eMicrostructure fingerprinting, a python library doing estimation of white matter microstructural properties from a dictionary of Monte Carlo diffusion MRI fingerprints.\u003c/li\u003e\n\u003cli\u003eFSL, a comprehensive library of analysis tools for FMRI, MRI and DTI brain imaging data.\u003c/li\u003e\n\u003cli\u003eDIAMOND, a c software that is characterizing brain tissue by assessment of the distribution of anisotropic microstructural environments in diffusion\u2010compartment imaging.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eElikoPy requires \u003ca href=\"https://www.python.org/\" rel=\"nofollow\"\u003ePython\u003c/a\u003e v3.7+ to run.\u003c/p\u003e\n\u003cp\u003eAfter cloning the repo, you can either firstly install all the python dependencies including optionnal dependency used to speed up the code:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ pip install -r requirements.txt --user\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOr you can install directly the library with only the mandatory dependencies (if you performed the previous step, you still need to perform this step):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ python3 setup.py install --user\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eMicrostructure Fingerprinting is currently not avaible in the standard python repo, you can clone and install this library manually.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone git@github.com:rensonnetg/microstructure_fingerprinting.git\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e microstructure_fingerprinting\n$ python setup.py install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFSL also needs to be installed and availabe in our path if you want to perform mouvement correction or tbss.\u003c/p\u003e\n\u003cp\u003eUnfortunatly, the DIAMOND code is not publically available. If you do not have it in your possesion, you will not be able to use this algorithm. If you have it, simply add the executable to your path.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cp\u003eTodo\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h3\u003e\n\u003cp\u003eWant to contribute? Great!\u003c/p\u003e\n\u003cp\u003eDo not hesitate to open issue or pull request!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTodos\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRelease a complete and accurate documentation for the library\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eFree Software, Hell Yeah!\u003c/strong\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "microstructure-fingerprinting",
      "fsl",
      "tbss",
      "python-library",
      "diffusion-imaging",
      "preprocessing",
      "dmri",
      "diamond",
      "noddi",
      "dti"
    ],
    "updated_at": 1627366965.0
  },
  {
    "data_format": 2,
    "description": "Work with PLINK from R",
    "filenames": [
      "Singularity"
    ],
    "full_name": "richelbilderbeek/plinkr",
    "latest_release": "v0.14",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-plinkr\" class=\"anchor\" href=\"#plinkr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eplinkr\u003c/h1\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eBranch\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://github.com/richelbilderbeek/plinkr/actions\"\u003e\u003cimg src=\"man/figures/GitHubActions.png\" alt=\"GitHub Actions logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://ci.appveyor.com/project/richelbilderbeek/plinkr/\" rel=\"nofollow\"\u003e\u003cimg src=\"man/figures/AppVeyor.png\" alt=\"AppVeyor logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/th\u003e\n\u003cth\u003e\u003ca href=\"https://www.codecov.io\" rel=\"nofollow\"\u003e\u003cimg src=\"man/figures/Codecov.png\" alt=\"Codecov logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003emaster\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/richelbilderbeek/plinkr/workflows/R-CMD-check/badge.svg?branch=master\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/richelbilderbeek/plinkr/workflows/R-CMD-check/badge.svg?branch=master\" alt=\"R-CMD-check\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://ci.appveyor.com/project/richelbilderbeek/plinkr\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cc491fb91e90bff218654de25d88bac87bb154025c9c13099b8c3cdb0412f373/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f72696368656c62696c6465726265656b2f706c696e6b723f6272616e63683d6d6173746572267376673d74727565\" alt=\"AppVeyor build status\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/github/richelbilderbeek/plinkr?branch=master\u0026amp;svg=true\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://codecov.io/github/richelbilderbeek/plinkr/branch/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8fafd8823f437cd6a912937658b53c50edd357b324f8d239d71a476d11c8859c/68747470733a2f2f636f6465636f762e696f2f6769746875622f72696368656c62696c6465726265656b2f706c696e6b722f636f7665726167652e7376673f6272616e63683d6d6173746572\" alt=\"codecov.io\" data-canonical-src=\"https://codecov.io/github/richelbilderbeek/plinkr/coverage.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003edevelop\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/richelbilderbeek/plinkr/workflows/R-CMD-check/badge.svg?branch=develop\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/richelbilderbeek/plinkr/workflows/R-CMD-check/badge.svg?branch=develop\" alt=\"R-CMD-check\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://ci.appveyor.com/project/richelbilderbeek/plinkr\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cb33698aaa88cd566bc4aa690db394e31327125a7bb4205164d31dbab7216050/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f6769746875622f72696368656c62696c6465726265656b2f706c696e6b723f6272616e63683d646576656c6f70267376673d74727565\" alt=\"AppVeyor build status\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/github/richelbilderbeek/plinkr?branch=develop\u0026amp;svg=true\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ca href=\"https://codecov.io/github/richelbilderbeek/plinkr/branch/develop\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d136bb6a044d0bd05e2f4f06a5a96494925547304deabd0674fbf0c9c1dd929c/68747470733a2f2f636f6465636f762e696f2f6769746875622f72696368656c62696c6465726265656b2f706c696e6b722f636f7665726167652e7376673f6272616e63683d646576656c6f70\" alt=\"codecov.io\" data-canonical-src=\"https://codecov.io/github/richelbilderbeek/plinkr/coverage.svg?branch=develop\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWork with PLINK and PLINK2 from R.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDoing the first PLINK example: \u003ca href=\"https://youtu.be/LsfKQw2oIUg\" rel=\"nofollow\"\u003eYouTube\u003c/a\u003e \u003ca href=\"http://richelbilderbeek.nl/plinkr_basic_usage.ogv\" rel=\"nofollow\"\u003edownload (.ogv)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eDetect an association with one or more quantitative traits: \u003ca href=\"https://youtu.be/IicNdc8sDfI\" rel=\"nofollow\"\u003eYouTube\u003c/a\u003e \u003ca href=\"http://richelbilderbeek.nl/plinkr_assoc_qt.ogv\" rel=\"nofollow\"\u003edownload (.ogv)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eDetect an association with ideal quantitative traits: \u003ca href=\"https://youtu.be/oXGy83WiHm4\" rel=\"nofollow\"\u003eYouTube\u003c/a\u003e \u003ca href=\"http://richelbilderbeek.nl/plinkr_demo_qt_assoc.ogv\" rel=\"nofollow\"\u003edownload (.ogv)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSimulate quantitative traits: \u003ca href=\"https://youtu.be/H0XlLVsFry4\" rel=\"nofollow\"\u003eYouTube\u003c/a\u003e \u003ca href=\"http://richelbilderbeek.nl/plinkr_create_demo_assoc_qt_params.ogv\" rel=\"nofollow\"\u003edownload (.ogv)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSimulate custom traits: \u003ca href=\"https://youtu.be/5X1kLkiQbtw\" rel=\"nofollow\"\u003eYouTube\u003c/a\u003e \u003ca href=\"http://richelbilderbeek.nl/plinkr_create_custom_trait.ogv\" rel=\"nofollow\"\u003edownload (.ogv)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eDetect an association with a binary trait/case-control phenotype: \u003ca href=\"https://youtu.be/LhXQcDQvZS0\" rel=\"nofollow\"\u003eYouTube\u003c/a\u003e \u003ca href=\"http://richelbilderbeek.nl/plinkr_assoc.ogv\" rel=\"nofollow\"\u003edownload (.ogv)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eSee \u003ca href=\"doc/install.md\"\u003edoc/install.md\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-plink\" class=\"anchor\" href=\"#running-plink\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning PLINK\u003c/h3\u003e\n\u003cp\u003eRun PLINK:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003elibrary(plinkr)\nrun_plink(\"--help\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo call a specific version of PLINK:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_plink(\"--help\", create_plink_v1_7_options())\nrun_plink(\"--help\", create_plink_v1_9_options())\nrun_plink(\"--help\", create_plink_v2_0_options())\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOf course, you can also call PLINK to detect genetic associations :-) :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Use the PLINK v1.9 example files\nplink_v1_9 \u0026lt;- create_plink_v1_9_options()\nped_filename \u0026lt;- get_plink_example_filename(\"toy.ped\", plink_v1_9)\nmap_filename \u0026lt;- get_plink_example_filename(\"toy.map\", plink_v1_9)\n\n# Do a case-control association\nplinkr::run_plink(\n  args = c(\n    \"--ped\", ped_filename, \n    \"--map\", map_filename\n  )\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eSee the vignette \u003ccode\u003ebasic_usage\u003c/code\u003e for basic usage of PLINK,\nas taken from the PLINK website, which shows a\nquantitative trait analysis\u003c/li\u003e\n\u003cli\u003eSee the vignette \u003ccode\u003etest_assoc_qt\u003c/code\u003e for the same basic usage of PLINK,\nusing the \u003ccode\u003eplinkr\u003c/code\u003e interface\u003c/li\u003e\n\u003cli\u003eSee the vignette \u003ccode\u003edemo_assoc_qt\u003c/code\u003e for doing a quantitative trait\nanalysis using simulated data and the \u003ccode\u003eplinkr\u003c/code\u003e interface\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-demonstrate-a-quantitative-trait-analysis\" class=\"anchor\" href=\"#demonstrate-a-quantitative-trait-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemonstrate a quantitative trait analysis\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003elibrary(plinkr)\nassoc_qt_params \u0026lt;- create_demo_assoc_qt_params()\nassoc_qt(assoc_qt_params)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eSee the vignette \u003ccode\u003edemo_assoc_qt\u003c/code\u003e for a walk-through of\nthe data that is simulated by default\u003c/li\u003e\n\u003cli\u003eSee the vignette \u003ccode\u003ecreate_demo_assoc_qt_params\u003c/code\u003e for many\nexamples how data can be simulated\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-faq\" class=\"anchor\" href=\"#faq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFAQ\u003c/h2\u003e\n\u003cp\u003eSee \u003ca href=\"doc/faq.md\"\u003edoc/faq.md\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627308092.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for bonito (https://github.com/nanoporetech/bonito)",
    "filenames": [
      "Singularity.0.3.6",
      "Singularity",
      "Singularity.0.4.0"
    ],
    "full_name": "powerPlant/bonito-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for bonito, a PyTorch Basecaller for Oxford Nanopore Reads\n\u003ca href=\"https://github.com/nanoporetech/bonito\"\u003ehttps://github.com/nanoporetech/bonito\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627353613.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v1.0.0"
    ],
    "full_name": "mchugomk/cat12_long",
    "latest_release": null,
    "readme": "",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1627066402.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "scripts/Singularity"
    ],
    "full_name": "SCXsunchenxi/Auto-Pytorch",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-auto-pytorch\" class=\"anchor\" href=\"#auto-pytorch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuto-PyTorch\u003c/h1\u003e\n\u003cp\u003eCopyright (C) 2019  \u003ca href=\"http://www.automl.org/\" rel=\"nofollow\"\u003eAutoML Group Freiburg\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis a very early pre-alpha version of our upcoming Auto-PyTorch.\nSo far, Auto-PyTorch supports featurized data (classification, regression) and image data (classification).\u003c/p\u003e\n\u003cp\u003eThe newest features in Auto-PyTorch for tabular data are described in the paper \u003ca href=\"https://arxiv.org/abs/2006.13799\" rel=\"nofollow\"\u003e\"Auto-PyTorch Tabular: Multi-Fidelity MetaLearning for Efficient and Robust AutoDL\"\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eClone repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e install/path\n$ git clone https://github.com/automl/Auto-PyTorch.git\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e Auto-PyTorch\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you want to contribute to this repository switch to our current develop branch\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git checkout develop\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eInstall pytorch:\n\u003ca href=\"https://pytorch.org/\" rel=\"nofollow\"\u003ehttps://pytorch.org/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eInstall Auto-PyTorch:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ cat requirements.txt \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e xargs -n 1 -L 1 pip install\n$ python setup.py install\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eCode for the \u003ca href=\"https://arxiv.org/abs/2006.13799\" rel=\"nofollow\"\u003epaper\u003c/a\u003e is available under \u003ccode\u003eexamples/ensemble\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor a detailed tutorial, please refer to the jupyter notebook in \u003ca href=\"https://github.com/automl/Auto-PyTorch/tree/master/examples/basics\"\u003ehttps://github.com/automl/Auto-PyTorch/tree/master/examples/basics\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn a nutshell:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# data and metric imports\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esklearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emodel_selection\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esklearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003edatasets\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esklearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emetrics\u003c/span\u003e\n\u003cspan class=\"pl-v\"\u003eX\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esklearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003edatasets\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eload_digits\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ereturn_X_y\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e)\n\u003cspan class=\"pl-v\"\u003eX_train\u003c/span\u003e, \u003cspan class=\"pl-v\"\u003eX_test\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey_test\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \\\n        \u003cspan class=\"pl-s1\"\u003esklearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emodel_selection\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003etrain_test_split\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eX\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erandom_state\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# running Auto-PyTorch\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"tiny_cs\"\u003c/span\u003e,  \u003cspan class=\"pl-c\"\u003e# config preset\u003c/span\u003e\n                                    \u003cspan class=\"pl-s1\"\u003elog_level\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027info\u0027\u003c/span\u003e,\n                                    \u003cspan class=\"pl-s1\"\u003emax_runtime\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e300\u003c/span\u003e,\n                                    \u003cspan class=\"pl-s1\"\u003emin_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e30\u003c/span\u003e,\n                                    \u003cspan class=\"pl-s1\"\u003emax_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e90\u003c/span\u003e)\n\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003efit\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eX_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003evalidation_split\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e0.3\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003ey_pred\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003epredict\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eX_test\u003c/span\u003e)\n\n\u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"Accuracy score\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003esklearn\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emetrics\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eaccuracy_score\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ey_test\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey_pred\u003c/span\u003e))\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eMore examples with datasets:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e examples/\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003eHow to configure Auto-PyTorch for your needs:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\n\u003cspan class=\"pl-c\"\u003e# Print all possible configuration options.\u003c/span\u003e\n\u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e().\u003cspan class=\"pl-en\"\u003eprint_help\u003c/span\u003e()\n\n\u003cspan class=\"pl-c\"\u003e# You can use the constructor to configure Auto-PyTorch.\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003elog_level\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027info\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_runtime\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e300\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emin_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e30\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e90\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# You can overwrite this configuration in each fit call.\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003efit\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eX_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003elog_level\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027debug\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_runtime\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e900\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emin_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e50\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e150\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# You can use presets to configure the config space.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# Available presets: full_cs, medium_cs (default), tiny_cs.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# These are defined in autoPyTorch/core/presets.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# tiny_cs is recommended if you want fast results with few resources.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# full_cs is recommended if you have many resources and a very high search budget.\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"full_cs\"\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# Enable or disable components using the Auto-PyTorch config:\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enetworks\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e[\u003cspan class=\"pl-s\"\u003e\"resnet\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"shapedresnet\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"mlpnet\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"shapedmlpnet\"\u003c/span\u003e])\n\n\u003cspan class=\"pl-c\"\u003e# You can take a look at the search space.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# Each hyperparameter belongs to a node in Auto-PyTorch\u0027s ML Pipeline.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# The names of the hyperparameters are prefixed with the name of the node: NodeName:hyperparameter_name.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# If a hyperparameter belongs to a component: NodeName:component_name:hyperparameter_name.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e# Call with the same arguments as fit.\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_hyperparameter_search_space\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eX_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ey_train\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003evalidation_split\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e0.3\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# You can configure the search space of every hyperparameter of every component:\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eHyperparameterSearchSpaceUpdates\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003esearch_space_updates\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eHyperparameterSearchSpaceUpdates\u003c/span\u003e()\n\n\u003cspan class=\"pl-s1\"\u003esearch_space_updates\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eappend\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enode_name\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"NetworkSelector\"\u003c/span\u003e,\n                            \u003cspan class=\"pl-s1\"\u003ehyperparameter\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"shapedresnet:activation\"\u003c/span\u003e,\n                            \u003cspan class=\"pl-s1\"\u003evalue_range\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e[\u003cspan class=\"pl-s\"\u003e\"relu\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"sigmoid\"\u003c/span\u003e])\n\u003cspan class=\"pl-s1\"\u003esearch_space_updates\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eappend\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enode_name\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"NetworkSelector\"\u003c/span\u003e,\n                            \u003cspan class=\"pl-s1\"\u003ehyperparameter\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"shapedresnet:blocks_per_group\"\u003c/span\u003e,\n                            \u003cspan class=\"pl-s1\"\u003evalue_range\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e[\u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e,\u003cspan class=\"pl-c1\"\u003e5\u003c/span\u003e],\n                            \u003cspan class=\"pl-s1\"\u003elog\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ehyperparameter_search_space_updates\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esearch_space_updates\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eEnable ensemble building (for featurized data):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetEnsemble\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eautoPyTorchEnsemble\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetEnsemble\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"tiny_cs\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_runtime\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e300\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emin_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e30\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e90\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eDisable pynisher if you experience issues when using cuda:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s1\"\u003eautoPyTorch\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eAutoNetClassification\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"tiny_cs\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003elog_level\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027info\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_runtime\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e300\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emin_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e30\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emax_budget\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e90\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ecuda\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003euse_pynisher\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis program is free software: you can redistribute it and/or modify\nit under the terms of the Apache license 2.0 (please see the LICENSE file).\u003c/p\u003e\n\u003cp\u003eThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\u003c/p\u003e\n\u003cp\u003eYou should have received a copy of the Apache license 2.0\nalong with this program (see LICENSE file).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference\" class=\"anchor\" href=\"#reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-text-bibtex\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e@incollection\u003c/span\u003e{\u003cspan class=\"pl-en\"\u003emendoza-automlbook18a\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003eauthor\u003c/span\u003e    = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eHector Mendoza and Aaron Klein and Matthias Feurer and Jost Tobias Springenberg and Matthias Urban and Michael Burkart and Max Dippel and Marius Lindauer and Frank Hutter\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003etitle\u003c/span\u003e     = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eTowards Automatically-Tuned Deep Neural Networks\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003eyear\u003c/span\u003e      = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e2018\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003emonth\u003c/span\u003e     = dec,\n  \u003cspan class=\"pl-s\"\u003eeditor\u003c/span\u003e    = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eHutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003ebooktitle\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eAutoML: Methods, Sytems, Challenges\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003epublisher\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eSpringer\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003echapter\u003c/span\u003e   = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e7\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003epages\u003c/span\u003e     = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e141--156\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003enote\u003c/span\u003e      = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eTo appear.\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: Previously, the name of the project was AutoNet. Since this was too generic, we changed the name to AutoPyTorch. AutoNet 2.0 in the reference mention above is indeed AutoPyTorch.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003eAuto-PyTorch is developed by the \u003ca href=\"http://www.automl.org/\" rel=\"nofollow\"\u003eAutoML Group of the University of Freiburg\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1609655576.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.cosmic_tagging_tf_2010"
    ],
    "full_name": "maxpkatz/singularity_image_files",
    "latest_release": null,
    "readme": "",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1609779903.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "pchengi/cmorfixer_env",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-environment-for-cmor-fixer\" class=\"anchor\" href=\"#environment-for-cmor-fixer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment for cmor-fixer\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eA tool to create an environment to allow easy use of the \u003ca href=\"https://github.com/EC-Earth/cmor-fixer\"\u003ecmor-fixer tool\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecmorfixer_env is a singularity container which comes with preinstalled miniconda3\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h1\u003e\n\u003cp\u003eYou need the singularity program installed. Follow the instructions here, to install singularity on your machine.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity.lbl.gov/install-linux\" rel=\"nofollow\"\u003ehttps://singularity.lbl.gov/install-linux\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-to-download-a-prebuilt-singularity-image\" class=\"anchor\" href=\"#to-download-a-prebuilt-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo download a prebuilt singularity image:\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eIf you\u0027d like to use a prebuilt image, you could download from the link below; if you\u0027d rather build the container yourself, follow the build instructing in the To build section.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://esg-dn2.nsc.liu.se/virtualtestbed/cmorfixerenv.simg\" rel=\"nofollow\"\u003eLink to prebuilt image\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-to-build\" class=\"anchor\" href=\"#to-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build cmorfixerenv.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-to-initialize-container-and-optionally-mount-external-filesystems\" class=\"anchor\" href=\"#to-initialize-container-and-optionally-mount-external-filesystems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo initialize container (and optionally mount external filesystems)\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eIf you don\u0027t have to mount any non-root filesystems, you could start the container like this:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e singularity shell cmorfixerenv.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eIf you don\u0027t see on the container the filesystem which is accessible on the host machine, you could try this, and once inside the container, you\u0027ll be able to see the filesystem mounted on /mnt.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e singularity shell --bind \u0026lt;path to filesystem you want mounted on the container\u0026gt;:/mnt cmorfixerenv.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eInside the container, do the following\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esource /etc/bashrc\nactivateminiconda3\nconda activate cmorfixer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eExecute cmorfixer (present in /opt/cmor_fixer/cmor-fixer/cmor-fixer.py, in the container)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003ecd /root\nscript -c \u0027/opt/cmor_fixer/cmor-fixer/cmor-fixer.py --verbose --forceid --olist --npp 1 --dry /mnt/CMIP6/ScenarioMIP/EC-Earth-Consortium/EC-Earth3/ssp126/\u0027 scriptout_cmorfix_dryrun\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611252471.0
  },
  {
    "data_format": 2,
    "description": "GitHub repo for storing scripts related to simulation using JModelica. The initial focus is on simulation in HPC environments.",
    "filenames": [
      "Singularity_Recipes/Singularity_Recipe_Py3_Simulation",
      "Singularity_Recipes/Singularity_Recipe_Py2_Compilation_Simulation"
    ],
    "full_name": "urbanopt/JModelica_simulation",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-jmodelica-simulation\" class=\"anchor\" href=\"#jmodelica-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJModelica Simulation\u003c/h1\u003e\n\u003cp\u003eGitHub repo for storing scripts related to simulation of Modelica models using JModelica. The initial focus is on simulation in HPC environments.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipes\" class=\"anchor\" href=\"#singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipes\u003c/h1\u003e\n\u003cp\u003eRecipes for building Singularity containers for compilation and simulation of Modelica models using PyModelica and PyFMI. Note that the recipe that would support compilation and simulation is for use with Python2 only, while a separate recipe supports simulation in Python3.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1608681086.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "marcjwilliams1/rstudio_julia",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5054\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for R studio server with Rv4.0 + julia v1.5.3\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611507934.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jganong/ubuntu-focal-foiegras",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-create-study-specific-roi-image-in-mni-space\" class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate study-specific ROI image in MNI space\u003c/h1\u003e\n\u003cp\u003ePMAT resting state connectivity study.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs:\u003c/h2\u003e\n\u003cp\u003eAll should be matched to the same T1 image.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eT1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)\u003c/li\u003e\n\u003cli\u003eDeformation from T1 subject space to atlas space (typically DEF_FWD resource of cat12 assessor)\u003c/li\u003e\n\u003cli\u003eSUBJECT directory of Freesurfer output (typically SUBJECT resource of freesurfer_dev assessor)\u003c/li\u003e\n\u003cli\u003eTemporal lobe segmentation (typically SEG resource of Temporal_Lobe assessor)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003erois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv        Region labels and volumes\nmakerois-PMAT.pdf           Visual report of final ROI image\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-regions-of-interest\" class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRegions of interest\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpheres (atlas space)\u003c/h3\u003e\n\u003cp\u003eSource: \u003cem\u003eLibby LA, Ekstrom AD, Ragland JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal cortices within human hippocampal subregions revealed by high-resolution functional imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eMethod: \u003cem\u003eSchr\u00f6der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"#entorhinal-cortex-atlas-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEntorhinal cortex (atlas space)\u003c/h3\u003e\n\u003cp\u003eAnterior lateral and posterior medial sections. Source and method: \u003cem\u003eSchr\u00f6der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\" href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTemporal lobe (Subject space, warped)\u003c/h3\u003e\n\u003cp\u003eHead for anterior hippocampus; body and tail combined for posterior hippocampus. Method: \u003cem\u003ePlassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017 Feb 24. PMID: 28781411; PMCID: PMC5544133.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-parahippocampal-perirhinal-subject-space-warped\" class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParahippocampal, perirhinal (Subject space, warped)\u003c/h3\u003e\n\u003cp\u003eGenerated by Freesurfer 6. Parahippocampal (1016,2016) and perirhinal (surface patch resampled to volume, overlap with parahippocampus was assigned to perirhinal). Method: \u003cem\u003eBruce Fischl, Andre van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H. Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607375887.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/makerois-PMAT",
    "latest_release": "v1.0.13",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-create-study-specific-roi-image-in-mni-space\" class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate study-specific ROI image in MNI space\u003c/h1\u003e\n\u003cp\u003ePMAT resting state connectivity study.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs:\u003c/h2\u003e\n\u003cp\u003eAll should be matched to the same T1 image.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eT1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)\u003c/li\u003e\n\u003cli\u003eDeformation from T1 subject space to atlas space (typically DEF_FWD resource of cat12 assessor)\u003c/li\u003e\n\u003cli\u003eSUBJECT directory of Freesurfer output (typically SUBJECT resource of freesurfer_dev assessor)\u003c/li\u003e\n\u003cli\u003eTemporal lobe segmentation (typically SEG resource of Temporal_Lobe assessor)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003erois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv        Region labels and volumes\nmakerois-PMAT.pdf           Visual report of final ROI image\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-regions-of-interest\" class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRegions of interest\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpheres (atlas space)\u003c/h3\u003e\n\u003cp\u003eSource: \u003cem\u003eLibby LA, Ekstrom AD, Ragland JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal cortices within human hippocampal subregions revealed by high-resolution functional imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eMethod: \u003cem\u003eSchr\u00f6der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"#entorhinal-cortex-atlas-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEntorhinal cortex (atlas space)\u003c/h3\u003e\n\u003cp\u003eAnterior lateral and posterior medial sections. Source and method: \u003cem\u003eSchr\u00f6der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\" href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTemporal lobe (Subject space, warped)\u003c/h3\u003e\n\u003cp\u003eHead for anterior hippocampus; body and tail combined for posterior hippocampus. Method: \u003cem\u003ePlassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017 Feb 24. PMID: 28781411; PMCID: PMC5544133.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-parahippocampal-perirhinal-subject-space-warped\" class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParahippocampal, perirhinal (Subject space, warped)\u003c/h3\u003e\n\u003cp\u003eGenerated by Freesurfer 6. Parahippocampal (1016,2016) and perirhinal (surface patch resampled to volume, overlap with parahippocampus was assigned to perirhinal). Method: \u003cem\u003eBruce Fischl, Andre van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H. Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607988459.0
  },
  {
    "data_format": 2,
    "description": "singularity container to run Ian Jonsen\u0027s foieGras package",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jganong/ubuntu-bionic-R-4.0.3-foieGras",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"#-cgo21_yasksite_ad-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cins\u003e CGO21_YaskSite_AD \u003c/ins\u003e\n\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup phase\u003c/h1\u003e\n\u003cp\u003eSteps 1 to 3 guide you through setting up.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.1\u003c/h2\u003e\n\u003cp\u003eClone this repository and go to the cloned directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\ngit checkout CGO21v0.3\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-12\" class=\"anchor\" href=\"#step-12\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.2\u003c/h2\u003e\n\u003cp\u003eFor the next steps we need singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install singularity with the following script if you have root access.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./install_singularity.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2\u003c/h2\u003e\n\u003cp\u003eDownload the singularity container.\u003c/p\u003e\n\u003cp\u003eThe pre-build container is available under the following link \u003ca href=\"https://doi.org/10.5281/zenodo.4415558\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.4415558\u003c/a\u003e\nand can be installed using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1 -O YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-3\" class=\"anchor\" href=\"#step-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3\u003c/h2\u003e\n\u003cp\u003eOnce singularity image is downloaded on the benchmarking system the first step is to run the app called build.\nThis installs YaskSite. It should be done at runtime since the YaskSite does machine specific configuration\nat build time. Run the following to do this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app build YS_CGO.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-phase\" class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun phase\u003c/h1\u003e\n\u003cp\u003eStep 4 illustrates how to run the app to reproduce results.\nIt is recommended the settings in the paper are followed to get comparable results.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-4\" class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 4\u003c/h2\u003e\n\u003cp\u003eRun the apps corresponding to YaskSite and Offsite. There are also pre-configured apps that helps to\nreproduce data in figures of the paper. To see the list of available apps use:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe method to run each apps are described in corresponding app\u0027s help. For example help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be obtained using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help --app Fig4 YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607375064.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "jganong/singularity-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"#-cgo21_yasksite_ad-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cins\u003e CGO21_YaskSite_AD \u003c/ins\u003e\n\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup phase\u003c/h1\u003e\n\u003cp\u003eSteps 1 to 3 guide you through setting up.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.1\u003c/h2\u003e\n\u003cp\u003eClone this repository and go to the cloned directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\ngit checkout CGO21v0.3\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-12\" class=\"anchor\" href=\"#step-12\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.2\u003c/h2\u003e\n\u003cp\u003eFor the next steps we need singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install singularity with the following script if you have root access.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./install_singularity.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2\u003c/h2\u003e\n\u003cp\u003eDownload the singularity container.\u003c/p\u003e\n\u003cp\u003eThe pre-build container is available under the following link \u003ca href=\"https://doi.org/10.5281/zenodo.4415558\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.4415558\u003c/a\u003e\nand can be installed using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1 -O YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-3\" class=\"anchor\" href=\"#step-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3\u003c/h2\u003e\n\u003cp\u003eOnce singularity image is downloaded on the benchmarking system the first step is to run the app called build.\nThis installs YaskSite. It should be done at runtime since the YaskSite does machine specific configuration\nat build time. Run the following to do this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app build YS_CGO.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-phase\" class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun phase\u003c/h1\u003e\n\u003cp\u003eStep 4 illustrates how to run the app to reproduce results.\nIt is recommended the settings in the paper are followed to get comparable results.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-4\" class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 4\u003c/h2\u003e\n\u003cp\u003eRun the apps corresponding to YaskSite and Offsite. There are also pre-configured apps that helps to\nreproduce data in figures of the paper. To see the list of available apps use:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe method to run each apps are described in corresponding app\u0027s help. For example help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be obtained using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help --app Fig4 YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1606934860.0
  },
  {
    "data_format": 2,
    "description": "Singularity description files",
    "filenames": [
      "mousegwas/Singularity",
      "fusorsv/Singularity"
    ],
    "full_name": "asafpr/singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"#-cgo21_yasksite_ad-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cins\u003e CGO21_YaskSite_AD \u003c/ins\u003e\n\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup phase\u003c/h1\u003e\n\u003cp\u003eSteps 1 to 3 guide you through setting up.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.1\u003c/h2\u003e\n\u003cp\u003eClone this repository and go to the cloned directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\ngit checkout CGO21v0.3\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-12\" class=\"anchor\" href=\"#step-12\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.2\u003c/h2\u003e\n\u003cp\u003eFor the next steps we need singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install singularity with the following script if you have root access.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./install_singularity.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2\u003c/h2\u003e\n\u003cp\u003eDownload the singularity container.\u003c/p\u003e\n\u003cp\u003eThe pre-build container is available under the following link \u003ca href=\"https://doi.org/10.5281/zenodo.4415558\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.4415558\u003c/a\u003e\nand can be installed using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1 -O YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-3\" class=\"anchor\" href=\"#step-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3\u003c/h2\u003e\n\u003cp\u003eOnce singularity image is downloaded on the benchmarking system the first step is to run the app called build.\nThis installs YaskSite. It should be done at runtime since the YaskSite does machine specific configuration\nat build time. Run the following to do this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app build YS_CGO.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-phase\" class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun phase\u003c/h1\u003e\n\u003cp\u003eStep 4 illustrates how to run the app to reproduce results.\nIt is recommended the settings in the paper are followed to get comparable results.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-4\" class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 4\u003c/h2\u003e\n\u003cp\u003eRun the apps corresponding to YaskSite and Offsite. There are also pre-configured apps that helps to\nreproduce data in figures of the paper. To see the list of available apps use:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe method to run each apps are described in corresponding app\u0027s help. For example help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be obtained using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help --app Fig4 YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1616613441.0
  },
  {
    "data_format": 2,
    "description": "This is the Artifact Description repository for the CGO21 paper: YaskSite \u2013 Stencil Optimization Techniques Applied to Explicit ODE Methods on Modern Architectures",
    "filenames": [
      "Singularity"
    ],
    "full_name": "seasite-project/CGO21_YaskSite_AD",
    "latest_release": "CGO21v0.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content--cgo21_yasksite_ad-\" class=\"anchor\" href=\"#-cgo21_yasksite_ad-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cins\u003e CGO21_YaskSite_AD \u003c/ins\u003e\n\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup-phase\" class=\"anchor\" href=\"#setup-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup phase\u003c/h1\u003e\n\u003cp\u003eSteps 1 to 3 guide you through setting up.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-11\" class=\"anchor\" href=\"#step-11\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.1\u003c/h2\u003e\n\u003cp\u003eClone this repository and go to the cloned directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/seasite-project/CGO21_YaskSite_AD.git\ncd CGO21_YaskSite_AD\ngit checkout CGO21v0.3\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-12\" class=\"anchor\" href=\"#step-12\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1.2\u003c/h2\u003e\n\u003cp\u003eFor the next steps we need singularity v 3.6.4 or higher.\nIf singularity is not installed, you can install singularity with the following script if you have root access.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./install_singularity.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-2\" class=\"anchor\" href=\"#step-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2\u003c/h2\u003e\n\u003cp\u003eDownload the singularity container.\u003c/p\u003e\n\u003cp\u003eThe pre-build container is available under the following link \u003ca href=\"https://doi.org/10.5281/zenodo.4415558\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.4415558\u003c/a\u003e\nand can be installed using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://zenodo.org/record/4415558/files/YS_CGO.sif?download=1 -O YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-3\" class=\"anchor\" href=\"#step-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3\u003c/h2\u003e\n\u003cp\u003eOnce singularity image is downloaded on the benchmarking system the first step is to run the app called build.\nThis installs YaskSite. It should be done at runtime since the YaskSite does machine specific configuration\nat build time. Run the following to do this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app build YS_CGO.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-phase\" class=\"anchor\" href=\"#run-phase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun phase\u003c/h1\u003e\n\u003cp\u003eStep 4 illustrates how to run the app to reproduce results.\nIt is recommended the settings in the paper are followed to get comparable results.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-step-4\" class=\"anchor\" href=\"#step-4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 4\u003c/h2\u003e\n\u003cp\u003eRun the apps corresponding to YaskSite and Offsite. There are also pre-configured apps that helps to\nreproduce data in figures of the paper. To see the list of available apps use:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe method to run each apps are described in corresponding app\u0027s help. For example help on how to run Fig4 app\n(reproduces results in Fig4 of the paper) can be obtained using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run-help --app Fig4 YS_CGO.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1609764345.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.torch_mmf",
      "Singularity.torch"
    ],
    "full_name": "ChunCun/container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos8_roar\" class=\"anchor\" href=\"#centos8_roar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos8_roar\u003c/h1\u003e\n\u003cp\u003eCentos 8 base image for Roar\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-note\" class=\"anchor\" href=\"#note\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNOTE\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThis recipe may include unnecessary packages for certain software installation\u003c/li\u003e\n\u003cli\u003eMore packages will be added in the future\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-updates\" class=\"anchor\" href=\"#updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUpdates\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e2020/11/13\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitial recipe added\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e2021/03/22\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefault Python3 is updated to Python 3.8\u003c/li\u003e\n\u003cli\u003eLapack, BLAS, OpenBLAS, ATLAS, and NetCDF are added\u003c/li\u003e\n\u003cli\u003eCMake 3.19.7, Boost 1.75.0, and R 4.0.4 are added\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605677713.0
  },
  {
    "data_format": 2,
    "description": "Centos 8 base image for Roar",
    "filenames": [
      "Singularity.gpu",
      "Singularity"
    ],
    "full_name": "willgpaik/centos8_roar",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos8_roar\" class=\"anchor\" href=\"#centos8_roar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos8_roar\u003c/h1\u003e\n\u003cp\u003eCentos 8 base image for Roar\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-note\" class=\"anchor\" href=\"#note\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNOTE\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eThis recipe may include unnecessary packages for certain software installation\u003c/li\u003e\n\u003cli\u003eMore packages will be added in the future\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-updates\" class=\"anchor\" href=\"#updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUpdates\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e2020/11/13\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInitial recipe added\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e2021/03/22\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDefault Python3 is updated to Python 3.8\u003c/li\u003e\n\u003cli\u003eLapack, BLAS, OpenBLAS, ATLAS, and NetCDF are added\u003c/li\u003e\n\u003cli\u003eCMake 3.19.7, Boost 1.75.0, and R 4.0.4 are added\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1616617880.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "mmirko/singularitytest",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularitytest\" class=\"anchor\" href=\"#singularitytest\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularitytest\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605257877.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "nicspalla/openmpi_centos_x86_64",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-openmpi_centos_x86_64\" class=\"anchor\" href=\"#openmpi_centos_x86_64\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eopenmpi_centos_x86_64\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605260984.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v1.0.0"
    ],
    "full_name": "baxpr/segwarp",
    "latest_release": null,
    "readme": "\u003cp\u003eWarp SEG output of a multi-atlas assessor to MNI space using the supplied SPM warp field.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605062943.0
  },
  {
    "data_format": 2,
    "description": "Bayesian Atmospheric Radiative Transfer (BART) packaged in a Singularity container https://github.com/davecwright3/bart-singularity",
    "filenames": [
      "Singularity"
    ],
    "full_name": "davecwright3/bart-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4946\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-bart-singularity-guide\" class=\"anchor\" href=\"#bart-singularity-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBART Singularity Guide\u003c/h1\u003e\n\u003cp\u003eThe Singularity image has BART installed at \u003ccode\u003e/bart_dir\u003c/code\u003e. The \u003ccode\u003e$topdir\u003c/code\u003e environment variable is set to this directory inside the image. This means that the instructions for the demo listed here \u003ca href=\"https://github.com/exosports/BART/tree/master/examples/demo\"\u003ehttps://github.com/exosports/BART/tree/master/examples/demo\u003c/a\u003e still work, but we need to mount a directory for outputs into the container for two reasons:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe demo expects your output directory to be parallel to the BART directory\u003c/li\u003e\n\u003cli\u003eThe container file system is read-only (this is only a problem because of (1); being read-only is actually preferred because it helps ensure reproducible results)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eIf the output directory wasn\u0027t required to be parallel to BART, you could run the container anywhere in \u003ccode\u003e$HOME\u003c/code\u003e because Singularity mounts \u003ccode\u003e$HOME\u003c/code\u003e of the current user into the container by default\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe image has a directory parallel to BART that is meant for output at \u003ccode\u003e/bart_dir/run\u003c/code\u003e. Make a directory on your host system where you want to store results. For the sake of this guide, let\u0027s say it\u0027s under your current directory at \u003ccode\u003edemo/run\u003c/code\u003e and you have pulled the singularity image\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name bart.sif shub://davecwright3/bart-singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto your current directory as well. Then start a shell in the singularity container with the bind mount specified\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell -B demo/run:/bart_dir/run bart.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe BART conda environment will be automatically activated. Now just \u003ccode\u003ecd $topdir/run\u003c/code\u003e and follow the instructions here \u003ca href=\"https://github.com/exosports/BART/tree/master/examples/demo\"\u003ehttps://github.com/exosports/BART/tree/master/examples/demo\u003c/a\u003e if you would like to do a demo run. You can \u003ccode\u003eexit\u003c/code\u003e the container whenever you are done, and your results will remain in your \u003ccode\u003edemo/run\u003c/code\u003e directory.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h1\u003e\n\u003cp\u003eBayesian Atmospheric Radiative Transfer (BART), a code to infer\nproperties of planetary atmospheres based on observed spectroscopic\ninformation.\u003c/p\u003e\n\u003cp\u003eThis project was completed with the support of the NASA Planetary\nAtmospheres Program, grant NNX12AI69G, held by Principal Investigator\nJoseph Harrington. Principal developers included graduate students\nPatricio E. Cubillos and Jasmina Blecic, programmer Madison Stemm, and\nundergraduates M. Oliver Bowman and Andrew S. D. Foster.  The included\n\u0027transit\u0027 radiative transfer code is based on an earlier program of\nthe same name written by Patricio Rojo (Univ. de Chile, Santiago) when\nhe was a graduate student at Cornell University under Joseph\nHarrington.  Statistical advice came from Thomas J. Loredo and Nate\nB. Lust.\u003c/p\u003e\n\u003cp\u003eCopyright (C) 2015-2016 University of Central Florida.\nAll rights reserved.\u003c/p\u003e\n\u003cp\u003eThis is a test version only, and may not be redistributed to any third\nparty.  Please refer such requests to us.  This program is distributed\nin the hope that it will be useful, but WITHOUT ANY WARRANTY; without\neven the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR\nPURPOSE.\u003c/p\u003e\n\u003cp\u003eOur intent is to release this software under an open-source,\nreproducible-research license, once the code is mature and the first\nresearch paper describing the code has been accepted for publication\nin a peer-reviewed journal.  We are committed to development in the\nopen, and have posted this code on github.com so that others can test\nit and give us feedback.  However, until its first publication and\nfirst stable release, we do not permit others to redistribute the code\nin either original or modified form, nor to publish work based in\nwhole or in part on the output of this code.  By downloading, running,\nor modifying this code, you agree to these conditions.  We do\nencourage sharing any modifications with us and discussing them\nopenly.\u003c/p\u003e\n\u003cp\u003eWe welcome your feedback, but do not guarantee support.  Please send\nfeedback or inquiries to:\nPatricio Cubillos \u003ca href=\"mailto:patricio.cubillos@oeaw.ac.at\"\u003epatricio.cubillos@oeaw.ac.at\u003c/a\u003e\nJasmina Blecic \u003ca href=\"mailto:jasmina@physics.ucf.edu\"\u003ejasmina@physics.ucf.edu\u003c/a\u003e\nJoseph Harrington \u003ca href=\"mailto:jh@physics.ucf.edu\"\u003ejh@physics.ucf.edu\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eor alternatively,\nJoseph Harrington, Patricio Cubillos, and Jasmina Blecic\nUCF PSB 441\n4111 Libra Drive\nOrlando, FL 32816-2385\nUSA\u003c/p\u003e\n\u003cp\u003eThank you for testing BART!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1604965509.0
  },
  {
    "data_format": 2,
    "description": "a Singularity recipe with openmpi 2.1.1 on base centos 7 to run on the Cineca clusters x86_64 based",
    "filenames": [
      "Singularity"
    ],
    "full_name": "simarocchi/openmpi_centos7_x86_64",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-openmpi_centos7_x86_64\" class=\"anchor\" href=\"#openmpi_centos7_x86_64\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eopenmpi_centos7_x86_64\u003c/h1\u003e\n\u003cp\u003ea Singularity recipe with openmpi 2.1.1 on base centos 7 to run on the Cineca clusters x86_64 based\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605098444.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for slim (https://github.com/MesserLab/SLiM)",
    "filenames": [
      "Singularity",
      "Singularity.3.4+1c85d00",
      "Singularity.3.5"
    ],
    "full_name": "powerPlant/slim-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for Selection on Linked Mutations: A forward population genetic simulation for studying linkage effects, such as hitchhiking, background selection, and Hill-Robertson interference\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607459916.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "tpall/htseq-paper-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-htseq-paper-singularity\" class=\"anchor\" href=\"#htseq-paper-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehtseq-paper-singularity\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1604657436.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.FMS-gcc10-openmpi-netcdf4.6.3-ubuntu",
      "Singularity.FMS-gcc10-openmpi-netcdf4.6.3-ubuntu-compile"
    ],
    "full_name": "thomas-robinson/fms_containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fms_containers\" class=\"anchor\" href=\"#fms_containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efms_containers\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1604411747.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "marcjwilliams1/rstudiosrvrV4",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4911\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for R studio server with Rv4.0.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1605122458.0
  },
  {
    "data_format": 2,
    "description": "The definition files for creating singularity containers that can run in the WashU HPC",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "humanconnectome/hcp-pipelines-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-definitions-for-hcp-pipelines\" class=\"anchor\" href=\"#singularity-definitions-for-hcp-pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Definitions for HCP Pipelines\u003c/h1\u003e\n\u003cp\u003eThe definition files for creating singularity containers for the XNAT pipelines\nwrapper code so that it can run in the WashU HPC.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cloning-with-submodules\" class=\"anchor\" href=\"#cloning-with-submodules\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCloning with Submodules\u003c/h2\u003e\n\u003cp\u003eDon\u0027t forget to pull down the submodules as well, with the \u003ccode\u003e--recursive\u003c/code\u003e flag.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/humanconnectome/hcp-pipelines-singularity --recursive\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCommand\u003c/th\u003e\n\u003cth\u003eTask\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003emake clean\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRemove previous container image.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003emake update\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eUpdate all the git submodule repos.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003emake build\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eGenerate a container image from .def file\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003emake upload\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eUpload the container to correct location in the HPC.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1610395015.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "other_images/Singularity.custom_openspiel"
    ],
    "full_name": "buregab/openspiel_singularity",
    "latest_release": null,
    "readme": "\u003cp\u003eFor building openspiel singularity containers.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1604380357.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "shreyaskamathkm/singularity_meshroom",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_meshroom\" class=\"anchor\" href=\"#singularity_meshroom\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_meshroom\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602807348.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "lehtiolab/nf-deqms",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lehtiolabnf-deqms\" class=\"anchor\" href=\"#lehtiolabnf-deqms\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elehtiolab/nf-deqms\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eA small pipeline to re-run DEqMS on existing results\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0fcfc6847f4944e0c46cb62bb190c0110bafa56ce455c12dd23051df8d710a4a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413532302e30312e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A520.01.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/lehtiolab/nf-deqms\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4068dc15ebffdfaa7d220510750dd7bcde75393d91d3fe2d05dc15190c515246/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6c656874696f6c61622f6e662d6465716d732e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/lehtiolab/nf-deqms.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThis workflow reruns DEqMS analysis on existing results, e.g. from the \u003ca href=\"https://github.com/lehtiolab/ddamsproteomics\"\u003elehtiolab/ddamsproteomics\u003c/a\u003e pipeline. It exists so one can use orthogonal sample groups (CTRL vs TREAT, old vs young) and rerun, or perhaps correct a mistake in the sample annotation, without having to re-search an entire set of spectra against a protein sequence database.\u003c/p\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run\" class=\"anchor\" href=\"#how-to-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003einstall \u003ca href=\"https://nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003einstall \u003ca href=\"https://docs.docker.com/engine/installation/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e, \u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e, or \u003ca href=\"https://conda.io/miniconda.html\" rel=\"nofollow\"\u003eConda\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003erun pipeline:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run lehtiolab/nf-deqms --proteins proteins.txt --peptides peptides.txt --genes genes.txt --ensg ensg.txt --sampletable samples.txt -profile standard,docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can leave out any accession that you do not have or are not interested in (e.g. \u003ccode\u003e--ensg\u003c/code\u003e in a Swissprot analysis).\u003c/p\u003e\n\u003cp\u003eThe lehtiolab/nf-deqms pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/troubleshooting\" rel=\"nofollow\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere is more extensive documentation on the options inside the main.nf file.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003elehtiolab/nf-deqms was originally written by Jorrit Boekel and tries to follow the \u003ca href=\"https://nf-co.re\" rel=\"nofollow\"\u003enf-core\u003c/a\u003e best practices and templates.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1605692054.0
  },
  {
    "data_format": 2,
    "description": "Recipe for funannotate pipeline Singularity recipy for UA HPC",
    "filenames": [
      "Singularity"
    ],
    "full_name": "dshyshlov/funannotate_singularity",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for SEX-DETector, a tool for the statistical inferrence of sex-linked genes from RNA / DNA reads from a cross (parents and set of childrens)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602202847.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for sex-detector-plusplus (https://gitlab.in2p3.fr/sex-det-family/sex-detector-plusplus)",
    "filenames": [
      "Singularity",
      "Singularity.00f7d723"
    ],
    "full_name": "powerPlant/sex-detector-plusplus-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for SEX-DETector, a tool for the statistical inferrence of sex-linked genes from RNA / DNA reads from a cross (parents and set of childrens)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1600917082.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v2.1.0"
    ],
    "full_name": "baxpr/cersuit",
    "latest_release": "v2.1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cersuit\" class=\"anchor\" href=\"#cersuit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecersuit\u003c/h1\u003e\n\u003cp\u003eCerebellar segmentation with the \u003ca href=\"http://diedrichsenlab.org/imaging/suit.htm\" rel=\"nofollow\"\u003eSUIT atlas and toolbox\u003c/a\u003e. In the container, the pipeline is installed in the \u003ccode\u003e/opt/cersuit\u003c/code\u003e directory. Matlab code is in the \u003ccode\u003esrc\u003c/code\u003e directory, and the entrypoint is \u003ccode\u003esrc/cersuit.m\u003c/code\u003e. Compiled Matlab code for use in the singularity container without a Matlab license is in \u003ccode\u003ebin\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSee the \u003ccode\u003eexternal\u003c/code\u003e directory for links, references, and license information for the underlying SPM12 and SUIT Matlab software. \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki\" rel=\"nofollow\"\u003eFSL version 6.0.2\u003c/a\u003e is also used for image file manipulation and creating the QA PDF.\u003c/p\u003e\n\u003cp\u003eThe container has a full installation of both SPM12 (compiled) and FSL.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references-for-suit\" class=\"anchor\" href=\"#references-for-suit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences for SUIT\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.1016/j.neuroimage.2006.05.056\" rel=\"nofollow\"\u003eDiedrichsen, J. (2006). A spatially unbiased atlas template of the human cerebellum. Neuroimage, 33, 1, p. 127-138.\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.1016/j.neuroimage.2009.01.045\" rel=\"nofollow\"\u003eDiedrichsen, J., Balsters, J. H., Flavell, J., Cussans, E., \u0026amp; Ramnani, N. (2009). A probabilistic atlas of the human cerebellum. Neuroimage 46(1):39-46.\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.1016/j.neuroimage.2010.10.035\" rel=\"nofollow\"\u003eDiedrichsen, J., Maderwald, S., Kuper, M., Thurling, M., Rabe, K., Gizewski, E. R., et al. (2011). Imaging the deep cerebellar nuclei: A probabilistic atlas and normalization procedure. Neuroimage 54(3):1786-94\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.1371/journal.pone.0133402\" rel=\"nofollow\"\u003eDiedrichsen, J. \u0026amp; Zotow, E. (2015). Surface-based display of volume-averaged cerebellar data. PLoS One, 7, e0133402.\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAdjustment of the source T1 file to axial data ordering using fslreorient2std, to meet a requirement of the SUIT toolbox.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTranslation-only alignment of the supplied gray matter image to SPM12\u0027s gray matter probabilistic atlas (TPM.nii). This is accomplished by aligning the centers of mass. Rotations are not estimated, to avoid an issue with SUIT\u0027s bounding box computation. The supplied gray matter image must be in register with the supplied T1. The estimated registration is saved to file and also applied to the T1.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSUIT estimation of the affine transformation and warp of the cerebellar area of the T1 to the SUIT atlas.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eResampling of the T1 and related images to the SUIT atlas space. Gray matter and white matter images are resampled both with and without modulation by the Jacobian.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eResampling of the SUIT-supplied atlases to the original T1 native space.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eComputation of regional volumes for the Lobules_SUIT atlas in the native T1 space.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage-of-the-singularity-container\" class=\"anchor\" href=\"#usage-of-the-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage of the singularity container\u003c/h2\u003e\n\u003cp\u003eSee \u003ccode\u003esingularity_examples.sh\u003c/code\u003e for examples of using the container for SUIT warp estimation, and transformation from native to SUIT space and back using an existing estimated warp. The transformations can also be done directly from matlab with the \u003ccode\u003etransform_???.m\u003c/code\u003e functions in \u003ccode\u003esrc\u003c/code\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters-and-inputs\" class=\"anchor\" href=\"#parameters-and-inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters and inputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;temporary-home-dir\u0026gt;      Matlab will use this for temp files\n\u0026lt;tmp-dir\u0026gt;                 Other location for temp files          \n\u0026lt;input-dir\u0026gt;               Directory containing the input T1 image file\n\u0026lt;output-dir\u0026gt;              Outputs will be stored here\n\u0026lt;t1-niigz-filename\u0026gt;       Filename of the input T1 - expecting \u0026lt;something\u0026gt;.nii.gz\n\u0026lt;mask-threshold\u0026gt;          SPM mask threshold for separating brain from background\n\u0026lt;project-name\u0026gt;            Project/subject/session/scan names from XNAT, if XNAT is\n\u0026lt;subject-name\u0026gt;               used. These are only used to decorate the PDF report.\n\u0026lt;session-name\u0026gt;    \n\u0026lt;scan-name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cp\u003ePDF report for quality assurance\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ePDF               cersuit.pdf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTransformation from native to atlas space. Apply in this order\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRIGID             coreg_t1_to_mni.mat\nAFFINE            Affine_c_t1_seg1.mat\nFLOWFIELD         u_a_c_t1_seg1.nii.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCropped T1 in both spaces\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eT1_CROP_NATIVE    c_t1.nii.gz\nT1_CROP_SUIT      wc_t1.nii.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCerebellum mask, segmented gray matter and white matter volume fraction images in native and atlas space\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eMASK_NATIVE       c_t1_pcereb.nii.gz\nGRAY_NATIVE       c_t1_seg1.nii.gz\nWHITE_NATIVE      c_t1_seg2.nii.gz\nMASK_SUIT         wc_t1_pcereb.nii.gz\nGRAY_SUIT         wc_t1_seg1.nii.gz\nWHITE_SUIT        wc_t1_seg2.nii.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eJacobian-modulated gray and white matter images in atlas space\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eGRAYMOD_SUIT      wdc_t1_seg1.nii.gz\nWHITEMOD_SUIT     wdc_t1_seg2.nii.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSegmented regions in native and atlas space, with lookup table\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eATLASES_NATIVE    SUIT-supplied atlases resampled to original T1 space\nATLASES_SUIT      The SUIT-supplied atlases themselves\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eVolumetry of segmented regions, computed from native space images. The \"Total\" is the volume of the atlas region after transformation to native space. The \"Gray\" is the sum of voxel gray matter fraction within the atlas region, in native space; similar for \"White\".\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eNATIVE_VOLS       iw_Lobules-SUIT_u_a_c_t1_seg1-volumes.csv\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1601771070.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "nwchem-702.ompi313.ivybridge/Singularity",
      "nwchem-dev.ompi40x.skylake/Singularity",
      "nwchem-dev.ompi40x.ifort.skylake/Singularity",
      "nwchem-701.mpich321.ivybridge/Singularity",
      "nwchem-701.ifort/Singularity",
      "nwchem-701.ompi313.ivybridge/Singularity"
    ],
    "full_name": "edoapra/nwchem-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nwchem-singularity\" class=\"anchor\" href=\"#nwchem-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNWChem singularity\u003c/h1\u003e\n\u003cp\u003eSingularity recipes for NWChem\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1613067966.0
  },
  {
    "data_format": 2,
    "description": "launch the C++ IDE Anjuta from a Singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "d-w-moore/anjuta_via_singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-anjuta-ide-via-singularity\" class=\"anchor\" href=\"#anjuta-ide-via-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnjuta IDE via Singularity\u003c/h1\u003e\n\u003cp\u003eThe container includes libraries for building and debugging  C++\nprograms with GCC 9, with C++17 support and Boost libraries. C/Xlib\napplications are also supported.\u003c/p\u003e\n\u003cp\u003eTo build the container under Singularity ~2.5.1 :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eget \u003ca href=\"http://sylabs.io\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e . If you\u0027re on Ubuntu/Debian,\nthe \u003ca href=\"https://neuro.debian.net\" rel=\"nofollow\"\u003eNeuroDebian\u003c/a\u003e repo can offer the\nmost up-to-date Singularity packages\u003c/li\u003e\n\u003cli\u003ein a local copy of this repo, use the build command:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build anjuta.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eThe IDE can be lauched by running anjuta.simg as an executable\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e$ ./anjuta.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor via the singularity application\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity run anjuta.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo alter an existing image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build --sandbox anjuta anjuta.simg\n$ sudo singularity shell --writable anjuta\nSingularity\u0026gt; apt update; apt install {custom-packages...}\nSingularity\u0026gt; exit\n$ sudo singularity build anjuta_updated.simg anjuta\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1600000384.0
  },
  {
    "data_format": 2,
    "description": "Optimal classical planner based on saturated cost partitioning",
    "filenames": [
      "misc/releases/19.12/Singularity.19.12",
      "misc/releases/20.06/Singularity.20.06",
      "misc/releases/19.06/Singularity.19.06",
      "misc/releases/latest/Singularity"
    ],
    "full_name": "jendrikseipp/scorpion",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-scorpion\" class=\"anchor\" href=\"#scorpion\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eScorpion\u003c/h1\u003e\n\u003cp\u003eScorpion is an optimal classical planner that uses saturated cost\npartitioning to combine multiple abstraction heuristics. It also contains\nimplementations of many other cost partitioning algorithms over\nabstraction and landmark heuristics. Scorpion is based on the \u003ca href=\"https://github.com/aibasel/downward\"\u003eFast\nDownward planning system\u003c/a\u003e, which is\ndescribed below. We regularly port the latest changes from Fast Downward\nto Scorpion and also try to port Scorpion features back to Fast Downward.\u003c/p\u003e\n\u003cp\u003ePlease use the following reference when citing Scorpion:\nJendrik Seipp, Thomas Keller and Malte Helmert.\n\u003ca href=\"https://www.jair.org/index.php/jair/article/view/11673\" rel=\"nofollow\"\u003eSaturated Cost Partitioning for Optimal Classical Planning\u003c/a\u003e.\nJournal of Artificial Intelligence Research 67, pp. 129-167. 2020.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-instructions\" class=\"anchor\" href=\"#instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstructions\u003c/h2\u003e\n\u003cp\u003eAfter installing the requirements (see below), compile the planner with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./build.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand see the available options with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./fast-downward.py --help  # driver\n./fast-downward.py --search -- --help  # search component\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more details (including build instructions for Windows), see the\ndocumentation about\n\u003ca href=\"http://www.fast-downward.org/ObtainingAndRunningFastDownward\" rel=\"nofollow\"\u003ecompiling\u003c/a\u003e\nand \u003ca href=\"http://www.fast-downward.org/PlannerUsage\" rel=\"nofollow\"\u003erunning\u003c/a\u003e the planner.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-recommended-configuration\" class=\"anchor\" href=\"#recommended-configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecommended Configuration\u003c/h3\u003e\n\u003cp\u003eWe recommend the following configuration, which is similar to the one\nScorpion used in the IPC 2018. It prunes irrelevant operators in a\npreprocessing step, uses partial order reduction, and maximizes over\nmultiple diverse SCP heuristics computed for projections and Cartesian\nabstractions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./fast-downward.py --transform-task preprocess-h2\n  ../benchmarks/gripper/prob01.pddl\n  --search \"astar(scp([\n    projections(hillclimbing(max_time=100, random_seed=0)),\n    projections(systematic(2)),\n    cartesian()],\n    max_orders=infinity, max_time=200, max_optimization_time=2, diversify=true,\n    orders=greedy_orders(random_seed=0), random_seed=0),\n    pruning=atom_centric_stubborn_sets(min_required_pruning_ratio=0.2))\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(In \u003ca href=\"https://lab.readthedocs.io/\" rel=\"nofollow\"\u003eDownward Lab\u003c/a\u003e you can use the\n\u003ccode\u003edriver_options\u003c/code\u003e argument of \u003ccode\u003eadd_algorithm\u003c/code\u003e to specify the\n\u003ccode\u003e--transform-task\u003c/code\u003e argument.)\u003c/p\u003e\n\u003cp\u003eIf you want to run exactly the same Scorpion version as in IPC 2018, we\nrecommend using the \u003ca href=\"https://bitbucket.org/ipc2018-classical/team44/src/ipc-2018-seq-opt/\" rel=\"nofollow\"\u003eScorpion IPC\nrepo\u003c/a\u003e.\nIt also includes a \u003ca href=\"https://github.com/hpcng/singularity\"\u003eSingularity\u003c/a\u003e\nimage.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-differences-between-scorpion-and-fast-downward\" class=\"anchor\" href=\"#differences-between-scorpion-and-fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDifferences between Scorpion and Fast Downward\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eScorpion comes with the\n\u003ca href=\"https://ojs.aaai.org/index.php/ICAPS/article/view/13708\" rel=\"nofollow\"\u003eh\u00b2-preprocessor\u003c/a\u003e\nby Vidal Alc\u00e1zar and \u00c1lvaro Torralba that prunes irrelevant operators.\nPass \u003ccode\u003e--transform-task builds/release/bin/preprocess-h2\u003c/code\u003e to use it.\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003e--transform-task\u003c/code\u003e command allows you to run arbitrary preprocessing\ncommands that transform the SAS+ output from the translator before\npassing it to the search.\u003c/li\u003e\n\u003cli\u003eIf \u003ca href=\"https://ccache.dev/\" rel=\"nofollow\"\u003eccache\u003c/a\u003e is installed, Scorpion uses it to cache\ncompilation files.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-new-plugin-options\" class=\"anchor\" href=\"#new-plugin-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNew plugin options\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ecegar(..., search_strategy=incremental)\u003c/code\u003e: use \u003ca href=\"https://ojs.aaai.org/index.php/ICAPS/article/view/6667\" rel=\"nofollow\"\u003eincremental search for\nCartesian abstraction\nrefinement\u003c/a\u003e\n(default).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ehillclimbing(..., max_generated_patterns=200)\u003c/code\u003e: limit the number of\npatterns generated by hill climbing.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-new-cost-partitioning-algorithms-for-abstraction-heuristics\" class=\"anchor\" href=\"#new-cost-partitioning-algorithms-for-abstraction-heuristics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNew cost partitioning algorithms for abstraction heuristics\u003c/h3\u003e\n\u003cp\u003eWe use Cartesian abstractions in the example configurations below\n(\u003ccode\u003e[cartesian()]\u003c/code\u003e). You can also use pattern database heuristics, e.g.,\n\u003ccode\u003e[projections(systematic(2))]\u003c/code\u003e, or mix abstractions, e.g.,\n\u003ccode\u003e[projections(systematic(3)), cartesian()]\u003c/code\u003e. Some of the algorithms are\nalso part of vanilla Fast Downward, but only for PDB heuristics.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOptimal cost partitioning:\n\u003ccode\u003eoptimal_cost_partitioning([cartesian()])\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eCanonical heuristic:\n\u003ccode\u003ecanonical_heuristic([cartesian()])\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ePost-hoc optimization:\n\u003ccode\u003eoperatorcounting([pho_abstraction_constraints([cartesian()], saturated=false)])\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eUniform cost partitioning:\n\u003ccode\u003euniform_cost_partitioning([cartesian()], opportunistic=false)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eOpportunistic uniform cost partitioning:\n\u003ccode\u003euniform_cost_partitioning([cartesian()], ..., opportunistic=true)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eGreedy zero-one cost partitioning:\n\u003ccode\u003ezero_one_cost_partitioning([cartesian()], ...)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eSaturated post-hoc optimization:\n\u003ccode\u003eoperatorcounting([pho_abstraction_constraints([cartesian()], saturated=true)])\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can also compute the maximum over abstraction heuristics:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emaximize([cartesian()])\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-new-cost-partitioning-algorithms-for-landmark-heuristics\" class=\"anchor\" href=\"#new-cost-partitioning-algorithms-for-landmark-heuristics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNew cost partitioning algorithms for landmark heuristics\u003c/h3\u003e\n\u003cp\u003eExample using A* search and saturated cost partitioning over BJOLP\nlandmarks:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--evaluator\n  \"lmc=lmcount(lm_merged([lm_rhw(), lm_hm(m=1)]),\n  admissible=true, cost_partitioning=suboptimal, greedy=true,\n  reuse_costs=true, scoring_function=max_heuristic_per_stolen_costs)\"\n--search\n  \"astar(lmc, lazy_evaluator=lmc)\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDifferent cost partitioning algorithms (all need \u003ccode\u003eadmissible=true\u003c/code\u003e):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOptimal cost partitioning (part of vanilla Fast Downward):\n\u003ccode\u003elmcount(..., cost_partitioning=optimal)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eCanonical heuristic:\n\u003ccode\u003elmcount(..., cost_partitioning=canonical)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ePost-hoc optimization:\n\u003ccode\u003elmcount(..., cost_partitioning=pho)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eUniform cost partitioning:\n\u003ccode\u003elmcount(..., cost_partitioning=suboptimal, greedy=false, reuse_costs=false)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eOpportunistic uniform cost partitioning (part of vanilla Fast Downward):\n\u003ccode\u003elmcount(..., cost_partitioning=suboptimal, greedy=false, reuse_costs=true, scoring_function=min_stolen_costs)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eGreedy zero-one cost partitioning:\n\u003ccode\u003elmcount(..., cost_partitioning=suboptimal, greedy=true, reuse_costs=false, scoring_function=max_heuristic)\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eSaturated cost partitioning:\n\u003ccode\u003elmcount(..., cost_partitioning=suboptimal, greedy=true, reuse_costs=true, scoring_function=max_heuristic_per_stolen_costs)\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast Downward\u003c/h1\u003e\n\u003cp\u003eFast Downward is a domain-independent classical planning system.\u003c/p\u003e\n\u003cp\u003eCopyright 2003-2020 Fast Downward contributors (see below).\u003c/p\u003e\n\u003cp\u003eFor further information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Downward website: \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReport a bug or file an issue: \u003ca href=\"http://issues.fast-downward.org\" rel=\"nofollow\"\u003ehttp://issues.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward mailing list: \u003ca href=\"https://groups.google.com/forum/#!forum/fast-downward\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/fast-downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward main repository: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tested-software-versions\" class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTested software versions\u003c/h2\u003e\n\u003cp\u003eThis version of Fast Downward has been tested with the following software versions:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOS\u003c/th\u003e\n\u003cth\u003ePython\u003c/th\u003e\n\u003cth\u003eC++ compiler\u003c/th\u003e\n\u003cth\u003eCMake\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 20.04\u003c/td\u003e\n\u003ctd\u003e3.8\u003c/td\u003e\n\u003ctd\u003eGCC 9, GCC 10, Clang 10, Clang 11\u003c/td\u003e\n\u003ctd\u003e3.16\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 18.04\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eGCC 7, Clang 6\u003c/td\u003e\n\u003ctd\u003e3.10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emacOS 10.15\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eAppleClang 12\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWindows 10\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eVisual Studio Enterprise 2017 (MSVC 19.16) and 2019 (MSVC 19.28)\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu, we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and on macOS, we do not test LP solvers (yet).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003eThe following list includes all people that actively contributed to\nFast Downward, i.e. all people that appear in some commits in Fast\nDownward\u0027s history (see below for a history on how Fast Downward\nemerged) or people that influenced the development of such commits.\nCurrently, this list is sorted by the last year the person has been\nactive, and in case of ties, by the earliest year the person started\ncontributing, and finally by last name.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2003-2020 Malte Helmert\u003c/li\u003e\n\u003cli\u003e2008-2016, 2018-2020 Gabriele Roeger\u003c/li\u003e\n\u003cli\u003e2010-2020 Jendrik Seipp\u003c/li\u003e\n\u003cli\u003e2010-2011, 2013-2020 Silvan Sievers\u003c/li\u003e\n\u003cli\u003e2012-2020 Florian Pommerening\u003c/li\u003e\n\u003cli\u003e2013, 2015-2020 Salome Eriksson\u003c/li\u003e\n\u003cli\u003e2016-2020 Cedric Geissmann\u003c/li\u003e\n\u003cli\u003e2017-2020 Guillem Franc\u00e8s\u003c/li\u003e\n\u003cli\u003e2018-2020 Augusto B. Corr\u00eaa\u003c/li\u003e\n\u003cli\u003e2018-2020 Patrick Ferber\u003c/li\u003e\n\u003cli\u003e2015-2019 Manuel Heusner\u003c/li\u003e\n\u003cli\u003e2017 Daniel Killenberger\u003c/li\u003e\n\u003cli\u003e2016 Yusra Alkhazraji\u003c/li\u003e\n\u003cli\u003e2016 Martin Wehrle\u003c/li\u003e\n\u003cli\u003e2014-2015 Patrick von Reth\u003c/li\u003e\n\u003cli\u003e2015 Thomas Keller\u003c/li\u003e\n\u003cli\u003e2009-2014 Erez Karpas\u003c/li\u003e\n\u003cli\u003e2014 Robert P. Goldman\u003c/li\u003e\n\u003cli\u003e2010-2012 Andrew Coles\u003c/li\u003e\n\u003cli\u003e2010, 2012 Patrik Haslum\u003c/li\u003e\n\u003cli\u003e2003-2011 Silvia Richter\u003c/li\u003e\n\u003cli\u003e2009-2011 Emil Keyder\u003c/li\u003e\n\u003cli\u003e2010-2011 Moritz Gronbach\u003c/li\u003e\n\u003cli\u003e2010-2011 Manuela Ortlieb\u003c/li\u003e\n\u003cli\u003e2011 Vidal Alc\u00e1zar Saiz\u003c/li\u003e\n\u003cli\u003e2011 Michael Katz\u003c/li\u003e\n\u003cli\u003e2011 Raz Nissim\u003c/li\u003e\n\u003cli\u003e2010 Moritz Goebelbecker\u003c/li\u003e\n\u003cli\u003e2007-2009 Matthias Westphal\u003c/li\u003e\n\u003cli\u003e2009 Christian Muise\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cp\u003eThe current version of Fast Downward is the merger of three different\nprojects:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe original version of Fast Downward developed by Malte Helmert\nand Silvia Richter\u003c/li\u003e\n\u003cli\u003eLAMA, developed by Silvia Richter and Matthias Westphal based on\nthe original Fast Downward\u003c/li\u003e\n\u003cli\u003eFD-Tech, a modified version of Fast Downward developed by Erez\nKarpas and Michael Katz based on the original code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition to these three main sources, the codebase incorporates\ncode and features from numerous branches of the Fast Downward codebase\ndeveloped for various research papers. The main contributors to these\nbranches are Malte Helmert, Gabi R\u00f6ger and Silvia Richter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe following directory is not part of Fast Downward as covered by\nthis license:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or (at\nyour option) any later version.\n\nFast Downward is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see \u0026lt;https://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1621274654.0
  },
  {
    "data_format": 2,
    "description": "An implementation for solving 3SAT (Exact Cover) using the Quantum Approximate Optimization Algorithm",
    "filenames": [
      "SingularityFile.def"
    ],
    "full_name": "vivekkatial/qaoa-three-sat",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-qaoa-3sat--\" class=\"anchor\" href=\"#qaoa-3sat--\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQAOA 3SAT \u003ca href=\"https://camo.githubusercontent.com/4beb7225857c50a9391b71fbe998bc23c33b4d87ee15e3da9b7c1b7dfdc67a11/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f71616f612d74687265652d7361742e7376673f6272616e63683d6d6173746572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4beb7225857c50a9391b71fbe998bc23c33b4d87ee15e3da9b7c1b7dfdc67a11/68747470733a2f2f7472617669732d63692e636f6d2f766976656b6b617469616c2f71616f612d74687265652d7361742e7376673f6272616e63683d6d6173746572\" alt=\"\" data-canonical-src=\"https://travis-ci.com/vivekkatial/qaoa-three-sat.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://qaoa-three-sat.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/253d508d956ec9315fd5509c8d9cb82640904ab96c15672f2c65c9ec5c2de390/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f71616f612d74687265652d7361742f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/qaoa-three-sat/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h1\u003e\n\u003cp\u003eAn implementation for solving 3SAT (Exact Cover) using the Quantum Approximate Optimization Algorithm\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607596883.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for busco (https://gitlab.com/ezlab/busco)",
    "filenames": [
      "Singularity.4.1.0",
      "Singularity.4.1.1",
      "Singularity.5.1.2",
      "Singularity",
      "Singularity.4.0.6",
      "Singularity.4.0.4",
      "Singularity.4.0.2",
      "Singularity.4.0.5",
      "Singularity.4.1.4",
      "Singularity.4.1.2",
      "Singularity.4.0.1",
      "Singularity.4.0.0"
    ],
    "full_name": "powerPlant/busco-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the BUSCO tool for Benchmarking Universal Single-Copy Ortholog assessment\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618269021.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for aws-cli (https://github.com/aws/aws-cli)",
    "filenames": [
      "Singularity",
      "Singularity.2.0.43"
    ],
    "full_name": "powerPlant/aws-cli-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the AWS CLI v2 tool\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1598486009.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for entrez-direct (https://ftp.ncbi.nlm.nih.gov/entrez/entrezdirect/)",
    "filenames": [
      "Singularity",
      "Singularity.13.8.20200819"
    ],
    "full_name": "powerPlant/entrez-direct-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for Entrez Direct: E-utilities on the Unix Command Line to provide access to the NCBI\u0027s suite of interconnected databases\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1598244762.0
  },
  {
    "data_format": 2,
    "description": "The purpose of this project is to map Oxford Nanopore Sequencing data down to the species level",
    "filenames": [
      "setup/Singularity"
    ],
    "full_name": "JoshLoecker/MAPT",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mapt\" class=\"anchor\" href=\"#mapt\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMAPT\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/JoshLoecker/MAPT/wiki\"\u003ePlease view the Wiki\u003c/a\u003e for more information.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h3\u003e\n\u003cp\u003eIf you need help, have questions, or have feature ideas please \u003ca href=\"https://github.com/JoshLoecker/MAPT/issues\"\u003eopen a new issue\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1620064890.0
  },
  {
    "data_format": 2,
    "description": "This contains the latest docker and singularity images",
    "filenames": [
      "Singularity_Ubuntu_18_04_Cuda_11_1",
      "Singularity_Ubuntu_20_04_Cuda_11_1",
      "Singularity_Ubuntu_16_04",
      "Singularity_Ubuntu_18_04_Cuda_10_2"
    ],
    "full_name": "shreyaskamathkm/Cluster_Images",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-examples-with-drake-and-ros-2\" class=\"anchor\" href=\"#examples-with-drake-and-ros-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples with Drake and ROS 2\u003c/h1\u003e\n\u003cp\u003eThis repo shows examples of using \u003ca href=\"https://drake.mit.edu/\" rel=\"nofollow\"\u003eDrake\u003c/a\u003e and \u003ca href=\"https://www.ros.org/\" rel=\"nofollow\"\u003eROS 2\u003c/a\u003e together.\nIt uses the pydrake API and is of prototype quality.\nFor a similar effort in ROS 1, see \u003ca href=\"https://github.com/EricCousineau-TRI/repro/tree/master/ros/drake_ros1_hacks\"\u003eEricCousineau-TRI/repro \u003ccode\u003edrake_ros1_hacks\u003c/code\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu Focal (20.04)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://index.ros.org/doc/ros2/Installation/Rolling/\" rel=\"nofollow\"\u003eROS 2 Rolling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://drake.mit.edu/from_binary.html\" rel=\"nofollow\"\u003eSome Drake binary installation from October 2020\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInstall ROS 2 Rolling using the \u003ca href=\"https://index.ros.org/doc/ros2/Installation/Rolling/Linux-Install-Debians/\" rel=\"nofollow\"\u003eLinux binary instructions\u003c/a\u003e and \u003ca href=\"https://index.ros.org/doc/ros2/Installation/Prerelease-Testing/\" rel=\"nofollow\"\u003eenable the ROS 2 testing apt repo\u003c/a\u003e.\nInstall the apt packages \u003ccode\u003eros-rolling-desktop\u003c/code\u003e and \u003ccode\u003eros-rolling-sdformat-urdf\u003c/code\u003e.\nExtract the Drake binary installation, install it\u0027s prerequisites, and \u003ca href=\"https://drake.mit.edu/python_bindings.html#inside-virtualenv\" rel=\"nofollow\"\u003euse this Python virutalenv trick\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h2\u003e\n\u003cp\u003eOnce the prerequisites are met, install \u003ccode\u003edrake_ros\u003c/code\u003e into the Drake virtualenv.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e. path/to/drake/bin/activate\ncd path/to/this/repo\ncd drake_ros/\npython setup.py develop\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ros-2-tf-and-robot-model-demo\" class=\"anchor\" href=\"#ros-2-tf-and-robot-model-demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eROS 2 tf and Robot Model Demo\u003c/h2\u003e\n\u003cp\u003eThis demo shows RViz visualizing a single UR10 robot being simulated by Drake.\nSet up two terminals: one for launching RViz, and another for launching the Drake simulation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e. /opt/ros/rolling/setup.bash\ncd path/to/this/repo\nAMENT_PREFIX_PATH=\"$AMENT_PREFIX_PATH:$(pwd)\" rviz2 -d view.rviz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e. /opt/ros/rolling/setup.bash\n. path/to/drake/bin/activate\ncd path/to/this/repo\nAMENT_PREFIX_PATH=\"$AMENT_PREFIX_PATH:$(pwd)\" ./ros2_demo.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/4175662/90415417-e7976980-e065-11ea-9564-96c820f51680.gif\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/4175662/90415417-e7976980-e065-11ea-9564-96c820f51680.gif\" alt=\"ur10_rviz_drake\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-interactive-markers-demo\" class=\"anchor\" href=\"#interactive-markers-demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInteractive Markers Demo\u003c/h2\u003e\n\u003cp\u003eThis demonstrates using interactive markers to control an iiwa14 being simulated by Drake.\nSet up two terminals: one for launching RViz, and another for launching the Drake simulation.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e. /opt/ros/rolling/setup.bash\ncd path/to/this/repo\nAMENT_PREFIX_PATH=\"$AMENT_PREFIX_PATH:$(pwd)\" rviz2 -d interactive_demo.rviz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e. /opt/ros/rolling/setup.bash\n. path/to/drake/bin/activate\ncd path/to/this/repo\nAMENT_PREFIX_PATH=\"$AMENT_PREFIX_PATH:$(pwd)\" ./interactive_demo.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/4175662/96510753-dcea8380-1212-11eb-89ca-4a9019a8a9cd.gif\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/4175662/96510753-dcea8380-1212-11eb-89ca-4a9019a8a9cd.gif\" alt=\"iiwa14_interactive_drake\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-a-container\" class=\"anchor\" href=\"#using-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing a Container\u003c/h2\u003e\n\u003cp\u003eThere is a definition file for a \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container.\u003c/p\u003e\n\u003cp\u003eFirst \u003ca href=\"https://sylabs.io/guides/3.7/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003ebuild and install Singularity\u003c/a\u003e.\nAfterwards, build a Singularity sandbox from the definition file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build --fakeroot --sandbox ~/drake-ros2-demos.sandbox demos.singularity.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCreate a shell with access to an NVidia graphics card and run one of the RViz configs for your chosen demo.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity shell --nv --writable-tmpfs \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/drake-ros2-demos.sandbox\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eSingularity\u0026gt; rviz2 -d view.rviz\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eCreate a shell into the sandbox and run one of the demos.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity shell --writable \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/drake-ros2-demos.sandbox\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eSingularity\u0026gt; ./ros2_demo.py\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1616693700.0
  },
  {
    "data_format": 2,
    "description": "Open-Source Computational Structural Biology Framework",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "sailfish009/openstructure",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h1\u003e\n\u003cp\u003eThis README is pulled from a default template for workflows.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-workflow-template-setup\" class=\"anchor\" href=\"#workflow-template-setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow template setup\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-lib\" class=\"anchor\" href=\"#lib\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elib\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ccode\u003elib\u003c/code\u003e directory contains general libraries that may be referenced by multiple workflows, for instance cromwell configs and python configs. Currently nothing in this directory is used.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipelines\" class=\"anchor\" href=\"#pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epipelines\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eEach pipeline is a full analysis. Think of it like the heading of a methods section in a paper. For instance if this were genetic summary statistics workflow, a pipeline might be \"fine-mapping\" that does both conditional and credible set analysis. Another pipeline may be \"colocalization\".\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePipelines may have numbers prior to their name (e.g., \u003ccode\u003eexample_pipeline_1\u003c/code\u003e to \u003ccode\u003e0025-example_pipeline_1\u003c/code\u003e). These numbers do not mean anything, but merely used to keep pipelines in their general order of execution. These are optional.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA pipeline consists of :\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003eA workflow.\u003c/li\u003e\n\u003cli\u003eA \u003ccode\u003escripts\u003c/code\u003e directory with \u003cem\u003eall\u003c/em\u003e scripts referenced by that workflow (unless a general lib script is called). Scripts may have numbers prior to their name. These numbers do not mean anything, but merely used to keep scripts in their general order of execution. These are optional.\u003c/li\u003e\n\u003cli\u003eA \u003ccode\u003edocs\u003c/code\u003e directory that contains a documentation of the default parameters written in a style that is publishable as methods in a paper (including citations). Within the \u003ccode\u003edocs\u003c/code\u003e directory there may be a \u003ccode\u003ereference\u003c/code\u003e with any additional reference materials.\u003c/li\u003e\n\u003cli\u003eAn \u003ccode\u003eexample_runtime_setup\u003c/code\u003e directory contains files that give an example of actual config files and any other files used to run the pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-studies\" class=\"anchor\" href=\"#studies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003estudies\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA studies directory should either exist within the workflow repo or be a separate repo that has the same name as the workflow repo, but with \u003ccode\u003estudies\u003c/code\u003e appended to it (e.g. \u003ccode\u003etemplate-workflow\u003c/code\u003e becomes \u003ccode\u003etemplate-workflow-studies\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003eIf there is a standard set of plots that will always look the same way, a pipeline should generate such plots. Otherwise, all code to analyze the results of a pipeline run should be in the \u003ccode\u003estudies\u003c/code\u003e directory. For instance if this were genetic summary statistics workflow, \u003ccode\u003estudies\u003c/code\u003e may contain a \u003ccode\u003et2d\u003c/code\u003e directory and a \u003ccode\u003eweight\u003c/code\u003e directory.\u003c/li\u003e\n\u003cli\u003eWithin a study is either an Jupyter notebook (either python or R kernel) or an R markdown file. Nearly all plots / analysis of the results of running the various pipelines should be done in the notebook / markdown file.\u003c/li\u003e\n\u003cli\u003eA study may also contain a scripts directory with scripts to aggregate data for a one off analysis (if the analysis is going to be repeated, consider making a new pipeline or adding it to an existing pipeline) or for special plots that cannot be done in the notebook / markdown file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-new-workflow-reminders\" class=\"anchor\" href=\"#new-workflow-reminders\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNew workflow reminders\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e[ ] Documentation\u003c/li\u003e\n\u003cli\u003e[ ] Environment version control\u003c/li\u003e\n\u003cli\u003e[ ] Pipeline version control\u003c/li\u003e\n\u003cli\u003e[ ] Git branches\u003c/li\u003e\n\u003cli\u003e[ ] Code review\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h1\u003e\n\u003cp\u003eBe sure to document your code!\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-environment-version-control\" class=\"anchor\" href=\"#environment-version-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment version control\u003c/h1\u003e\n\u003cp\u003eAnalysis environment is controlled using conda. Each pipeline should have an \u003ccode\u003eenvironment.yml\u003c/code\u003e file with all of the packages used. If a required package or library is missing from conda (and therefore not in the \u003ccode\u003eenvironment.yml\u003c/code\u003e), it should be noted in the \u003ccode\u003eREADME.md\u003c/code\u003e of the pipeline.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda env \u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e --no-builds \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e grep -v prefix \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e grep -v name \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e environment.yml\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pipeline-version-control\" class=\"anchor\" href=\"#pipeline-version-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline version control\u003c/h1\u003e\n\u003cp\u003eEach pipeline within this workflow uses \u003ca href=\"https://pypi.org/project/bumpversion\" rel=\"nofollow\"\u003ebumpversion\u003c/a\u003e for automatic \u003ca href=\"https://semver.org\" rel=\"nofollow\"\u003esemantic versioning\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e bump the appropriate increment\u003c/span\u003e\nbumpversion patch --verbose --dry-run\nbumpversion minor --verbose --dry-run\nbumpversion major --verbose --dry-run\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e commit with tags\u003c/span\u003e\ngit push --tags\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-github-forks\" class=\"anchor\" href=\"#github-forks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGitHub forks\u003c/h1\u003e\n\u003cp\u003eForking the repository allows developers to work independently while retaining well-maintained code on the master fork. For instructions on how to fork, follow the \u003ca href=\"https://help.github.com/en/articles/fork-a-repo\"\u003eFork a repo\u003c/a\u003e instructions.\u003c/p\u003e\n\u003cp\u003eAfter forking the repo, clone the repo to your local desktop:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to use SSH\u003c/span\u003e\ngit clone git@github.com:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/template-workflow.git\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to use Https\u003c/span\u003e\ngit clone https://github.com/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/template-workflow.git\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis creates a replica of the remote repository on your local desktop. \u003cem\u003eNote\u003c/em\u003e: When you create your local repository, it will also make a local clone of the remote repository (typically as \u003ccode\u003eorigin\u003c/code\u003e). So, your local master branch would simply be \u003ccode\u003emaster\u003c/code\u003e. But, your remote master branch will be \u003ccode\u003eorigin/master\u003c/code\u003e. You can also add multiple remote repositories. For instance, let us say our main repository is under the remote repository \u003ccode\u003emy_repo\u003c/code\u003e. We will want to add it as a remote repository, so we can fetch the most up-to-date code. You could add it by:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Add the my_repo remote repo to your local desktop -- this will allow you to pull and push to branches on the my_repo repository\u003c/span\u003e\ngit remote add my_repo git@github.com:my_repo/template-workflow.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-git-branches\" class=\"anchor\" href=\"#git-branches\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGit branches\u003c/h1\u003e\n\u003cp\u003eBranching is how git actually tracks code development. For more information, see the \u003ca href=\"https://www.atlassian.com/git/tutorials/using-branches\" rel=\"nofollow\"\u003eGit Branch Tutorial\u003c/a\u003e on Atlassian. If you want to add a new feature, pipeline, or fix a bug, a common work flow would look like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Update your local copy of the master branch to make sure you are getting the most up-to-date code\u003c/span\u003e\ngit pull\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Create the branch on your local machine and switch in this branch \u003c/span\u003e\ngit checkout -b [name_of_your_new_branch]\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Push the branch on github\u003c/span\u003e\ngit push origin [name_of_your_new_branch]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAs you develop, you want to commit your work to your branch, so you don\u0027t lose it all if something happens!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Confirm we\u0027re on the right branch\u003c/span\u003e\ngit branch -a\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Add all your work to be tracked (Note: there are many ways to add specific files, etc. See https://git-scm.com/docs/git-add for more information). The following command adds everything in your currently directory.\u003c/span\u003e\ngit add \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Commit your work to the branch with a message describing what\u0027s in the commit\u003c/span\u003e\ngit commit -m \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eCreated the scATAC-seq pipeline!\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e You can add a -u parameter to set-upstream for a push\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Alternatively, git will also automatically query you when you do your first push.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e You can also set this manually by adding a new remote for your branch:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003egit remote add [name_of_your_remote] [name_of_your_new_branch]\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Here is another push where we specify HEAD\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003egit push origin HEAD # HEAD pushes everything up to the most recent commit\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-code-review\" class=\"anchor\" href=\"#code-review\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCode review\u003c/h1\u003e\n\u003cp\u003eCreate a \u003ca href=\"https://help.github.com/en/articles/creating-a-pull-request\"\u003eGitHub Pull Request\u003c/a\u003e. A PR allows other developers a chance to go through and comment on lines of code they believe can be improved. In addition, it will tell you if the code you are trying to merge into the \u003ccode\u003emy_repo\u003c/code\u003e branch actually conflicts with code that already exists in the branch, so you don\u0027t overwrite someone else\u0027s work.\u003c/p\u003e\n\u003cp\u003eOnce another developer approves the PR, you have the go-ahead to merge your code! Congrats, you finished your feature!\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote\u003c/em\u003e: There are some cases where you may just want to push directly to the my_repo fork, thereby avoiding code reviews. For instance, if you\u0027re working on a one-off project that you want people to be able to see, but no one else is necessarily working on, you can always push directly to the branches on my_repo fork. Or, you could also still go through the steps of a PR, but simply merge your own code without CR.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1597367982.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "pipelines/0037-cell_cell_interaction/env/Singularity.cell_cell_interaction",
      "pipelines/0025-qc_cluster/env/Singularity.sc_qc_cluster",
      "pipelines/0015-preprocessing/env/Singularity.preprocessing"
    ],
    "full_name": "ckrilow/dev-ckrilow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h1\u003e\n\u003cp\u003eThis README is pulled from a default template for workflows.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-workflow-template-setup\" class=\"anchor\" href=\"#workflow-template-setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow template setup\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-lib\" class=\"anchor\" href=\"#lib\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elib\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ccode\u003elib\u003c/code\u003e directory contains general libraries that may be referenced by multiple workflows, for instance cromwell configs and python configs. Currently nothing in this directory is used.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipelines\" class=\"anchor\" href=\"#pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epipelines\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eEach pipeline is a full analysis. Think of it like the heading of a methods section in a paper. For instance if this were genetic summary statistics workflow, a pipeline might be \"fine-mapping\" that does both conditional and credible set analysis. Another pipeline may be \"colocalization\".\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePipelines may have numbers prior to their name (e.g., \u003ccode\u003eexample_pipeline_1\u003c/code\u003e to \u003ccode\u003e0025-example_pipeline_1\u003c/code\u003e). These numbers do not mean anything, but merely used to keep pipelines in their general order of execution. These are optional.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA pipeline consists of :\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003eA workflow.\u003c/li\u003e\n\u003cli\u003eA \u003ccode\u003escripts\u003c/code\u003e directory with \u003cem\u003eall\u003c/em\u003e scripts referenced by that workflow (unless a general lib script is called). Scripts may have numbers prior to their name. These numbers do not mean anything, but merely used to keep scripts in their general order of execution. These are optional.\u003c/li\u003e\n\u003cli\u003eA \u003ccode\u003edocs\u003c/code\u003e directory that contains a documentation of the default parameters written in a style that is publishable as methods in a paper (including citations). Within the \u003ccode\u003edocs\u003c/code\u003e directory there may be a \u003ccode\u003ereference\u003c/code\u003e with any additional reference materials.\u003c/li\u003e\n\u003cli\u003eAn \u003ccode\u003eexample_runtime_setup\u003c/code\u003e directory contains files that give an example of actual config files and any other files used to run the pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-studies\" class=\"anchor\" href=\"#studies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003estudies\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA studies directory should either exist within the workflow repo or be a separate repo that has the same name as the workflow repo, but with \u003ccode\u003estudies\u003c/code\u003e appended to it (e.g. \u003ccode\u003etemplate-workflow\u003c/code\u003e becomes \u003ccode\u003etemplate-workflow-studies\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003eIf there is a standard set of plots that will always look the same way, a pipeline should generate such plots. Otherwise, all code to analyze the results of a pipeline run should be in the \u003ccode\u003estudies\u003c/code\u003e directory. For instance if this were genetic summary statistics workflow, \u003ccode\u003estudies\u003c/code\u003e may contain a \u003ccode\u003et2d\u003c/code\u003e directory and a \u003ccode\u003eweight\u003c/code\u003e directory.\u003c/li\u003e\n\u003cli\u003eWithin a study is either an Jupyter notebook (either python or R kernel) or an R markdown file. Nearly all plots / analysis of the results of running the various pipelines should be done in the notebook / markdown file.\u003c/li\u003e\n\u003cli\u003eA study may also contain a scripts directory with scripts to aggregate data for a one off analysis (if the analysis is going to be repeated, consider making a new pipeline or adding it to an existing pipeline) or for special plots that cannot be done in the notebook / markdown file.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-new-workflow-reminders\" class=\"anchor\" href=\"#new-workflow-reminders\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNew workflow reminders\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e[ ] Documentation\u003c/li\u003e\n\u003cli\u003e[ ] Environment version control\u003c/li\u003e\n\u003cli\u003e[ ] Pipeline version control\u003c/li\u003e\n\u003cli\u003e[ ] Git branches\u003c/li\u003e\n\u003cli\u003e[ ] Code review\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h1\u003e\n\u003cp\u003eBe sure to document your code!\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-environment-version-control\" class=\"anchor\" href=\"#environment-version-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment version control\u003c/h1\u003e\n\u003cp\u003eAnalysis environment is controlled using conda. Each pipeline should have an \u003ccode\u003eenvironment.yml\u003c/code\u003e file with all of the packages used. If a required package or library is missing from conda (and therefore not in the \u003ccode\u003eenvironment.yml\u003c/code\u003e), it should be noted in the \u003ccode\u003eREADME.md\u003c/code\u003e of the pipeline.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda env \u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e --no-builds \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e grep -v prefix \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e grep -v name \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e environment.yml\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pipeline-version-control\" class=\"anchor\" href=\"#pipeline-version-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline version control\u003c/h1\u003e\n\u003cp\u003eEach pipeline within this workflow uses \u003ca href=\"https://pypi.org/project/bumpversion\" rel=\"nofollow\"\u003ebumpversion\u003c/a\u003e for automatic \u003ca href=\"https://semver.org\" rel=\"nofollow\"\u003esemantic versioning\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e bump the appropriate increment\u003c/span\u003e\nbumpversion patch --verbose --dry-run\nbumpversion minor --verbose --dry-run\nbumpversion major --verbose --dry-run\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e commit with tags\u003c/span\u003e\ngit push --tags\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-github-forks\" class=\"anchor\" href=\"#github-forks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGitHub forks\u003c/h1\u003e\n\u003cp\u003eForking the repository allows developers to work independently while retaining well-maintained code on the master fork. For instructions on how to fork, follow the \u003ca href=\"https://help.github.com/en/articles/fork-a-repo\"\u003eFork a repo\u003c/a\u003e instructions.\u003c/p\u003e\n\u003cp\u003eAfter forking the repo, clone the repo to your local desktop:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to use SSH\u003c/span\u003e\ngit clone git@github.com:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/template-workflow.git\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e to use Https\u003c/span\u003e\ngit clone https://github.com/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eusername\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/template-workflow.git\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis creates a replica of the remote repository on your local desktop. \u003cem\u003eNote\u003c/em\u003e: When you create your local repository, it will also make a local clone of the remote repository (typically as \u003ccode\u003eorigin\u003c/code\u003e). So, your local master branch would simply be \u003ccode\u003emaster\u003c/code\u003e. But, your remote master branch will be \u003ccode\u003eorigin/master\u003c/code\u003e. You can also add multiple remote repositories. For instance, let us say our main repository is under the remote repository \u003ccode\u003emy_repo\u003c/code\u003e. We will want to add it as a remote repository, so we can fetch the most up-to-date code. You could add it by:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Add the my_repo remote repo to your local desktop -- this will allow you to pull and push to branches on the my_repo repository\u003c/span\u003e\ngit remote add my_repo git@github.com:my_repo/template-workflow.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-git-branches\" class=\"anchor\" href=\"#git-branches\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGit branches\u003c/h1\u003e\n\u003cp\u003eBranching is how git actually tracks code development. For more information, see the \u003ca href=\"https://www.atlassian.com/git/tutorials/using-branches\" rel=\"nofollow\"\u003eGit Branch Tutorial\u003c/a\u003e on Atlassian. If you want to add a new feature, pipeline, or fix a bug, a common work flow would look like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Update your local copy of the master branch to make sure you are getting the most up-to-date code\u003c/span\u003e\ngit pull\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Create the branch on your local machine and switch in this branch \u003c/span\u003e\ngit checkout -b [name_of_your_new_branch]\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Push the branch on github\u003c/span\u003e\ngit push origin [name_of_your_new_branch]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAs you develop, you want to commit your work to your branch, so you don\u0027t lose it all if something happens!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Confirm we\u0027re on the right branch\u003c/span\u003e\ngit branch -a\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Add all your work to be tracked (Note: there are many ways to add specific files, etc. See https://git-scm.com/docs/git-add for more information). The following command adds everything in your currently directory.\u003c/span\u003e\ngit add \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Commit your work to the branch with a message describing what\u0027s in the commit\u003c/span\u003e\ngit commit -m \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eCreated the scATAC-seq pipeline!\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e You can add a -u parameter to set-upstream for a push\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Alternatively, git will also automatically query you when you do your first push.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e You can also set this manually by adding a new remote for your branch:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003egit remote add [name_of_your_remote] [name_of_your_new_branch]\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Here is another push where we specify HEAD\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003egit push origin HEAD # HEAD pushes everything up to the most recent commit\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-code-review\" class=\"anchor\" href=\"#code-review\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCode review\u003c/h1\u003e\n\u003cp\u003eCreate a \u003ca href=\"https://help.github.com/en/articles/creating-a-pull-request\"\u003eGitHub Pull Request\u003c/a\u003e. A PR allows other developers a chance to go through and comment on lines of code they believe can be improved. In addition, it will tell you if the code you are trying to merge into the \u003ccode\u003emy_repo\u003c/code\u003e branch actually conflicts with code that already exists in the branch, so you don\u0027t overwrite someone else\u0027s work.\u003c/p\u003e\n\u003cp\u003eOnce another developer approves the PR, you have the go-ahead to merge your code! Congrats, you finished your feature!\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNote\u003c/em\u003e: There are some cases where you may just want to push directly to the my_repo fork, thereby avoiding code reviews. For instance, if you\u0027re working on a one-off project that you want people to be able to see, but no one else is necessarily working on, you can always push directly to the branches on my_repo fork. Or, you could also still go through the steps of a PR, but simply merge your own code without CR.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1596817893.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for bedops (https://github.com/bedops/bedops)",
    "filenames": [
      "Singularity",
      "Singularity.2.4.39"
    ],
    "full_name": "powerPlant/bedops-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the BEDOPS open-source command-line toolkit that performs highly efficient and scalable Boolean and other set operations, statistical calculations, archiving, conversion and other management of genomic data of arbitrary scale.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1596773368.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for REPET (https://urgi.versailles.inra.fr/Tools/REPET)",
    "filenames": [
      "Singularity",
      "Singularity.3.0"
    ],
    "full_name": "powerPlant/repet-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for REPET\n(\u003ca href=\"https://urgi.versailles.inra.fr/Tools/REPET\" rel=\"nofollow\"\u003ehttps://urgi.versailles.inra.fr/Tools/REPET\u003c/a\u003e), used to detect, annotate and\nanalyse repeats in genomic sequences, specifically designed for transposable\nelements (TEs).\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602104190.0
  },
  {
    "data_format": 2,
    "description": "Comparison of batch correction methods for scRNA-seq data - basically a clone of BatchBench",
    "filenames": [
      "Singularity"
    ],
    "full_name": "Sarah145/batch_correct",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-batch-correction-pipeline\" class=\"anchor\" href=\"#batch-correction-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBatch correction pipeline\u003c/h2\u003e\n\u003cp\u003eThis repository contains scripts to run a Nextflow pipeline to compare different batch correction methods for single-cell RNA-seq data. This is mostly just a clone of the \u003ca href=\"https://github.com/cellgeni/batchbench\"\u003eBatchBench\u003c/a\u003e pipeline from the CellGen IT team at Sanger but I couldn\u0027t get that to run so made some edits and added one or two extra things.\u003c/p\u003e\n\u003cp\u003eThe input files for this pipeline must be .Rds files of the uncorrected data as a SingleCellExperiment object (all batches in one object) with batch labels stored in the \u003ccode\u003ebatch_key\u003c/code\u003e (\u0027Batch\u0027 by default) column and cell type labels stored in the \u003ccode\u003ecelltype_key\u003c/code\u003e (\u0027cell_type1\u0027 by default) column.\u003c/p\u003e\n\u003cp\u003eThe pipeline will run 7 different batch correction methods on the data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eScanorama\u003c/li\u003e\n\u003cli\u003eBBKNN\u003c/li\u003e\n\u003cli\u003eSeurat 3\u003c/li\u003e\n\u003cli\u003eCombat\u003c/li\u003e\n\u003cli\u003eHarmony\u003c/li\u003e\n\u003cli\u003elimma\u003c/li\u003e\n\u003cli\u003eMNNCorrect\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor each method, 5 different evaluation metrics are returned:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBatch entropy (from \u003ca href=\"https://www.biorxiv.org/content/10.1101/2020.05.22.111211v2\" rel=\"nofollow\"\u003eBatchBench\u003c/a\u003e) - measure of how well batches are aligned after correction - related to the probability that for each cell, its \u003cem\u003ek\u003c/em\u003e nearest neighbors come from a different batch - value reported is average entropy scaled between 0-1 - high batch entropy = well-mixed batches, low batch entropy = poorly-mixed batches.\u003c/li\u003e\n\u003cli\u003eCell type entropy (from \u003ca href=\"https://www.biorxiv.org/content/10.1101/2020.05.22.111211v2\" rel=\"nofollow\"\u003eBatchBench\u003c/a\u003e) - same as batch entropy but using cell type labels instead - high cell type entropy = mixing of cell types (not good), low cell type entropy = cell types are not mixing (good).\u003c/li\u003e\n\u003cli\u003eBatch ASW (from \u003ca href=\"https://github.com/theislab/scib\"\u003escIB\u003c/a\u003e) - average silhouette width of batches - scaled between -1-1 - high batch ASW = dense, well-separated batches (bad), low batch ASW = well mixed batches (good).\u003c/li\u003e\n\u003cli\u003eCell type ASW (from \u003ca href=\"https://github.com/theislab/scib\"\u003escIB\u003c/a\u003e) - same as batch ASW but for cell type labels - high cell type ASW = good, low cell type ASW = bad.\u003c/li\u003e\n\u003cli\u003eRecovery of marker genes - this idea was taken from the BatchBench paper but couldn\u0027t find code for it so wrote my own - not sure if it\u0027s right. For methods that correct the expression matrix (Scanorama, Seurat3, Combat, limma, MNNCorrect), found marker genes for each cell type (by batch and in the merged dataset), before and after batch correction, then compared the list of total marker genes identified before batch correction to the list of total marker genes identified after batch correction and calculated the Jaccard similarity index of the two lists. High Jaccard index = gene expression was not distorted too much by batch correction, most markers genes could still be identified (good), low Jaccard index = batch correction highly distorted the gene expression values so not as many marker genes could be recovered (bad). Jaccard index = 1 - all marker genes recovered, Jaccard index = 0 - no marker genes recovered.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo run pipeline:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNeed to have Nextflow and Singularity installed.\u003c/li\u003e\n\u003cli\u003eClone this repo and \u003ccode\u003ecd\u003c/code\u003e into it.\u003c/li\u003e\n\u003cli\u003ePull Singularity image - \u003ccode\u003esingularity pull shub://Sarah145/batch_correct\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eEdit the \u003ccode\u003enextflow.config\u003c/code\u003e script with location of data, batch key, cell type key, etc. \u003cem\u003eNote: profile section of the nextflow.config script in this repo is configured to run on cluster with slurm.\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003eEdit \u003ccode\u003edataset_list.txt\u003c/code\u003e file with name of files - one file on each line, no file extension.\u003c/li\u003e\n\u003cli\u003eRun pipeline with \u003ccode\u003enextflow run main.nf -profile singularity -with-trace trace.txt -with-dag flowchart.png\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eCompile html report of run by running \u003ccode\u003e./compile_report.R \u0026lt;sample_name\u0026gt;\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eOverview of pipeline\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/Sarah145/batch_correct/blob/master/imgs/flowchart.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/Sarah145/batch_correct/raw/master/imgs/flowchart.png?raw=true\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "scrna-seq-analysis",
      "batch-correction"
    ],
    "updated_at": 1624883167.0
  },
  {
    "data_format": 2,
    "description": "parallel gzipper in pure python",
    "filenames": [
      "Singularity.alpine"
    ],
    "full_name": "d-w-moore/zipit",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-zipit\" class=\"anchor\" href=\"#zipit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ezipit\u003c/h1\u003e\n\u003cp\u003eThis repo contains two scripts useful for gzipping and checking large files\nas quickly as possible leveraging the parallelism of your machine.\u003c/p\u003e\n\u003cp\u003eThey require only that python be installed, and they depend only on modules\nincluded in the Python Standard Library -- particularly, of course, gzip.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-zipitpy\" class=\"anchor\" href=\"#zipitpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ezipit.py\u003c/h2\u003e\n\u003cp\u003eExample uses:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ ./zipit.py -v large.tar    # =\u0026gt; Creates large.tar.gz at default level of parallelism.\n                              #    (-v verbosely informs of the piece-wise gzip tasks)\n\n $ ./zipit.py -qm large.tar   # =\u0026gt; creates large.tar.gz using all available CPU\u0027s\n\n $ some_command | ./zipit.py - \u0026gt; out.gz   # =\u0026gt; gzips from the stdin stream, onto stdout\n\n $ docker export cimg | ./zipit.py \\      # =\u0026gt; export and compress the filesystem of\n      -d cimg.dig - \u0026gt;cimg.tgz             #    a docker container\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testzippy\" class=\"anchor\" href=\"#testzippy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etestzip.py\u003c/h2\u003e\n\u003cp\u003eExample use (for context, see the final \u003ccode\u003ezipit.py\u003c/code\u003e example above):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ ./testzip.py cimg.tgz cimg.dig      # =\u0026gt; tests the gzipped file\u0027s integrity using a digest file\n                                       #    (returns 0 if the integrity is good)\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1602285708.0
  },
  {
    "data_format": 2,
    "description": "singularity environment manager for NGS pipelines ",
    "filenames": [
      "damona/recipes/salmon/Singularity.salmon_1.3.0",
      "damona/recipes/conda/Singularity.conda_4.7.12",
      "damona/recipes/conda/Singularity.conda_4.9.2",
      "damona/recipes/minimap2/Singularity.minimap2_2.17.0",
      "damona/recipes/rtools/Singularity.Rtools_1.0.0",
      "damona/recipes/rtools/Singularity.Rtools_1.1.0",
      "damona/recipes/art/Singularity.art_3.11.14",
      "damona/recipes/damona/Singularity.damona_0.4.2",
      "damona/recipes/damona/Singularity.damona_0.3.0",
      "damona/recipes/falco/Singularity.falco_0.2.1",
      "damona/recipes/rnaseqc/Singularity.rnaseqc_2.35.0",
      "damona/recipes/trf/Singularity.trf_4.09",
      "damona/recipes/trf/Singularity.trf_4.10.0",
      "damona/recipes/rnadiff/Singularity.rnadiff_1.7.1",
      "damona/recipes/sequana_tools/Singularity.sequana_tools_0.9.0",
      "damona/recipes/sequana_tools/Singularity.sequana_tools_0.10.0",
      "damona/recipes/sequana_tools/Singularity.sequana_tools_0.11.0",
      "damona/recipes/R/Singularity.R_3.6.3",
      "damona/recipes/R/Singularity.R_4.0.2",
      "damona/recipes/kraken/Singularity.kraken_1.1",
      "damona/recipes/kraken/Singularity.kraken_2.0.9",
      "damona/recipes/fastqc/Singularity.fastqc_0.11.8",
      "damona/recipes/fastqc/Singularity.fastqc_0.11.9",
      "damona/recipes/gffread/Singularity.gffread_0.12.1",
      "damona/recipes/bcl2fastq/Singularity.bcl2fastq_2.20.0",
      "damona/recipes/prokka/Singularity.prokka_1.14.5",
      "damona/recipes/sequana_perl_tools/Singularity.sequana_perl_tools_0.1.0",
      "damona/recipes/ucsc/Singularity.ucsc_0.1.0",
      "damona/recipes/fitter/Singularity.fitter_1.3.0",
      "damona/recipes/phantompeakqualtools/Singularity.phantompeakqualtools_1.2.2",
      "damona/recipes/canu/Singularity.canu_1.8.0",
      "damona/recipes/canu/Singularity.canu_1.6.0",
      "damona/recipes/graphviz/Singularity.graphviz_2.43.0"
    ],
    "full_name": "cokelaer/damona",
    "latest_release": "v0.4.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc-singularity\" class=\"anchor\" href=\"#hpc-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehpc-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity for HPC\u003c/p\u003e\n\u003cp\u003eMake sure the sigularity is built on \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003ehttps://singularity-hub.org\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eif ready use:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers3.0.2-imagecrawl\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTransformer 2.11.0:\n\u003ccode\u003esingularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers2.11.0-imagecrawl\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eMake sure the imagecrawl is updated (latest commit)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1620052198.0
  },
  {
    "data_format": 2,
    "description": "Singularity for HPC",
    "filenames": [
      "Singularity.centos7-python3.7-transformers2.11.0-ImageCrawl",
      "Singularity.centos7-python3.7-transformers3.0.2-ImageCrawl"
    ],
    "full_name": "sina-ehsani/hpc-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc-singularity\" class=\"anchor\" href=\"#hpc-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehpc-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity for HPC\u003c/p\u003e\n\u003cp\u003eMake sure the sigularity is built on \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003ehttps://singularity-hub.org\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eif ready use:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers3.0.2-imagecrawl\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTransformer 2.11.0:\n\u003ccode\u003esingularity pull shub://sinaehsani6/hpc-singularity:centos7-python3.7-transformers2.11.0-imagecrawl\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eMake sure the imagecrawl is updated (latest commit)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1601682764.0
  },
  {
    "data_format": 2,
    "description": "HPC-AI 2020 | Training Project NEMO - Nucleus for European Modelling of the Ocean",
    "filenames": [
      "Slurm Script/Singularity.nemo.apps",
      "Slurm Script/Singularity.CENTOS-7.7-NEMO-MOFED"
    ],
    "full_name": "soycoder/nemo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content--nemo---ocean\" class=\"anchor\" href=\"#-nemo---ocean\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cg-emoji class=\"g-emoji\" alias=\"ocean\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30a.png\"\u003e\ud83c\udf0a\u003c/g-emoji\u003e NEMO - ocean\u003c/h1\u003e\n\u003cp\u003eHPC-AI 2020 | Training Project - NEMO: Nucleus for European Modelling of the Ocean\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content--docker-images---centos\" class=\"anchor\" href=\"#-docker-images---centos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cg-emoji class=\"g-emoji\" alias=\"floppy_disk\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png\"\u003e\ud83d\udcbe\u003c/g-emoji\u003e Docker Images - CentOS\u003c/h2\u003e\n\u003cp\u003eThank you for an image  (\u003ca href=\"https://hub.docker.com/r/wangyoucao577/centos7-gcc7.4\" rel=\"nofollow\"\u003ewangyoucao577/centos7-gcc7.4\u003c/a\u003e)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content--tag\" class=\"anchor\" href=\"#-tag\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cg-emoji class=\"g-emoji\" alias=\"bookmark\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f516.png\"\u003e\ud83d\udd16\u003c/g-emoji\u003e Tag\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://hub.docker.com/layers/soycoder/centos7/nemo-ocean/images/sha256-c7bdaa3614e1fc1bbef31bdb05ac997e64b11abff716d00315807b1b79ad13c3\" rel=\"nofollow\"\u003e:nemo-ocean\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content--environment\" class=\"anchor\" href=\"#-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cg-emoji class=\"g-emoji\" alias=\"sunrise_over_mountains\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f304.png\"\u003e\ud83c\udf04\u003c/g-emoji\u003e Environment\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eHPC-X to build an out-of-box MPI environment\u003c/li\u003e\n\u003cli\u003eBoost library\u003c/li\u003e\n\u003cli\u003eHDF5 Parallellibrary\u003c/li\u003e\n\u003cli\u003eNETCDF Parallel library with HDF5\u003c/li\u003e\n\u003cli\u003eNETCDF-FortranParallel library with NETCDF Parallel\u003c/li\u003e\n\u003cli\u003eXIOS\u003c/li\u003e\n\u003cli\u003eGYREwith GNUgfortran + HPC-X OpenMPI\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-text-html-basic\"\u003e\u003cpre\u003e/usr/mpi/gcc/openmpi-3.1.1rc1/bin/mpirun \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n-mca mpi_show_mca_params 1 -mca pml_ucx_verbose 9 \\\n/usr/mpi/gcc/openmpi-3.1.1rc1/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\n\n/usr/mpi/gcc/openmpi-3.1.1rc1/bin/mpirun -n 2 \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n/usr/mpi/gcc/openmpi-3.1.1rc1/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca pml ucx -x UCX_TLS=rc UCX_NET_DEVICES=mlx5_0:1 ./nemo\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca -x UCX_TLS=rc -x UCX_NET_DEVICES=mlx5_0:1 ./nemo\n\n/usr/bin/time -p mpirun -n 2 \\\n-mca -x UCX_TLS=rc -x UCX_NET_DEVICES=ib0 \\\n/home/hpc/nemo/apps/hpcx-v2.6.0-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-redhat7.7-x86_64/ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\nibstat\n\n\nNow step into the container and install MOFED:\n\n$ sudo singularity exec -w u16.04-sandbox/ bash\n(singularity)# cd MOFED/MLNX_OFED_LINUX-4.3-1.0.1.0-ubuntu16.04-x86_64\n(singularity)# ./mlnxofedinstall\n\n\n! -- (nemo) singularity exec -w nemo.sif bash\n\n\n## Run container\nTo use Singularity in Mellanox/HPCX need to load env module: `module load tools/singularity`\n.\n\nRun `osu_latency` test:\n```sh\n$ mpirun -np 2 --map-by node -mca btl self singularity exec hpcx-u16.04.simg /hpcx/ompi-a7df\nd94/tests/osu-micro-benchmarks-5.3.2/osu_latency\n# OSU MPI Latency Test v5.3.2\n# Size          Latency (us)\n0                       1.55\n1                       1.55\n2                       1.55\n4                       1.55\n8                       1.54\n16                      1.55\n32                      1.55\n64                      1.65\n128                     2.19\n256                     2.23\n512                     2.35\n1024                    2.64\n2048                    2.89\n4096                    3.51\n8192                    5.00\n16384                   6.44\n32768                   8.91\n65536                  14.12\n131072                 25.05\n262144                 27.31\n524288                 49.03\n1048576                92.53\n2097152               178.95\n4194304               351.24\n\n\n\n$hpcx_mpi_dir/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\ncd /home/hpc/nemo/apps/hpcx-v2.6.0-gcc-MLNX_OFED_LINUX-4.7-1.0.0.1-redhat7.7-x86_64\n\nmpirun \\\n-mca pml ucx -x UCX_NET_DEVICES=mlx5_0:1 \\\n-mca mpi_show_mca_params 1 -mca pml_ucx_verbose 9 \\\n./ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\n\nmpirun \\\n-mca mpi_show_mca_params 1 -mca pml_ucx_verbose 9 \\\n./ompi/tests/osu-micro-benchmarks-5.3.2/osu_get_bw\n\n\n\n/usr/bin/time -p mpirun -np 4 \\\n--map-by core -report-bindings \\\n-mca io ompio -x UCX_NET_DEVICES=mlx5_0:1 ./nemo\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1603363757.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for sortmerna (https://github.com/biocore/sortmerna)",
    "filenames": [
      "Singularity",
      "Singularity.4.2.0",
      "Singularity.4.3.2",
      "Singularity.3.0.3"
    ],
    "full_name": "powerPlant/sortmerna-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the SortMeRNA local sequence alignment tool for filtering, mapping and clustering.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618542140.0
  },
  {
    "data_format": 2,
    "description": "XCrySDen in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.1.6.2"
    ],
    "full_name": "OSC/sa_singularity_xcrysden",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-xcrysden\" class=\"anchor\" href=\"#singularity-xcrysden\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity XCrySDen\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4445\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"http://www.xcrysden.org/Download.html\" rel=\"nofollow\"\u003eXCrysDen\u003c/a\u003e. It was built on top of the base Docker image \u003ca href=\"https://hub.docker.com/_/ubuntu\" rel=\"nofollow\"\u003eubuntu\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003excrysden.sif\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build xcrysden.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from \u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull xcrysden.sif shub://OSC/sa_singularity_xcrysden\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-start-xcrysden\" class=\"anchor\" href=\"#start-xcrysden\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStart XCrysDen\u003c/h3\u003e\n\u003cp\u003eXCrysDen is started using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run xcrysden.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as a native command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./xcrysden.sif\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1592244254.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for trinityrnaseq (https://github.com/trinityrnaseq/trinityrnaseq)",
    "filenames": [
      "Singularity.2.9.1",
      "Singularity",
      "Singularity.2.8.6",
      "Singularity.2.10.0",
      "Singularity.2.9.0"
    ],
    "full_name": "powerPlant/trinityrnaseq-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the Trinity RNA-Seq de novo transcriptome assembly\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1591576526.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity_1.0.0"
    ],
    "full_name": "daviesdrew/variantcalling",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"docs/images/nf-core-illuminavariantcalling_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/nf-core-illuminavariantcalling_logo.png\" alt=\"nf-core/illuminavariantcalling\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eIllumina paired end reads variant calling pipeline\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/nf-core/illuminavariantcalling/actions\"\u003e\u003cimg src=\"https://github.com/nf-core/illuminavariantcalling/workflows/nf-core%20CI/badge.svg\" alt=\"GitHub Actions CI Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/nf-core/illuminavariantcalling/actions\"\u003e\u003cimg src=\"https://github.com/nf-core/illuminavariantcalling/workflows/nf-core%20linting/badge.svg\" alt=\"GitHub Actions Linting Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/illuminavariantcalling\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/609e7a6579baf2276f34ef713d9cc0b55f7fd62e2c5c7618d40423779d41fd44/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f696c6c756d696e6176617269616e7463616c6c696e672e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/illuminavariantcalling.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003ei. Install \u003ca href=\"https://nf-co.re/usage/installation\" rel=\"nofollow\"\u003e\u003ccode\u003enextflow\u003c/code\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eii. Install either \u003ca href=\"https://docs.docker.com/engine/installation/\" rel=\"nofollow\"\u003e\u003ccode\u003eDocker\u003c/code\u003e\u003c/a\u003e or \u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/\" rel=\"nofollow\"\u003e\u003ccode\u003eSingularity\u003c/code\u003e\u003c/a\u003e for full pipeline reproducibility (please only use \u003ca href=\"https://conda.io/miniconda.html\" rel=\"nofollow\"\u003e\u003ccode\u003eConda\u003c/code\u003e\u003c/a\u003e as a last resort; see \u003ca href=\"https://nf-co.re/usage/configuration#basic-configuration-profiles\" rel=\"nofollow\"\u003edocs\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eiii. Download the pipeline and test it on a minimal dataset with a single command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run nf-core/illuminavariantcalling -profile test,\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edocker/singularity/conda/institute\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cblockquote\u003e\n\u003cp\u003ePlease check \u003ca href=\"https://github.com/nf-core/configs#documentation\"\u003enf-core/configs\u003c/a\u003e to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use \u003ccode\u003e-profile \u0026lt;institute\u0026gt;\u003c/code\u003e in your command. This will enable either \u003ccode\u003edocker\u003c/code\u003e or \u003ccode\u003esingularity\u003c/code\u003e and set the appropriate execution settings for your local compute environment.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eiv. Start running your own analysis!\u003c/p\u003e\n\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run nf-core/illuminavariantcalling -profile \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edocker/singularity/conda/institute\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e --reads \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e*_R{1,2}.fastq.gz\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e --genome GRCh37\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee \u003ca href=\"docs/usage.md\"\u003eusage docs\u003c/a\u003e for all of the available options when running the pipeline.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eThe nf-core/illuminavariantcalling pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/installation\" rel=\"nofollow\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/local_installation\" rel=\"nofollow\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/adding_own_config\" rel=\"nofollow\"\u003eAdding your own system config\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/reference_genomes\" rel=\"nofollow\"\u003eReference genomes\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://nf-co.re/usage/troubleshooting\" rel=\"nofollow\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003enf-core/illuminavariantcalling was originally written by Drew Davies.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions-and-support\" class=\"anchor\" href=\"#contributions-and-support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions and Support\u003c/h2\u003e\n\u003cp\u003eIf you would like to contribute to this pipeline, please see the \u003ca href=\".github/CONTRIBUTING.md\"\u003econtributing guidelines\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor further information or help, don\u0027t hesitate to get in touch on \u003ca href=\"https://nfcore.slack.com/channels/illuminavariantcalling\" rel=\"nofollow\"\u003eSlack\u003c/a\u003e (you can join with \u003ca href=\"https://nf-co.re/join/slack\" rel=\"nofollow\"\u003ethis invite\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\n\n\u003cp\u003eYou can cite the \u003ccode\u003enf-core\u003c/code\u003e publication as follows:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eThe nf-core framework for community-curated bioinformatics pipelines.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePhilip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso \u0026amp; Sven Nahnsen.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eNat Biotechnol.\u003c/em\u003e 2020 Feb 13. doi: \u003ca href=\"https://dx.doi.org/10.1038/s41587-020-0439-x\" rel=\"nofollow\"\u003e10.1038/s41587-020-0439-x\u003c/a\u003e.\u003cbr\u003e\nReadCube: \u003ca href=\"https://rdcu.be/b1GjZ\" rel=\"nofollow\"\u003eFull Access Link\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1593036214.0
  },
  {
    "data_format": 2,
    "description": "graph clustering toolkit",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "Lizhen0909/graph_clustering_toolkit",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-graph-clustering-toolkit\" class=\"anchor\" href=\"#graph-clustering-toolkit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGraph Clustering Toolkit\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSummary:\u003c/h3\u003e\n\u003cp\u003eThe toolkit collects many academic graph clustering programs and make them avaliable as package. Docker image is provided for easy access.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation:\u003c/h3\u003e\n\u003cp\u003eUse docker is convenient as\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull lizhen0909/graph_clustering_toolkit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more information, please refer to \u003ca href=\"https://lizhen0909.github.io/graph_clustering_toolkit/\" rel=\"nofollow\"\u003eonline document\u003c/a\u003e for a full description\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage:\u003c/h3\u003e\n\u003cp\u003eStart python from docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -it --rm lizhen0909/graph_clustering_toolkit python\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRun the script from the command line:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egct\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egct\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003edataset\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erandom_dataset\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e#create a random graph use LFR generator\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eds\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003erandom_dataset\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003egenerate_undirected_unweighted_random_graph_LFR\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ename\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"random_graph\"\u003c/span\u003e, \\\n                                       \u003cspan class=\"pl-v\"\u003eN\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ek\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e16\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emaxk\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e32\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emu\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e0.2\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003eminc\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e32\u003c/span\u003e)\n\u003cspan class=\"pl-c\"\u003e# run pScan graph algorithm\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003epscan_clustering\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003egct\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003escan_pScan\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"get_start_pscan\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003eds\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee more to visit \u003ca href=\"https://lizhen0909.github.io/graph_clustering_toolkit/usage/usage.html\" rel=\"nofollow\"\u003eonline usage\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation:\u003c/h3\u003e\n\u003cp\u003ePlease cite \u003ca href=\"https://arxiv.org/abs/2005.04806\" rel=\"nofollow\"\u003eComparison and Benchmark of Graph Clustering Algorithms\u003c/a\u003e for this work.\u003c/p\u003e\n\u003cp\u003eFor individual algorithms, see \u003ca href=\"https://lizhen0909.github.io/graph_clustering_toolkit/usage/pydoc_alg.html\" rel=\"nofollow\"\u003eAlgorithms\u003c/a\u003e for their publications.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1589386210.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.gpu",
      "Singularity",
      "Singularity.devel"
    ],
    "full_name": "lamps24/neural_network_project",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-csci5980\" class=\"anchor\" href=\"#csci5980\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecsci5980\u003c/h1\u003e\n\u003cp\u003eFinal project for CSci 5980: deep learning for automatic music translation.\u003c/p\u003e\n\u003cp\u003eFollow theses steps to install all package dependencies for running the model:\u003c/p\u003e\n\u003cp\u003eWe first install software dependencies for manipulating raw audio (\u003ccode\u003effmpeg\u003c/code\u003e):\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a local software directory\n\u003ccode\u003emkdir ~/software\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the NASM assembler (dependency of ffmpeg):\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software\nwget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2\ntar -xvf nasm-2.14.02.tar.bz2\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e nasm-2.14.02\n./configure --prefix=\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/nasm/\nmake install\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"pl-smi\"\u003e$PATH\u003c/span\u003e:\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/nasm/bin/\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMake sure that NASM assembler installed correctly:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enasm -v\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe output should look something like:\n\u003ccode\u003eNASM version 2.14.02 compiled on Mar 11 2020\u003c/code\u003e\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eInstall ffmpeg:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software\nwget https://ffmpeg.org/releases/ffmpeg-4.2.2.tar.bz2\ntar -xvf ffmpeg-4.2.2.tar.bz2\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e ffmpeg-4.2.2\n./configure --prefix=\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/ffmpeg/\nmake install\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"pl-smi\"\u003e$PATH\u003c/span\u003e:\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/ffmpeg/bin/\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003eMake sure that ffmpeg installed correctly:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003effmpeg -version\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe output should look something like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003effmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\nbuilt with gcc 4.4.7 (GCC) 20120313 (Red Hat 4.4.7-23)\nconfiguration: --prefix=/home/csci5980/piehl008/software/ffmpeg/\nlibavutil      56. 31.100 / 56. 31.100\nlibavcodec     58. 54.100 / 58. 54.100\nlibavformat    58. 29.100 / 58. 29.100\nlibavdevice    58.  8.100 / 58.  8.100\nlibavfilter     7. 57.100 /  7. 57.100\nlibswscale      5.  5.100 /  5.  5.100\nlibswresample   3.  5.100 /  3.  5.100\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eNow, we can make the virtual environment and install python packages.  First, create the virtual environment by running:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ccode\u003econda create --name audio-proj python=3.7\u003c/code\u003e\u003c/p\u003e\n\u003col start=\"7\"\u003e\n\u003cli\u003eNext, install packages by running\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/csci5980\nconda install --name audio-proj --file requirements.txt --channel defaults --channel conda-forge\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e(Note: this can take a while - and you need to say yes to installing everything after it solves the environment)\u003c/p\u003e\n\u003col start=\"8\"\u003e\n\u003cli\u003eTo activate the virtual environment, you can now run \u003ccode\u003esource activate audio-proj\u003c/code\u003e. Note: you should do this to test that you can activate the virtual evironment, but you probably shouldn\u0027t run a lot unless you are submitting jobs to the queue.  If you want to use this virtual environment through the MSI notebooks, check out the tutorial at \u003ca href=\"https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\" rel=\"nofollow\"\u003ehttps://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-adding-the-virtual-environment-to-jupyter-notebooks\" class=\"anchor\" href=\"#adding-the-virtual-environment-to-jupyter-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdding the Virtual Environment to Jupyter Notebooks\u003c/h3\u003e\n\u003cp\u003eNow that we have created the virtual environment, we can add it to the Jupyter notebook kernels so that we can use the virtual environment through MSI\u0027s notebook server. To do this, we have to add the kernel specifications to the known Jupyter kernels for our user:\u003c/p\u003e\n\u003col start=\"9\"\u003e\n\u003cli\u003eIf you haven\u0027t already, activate your virtual environment by running \u003ccode\u003esource activate audio-proj\u003c/code\u003e. Then enter\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ewhich python\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYour output should tell you where the python executable for this virtual environment lives - the output for me displays \u003ccode\u003e~/.conda/envs/audio-proj/bin/python\u003c/code\u003e.  If you see something that looks like \u003ccode\u003e/panfs/roc/msisoft/anaconda/anaconda3-2018.12/bin/python\u003c/code\u003e, go back and make sure that you have the virtual environment active and try again. After you have an ouput that clearly has the name of the virtual environment in the directory path (i.e. contains audio-proj in it), continue to the next step.\u003c/p\u003e\n\u003col start=\"10\"\u003e\n\u003cli\u003eNow, we need to create the kernel configuration. To do this run\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.local/share/jupyter/kernels/audio-proj\nnano \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.local/share/jupyter/kernels/audio-proj/kernel.json\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe nano command will open a very basic text editor that you can navigate with the arrow keys. Enter the following:\u003c/p\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e{\n \"argv\": [\n  \"~/.conda/envs/audio-proj/bin/python\", #replace this with your path from step 9 above! (and delete this comment)\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Audio Project Kernel\",\n \"language\": \"python\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere you replace the first line of the argv array with whatever executable path was output from step 9 above (it likely will be identical to this). To exit the nano text editor, type \u003ccode\u003eCtrl-x \u0026lt;RETURN\u0026gt;\u003c/code\u003e and then type \u003ccode\u003eY \u0026lt;RETURN\u0026gt;\u003c/code\u003e to save the file.\u003c/p\u003e\n\u003col start=\"11\"\u003e\n\u003cli\u003eNow that you have saved the kernel file, you should be able to go to \u003ccode\u003ehttps://notebooks.msi.umn.edu/\u003c/code\u003e and when you click on the \u003ccode\u003eNew\u003c/code\u003e tab to create a new file, you should be able to select \u003ccode\u003eAudio Project Kernel\u003c/code\u003e as an available kernel to run your newly created file in.\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1588946712.0
  },
  {
    "data_format": 2,
    "description": " Molecular graphics systems in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.1.0"
    ],
    "full_name": "OSC/sa_singularity_molgfx",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-molgfx\" class=\"anchor\" href=\"#singularity-molgfx\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Molgfx\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4301\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://github.com/OpenChemistry\"\u003eOpen Chemistry\u003c/a\u003e, Gabedit and Jmol. It was built on top of the base Docker image \u003ca href=\"https://hub.docker.com/_/ubuntu\" rel=\"nofollow\"\u003eubuntu\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003emolgfx.sif\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build molgfx.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from \u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull molgfx.sif shub://OSC/sa_singularity_molgfx\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-find-versions-of-molecular-graphics-systems\" class=\"anchor\" href=\"#find-versions-of-molecular-graphics-systems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFind versions of molecular graphics systems\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity inspect -H molgfx.sif\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-start-avogadro2\" class=\"anchor\" href=\"#start-avogadro2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStart Avogadro2\u003c/h3\u003e\n\u003cp\u003eAvogadro2 is started using the default exec command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e molgfx.sif avogadro2\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1588619360.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for APSIM Classic (https://github.com/APSIMInitiative/APSIMClassic)",
    "filenames": [
      "Singularity",
      "Singularity.7.9-r4047",
      "Singularity.7.10-r49ace54f9c8a670190aef9d8d0fb9d5477bb1534"
    ],
    "full_name": "powerPlant/apsim-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the APSIM Classic version of the Agricultural Production Systems sIMulator\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-maintainer-notes\" class=\"anchor\" href=\"#maintainer-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaintainer Notes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRecipes for APSIM 7.9 use the upstream SVN repository (no longer available)\u003c/li\u003e\n\u003cli\u003ePlease see comments inside the recipes for the reasons why some upstream files are overwritten during the build process\u003c/li\u003e\n\u003cli\u003eThe Cotton Model requires a password, which needs to be obtained by the model owner and placed under \u003ccode\u003efiles/CottonPassword.txt\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1586904956.0
  },
  {
    "data_format": 2,
    "description": "Rnnotator is an automated software pipeline that generates transcript models by de novo assembly of RNA-Seq data without the need for a reference genome.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sghignone/Rnnotator",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rnnotator\" class=\"anchor\" href=\"#rnnotator\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRnnotator\u003c/h1\u003e\n\u003cp\u003eRnnotator is an automated software pipeline that generates transcript models by de novo assembly of RNA-Seq data without the need for a reference genome.\u003c/p\u003e\n\u003cp\u003eRnnotator must be run on a 64-bit Linux architecture. Before running Rnnotator the\nfollowing prerequisites must be installed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBlat v. 34 (\u003ca href=\"http://genome.ucsc.edu/FAQ/FAQblat.html#blat3\" rel=\"nofollow\"\u003ehttp://genome.ucsc.edu/FAQ/FAQblat.html#blat3\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003eVelvet 1.0.15 (\u003ca href=\"http://www.ebi.ac.uk/~zerbino/velvet/\" rel=\"nofollow\"\u003ehttp://www.ebi.ac.uk/~zerbino/velvet/\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003eAMOS (\u003ca href=\"http://sourceforge.net/apps/mediawiki/amos/index.php\" rel=\"nofollow\"\u003ehttp://sourceforge.net/apps/mediawiki/amos/index.php\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003eVmatch 2.0 (\u003ca href=\"http://www.vmatch.de/\" rel=\"nofollow\"\u003ehttp://www.vmatch.de/\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003ebwa 0.5.8c (\u003ca href=\"http://bio-bwa.sourceforge.net/\" rel=\"nofollow\"\u003ehttp://bio-bwa.sourceforge.net/\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003eMUMmer (\u003ca href=\"http://sourceforge.net/projects/mummer/\" rel=\"nofollow\"\u003ehttp://sourceforge.net/projects/mummer/\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003eBioPerl (\u003ca href=\"http://www.bioperl.org\" rel=\"nofollow\"\u003ehttp://www.bioperl.org\u003c/a\u003e) -- base system\u003c/li\u003e\n\u003cli\u003ePerl modules: Parallel::ForkManager, Tree (\u003ca href=\"http://search.cpan.org/\" rel=\"nofollow\"\u003ehttp://search.cpan.org/\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOptional prerequisites are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOases 0.1.18 (\u003ca href=\"http://www.ebi.ac.uk/~zerbino/oases/\" rel=\"nofollow\"\u003ehttp://www.ebi.ac.uk/~zerbino/oases/\u003c/a\u003e) -- DONE\u003c/li\u003e\n\u003cli\u003eBambus 2.33 (\u003ca href=\"http://www.cbcb.umd.edu/software/bambus/\" rel=\"nofollow\"\u003ehttp://www.cbcb.umd.edu/software/bambus/\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSopra 1.0 (\u003ca href=\"mailto:dayarian@physics.rutgers.edu\"\u003edayarian@physics.rutgers.edu\u003c/a\u003e) x1 \u2013 x4 scripts\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003esg\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "pipeline",
      "singularity",
      "singularity-recipe",
      "rnaseq",
      "docker",
      "dockerfile"
    ],
    "updated_at": 1612716290.0
  },
  {
    "data_format": 2,
    "description": "gpu image for folding at home",
    "filenames": [
      "Singularity"
    ],
    "full_name": "slaclab/folding-at-home-gpu",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-folding-at-home-gpu\" class=\"anchor\" href=\"#folding-at-home-gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efolding-at-home-gpu\u003c/h1\u003e\n\u003cp\u003egpu image for folding at home\u003c/p\u003e\n\u003cp\u003esimple merge of nvidia cl image with folding at home v7.5.1 to enable gpu processing.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1584940583.0
  },
  {
    "data_format": 2,
    "description": "Singularity container description for BigStitcher",
    "filenames": [
      "Singularity-BigStitcher"
    ],
    "full_name": "PreibischLab/BigStitcher-Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-bigstitcher-singularity\" class=\"anchor\" href=\"#bigstitcher-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBigStitcher-Singularity\u003c/h1\u003e\n\u003cp\u003eSingularity container description that automatically creates an Uber-JAR of the current BigStitcher version (including all dependencies) using local copy of the Oracle JDK.\u003c/p\u003e\n\u003cp\u003eCan easily be deployed for example on a cluster for parallel resaving.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 11,
    "topics": [],
    "updated_at": 1584624624.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for GroIMP (http://www.grogra.de/software/groimp)",
    "filenames": [
      "Singularity",
      "Singularity.1.6-jre8-cuda",
      "Singularity.1.6-cuda"
    ],
    "full_name": "powerPlant/groimp-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for GroIMP, a 3D-modelling platform\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583894106.0
  },
  {
    "data_format": 2,
    "description": "Docker images",
    "filenames": [
      "images/sc_qc_cluster/Singularity.sc_qc_cluster"
    ],
    "full_name": "letaylor/docker-letaylor",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-docker-letaylor\" class=\"anchor\" href=\"#docker-letaylor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edocker-letaylor\u003c/h1\u003e\n\u003cp\u003eThis repo contains Docker images that are automatically built using Travis CI. It is not designed to scale to many images as each image is updated if any one image changes.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-automatically-push-images-to-docker-hub-using-travis-ci\" class=\"anchor\" href=\"#automatically-push-images-to-docker-hub-using-travis-ci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAutomatically push images to Docker Hub using Travis CI\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-1-edit-config-files\" class=\"anchor\" href=\"#1-edit-config-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Edit config files\u003c/h2\u003e\n\u003cp\u003eEdit the following files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e.travis.yml\u003c/code\u003e : alter \u003ccode\u003e$IMAGE_NAME\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-2-give-travis-ci-access-to-upload-to-docer-hub\" class=\"anchor\" href=\"#2-give-travis-ci-access-to-upload-to-docer-hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Give Travis CI access to upload to Docer Hub\u003c/h2\u003e\n\u003cp\u003eStore both \u003ccode\u003e$DOCKER_PASSWORD\u003c/code\u003e and \u003ccode\u003e$DOCKER_USERNAME\u003c/code\u003e securely in on Travis CI. These are used for authentication.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLogin to the account you want Travis to use to upload on \u003ca href=\"https://hub.docker.com/\" rel=\"nofollow\"\u003ehub.docker.com\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eClick on your username on the top left and go to \u0027Account Settings\u0027.\u003c/li\u003e\n\u003cli\u003eOn the left hand panel, go to \u0027Security\u0027 and enter your password as requested.\u003c/li\u003e\n\u003cli\u003eNow we\u0027ll create an API token. Name it Travis CI.\u003c/li\u003e\n\u003cli\u003eCreate the token and copy it.\u003c/li\u003e\n\u003cli\u003eLogin to your account on \u003ca href=\"https://travis-ci.org\" rel=\"nofollow\"\u003etravis-ci.org\u003c/a\u003e and go to the repository that you want to add this automatic functionality to.\u003c/li\u003e\n\u003cli\u003eOn the right next to \u0027More options\u0027 go to \u0027Settings\u0027 in the hamburger menu.\u003c/li\u003e\n\u003cli\u003eAdd an environment variable with the name \u003ccode\u003eDOCKER_PASSWORD\u003c/code\u003e and give it the value of the API token that you copied from \u003ca href=\"https://hub.docker.com/\" rel=\"nofollow\"\u003ehub.docker.com\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eAdd an environment variable with the name \u003ccode\u003eDOCKER_USERNAME\u003c/code\u003e and give it your \u003ca href=\"https://hub.docker.com/\" rel=\"nofollow\"\u003ehub.docker.com\u003c/a\u003e user name.\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1611328575.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for pinfish (https://github.com/nanoporetech/pinfish)",
    "filenames": [
      "Singularity",
      "Singularity.0.1.0"
    ],
    "full_name": "powerPlant/pinfish-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the pinfish collection of tools helping to make sense of long transcriptomics data (long cDNA reads, direct RNA reads)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583274123.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for Mandalorion-Episode-II (https://github.com/rvolden/Mandalorion-Episode-II)",
    "filenames": [
      "Singularity.6219d58",
      "Singularity"
    ],
    "full_name": "powerPlant/mandalorion-episode-ii-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for Mandalorion Episode II, Attack of the Isoforms\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583274107.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "arezaii/pf_singularity_demo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-parflow-singularity-container-demonstration\" class=\"anchor\" href=\"#parflow-singularity-container-demonstration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParFlow Singularity Container Demonstration\u003c/h1\u003e\n\u003cp\u003eThe Singularity container is built with ParFlow installed as a SCIF-app, providing access to both sequential and parallel\nbuilds of ParFlow. See additional information about \u003ca href=\"https://sylabs.io/guides/3.3/user-guide/definition_files.html?highlight=apps#apps\" rel=\"nofollow\"\u003eApps in Singularity\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eHost OS must have Singularity installed (See \u003ca href=\"https://sylabs.io/guides/3.3/user-guide/installation.html\" rel=\"nofollow\"\u003eInstalling Singularity\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-linux-hosts\" class=\"anchor\" href=\"#linux-hosts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinux Hosts\u003c/h2\u003e\n\u003cp\u003eVerify Singularity is installed with the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity --version\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, see the Quickstart directions below\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-windowsmac-hosts\" class=\"anchor\" href=\"#windowsmac-hosts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWindows/Mac Hosts\u003c/h2\u003e\n\u003cp\u003eFollow the instructions to \u003ca href=\"https://sylabs.io/guides/3.3/user-guide/installation.html#install-on-windows-or-mac\" rel=\"nofollow\"\u003einstall Singularity\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMake sure you are ssh\u0027d into the Vagrant box before beginning the Quickstart steps below\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evagrant ssh\nvagrant@vagrant:~$ singularity --version\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h2\u003e\n\u003cp\u003eSteps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/arezaii/pf_singularity_demo\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ecd to the repository directory\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003ecd pf_singularity_demo\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003erun the shell script to execute tests for Little Washita domain on 1 processor, for 1 timestep\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e./run_test.sh LW 1 1 1 1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-performance-test-cases\" class=\"anchor\" href=\"#running-performance-test-cases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Performance Test Cases\u003c/h2\u003e\n\u003cp\u003eThe shell script run_test.sh facilitates running tests on different domains.\u003c/p\u003e\n\u003cp\u003eUsage:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ./run_test.sh \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edomain\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eP\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eQ\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eR\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eTimeSteps\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhere\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edomain is a test domain defined below\u003c/li\u003e\n\u003cli\u003eP, Q, R are integers defining processor topology in X, Y, Z directions\u003c/li\u003e\n\u003cli\u003eTimesteps is number of timesteps to execute\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-test-domains\" class=\"anchor\" href=\"#test-domains\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest Domains\u003c/h2\u003e\n\u003cp\u003eThere are several test domains for performance analysis contained in the perf_tests folder.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLW - Little Washita\u003c/li\u003e\n\u003cli\u003eclayl - ClayL\u003c/li\u003e\n\u003cli\u003econus_ru - CONUS Clip - Run off\u003c/li\u003e\n\u003cli\u003econus_tfg - CONUS Clip - Terrain Following Grid\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-little-washita\" class=\"anchor\" href=\"#little-washita\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLittle Washita\u003c/h3\u003e\n\u003cp\u003eNatural model of the Little Washita watershed in Oklahoma.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eDomain Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNumber of Cells: 84,050, 41x41x50 (X,Y,Z)\u003c/li\u003e\n\u003cli\u003eHorizontal Resolution: 1km\u003c/li\u003e\n\u003cli\u003eVertical Resolution: 2m\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eTechnical Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCLM enabled with NLDAS Forcings\u003c/li\u003e\n\u003cli\u003eTimestep: 1hr\u003c/li\u003e\n\u003cli\u003eSuburface: Heterogeneous\u003c/li\u003e\n\u003cli\u003eInitial Condition: Pressure file from spin-up\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-clayl\" class=\"anchor\" href=\"#clayl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClayL\u003c/h3\u003e\n\u003cp\u003eSynthetic model with completely flat surface and many thin, vertical layers\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eDomain Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNumber of Cells: 2.4M for 1 core. Scales with processor count, 100Px100Qx240 (X,Y,Z)\u003c/li\u003e\n\u003cli\u003eHorizontal Resolution: 1m\u003c/li\u003e\n\u003cli\u003eVertical Resolution: 0.025m\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eTechnical Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNo CLM, constant simulated rain on top surface @ .0008 mm/hr\u003c/li\u003e\n\u003cli\u003eTimestep 1hr\u003c/li\u003e\n\u003cli\u003eSubsurface: Homogeneous\u003c/li\u003e\n\u003cli\u003eInitial Condition: Dry\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conus-run-off\" class=\"anchor\" href=\"#conus-run-off\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCONUS Run-off\u003c/h3\u003e\n\u003cp\u003eNatural topography with an impervious surface (parking lot simulation)\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eDomain Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNumber of Cells: 1,562,500 1250x1250x1 (X,Y,Z)\u003c/li\u003e\n\u003cli\u003eHorizontal Resolution: 1km\u003c/li\u003e\n\u003cli\u003eVertical Resolution: 0.10m\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eTechnical Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNo CLM, period of 1 hour simulated rain on top surface @ .005 mm/hr, then recession for 1000 hours\u003c/li\u003e\n\u003cli\u003eTimestep: 6 minutes\u003c/li\u003e\n\u003cli\u003eSubsurface: Homogeneous\u003c/li\u003e\n\u003cli\u003eInitial Condition: Dry\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conus-terrain-following-grid\" class=\"anchor\" href=\"#conus-terrain-following-grid\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCONUS Terrain Following Grid\u003c/h3\u003e\n\u003cp\u003eNatural topography with the terrain following grid (TFG) feature enabled\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eDomain Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNumber of Cells: 1,125,000 750x750x2 (X,Y,Z)\u003c/li\u003e\n\u003cli\u003eHorizontal Resolution: 1km\u003c/li\u003e\n\u003cli\u003eVertical Resolution: toplayer=1m, bottomlayer=100m\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eTechnical Details\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNo CLM, seepage face boundary condition type on top layer, @ 0.00001\u003c/li\u003e\n\u003cli\u003eTimestep: 100000\u003c/li\u003e\n\u003cli\u003eSubsurface: Homogeneous\u003c/li\u003e\n\u003cli\u003eInitial Condition: Water Table at 45m above lower boundary\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-apps\" class=\"anchor\" href=\"#about-apps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout Apps\u003c/h2\u003e\n\u003cp\u003eThe demo container has two apps installed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003epar = distributed build of ParFlow, -DPARFLOW_AMPS_SEQUENTIAL_IO=False\u003c/li\u003e\n\u003cli\u003eseq = sequential build of ParFlow, -DPARFLOW_AMPS_SEQUENTIAL_IO=True\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity run --app \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eapp_name\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e/path/to/singularity_container.sif\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003e.tcl input file\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee additional information about \u003ca href=\"https://sylabs.io/guides/3.3/user-guide/definition_files.html?highlight=apps#apps\" rel=\"nofollow\"\u003eApps in Singularity\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-build-container\" class=\"anchor\" href=\"#to-build-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo Build Container\u003c/h2\u003e\n\u003cp\u003eThe quickest way to build is to use a remote build service such as \u003ca href=\"https://cloud.sylabs.io/builder\" rel=\"nofollow\"\u003ecloud.sylabs.io\u003c/a\u003e\nIf a user has root access, they can build from the definition file, conventionally named Singularity.\u003c/p\u003e\n\u003cp\u003eGeneral build command is of the form:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo singularity build \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edestination/path/to/singularity_container.sif\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eSingularity definition file\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eas a specific example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo singularity build \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/pf_singularity_demo.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-use-parflow-in-container\" class=\"anchor\" href=\"#to-use-parflow-in-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo Use ParFlow in Container\u003c/h2\u003e\n\u003cp\u003eExample of running the LW test case in \u003ccode\u003eparflow/test/washita/tcl_scripts\u003c/code\u003e directory\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity run --app par \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/pf_singularity_demo.sif LW_Test.tcl\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pull-from-sylabs-cloud\" class=\"anchor\" href=\"#pull-from-sylabs-cloud\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePull from Sylabs Cloud\u003c/h2\u003e\n\u003cp\u003eTo pull the pre-built image from Sylabs Cloud:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity pull [destination image name] library://arezaii/default/parflow_demo:master\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003eBecause singularity containers are write protected and ParFlow tests write to disk, you must expand the image to a writable sandbox.\nThis requires super user access, similar to building a container from the definition file.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-make-container-writable\" class=\"anchor\" href=\"#make-container-writable\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMake Container Writable\u003c/h3\u003e\n\u003cp\u003eFirst, create a writable sandbox from the immutable container using Singularity\u0027s build command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build --sandbox \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edirectory_to_create_for_sandbox/\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003esingularity_container\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eas an example, if you had pulled the parflow_ompi image from shub:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build --sandbox parflow_demo_master_sandbox/ parflow_demo_master.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThere will now be a new directory parflow_demo_master_sandbox/ that is the root of the container.\nEditing any of the folder contents will require super user permissions.\u003c/p\u003e\n\u003cp\u003eYou can enter a console into the container now by using the Singularity shell command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity shell --writable \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edirectory_to_create_for_sandbox/\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run-tests\" class=\"anchor\" href=\"#run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Tests\u003c/h3\u003e\n\u003cp\u003eAfter making the container writable and accessing it through a shell, both documented above, running the ParFlow\ntests can be done by changing directories and exporting the PARFLOW_DIR environment variable for either distributed\nor sequential builds of ParFlow.\u003c/p\u003e\n\u003cp\u003eTake note of the ParFlow build and install directories in the container:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSequential Build\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebuild directory: /home/parflow/build_seq\u003c/li\u003e\n\u003cli\u003einstall directory: /home/parflow/pfdir_seq\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eDistributed Build\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebuild directory: /home/parflow/build_par\u003c/li\u003e\n\u003cli\u003einstall directory: /home/parflow/pfdir_par\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e /home/parflow/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ebuild_dir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PARFLOW_DIR=/home/parflow/\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003einstall_dir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \n\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e make \u003cspan class=\"pl-c1\"\u003etest\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583512107.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.iqtree",
      "Singularity.ncbi-downloader",
      "Singularity.BUSCO4",
      "Singularity.mashmap",
      "Singularity.snakemake",
      "Singularity.bioconvert",
      "Singularity.metawrap",
      "Singularity.VAMB",
      "Singularity.nanofilt",
      "Singularity.biopython",
      "Singularity.bwa",
      "Singularity.antismash_standalone",
      "Singularity.pysam",
      "Singularity.puntseq",
      "Singularity.eukcc_vanilla",
      "Singularity.METAMVGL",
      "Singularity.CAT_update",
      "Singularity.dbcan",
      "Singularity.metawap_docker",
      "Singularity.minimap2",
      "Singularity.kraken2",
      "Singularity.BUSCO414",
      "Singularity.bbmap",
      "Singularity.trimal",
      "Singularity.krona",
      "Singularity.bioinfo",
      "Singularity.sepp",
      "Singularity.dRep",
      "Singularity.cmseq",
      "Singularity.VAMB_10.1",
      "Singularity.mmseq2",
      "Singularity.mummer",
      "Singularity.BUSCO5",
      "Singularity.repeatmasker",
      "Singularity.ploidyNGS",
      "Singularity.ete3",
      "Singularity.CAT",
      "Singularity.deeptools",
      "Singularity.metaeuk",
      "Singularity.megahit",
      "Singularity.EukRep",
      "Singularity.famsa",
      "Singularity.fastani",
      "Singularity.comparem",
      "Singularity.nQuire",
      "Singularity.qiime2",
      "Singularity.euk_decide",
      "Singularity.pasta",
      "Singularity.sourmash",
      "Singularity.spades",
      "Singularity.raxml-ng",
      "Singularity.metabat2",
      "Singularity.tree",
      "Singularity.spades_3.13",
      "Singularity.bamm",
      "Singularity.mafft",
      "Singularity.mash",
      "Singularity.dRep3",
      "Singularity.art",
      "Singularity.cmseq_conda",
      "Singularity.spades_3.15",
      "Singularity.seqtk",
      "Singularity.VAMP",
      "Singularity.R"
    ],
    "full_name": "hexmek/container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"#singularity-rnaseq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-rnaseq\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-jupyter\" class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Jupyter\u003c/h2\u003e\n\u003cp\u003eRun this to start Jupyter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen follow the instructions that Jupyter printed to the terminal when you started it up to access Jupyter in your web browser\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAccessing Jupyter on a remote server\u003c/h3\u003e\n\u003cp\u003eIf you are running the container on a remote server, you will need to set up port forwarding with ssh to be able to access Jupyter.  Run this command to forward the default Jupyter port (8888)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -L 8888:localhost:8888 bug\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote if the default Jupyter port is not available, Jupyter will choose a different port.  In this case you will need to substitute the port that Jupyter outputs for 8888 in the ssh port forwarding command above.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-a-slurm-cluster\" class=\"anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on a SLURM Cluster\u003c/h2\u003e\n\u003cp\u003eYou can use this image interactively on a SLURM-managed cluster by running launching RStudio or Jupyter. The following instructions work on the Duke Compute Cluster (DCC).  Doing this on other cluster will require some modification and may not work, depending on how the cluster is configured.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRStudio\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003essh to DCC login node: \u003ccode\u003essh NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003erun tmux on login node: \u003ccode\u003etmux new -s container_demo\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun this on login node: \u003ccode\u003esrun -A chsi -p chsi --mem=100G -c 30 --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003ehostname -A\u003c/code\u003e on compute node and record results\u003c/li\u003e\n\u003cli\u003eRun on the following on a compute node and note the port, username, and password that the command prints:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eRun on local machine: \u003ccode\u003essh -L PORT:COMPUTE_HOSTNAME:PORT NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eWhere PORT is the port returned but the \"singularity run\" commmand\u003c/li\u003e\n\u003cli\u003eWhere COMPUTE_HOSTNAME is the hostname returned by running \"hostname -A\" on the compute node\u003c/li\u003e\n\u003cli\u003eWhere NETID is your NetID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGo to \"localhost:PORT\" in a webrowser and enter the username and password printed by the \"singularity run\" commmand\u003c/li\u003e\n\u003cli\u003eHave fun!!\u003c/li\u003e\n\u003cli\u003eAt the end of an analysis you will probably want to copy results to your directory in \u003ccode\u003e/work\u003c/code\u003e or \u003ccode\u003e/hpc/group\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003essh to dcc-login-01.rc.duke.edu\u003c/li\u003e\n\u003cli\u003erun tmux on login node: \u003ccode\u003etmux new -s container_demo\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun this on login node: \u003ccode\u003esrun -A chsi -p chsi --mem=100G -c 30 --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun on compute node:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eRun on local machine: \u003ccode\u003essh -L PORT:COMPUTE_HOSTNAME:PORT NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eWhere PORT is the number after \u003ccode\u003ehttp://127.0.0.1:\u003c/code\u003e in the URL given by Jupyter (defaults to 8888, but Jupyter will use a different one if the default is in use, or if a different port is supplied as an argument using \u003ccode\u003e--port\u003c/code\u003e when running the singularity container\u003c/li\u003e\n\u003cli\u003eWhere COMPUTE_HOSTNAME is the hostname returned by running \"hostname -A\" on the compute node\u003c/li\u003e\n\u003cli\u003eWhere NETID is your NetID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCopy the URL supplied by jupyter that starts \u003ccode\u003ehttp://127.0.0.1:\u003c/code\u003e and paste it in a webbrowser\u003c/li\u003e\n\u003cli\u003eHave fun!!\u003c/li\u003e\n\u003cli\u003eAt the end of an analysis you will probably want to copy results to your directory in \u003ccode\u003e/work\u003c/code\u003e or \u003ccode\u003e/hpc/group\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter-on-gpu-node\" class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter on GPU node\u003c/h3\u003e\n\u003cp\u003eSame as above, but the srun command should use the \u003ccode\u003echsi-gpu\u003c/code\u003e partition and request a gpu, but less CPUs and Memory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esrun -A chsi -p chsi-gpu --gres=gpu:1 --mem=15866 -c 2 --pty bash -i\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1619700602.0
  },
  {
    "data_format": 2,
    "description": "start with raw plink, end with standardized QCed plink",
    "filenames": [
      "workflow/Singularity_defs.def"
    ],
    "full_name": "pmonnahan/DataPrep",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pre-imputation-qc-pipeline\" class=\"anchor\" href=\"#pre-imputation-qc-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePre-imputation QC pipeline\u003c/h1\u003e\n\u003cp\u003eThe purpose of this pipeline is to perform the following for a set of input PLINK datasets:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebasic QC (genotype/variant missingness, HWE, and minor allele frequency)\u003c/li\u003e\n\u003cli\u003eharmonize allele specifications with the GRCh37 reference genome\u003c/li\u003e\n\u003cli\u003eproduce a set of VCF files (separated by chromosome) for imputation\u003c/li\u003e\n\u003cli\u003emerge filtered datasets into a single dataset consisting only of overlapping sites.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA companion pipeline, which performs post-imputation QC, will download alongside the pre-imputation pipeline.  To use the post-imputation pipeline, see the README in the postImpute directory.\u003c/p\u003e\n\n\n\u003cp\u003e\u003cstrong\u003eTable of Contents\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#requirements\"\u003eRequirements\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#snakemake\"\u003eSnakemake\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#singularity\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-the-workflow\"\u003eRunning the workflow\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#other-notes\"\u003eOther Notes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#debugging-and-error-reports\"\u003eDebugging and error reports\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#pipeline-overview\"\u003ePipeline Overview\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#input-data\"\u003eInput Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#output\"\u003eOutput\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#dataset-harmonization\"\u003eData Harmonization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reference-allele-fixing\"\u003eReference allele fixing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#basic-qc\"\u003eBasic QC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#merging-inputs-optional\"\u003eMerging Inputs (Optional)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#imputaton-preparation\"\u003eImputation Preparation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/pmonnahan/DataPrep/blob/master/Pipeline_DAG.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pmonnahan/DataPrep/raw/master/Pipeline_DAG.png\" alt=\"Pipeline DAG\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-snakemake\" class=\"anchor\" href=\"#snakemake\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSnakemake\u003c/h3\u003e\n\u003cp\u003eThe pipeline is coordinated and run on an HPC (or locally) using \u003cem\u003eSnakemake\u003c/em\u003e.  To install snakemake, first create a virtual environment via:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load python3/3.6.3_anaconda5.0.1\nconda install -c conda-forge mamba\nmamba create -c conda-forge -c bioconda -n \u0026lt;your_environment_name\u0026gt; snakemake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will create a new virtual environment and install \u003ccode\u003esnakemake\u003c/code\u003e.  Then, activate this environment and perform following installations:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate \u0026lt;your_environment_name\u0026gt;\nconda install numpy yaml pandas\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnytime you need to run the pipeline, activate this environment beforehand via:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate \u0026lt;environment_name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you choose not to create an environment, you must ensure that these packages are installed and available for your python installation.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h3\u003e\n\u003cp\u003eThe installation of the individual programs  used throughout this pipeline can be completely avoid by utilizing a Singularity image.  This image is too large to be hosted on Github, although you can find the definitions file used to create the image \u003ca href=\"https://github.com/pmonnahan/AncInf/blob/master/singularity/Singularity_defs.def\"\u003ehere\u003c/a\u003e.  Building of images is still not currently supported at MSI, so I used a Vagrant virtual machine, which comes with Singularity pre-configured/installed (\u003ca href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\" rel=\"nofollow\"\u003ehttps://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\u003c/a\u003e).  I can also share the img file directly upon request.\u003c/p\u003e\n\u003cp\u003eHowever, in order to utilize the singularity image, \u003cem\u003eSingularity\u003c/em\u003e must be installed on the HPC.  Currently, the pipeline assumes that \u003cem\u003eSingularity\u003c/em\u003e will be available as a module and can be loaded into the environment via the command specified in the config.yml file, where it says \u0027singularity_module\u0027.  The default setting will work for MSI at UMN.\u003c/p\u003e\n\u003cp\u003eSingularity settings in config.yml\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity:\n  use_singularity: \u0027true\u0027\n  image: \u0027/home/pmonnaha/pmonnaha/singularity/AncestryInference.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-workflow\" class=\"anchor\" href=\"#running-the-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the workflow\u003c/h2\u003e\n\u003cp\u003eFirst, activate the virtual environment into which snakemake was installed:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate \u0026lt;environment_name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eClone the parent repository to the location where you want to store the output of the pipeline.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/pmonnahan/DataPrep.git preImputeQC\ncd preImputeQC\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe critical files responsible for executing the pipeline are contained in the \u003cem\u003e./workflow\u003c/em\u003e subdirectory contained within the cloned repo.  They are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSnakefile\u003c/li\u003e\n\u003cli\u003econfig.yml\u003c/li\u003e\n\u003cli\u003ecluster.yaml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cem\u003eSnakefile\u003c/em\u003e is the primary workhouse of snakemake, which specifies the dependencies of various parts of the pipeline and coordinates execution.  No modifications to the \u003cem\u003eSnakefile\u003c/em\u003e are necessary.\u003c/p\u003e\n\u003cp\u003eIn order for the \u003cem\u003eSnakefile\u003c/em\u003e to locate all of the necessary input and correctly submit jobs to the cluster, \u003cstrong\u003eboth\u003c/strong\u003e the \u003cem\u003econfig.yaml\u003c/em\u003e and \u003cem\u003ecluster.yaml\u003c/em\u003e need to be modified. Open these files and change the required entries that are indicated with \u0027MODIFY\u0027.  Other fields do not require modification, although this may be desired given the particulars of the run you wish to implement.  Details on each entry in the config file (e.g. what the program expects in each entry as well as the purpose of the entry) are provided in the \u003cem\u003ePipeline Overview\u003c/em\u003e at the bottom.  Note: Only use letters and numbers when naming output files or datasets as this may cause issues with the report creation.\u003c/p\u003e\n\u003cp\u003eThe entire pipeline can be executed on a local machine (not recommended) or on an HPC, and the \u003cem\u003ecluster.yaml\u003c/em\u003e file is required only for the latter.  For a local run, change the \u003ccode\u003elocal_run\u003c/code\u003e entry to \u003ccode\u003etrue\u003c/code\u003e under the \u003ccode\u003erun_settings\u003c/code\u003e section of the config file, and launch snakemake from within the parent directory by the simple command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHowever, multiple steps in the pipeline have high resource demands, and so are unlikely to be able to be run locally.  This option exists primarily for testing and troubleshooting, so the remainder of the  documentation assumes that the pipeline will be executed on an HPC.  In order to coordinate the use of the HPC, the following modifications to the snakemake command are required:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --cluster \"sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type} -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml -j 32\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere -j specifies the number of jobs that can be submitted at once.\u003c/p\u003e\n\u003cp\u003eOne additional setting in the \u003cem\u003econfig.yml\u003c/em\u003e is needed in order to correctly submit jobs to the HPC.  The relevant entries are under the \u003ccode\u003erun_settings\u003c/code\u003e section of the config file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_settings:\n  local_run: \u0027false\u0027\n  cluster_config: \u0027workflow/cluster_slurm.yaml\u0027\n  scheduler: \u0027slurm\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, it is necessary that the \u003ccode\u003ecluster_config\u003c/code\u003e entry is set to the path of the cluster_slurm.yaml file that will be used in the snakemake command.  Also, the scheduler must correspond to the syntax used in the snakemake command and cluster.yaml file.  I should point out that these additional changes are needed for responsibly using PLINK within a snakemake framework, and are not directly needed for snakemake.  PLINK will attempt to auto-detect available resources upon running regardless of the resources that were requested when the job was submitted.  Therefore, we have to read and parse the requested resources in the cluster config file in order for them to be communicated to PLINK from within the Snakefile.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-other-notes\" class=\"anchor\" href=\"#other-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOther notes\u003c/h3\u003e\n\u003cp\u003eIt is recommended that \u003cem\u003esnakemake\u003c/em\u003e is run as an interactive session on an HPC.  \u003cem\u003eSnakemake\u003c/em\u003e will launch the specified number (via the -j flag) of jobs, and then will hang and wait for them to finish.  As jobs finish (and assuming no errors), \u003cem\u003esnakemake\u003c/em\u003e will launch additional jobs keeping the total running jobs at whatever -j is set for.  Although \u003cem\u003esnakemake\u003c/em\u003e should not use a lot of memory, it could have long run times, which is generally not advisable on login nodes.\u003c/p\u003e\n\u003cp\u003eOne attractive feature of \u003cem\u003esnakemake\u003c/em\u003e is its ability to keep track of the progress and dependencies of the different stages of the pipeline.  Specifically, if an error is encountered or the pipeline otherwise stops before the final step, \u003cem\u003esnakemake\u003c/em\u003e can resume the pipeline where it left off, avoiding redundant computation for previously completed tasks.  To do so, simply resubmit the original \u003cem\u003esnakemake\u003c/em\u003e command.\u003c/p\u003e\n\u003cp\u003eTo run a specific part of the pipeline, do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake -R \u0026lt;rule_name\u0026gt; --cluster \"sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type} -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml -j 20 --rerun-incomplete\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003cem\u003erule_name\u003c/em\u003e indicates the \u0027rule\u0027 (i.e. job) in the Snakefile that you wish to run.  Or, you can request a specific file by providing the filename at the end of the command.  You may need to include the -F (i.e. force) if the output file already exists and you want to overwrite it.\u003c/p\u003e\n\u003cp\u003eAlso, it is often very helpful to do a \u0027dry-run\u0027 of the pipeline in which the different steps and dependencies are printed to screen, but no actual jobs are executed.  This can be helpful to ensure that config entries are correct, etc.  To perform a dry-run, do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake -nrp\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: It is convenient to make an alias in your ~/.bashrc file to run snakemake on the cluster without having to type the --cluster... part of the command every time.  For me, it looked like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ealias snakeslurm=\"snakemake -k --cluster \u0027sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --job-name={cluster.job-name} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type} -o {cluster.o} -e {cluster.e} -A {cluster.A}\u0027 --cluster-config workflow/cluster_slurm.yaml\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis way, I can just do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakeslurm -j 25\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo launch snakemake on the cluster.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-debugging-and-error-reports\" class=\"anchor\" href=\"#debugging-and-error-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDebugging and error reports\u003c/h4\u003e\n\u003cp\u003eShould an error be encountered in a job, snakemake will halt the pipeline and indicate in the terminal that an error has occurred.  The offending job will also be printed in red in the terminal window.  More information on why the job failed can be found in the \u0027stdout\u0027 and \u0027stderr\u0027 files that are output to the \u003cem\u003e\u0027OandE\u0027\u003c/em\u003e directory and will be labelled with the jobname.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-overview\" class=\"anchor\" href=\"#pipeline-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Overview\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-input-data\" class=\"anchor\" href=\"#input-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput Data\u003c/h3\u003e\n\u003cp\u003eUnder the \u0027query\u0027 section, you can specify the inputs for one or more datasets.  Each dataset should be uniquely named (Note: avoid using periods or underscores when naming output files or datasets as this may cause issues with the report creation.) with values specified for the following \"keys\":\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003edata\u003c/strong\u003e: path to the PLINK files (just the PLINK prefix).\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003echrom_key\u003c/strong\u003e: tab-delimited text file with 2 columns (no header).  The first column contains the old chromosome names, and the second column contains the new names.\n\u003cul\u003e\n\u003cli\u003eUsed for converting to numeric names. e.g chr10 to 10.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eallele_key\u003c/strong\u003e: tab-delimited text file with 5 columns (no header).  First column is snpID and following columns are: old_allele1 old_allele2 new_allele1 new_allele2.\n\u003cul\u003e\n\u003cli\u003eUsed for converting alleles with A/B specification to ACGT.  Oftentimes provided in the dbGaP download.  If alleles are already specified in ACGT format, this field can be set to \u0027none\u0027\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eID_key\u003c/strong\u003e: tab-delimited text file with 2 columns (no header).  First column is old SNP ID and second column is new SNP ID.\n\u003cul\u003e\n\u003cli\u003eUsed for converting to rsID format.  If SNP IDs are already in rs-format, this field can be set to \u0027none\u0027\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eflip_key\u003c/strong\u003e: text file with single column containing SNP rsIDs that need to be flipped in order to align strand to the hg19 reference genome.\n\u003cul\u003e\n\u003cli\u003eUsed to harmonize strand across datasets to the hg19 reference genome.  Set this field to \u0027none\u0027 if all alleles are already on the same strand as the target reference genome.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEach of these fields are optional and providing \u0027none\u0027 as the entry will disable the steps associated with each key.  However, these fields should only be set to \u0027none\u0027 if you are sure that they are not necessary (e.g. you have already fixed any existing strand issues across datasets).\u003c/p\u003e\n\u003cp\u003eExample of input specifications in the config file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003equery:\n  \"dataset1\":\n    data: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    chrom_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    allele_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    ID_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n    flip_key: \"PATH/TO/PLINK/PREFIX/FOR/DATASET1\"\n  \"dataset2\":\n    data: \"PATH/TO/PLINK/PREFIX/FOR/DATASET2\"\n    chrom_key: \"none\"\n    allele_key: \"none\"\n    ID_key: \"none\"\n    flip_key: \"none\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePhenotypes of the samples must be specified by a tab-delimited text file where the first column contains the sample IDs (as they appear in the imputed VCF file) and the second column contains the phenotype. The path to this file can be provided in the field labelled \u0027phenotype_file\u0027 under the \u0027phenotype_data\u0027 field in the config.yml file.\u003c/p\u003e\n\u003cp\u003eSex of the samples must also be specified in a tab-delimited text file where the first column is sample ID and the second column is the sex specification according to PLINK.  The path to this file can be provided in the field labelled \u0027sex_file\u0027 under the \u0027phenotype_data\u0027 field in the config.yml file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ephenotype_data: \n  pheno_file: \"none\"\n  sex_file: \"/path/to/sex/file\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h3\u003e\n\u003cp\u003eThe output is a set of PLINK files in the parent directory labelled with the value provided in the \u0027outname\u0027 entry of the config file.  However, if \u0027merge\u0027 is set to \u0027false\u0027 in the config file, this final merge step is skipped, and the final output would be the set of QC\u0027ed plink files within each subdirectory labelled with the dataset names.  Within each of these subdirectories, there will also be a set of VCF files, which are suitable for use in either the Michigan or TOPMed imputation servers.\u003c/p\u003e\n\u003cp\u003eThe other primary output is a PDF report containing a summary of various steps in the pipeline.  It is \u003cstrong\u003ehighly recommended\u003c/strong\u003e that the user carefully review this report to confirm that everything seems in order.  Particular attention should be paid to whether specific steps have resulted in major loss of markers as well as whether there is a positive correlation between allele frequencies in the 1000Genomes dataset and allele frequencies in each of the query datasets.  These scatter plots are provided towards the end of the report, and if a substantial subset of the points exhibit an anti-correlation, this is indicative of a preponderance of strand errors that ought to be corrected (via the \u0027flip_key\u0027) prior to proceeding.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dataset-harmonization\" class=\"anchor\" href=\"#dataset-harmonization\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDataset harmonization\u003c/h3\u003e\n\u003cp\u003eThe first step(s) in the pipeline aims to harmonize the naming of chromosomes, alleles, and variant IDs.  This is accomplished via the 4 keys described above.  While this pipeline generally attempts to simplify the QC process, it is extremely important that the user is acquainted well enough with each individual dataset to ensure that the appropriate keys are specified (or not specified).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-reference-allele-fixing\" class=\"anchor\" href=\"#reference-allele-fixing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference allele fixing\u003c/h3\u003e\n\u003cp\u003eIn contrast to a VCF, where alleles are specified with respect to a specified reference genome (reference versus alternative alleles), PLINK-formatted files often specify alleles as major/minor alleles based on the frequency in the dataset.  Furthermore, many commonly used arrays will contain a mixture of SNPs genotyped on either the + or - strand.  Lastly, the default behavior of PLINK is to automatically set the minor to A1 and the major allele to A2, which can unintentionally generate inconsistencies in allele specifications across datasets.\u003c/p\u003e\n\u003cp\u003eWith respect to a reference genome, two possible types of errors can occur:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFlipped strand:  The genotype is specified with respect to the opposite strand relative to the reference genome.\u003c/li\u003e\n\u003cli\u003eSwapped allele:  The genotype is specified on the same strand as the reference genome, but the A1 (minor) allele has been set to equal the \u0027reference\u0027 allele when it ought to be set to equal the non-reference/\u0027alternative\u0027 allele\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo identify these errors, we use the bcftools plugin \u0027+fixref\u0027, which requires not only the reference sequence (fasta) file, but also a VCF file containing variant sites that are used to identify mismatching alleles in the query dataset.  Importantly, if the program determines that no strand issues exist and that the reference/alternative alleles have simply been swapped, then program will swap the major/minor alleles to match the reference.  It will not perform any strand flipping, where it converts genotypes to be specified with respect to the nucleotide on the opposite strand.  Although the program will attempt to identify these strand flips, it doesn\u0027t make the correction as the authors consider this a risky move that should not be handled in an automated fashion.  Thus, flip-strand mismatches are ultimately removed.  If there are a large number of these, the user should attempt to understand and resolve the source of the issue and rerun this pipeline.\u003c/p\u003e\n\u003cp\u003eBy default, the pipeline will download the following files for the hg19 reference genome:\u003c/p\u003e\n\u003cp\u003eReference fasta:\n\u003ca href=\"ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz\" rel=\"nofollow\"\u003eftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/reference/human_g1k_v37.fasta.gz\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eReference VCF (1000Genomes):\n\u003ca href=\"ftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz\" rel=\"nofollow\"\u003eftp://ftp.ncbi.nih.gov/snp/organisms/human_9606_b150_GRCh37p13/VCF/All_20170710.vcf.gz\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn indication of whether alleles are now specified correctly is to plot frequency of an allele in the query population against the frequency in the reference population and look for an obviously positive correlation.  Such plots are automatically produced in the PDF report as the final step in the pipeline.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-basic-qc\" class=\"anchor\" href=\"#basic-qc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBasic QC\u003c/h3\u003e\n\u003cp\u003eAfter alleles have been fixed as described above, a series of basic QC steps are performed on each dataset by the script \u003cem\u003e\u0027scripts/QC.py\u0027\u003c/em\u003e, with the filtering thresholds specified in the config file (see below).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eperform_QC: \u0027true\u0027\nQC:\n  vm1: \"0.2\" # Initial variant missingness filter\n  gm: \"0.1\" # Individual missingness filter\n  vm2: \"0.05\"  # Ultimate call rate for variants after removing low-callrate samples\n  maf: \"0.01\"  # mimimum Minor allele frequency\n  hwe: \"0.0000001\"  # p-value threshold for whether site follows hardy-weinberg\n  mbs: \"0.0000001\"  # p-value treshold for test of whether missingness varies by sex\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe first wish to identify and remove individual samples that show high missingess across markers (specified by \u0027gm\u0027).  However, to identify these individuals, we first need to remove variants that imputed poorly across all individuals (specified by \u0027vm1\u0027).  After removing these individuals, we then remove variants with high missingness (specified by \u0027vm2\u0027).  Since poor imputation will result in missing genotypes, this missingness filter indirectly filters for low quality imputation sites.  Variants are also filtered based whether or not they show significant departures from Hardy-Weinberg Equilibrium (\u0027hwe\u0027 entry) and whether there is a significant association between missingness and sex (\u0027mbs\u0027 entry).  We also remove rare variants based on the \u0027maf\u0027 value.  Lastly, we remove indels, duplicate SNPs, and multi-allelic variants.\u003c/p\u003e\n\u003cp\u003eNote that testing for missigness by case/control status is generally recommended as well if the user wishes to proceed straight to SNP-based analyses such as GWAS.  However, if the data is to be used for ancestry inference, it may make more sense to retain these SNPs.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-merging-inputs-optional\" class=\"anchor\" href=\"#merging-inputs-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMerging inputs (Optional)\u003c/h3\u003e\n\u003cp\u003eIf multiple input datasets were provided, an optional final step is to create a single merged dataset consisting of only the sites that overlap (i.e. passed filters) across all component datasets.  This behavior is controlled by the \u0027merge\u0027 entry in the config file.  To enable the merging behavior, set this to:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emerge: \u0027true\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-imputaton-preparation\" class=\"anchor\" href=\"#imputaton-preparation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImputaton preparation\u003c/h3\u003e\n\u003cp\u003eAnother optional, final feature is to create a set of of VCF files (parsed by chromosome) for each of the input datasets.  These VCFs can be used directly as input into either the Michigan Imputation Server or the TOPMed Imputation Server.  The output of the imputation servers can then be used as input into the post-imputation QC pipeline (see README.md in the \u0027postImpute\u0027 directory).\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617574369.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for spades (git@github:powerPlant/spades-srf.git)",
    "filenames": [
      "Singularity.v3.13.0",
      "Singularity.v3.11.0",
      "Singularity.v3.10.0",
      "Singularity.cami2-submission",
      "Singularity.v3.8.2",
      "Singularity.template",
      "Singularity.spaligner-paper",
      "Singularity.v3.12.0",
      "Singularity.metaplasmid-paper",
      "Singularity.v3.11.1",
      "Singularity.v3.8.0",
      "Singularity.v3.9.0",
      "Singularity.v3.8.1",
      "Singularity.v3.13.1",
      "Singularity.v3.14.0",
      "Singularity.v3.10.1",
      "Singularity.cloudspades-paper",
      "Singularity.v0.5-recomb",
      "templates/Singularity.template"
    ],
    "full_name": "powerPlant/spades-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for spades\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1580700253.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for getorganelle (https://github.com/Kinggerm/GetOrganelle)",
    "filenames": [
      "Singularity",
      "Singularity.v1.6.2e"
    ],
    "full_name": "powerPlant/getorganelle-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the GetOrganelle toolkit to assembly organelle genomes from genome skimming data\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1579837325.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for hapcol (https://github.com/AlgoLab/HapCol)",
    "filenames": [
      "Singularity.97d4a5e",
      "Singularity"
    ],
    "full_name": "powerPlant/hapcol-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the HapCol tool, a fast and memory-efficient method for haplotype assembly from long gapless reads\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1579837367.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for sga (https://github.com/jts/sga)",
    "filenames": [
      "Singularity",
      "Singularity.0.10.15"
    ],
    "full_name": "powerPlant/sga-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3984\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the SGA tool, a de novo genome assembler based on the concept of string graphs\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1579231330.0
  },
  {
    "data_format": 2,
    "description": "WineHQ in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.5.0.0",
      "Singularity.4.0.3"
    ],
    "full_name": "OSC/sa_singularity_winehq",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-winehq\" class=\"anchor\" href=\"#singularity-winehq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity WineHQ\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3891\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://www.winehq.org/\" rel=\"nofollow\"\u003eWineHQ\u003c/a\u003e. It was built on top of the base Docker image \u003ca href=\"https://hub.docker.com/_/ubuntu\" rel=\"nofollow\"\u003eubuntu\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003ewinehq.sif\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build winehq.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from \u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull winehq.sif shub://OSC/sa_singularity_winehq\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run-64-bit-windows-binary\" class=\"anchor\" href=\"#run-64-bit-windows-binary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun 64-bit Windows binary\u003c/h3\u003e\n\u003cp\u003eWineHQ is started using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run winehq.sif /path/to/windows_64bit_exe\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as a native command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./winehq.sif /path/to/windows_64bit_exe\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run-32-bit-windows-binary\" class=\"anchor\" href=\"#run-32-bit-windows-binary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun 32-bit Windows binary\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e winehq.sif wine /path/to/windows_32bit_exe\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1581361908.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for MrBayes (http://nbisweden.github.io/MrBayes)",
    "filenames": [
      "Singularity",
      "Singularity.3.2.7a-gpu",
      "Singularity.3.2.7a"
    ],
    "full_name": "powerPlant/mrbayes-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3808\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the MrBayes program for Bayesian inference and model choice across a wide range of phylogenetic and evolutionary models\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1574325488.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.0.1.0"
    ],
    "full_name": "arcsUVA/cryoCARE",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-images\" class=\"anchor\" href=\"#singularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-images\u003c/h1\u003e\n\u003cp\u003eSetups for various images used on the dgx.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1574198978.0
  },
  {
    "data_format": 2,
    "description": "Setups for various images used on the dgx.",
    "filenames": [
      "Singularity-PyTorch"
    ],
    "full_name": "uri-ai-lab/singularity-images",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-images\" class=\"anchor\" href=\"#singularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-images\u003c/h1\u003e\n\u003cp\u003eSetups for various images used on the dgx.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1573772605.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "CN-Healthborn/el7tf1.12gpu",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nova-el7-tensorflow-gpu\" class=\"anchor\" href=\"#nova-el7-tensorflow-gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enova-el7-tensorflow-gpu\u003c/h1\u003e\n\u003cp\u003eConfigurations for docker and singularity for making OSG-compatible CENTOS7 container with GPU-accelerated tensorflow and keras installed.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1603475388.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for https://github.com/revbayes/revbayes",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ResearchIT/revbayes-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3722\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the PLINK association analysis toolset\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1589324901.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for plink2 (https://www.cog-genomics.org/plink/2.0/)",
    "filenames": [
      "Singularity.v2.00a2LM",
      "Singularity"
    ],
    "full_name": "powerPlant/plink2-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3722\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the PLINK association analysis toolset\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1572401216.0
  },
  {
    "data_format": 2,
    "description": "LSHVec pre-trained models and its Python bindings",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "Lizhen0909/PyLSHvec",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lshvec-pre-trained-models-and-its-python-bindings\" class=\"anchor\" href=\"#lshvec-pre-trained-models-and-its-python-bindings\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLSHVec pre-trained models and its Python bindings\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSummary\u003c/h2\u003e\n\u003cp\u003eThis repository presents a few of pre-tained models with JLSHVec (which is a rewritten java version of  \u003ca href=\"https://github.com/Lizhen0909/LSHVec\"\u003eLSHVec\u003c/a\u003e).  See \u003ca href=\"#remark\"\u003eRemark\u003c/a\u003e for technical details.\u003c/p\u003e\n\u003cp\u003ePython codes and examples to uses these models are also provided.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003ePython 3.6\u003c/li\u003e\n\u003cli\u003ecython\u0026gt;=0.28.5\u003c/li\u003e\n\u003cli\u003eJnius \u0026gt;=1.1.0\u003c/li\u003e\n\u003cli\u003ejava \u0026gt;=1.8\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build-from-source\" class=\"anchor\" href=\"#build-from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebuild from source\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/Lizhen0909/PyLSHvec.git \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e PyLSHvec \u003cspan class=\"pl-k\"\u003e\u0026amp;\u0026amp;\u003c/span\u003e python setup.py install\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-or-use-pip\" class=\"anchor\" href=\"#or-use-pip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eor use pip\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip install pylshvec\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-or-use-docker\" class=\"anchor\" href=\"#or-use-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eor use docker\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker pull lizhen0909/pylshvec\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-or-use-singularity-3\" class=\"anchor\" href=\"#or-use-singularity-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eor use singularity 3\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name pylshvec.sif shub://Lizhen0909/PyLSHvec\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to use\u003c/h2\u003e\n\u003cp\u003ePut things simply, just\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003epylshvec\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e#here needs jlshvec jar file, download it first\u003c/span\u003e\n\u003cspan class=\"pl-en\"\u003eset_lshvec_jar_path\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"/mnt/jlshvec-assembly-0.1.jar\"\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e#since vector model is usually large, set a big java memory limit is preferred. \u003c/span\u003e\n\u003cspan class=\"pl-en\"\u003eadd_java_options\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"-Xmx32G\"\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e#here need model file and lsh function file, download them first\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e#use help(model) to see all the methods and constructor options \u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003emodel\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eLSHVec\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003emodel_file\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"/mnt/refdb_viruses_model_gs_k23_l3000_rand_model_299\"\u003c/span\u003e, \n              \u003cspan class=\"pl-s1\"\u003ehash_file\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"/mnt/lsh_nt_NonEukaryota_k23_h25.crp\"\u003c/span\u003e)\n\n\u003cspan class=\"pl-s1\"\u003ereads\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e [\u003cspan class=\"pl-s\"\u003e\u0027ACGTACGT.....\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027ACGTACGT.....\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027ACGTACGT.....\u0027\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\u0027ACGTACGT.....\u0027\u003c/span\u003e, ....]\n\n\u003cspan class=\"pl-s1\"\u003epredicts\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003emodel\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003epredict\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ereads\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor more complete examples please see the notebooks (see \u003ca href=\"#download\"\u003eDownload\u003c/a\u003e for minimum memory requirement):\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"notebook/example_use_virus_classfication_model.ipynb\"\u003eexample_use_virus_classfication_model.ipynb\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"notebook/example_use_bacteria_classfication_model.ipynb\"\u003eexample_use_bacteria_classfication_model.ipynb\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"notebook/example_use_vectors_in_bacteria_classfication_model.ipynb\"\u003eexample_use_vectors_in_bacteria_classfication_model.ipynb\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"notebook/example_use_Illumina_bacteria_classfication_model.ipynb\"\u003eexample_use_Illumina_bacteria_classfication_model.ipynb\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"notebook/example_use_Pacbio_bacteria_classfication_model.ipynb\"\u003eexample_use_Pacbio_bacteria_classfication_model.ipynb\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-use-docker\" class=\"anchor\" href=\"#use-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse Docker\u003c/h3\u003e\n\u003cp\u003eAssume you put your data in /mnt/data and your notebook in /mnt/notebook.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erun python or ipython\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run -v /mnt/data:/data -it lizhen0909/pylshvec python \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003eor ipython\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003erun Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run -v /mnt/data:/data -v /mnt/notebook:/notebook -p 8888:8888  -it lizhen0909/pylshvec jupyter_notebook\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFind connection url in the console output.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-use-singularity\" class=\"anchor\" href=\"#use-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse Singularity\u003c/h3\u003e\n\u003cp\u003eSince singularity maps the $HOME directory, here just assumes data/model are going to locate in $HOME. Otherwise, you need map the directories like docker.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003erun python or ipython\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run pylshvec.sif python \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003ethe nrun any pylshvec code \u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003erun Jupyter notebook\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003eIt should work, however singularity maps too many things that host settings may affect the notebook\u003c/span\u003e\nsingularity run  --bind \u003cspan class=\"pl-smi\"\u003e$HOME\u003c/span\u003e/notebook:/notebook pylshvec.sif jupyter_notebook \u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jlshvec-jar-file\" class=\"anchor\" href=\"#jlshvec-jar-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJLSHVec jar file\u003c/h3\u003e\n\u003cp\u003eThe pre-trained models were trained with a rewritten  \u003ca href=\"https://github.com/Lizhen0909/LSHVec\"\u003eLSHVec\u003c/a\u003e in java.\nThe assembly jar file is needed to load the models.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.amazon.com/clouddrive/share/4NiogpuW1lzBMyGmMlkrDbjhSMYpQgWjW5GUcKFR7Q6\" rel=\"nofollow\"\u003eDownload jlshvec-assembly-0.1.jar\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e: aeb207b983b3adc27e14fd9c431e2130\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pre-trained-models\" class=\"anchor\" href=\"#pre-trained-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePre-trained models\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eBe Warned\u003c/strong\u003e that like all the machine learning models, the model cannot preform better beyond the data. If your data is significant other than the pre-trained model data, training your own model is preferred.\u003c/p\u003e\n\u003cp\u003eHere are issues I can think of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSome NCBI taxonomy id may never be predicted since not all ids have train data.\u003c/li\u003e\n\u003cli\u003eData is not balanced. Some ids (e.g. a specified species) have much more data than others, which makes prediction may prefer to the rich-data ids.\u003c/li\u003e\n\u003cli\u003eStrain (even some species) prediction is terrible. Don\u0027t expect it.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-refdb-viruses-classfication-model\" class=\"anchor\" href=\"#refdb-viruses-classfication-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRefDB viruses classfication model\u003c/h4\u003e\n\u003cp\u003eTrainned with 9.3k viruses assemblies of RefDB. Minimum Java memory: 16G.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003emodel file: \u003ca href=\"https://www.amazon.com/clouddrive/share/RmoJ1lduzlqstAJFnKg0aAlx82AyCjnzKncfGjQIQMg\" rel=\"nofollow\"\u003erefdb_viruses_model_gs_k23_l3000_rand_model_299\u003c/a\u003e [size: 5.3G]\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e 2502b284b336734300c2297d23d1d349\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ehash function file: \u003ca href=\"https://www.amazon.com/clouddrive/share/6ZNvMXMy30b4vc0RYNVG1lbf1ih8WgpoQ9w4lX91IXy\" rel=\"nofollow\"\u003elsh_nt_NonEukaryota_k23_h25.crp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e 5eea8a98d224b7ff505091bd483ca75c\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-refdb-bacteria-classfication-model\" class=\"anchor\" href=\"#refdb-bacteria-classfication-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRefDB bacteria classfication model\u003c/h4\u003e\n\u003cp\u003eTrainned with 42k bacteria assemblies of RefDB. Minimum Java memory: 32G.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003emodel file: \u003ca href=\"https://www.amazon.com/clouddrive/share/LoXz6k229SwYuElPTHvu0SSJOq56nJenvBbOTGVeb9a\" rel=\"nofollow\"\u003erefdb_bacteria_model_gs_k23_l3000_rand_model_214\u003c/a\u003e [size: 11G]\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e 402e9a286b71068999caa9766b2dbf8c\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ehash function file: \u003ca href=\"https://www.amazon.com/clouddrive/share/6ZNvMXMy30b4vc0RYNVG1lbf1ih8WgpoQ9w4lX91IXy\" rel=\"nofollow\"\u003elsh_nt_NonEukaryota_k23_h25.crp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e 5eea8a98d224b7ff505091bd483ca75c\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-genbank-bacteria-and-viruses-classfication-model-illumina-simulation\" class=\"anchor\" href=\"#genbank-bacteria-and-viruses-classfication-model-illumina-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenBank bacteria and viruses classfication model (Illumina Simulation)\u003c/h4\u003e\n\u003cp\u003eTrainned with 54k assemblies from GenBank. \u003cstrong\u003eOnly one assembly was sampled for each species.\u003c/strong\u003e Because viruses data is too samll compared to bateria, it rarely predicts any viruses. Just take it as a bateria model.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3278762/\" rel=\"nofollow\"\u003eart_illumina\u003c/a\u003e was used to simulate the paired-end reads with length of 150, mean size of 270 and stddev of 27.\u003c/p\u003e\n\u003cp\u003eMinimum Java memory: 48G.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003emodel file: \u003ca href=\"https://www.amazon.com/clouddrive/share/zQnu2ti1vfBMGcXrRqsohgfzuaYzZs4HGESP58vobRn\" rel=\"nofollow\"\u003egenbank_model_ill_k23_model_299\u003c/a\u003e [size: 12G]\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e d6b117a4c7ffe4f25e6c532a88bb3a47\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ehash function file: \u003ca href=\"https://www.amazon.com/clouddrive/share/efWceiTHId4EVhY1DEppmW6amyBQoEt3iIU6oW5FbcX\" rel=\"nofollow\"\u003elsh_CAMI2_illumina_k23_h25.crp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e 706633919e347f920ce6ab3277091efb\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-genbank-bacteria-and-viruses-classfication-model-pacbio-simulation\" class=\"anchor\" href=\"#genbank-bacteria-and-viruses-classfication-model-pacbio-simulation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenBank bacteria and viruses classfication model (Pacbio Simulation)\u003c/h4\u003e\n\u003cp\u003eTrainned with 54k assemblies from GenBank. \u003cstrong\u003eOnly one assembly was sampled for each species.\u003c/strong\u003e Because viruses data is too samll compared to bateria, it rarely predicts any viruses. Just take it as a bateria model.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/pfaucon/PBSIM-PacBio-Simulator\"\u003epbsim\u003c/a\u003e was used to simulate the pacbio reads with Continuous Long Read (CLR) profile, mean size of 3000 and stddev of 1000.\u003c/p\u003e\n\u003cp\u003eMinimum Java memory: 16G.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003emodel file: \u003ca href=\"https://www.amazon.com/clouddrive/share/OmU9cmVKknacpt0W9HpI6QY2jXC17dQpWaaERpLhOGl\" rel=\"nofollow\"\u003egenbank_model_pb_k9_model_299\u003c/a\u003e [size: 121M]\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e 351275531493a4866be4afcd9df3932c\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ehash function file: \u003ca href=\"https://www.amazon.com/clouddrive/share/zw4JwJCE4Lst5I4q36ijwrhc3db9rHYsCuyQ4KkihVC\" rel=\"nofollow\"\u003elsh_CAMI2_pacbio_k9_h25.crp\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e df7ee38cf8b58d5f8034bb9b266e3334\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sample-data\" class=\"anchor\" href=\"#sample-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSample data\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eActinoMock Nanopore Sample [size: 500M].\u003c/p\u003e\n\u003cp\u003eThe data is used in example notebook \u003ca href=\"notebook/example_use_vectors_in_bacteria_classfication_model.ipynb\"\u003eexample_use_vectors_in_bacteria_classfication_model.ipynb\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://ww2.cs.fsu.edu/~lshi/ActinoMock_Nanopore.seq.gz\" rel=\"nofollow\"\u003eDownload from FSU\u003c/a\u003e\n\u2003\u2003\n\u003ca href=\"https://www.amazon.com/clouddrive/share/eTIKYVLckXUCMnMQSpO8TCqZOwekmBrx23ZhMa3XO8d\" rel=\"nofollow\"\u003eDownload from Amazon Drive\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003emd5sum\u003c/strong\u003e: b7f3e55438fdc05920aee693a98ded2e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-remark\" class=\"anchor\" href=\"#remark\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemark\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-what-is-jlshvec--why-jlshvec-instead-of-lshvec\" class=\"anchor\" href=\"#what-is-jlshvec--why-jlshvec-instead-of-lshvec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat is JLSHVec ? Why JLSHVec instead of LSHVec?\u003c/h3\u003e\n\u003cp\u003eJLSHVec is a rewritten version of \u003ca href=\"https://github.com/Lizhen0909/LSHVec\"\u003eLSHVec\u003c/a\u003e in Java language.\u003c/p\u003e\n\u003cp\u003eWhen we use LSHVec with big dataset (e.g. \u003ca href=\"https://www.ncbi.nlm.nih.gov/genbank/\" rel=\"nofollow\"\u003eGenBank\u003c/a\u003e, \u003ca href=\"https://www.ncbi.nlm.nih.gov/pubmed/12652131\" rel=\"nofollow\"\u003eRefDB\u003c/a\u003e), we found that LSHVec is hard to process such a big data size.\u003c/p\u003e\n\u003cp\u003eThe reason is that LSHVec which inherits from \u003ca href=\"https://fasttext.cc/\" rel=\"nofollow\"\u003eFastText\u003c/a\u003e requires the input is text format separated by white space and then loads all the text in memory. This is acceptable for natural languages since the data size is at most tens GBs.\u003c/p\u003e\n\u003cp\u003eHowever in LSHVec k-mers are used instead of words. Suppose we want to train a k-mer embedding of simulated Illumina reads with RefDB bacteria assemblies (about 500G genetic bits). The number of kmers is about D*n, where D is the assembly data size and n is coverage. In our case, assuming n=10 and k=23, the number of kmers is 5T and requires a disk space of 125TB, of which the data preparation and loading process will take forever.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-were-jlshvec-pre-trained-models-trained-\" class=\"anchor\" href=\"#how-were-jlshvec-pre-trained-models-trained-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow were JLSHVec pre-trained models trained ?\u003c/h3\u003e\n\u003cp\u003eFirst we prepared a \u003ca href=\"https://rocksdb.org/\" rel=\"nofollow\"\u003eRockDB\u003c/a\u003e for the reference sequences (e.g. all bacteria assemblies in RefDB).\u003c/p\u003e\n\u003cp\u003eThen we have several nodes to train the model: one node (train node) trains the model and others (hash nodes) generate and hash kmers. The nodes communicates by passing \u003ca href=\"https://developers.google.com/protocol-buffers\" rel=\"nofollow\"\u003eprotocol-buf\u003c/a\u003e message with a \u003ca href=\"https://redis.io/\" rel=\"nofollow\"\u003eRedis\u003c/a\u003e server.\u003c/p\u003e\n\u003cp\u003eA hash node randomly reads reference sequences from the RockDB, simulates (e.g. simulations Illumina, Pacbio, Gold Standard) reads, generates kmers and hashes them, then feeds the hashed-kmer-sequences to a Redis queue.\u003c/p\u003e\n\u003cp\u003eTrain node reads from the Redis queue and does jobs of embedding or classification training.  Our training code supports hierarchical softmax using NCBI taxonomy tree, which is essential for multi-label(an instance can have a label for each rank) and multi-class(an instance can only have one label for a rank)  mixture classification model.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003ePlease cite:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.biorxiv.org/content/biorxiv/early/2019/08/06/726729.full.pdf\" rel=\"nofollow\"\u003eA Vector Representation of DNA Sequences Using Locality Sensitive Hashing\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.gnu.org/licenses/gpl-3.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\" alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1572883846.0
  },
  {
    "data_format": 2,
    "description": "A Nextflow pipeline for processing 16S rRNA sequences using dada2",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "nhoffman/dada2-nf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-dada2-nextflow-pipeline\" class=\"anchor\" href=\"#dada2-nextflow-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDada2 Nextflow pipeline\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local-execution-quickstart-for-the-truly-impatient\" class=\"anchor\" href=\"#local-execution-quickstart-for-the-truly-impatient\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal execution quickstart for the truly impatient\u003c/h2\u003e\n\u003cp\u003eInstall Docker and make sure that the Docker daemon is running.\u003c/p\u003e\n\u003cp\u003eInstall the nextflow binary in this directory\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget -qO- https://get.nextflow.io | bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExecute locally, using the minimal data set.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./nextflow run main.nf -params-file params-minimal.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-execution-on-aws-batch\" class=\"anchor\" href=\"#execution-on-aws-batch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecution on AWS Batch\u003c/h2\u003e\n\u003cp\u003eDetails will depend on your AWS batch configuration. General instructions TBD.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-infernal-16s-filtering\" class=\"anchor\" href=\"#infernal-16s-filtering\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInfernal 16s filtering\u003c/h3\u003e\n\u003cp\u003eCoveriance model used for Infernal sequence filtering obtained from the Rfam database:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://rfam.xfam.org/family/RF00177\" rel=\"nofollow\"\u003ehttps://rfam.xfam.org/family/RF00177\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTo cite Rfam see latest web site instructions:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://rfam.xfam.org/\" rel=\"nofollow\"\u003ehttps://rfam.xfam.org/\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1623959326.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "OSC/bc_osc_rstudio_server_quick",
    "latest_release": "v0.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-batch-connect---osc-rstudio-server\" class=\"anchor\" href=\"#batch-connect---osc-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBatch Connect - OSC RStudio Server\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667\" alt=\"GitHub Release\" data-canonical-src=\"https://img.shields.io/github/release/osc/bc_osc_rstudio_server.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn interactive app designed for OSC OnDemand that launches an RStudio Server\nwithin an Owens batch job.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eThis Batch Connect app requires the following software be installed on the\n\u003cstrong\u003ecompute nodes\u003c/strong\u003e that the batch job is intended to run on (\u003cstrong\u003eNOT\u003c/strong\u003e the\nOnDemand node):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.tacc.utexas.edu/research-development/tacc-projects/lmod\" rel=\"nofollow\"\u003eLmod\u003c/a\u003e 6.0.1+ or any other \u003ccode\u003emodule restore\u003c/code\u003e and \u003ccode\u003emodule load \u0026lt;modules\u0026gt;\u003c/code\u003e based\nCLI used to load appropriate environments within the batch job before\nlaunching the RStudio Server.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ewithout Singularity\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e 3.3.2+ (earlier versions are untested but may work for you)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.rstudio.com/products/rstudio-server/\" rel=\"nofollow\"\u003eRStudio Server\u003c/a\u003e 1.0.136+ (earlier versions are untested by may work for you)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://proot-me.github.io/\" rel=\"nofollow\"\u003ePRoot\u003c/a\u003e 5.1.0+ (used to setup fake bind mount)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eor with Singularity\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e 2.4.2+\u003c/li\u003e\n\u003cli\u003eA Singularity image similar to \u003ca href=\"https://www.singularity-hub.org/collections/463\" rel=\"nofollow\"\u003enickjer/singularity-rstudio\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCorresponding module to launch the above Singularity image (see\n\u003ca href=\"https://github.com/nickjer/singularity-rstudio/blob/master/example_module/\"\u003eexample_module\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eUse git to clone this app and checkout the desired branch/version you want to\nuse:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003escl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git clone \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003erepo\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git checkout \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etag/branch\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou will not need to do anything beyond this as all necessary assets are\ninstalled. You will also not need to restart this app as it isn\u0027t a Passenger\napp.\u003c/p\u003e\n\u003cp\u003eTo update the app you would:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git fetch\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git checkout \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etag/branch\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAgain, you do not need to restart the app as it isn\u0027t a Passenger app.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eFork it ( \u003ca href=\"https://github.com/OSC/bc_osc_rstudio_server/fork\"\u003ehttps://github.com/OSC/bc_osc_rstudio_server/fork\u003c/a\u003e )\u003c/li\u003e\n\u003cli\u003eCreate your feature branch (\u003ccode\u003egit checkout -b my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCommit your changes (\u003ccode\u003egit commit -am \u0027Add some feature\u0027\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003ePush to the branch (\u003ccode\u003egit push origin my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCreate a new Pull Request\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1570733859.0
  },
  {
    "data_format": 2,
    "description": "hackathon_intel_genci",
    "filenames": [
      "Sarek/Singularity",
      "Sarek/ScLifeLab/Singularity"
    ],
    "full_name": "larosap/hackathon_intel_genci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCNTdocker\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h2\u003e\n\u003cp\u003eDockerfiles to create Docker images used by the CNT at the university of Pennsylvania\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directory-contents-explanation\" class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirectory contents explanation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-eeg\" class=\"anchor\" href=\"#eeg\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eEEG\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common EEG analysis tools. Usually python 3\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eechobase\u003c/strong\u003e: Dockerfiles used to create images that can calculate functional connectivity of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase code is from \u003ca href=\"https://github.com/andyrevell/paper001\"\u003ehttps://github.com/andyrevell/paper001\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUbuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\nscipy 1.4.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-imaging\" class=\"anchor\" href=\"#imaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eImaging\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common MRI analysis tools.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n  FSL 6.0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ml\" class=\"anchor\" href=\"#ml\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eml\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common machine learning tools.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ewavenet\u003c/strong\u003e: Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet paper\n\u003ca href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\" rel=\"nofollow\"\u003eWavenet blog\u003c/a\u003e\n\u003ca href=\"https://arxiv.org/pdf/1609.03499.pdf\" rel=\"nofollow\"\u003eWavenet paper\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  tensorflow 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTensorflow_2.1\u003c/strong\u003e: Dockerfile to create compatible dependencies to with tensorflow 2.1\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  tensorflow 2.1\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1573750055.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.1.026"
    ],
    "full_name": "arcsUVA/patric",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCNTdocker\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h2\u003e\n\u003cp\u003eDockerfiles to create Docker images used by the CNT at the university of Pennsylvania\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directory-contents-explanation\" class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirectory contents explanation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-eeg\" class=\"anchor\" href=\"#eeg\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eEEG\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common EEG analysis tools. Usually python 3\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eechobase\u003c/strong\u003e: Dockerfiles used to create images that can calculate functional connectivity of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase code is from \u003ca href=\"https://github.com/andyrevell/paper001\"\u003ehttps://github.com/andyrevell/paper001\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUbuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\nscipy 1.4.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-imaging\" class=\"anchor\" href=\"#imaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eImaging\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common MRI analysis tools.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n  FSL 6.0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ml\" class=\"anchor\" href=\"#ml\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eml\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common machine learning tools.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ewavenet\u003c/strong\u003e: Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet paper\n\u003ca href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\" rel=\"nofollow\"\u003eWavenet blog\u003c/a\u003e\n\u003ca href=\"https://arxiv.org/pdf/1609.03499.pdf\" rel=\"nofollow\"\u003eWavenet paper\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  tensorflow 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTensorflow_2.1\u003c/strong\u003e: Dockerfile to create compatible dependencies to with tensorflow 2.1\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  tensorflow 2.1\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1570548606.0
  },
  {
    "data_format": 2,
    "description": "R containers",
    "filenames": [
      "Singularity.3.6.0"
    ],
    "full_name": "arcsUVA/R",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-r\" class=\"anchor\" href=\"#r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR\u003c/h1\u003e\n\u003cp\u003eR containers\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1573410996.0
  },
  {
    "data_format": 2,
    "description": "IQmol in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.2.11.2",
      "Singularity.2.13b",
      "Singularity.2.14"
    ],
    "full_name": "OSC/sa_singularity_iqmol",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-iqmol\" class=\"anchor\" href=\"#singularity-iqmol\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity IQmol\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3599\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"http://iqmol.org/index.html\" rel=\"nofollow\"\u003eIQmol\u003c/a\u003e. It was built on top of the base Docker image \u003ca href=\"https://hub.docker.com/_/ubuntu\" rel=\"nofollow\"\u003eubuntu\u003c/a\u003e or CentOS image \u003ca href=\"https://hub.docker.com/_/centos\" rel=\"nofollow\"\u003ecentos\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003eiqmol.sif\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build iqmol.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from \u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull iqmol.sif shub://OSC/sa_singularity_iqmol\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-start-iqmol\" class=\"anchor\" href=\"#start-iqmol\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStart IQmol\u003c/h3\u003e\n\u003cp\u003eIQmol is started using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run iqmol.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as a native command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./iqmol.sif\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1599018735.0
  },
  {
    "data_format": 2,
    "description": "QGIS in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.3.4.12"
    ],
    "full_name": "OSC/sa_singularity_qgis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-qgis\" class=\"anchor\" href=\"#singularity-qgis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity QGIS\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3587\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://qgis.org/en/site/index.html\" rel=\"nofollow\"\u003eQGIS\u003c/a\u003e. It was built on top of the base Docker image \u003ca href=\"https://hub.docker.com/_/ubuntu\" rel=\"nofollow\"\u003eubuntu\u003c/a\u003e. Packages installed: \u003ccode\u003eqgis qgis-plugin-grass\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003eqgis.sif\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build qgis.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from \u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull qgis.sif shub://OSC/sa_singularity_qgis\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-start-qgis\" class=\"anchor\" href=\"#start-qgis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStart QGIS\u003c/h3\u003e\n\u003cp\u003eQGIS is started using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run qgis.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as a native command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./qgis.sif\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1569951156.0
  },
  {
    "data_format": 2,
    "description": "OpenEXR in a Singularity container",
    "filenames": [
      "Singularity",
      "Singularity.2.2"
    ],
    "full_name": "OSC/sa_singularity_openexr",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-openexr\" class=\"anchor\" href=\"#singularity-openexr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity OpenEXR\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3586\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://www.openexr.com/\" rel=\"nofollow\"\u003eOpenEXR\u003c/a\u003e. It was built on top of the base Docker image \u003ca href=\"https://hub.docker.com/_/ubuntu\" rel=\"nofollow\"\u003eubuntu\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003eopenexr.sif\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build openexr.sif Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from \u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name openexr.sif shub://OSC/sa_singularity_openexr\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-render-exr-image\" class=\"anchor\" href=\"#render-exr-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRender .EXR image\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eexrdisplay\u003c/code\u003e command is launched using the command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e openexr.sif exrdisplay -h\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e openexr.sif exrdisplay rendertest_0001.exr\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1569951214.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for dropSeqPipe",
    "filenames": [
      "Singularity",
      "Singularity.v04"
    ],
    "full_name": "seb-mueller/singularity_dropSeqPipe",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\" alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\" alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\" class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHEMnet - Haematoxylin \u0026amp; Eosin and Molecular neural network\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA deep learning automated cancer diagnosis software using molecular labelling to improve pathological annotation of\nHaematoxylin and Eosin (H\u0026amp;E) stained tissue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDocker\u003c/p\u003e\n\u003cp\u003eYou can download and run the docker image using the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConda\u003c/p\u003e\n\u003cp\u003eInstall Openslide (this is necessary to open whole slide images) - download it \u003ca href=\"https://openslide.org/download/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCreate a conda environment from the \u003ccode\u003eenvironment.yml\u003c/code\u003e file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f environment.yml\nconda activate HEMnet\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-slide-preparation\" class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSlide Preparation\u003c/h3\u003e\n\u003cp\u003eName slides in the format: \u003ccode\u003eslide_id_TP53\u003c/code\u003e for TP53 slides and \u003ccode\u003eslide_id_HandE\u003c/code\u003e for H\u0026amp;E slides\nThe \u003ccode\u003eTP53\u003c/code\u003e and \u003ccode\u003eHandE\u003c/code\u003e suffix is used by HEMnet to identify the stain used.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-generate-training-and-testing-datasets\" class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Generate training and testing datasets\u003c/h3\u003e\n\u003cp\u003ea. Generate train dataset\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eb. Generate test dataset\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_test_dataset.py -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh -n non_cancer_thresh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-t\u003c/code\u003e is the relative path to the template slide from which all other slides will be normalised against. The template\nslide should be the same for each step.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is the tile magnification. e.g. if  the input is \u003ccode\u003e10\u003c/code\u003e then the tiles will be output at 10x\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-a\u003c/code\u003e is the align magnification. Paired TP53 and H\u0026amp;E slides will be registered at this magnification.\nTo reduce computation time we recommend this be less than the tile magnification - a five times downscale generally works well.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-c\u003c/code\u003e cancer threshold to apply to the DAB channel. DAB intensities less than this threshold indicate cancer.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-n\u003c/code\u003e non-cancer threshold to apply to the DAB channel. DAB intensities greater than this threshold indicate no cancer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\" href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Train and evaluate model\u003c/h3\u003e\n\u003cp\u003ea. Training model\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython train.py -b /path/to/base/directory -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size -s -w -f -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is CNN base model. eg. \u003ccode\u003eresnet50\u003c/code\u003e, \u003ccode\u003evgg16\u003c/code\u003e, \u003ccode\u003evgg19\u003c/code\u003e, \u003ccode\u003einception_v3\u003c/code\u003e and \u003ccode\u003exception\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-g\u003c/code\u003e is number of GPUs for training.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e\u003c/code\u003e is training epochs. Default is \u003ccode\u003e100\u003c/code\u003e epochs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-a\u003c/code\u003e is batch size. Default is \u003ccode\u003e32\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-s\u003c/code\u003e is option to save the trained model weights.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-w\u003c/code\u003e is option to used transfer learning. Model will used pre-trained weights from ImageNet at the initial stage.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-f\u003c/code\u003e is fine-tuning option. Model will re-train CNN base.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eb. Test model prediction\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython test.py  -b /path/to/base/directory -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory -w model_weights -m cnn_base -g num_gpus -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-w\u003c/code\u003e is path to trained model. eg. \u003ccode\u003etrained_model.h5\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is CNN base model (same to training step).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-g\u003c/code\u003e is number of GPUs for prediction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ec. Evaluate model performance and visualise model prediction\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython visualisation.py -b /path/to/base/directory -t /relative/path/to/training_output_directory -p /relative/path/to/test_output_directory  -o /relative/path/to/output/directory -i sample\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-t\u003c/code\u003e is path to training outputs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-p\u003c/code\u003e is path to test outputs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-i\u003c/code\u003e is name of Whole Slide Image for visualisation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\" href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Apply model to diagnose new images\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_inference.py -s \u0027/path/to/new/HE/Slides/\u0027 -o \u0027/path/to/output/directory/\u0027 -t \u0027/path/to/template/slide/\u0027 -nn \u0027/path/to/trained/model/\u0027 -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePredict on TCGA images with our pretrained model for colorectal cancer using \u003ca href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\" rel=\"nofollow\"\u003egoogle colab\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting HEMnet\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-the-team\" class=\"anchor\" href=\"#the-team\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe Team\u003c/h2\u003e\n\u003cp\u003ePlease contact Dr Quan Nguyen (\u003ca href=\"mailto:quan.nguyen@uq.edu.au\"\u003equan.nguyen@uq.edu.au\u003c/a\u003e), Andrew Su (\u003ca href=\"mailto:a.su@uqconnect.edu.au\"\u003ea.su@uqconnect.edu.au\u003c/a\u003e),\nand Xiao Tan (\u003ca href=\"mailto:xiao.tan@uqconnect.edu.au\"\u003exiao.tan@uqconnect.edu.au\u003c/a\u003e) for issues, suggestions,\nand we are very welcome to collaboration opportunities.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1569595505.0
  },
  {
    "data_format": 2,
    "description": "containers",
    "filenames": [
      "Singularity.py3_tfstable",
      "Singularity.pyhon3"
    ],
    "full_name": "LuisBonillaR/singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3399\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the PAML tool for phylogenetic analyses of DNA or protein sequences using maximum likelihood.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1610738330.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for paml (http://abacus.gene.ucl.ac.uk/software/paml.html)",
    "filenames": [
      "Singularity",
      "Singularity.4.9i"
    ],
    "full_name": "powerPlant/paml-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3399\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the PAML tool for phylogenetic analyses of DNA or protein sequences using maximum likelihood.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1565742033.0
  },
  {
    "data_format": 2,
    "description": "Nextflow workflow for finding conserved motifs intersecting with splice junctions",
    "filenames": [
      "Singularity"
    ],
    "full_name": "czbiohub/splicemotifs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-corebedtools-intersect\" class=\"anchor\" href=\"#nf-corebedtools-intersect\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-core/bedtools-intersect\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eIntersect lots of bed files with lots of other bed files\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nf-core/bedtools-intersect\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/811368779316af4f70b4dd35fc2c24cebcc4dc194cd63234e130384ec38ac89f/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f626564746f6f6c732d696e746572736563742e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nf-core/bedtools-intersect.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/bedtools-intersect\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca7e06b0d2929a9cba14da1892e90c6d4673a695806cb07ea82e89a1cbecef92/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f626564746f6f6c732d696e746572736563742e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/bedtools-intersect.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe nf-core/bedtools-intersect pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1564673719.0
  },
  {
    "data_format": 2,
    "description": "RNA-seq analysis pipeline based on Snakemake",
    "filenames": [
      "Singularity"
    ],
    "full_name": "tgac-vumc/RNA-seq",
    "latest_release": "v1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rna-seq-analysis-pipeline\" class=\"anchor\" href=\"#rna-seq-analysis-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRNA-seq analysis pipeline\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://snakemake.bitbucket.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9e0a726dc69516d51067fd9fc2074a9f2dc9d44eb069ae05434a36f580af32f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b653d3d352e32352e302d627269676874677265656e2e7376673f7374796c653d666c61742d737175617265\" alt=\"Snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake==5.25.0-brightgreen.svg?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://singularity-hub.org/collections/3066\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c9d2afb620129b7ba0f4d918b77bfdb2b91c595cd6c6d013e950ee6e3c2bbc55/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d73696e67756c61726974792d2d6875622d7265642e737667\" alt=\"singularity-hub\" data-canonical-src=\"https://img.shields.io/badge/install%20with-singularity--hub-red.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e225eb3891735f81d51e8e6aa377429328cfd43656973ff807bffe9234bc28c7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d636f6e64612d677265656e2e737667\" alt=\"miniconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-conda-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is a \u003ca href=\"https://snakemake.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003eSnakemake\u003c/a\u003e based pipeline for RNA-seq used in the \u003ca href=\"http://www.tgac.nl/\" rel=\"nofollow\"\u003eTumor Genome Core Analysis\u003c/a\u003e housed in the \u003ca href=\"https://www.vumc.com/departments/cancer-center-amsterdam.htm\" rel=\"nofollow\"\u003eCancer Center Amsterdam\u003c/a\u003e, at \u003ca href=\"https://www.vumc.nl/\" rel=\"nofollow\"\u003eAmsterdam UMC location VUmc\u003c/a\u003e and part of the Department of Pathology.\u003c/p\u003e\n\u003cp\u003eThe pipeline processes raw data from FastQ inputs (\u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\u003eFastQC\u003c/a\u003e, \u003ca href=\"http://www.usadellab.org/cms/?page=trimmomatic\" rel=\"nofollow\"\u003eTrimmomatic\u003c/a\u003e), aligns the reads (\u003ca href=\"https://github.com/alexdobin/STAR\"\u003eSTAR\u003c/a\u003e), generates gene counts (\u003ca href=\"http://bioinf.wehi.edu.au/featureCounts/\" rel=\"nofollow\"\u003efeatureCounts\u003c/a\u003e) and performs quality-control on the results (\u003ca href=\"https://multiqc.info/\" rel=\"nofollow\"\u003eMultiQC\u003c/a\u003e). Paired-end (PE) and single read (SR) are supported.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://github.com/tgac-vumc/RNA-seq/blob/master/DAG_RNAseq.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"850\" height=\"483\" src=\"https://github.com/tgac-vumc/RNA-seq/raw/master/DAG_RNAseq.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThe pipeline is preliminary used in linux environment with conda/singularity available.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-conda\" class=\"anchor\" href=\"#using-conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Conda\u003c/h3\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1-installing-miniconda-3\" class=\"anchor\" href=\"#step-1-installing-miniconda-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1: Installing Miniconda 3\u003c/h3\u003e\n\u003cp\u003eFirst, please open a terminal or make sure you are logged into your Linux VM. Assuming that you have a 64-bit system, on Linux, download and install Miniconda 3 with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn MacOS X, download and install with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2-downloading-repository--creating-environment\" class=\"anchor\" href=\"#step-2-downloading-repository--creating-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2: Downloading repository \u0026amp; creating environment\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003emkdir snakemake_RNAseq\ncd snakemake_RNAseq\ngit clone https://github.com/tgac-vumc/RNA-seq\nconda env create --name RNAseq --file env.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Singularity\u003c/h3\u003e\n\u003cp\u003eThe singularity container holds a virtual environment of CentOS 7 and it\u0027s available with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://tgac-vumc/RNA-seq\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-path-configuration--running-the-pipeline\" class=\"anchor\" href=\"#path-configuration--running-the-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePath Configuration \u0026amp; Running the pipeline\u003c/h2\u003e\n\u003cp\u003eBefore attempting to run the pipeline, please open \u003cem\u003econfig.yaml\u003c/em\u003e. Inside, you will encounter \u003cstrong\u003ePath Configuration\u003c/strong\u003e and \u003cstrong\u003eSoftware Options\u003c/strong\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eOn \u003cstrong\u003ePath configuration\u003c/strong\u003e, first, you have to choose whether your data is PE or SR and after change the fastq path to the path where your fastq files are actually stored.\u003c/li\u003e\n\u003cli\u003eOn \u003cstrong\u003eSoftware Options\u003c/strong\u003e, you will find several options that can be modified by the user. Please, have a look at it before running the pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAll the software used in the pipeline is installed by conda or executed in a wrapper. We recommend to run the pipeline from a different location than the pipeline path, like the example below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake -s PATH_TO_PIPELINE/Snakefile --use-conda --cores=24\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWith --use-conda option, the pipeline will create environments to run rules based on \u003cem\u003eenv.yaml\u003c/em\u003e.\n\u003cstrong\u003eNote\u003c/strong\u003e the pipeline assumes that \u003cem\u003econfig.yaml\u003c/em\u003e is available at the location where the pipeline is executed.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1625231941.0
  },
  {
    "data_format": 2,
    "description": "local settings",
    "filenames": [
      "examples/arch/Singularity",
      "examples/asciinema/Singularity",
      "examples/busybox/Singularity",
      "examples/shub/Singularity",
      "examples/apps/Singularity",
      "examples/apps/Singularity.cowsay",
      "examples/ubuntu/Singularity",
      "examples/instances/Singularity",
      "examples/docker/Singularity",
      "examples/raspbian/Singularity",
      "examples/centos/Singularity",
      "examples/scratch/Singularity.busybox",
      "examples/scratch/Singularity.alpine",
      "examples/opensuse/Singularity",
      "examples/multistage/Singularity",
      "examples/scientific/Singularity",
      "examples/debian/Singularity",
      "examples/library/Singularity",
      "examples/self/Singularity"
    ],
    "full_name": "frankwillmore/alcf-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/sylabs/singularity\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1646c42a348a1331feb3842e34171e866c139adbae2608ba5fbd2c022c9c20f/68747470733a2f2f7472617669732d63692e6f72672f73796c6162732f73696e67756c61726974792e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/sylabs/singularity.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://circleci.com/gh/sylabs/singularity/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://goreportcard.com/report/github.com/sylabs/singularity\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/179d3d939b6a64c4f021860776fdc6243bc26409e966f1aa6bd7d35ca9593fea/68747470733a2f2f676f7265706f7274636172642e636f6d2f62616467652f6769746875622e636f6d2f73796c6162732f73696e67756c6172697479\" alt=\"Go Report Card\" data-canonical-src=\"https://goreportcard.com/badge/github.com/sylabs/singularity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"CONTRIBUTING.md\"\u003eGuidelines for Contributing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\".github/PULL_REQUEST_TEMPLATE.md\"\u003ePull Request Template\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"LICENSE.md\"\u003eProject License\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459\" rel=\"nofollow\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSingularity is an open source container platform designed to be simple, fast, and secure. Singularity is optimized for \u003ca href=\"https://www.sylabs.io/2018/09/singularity-is-enterprise-performance-computing/\" rel=\"nofollow\"\u003eEPC\u003c/a\u003e and HPC workloads, allowing untrusted users to run untrusted containers in a trusted way.\u003c/p\u003e\n\u003cp\u003eCheck out \u003ca href=\"https://www.sylabs.io/singularity/whos-using-singularity/\" rel=\"nofollow\"\u003ewho is using Singularity\u003c/a\u003e and some \u003ca href=\"https://www.sylabs.io/category/how-tos/\" rel=\"nofollow\"\u003euse cases of Singularity\u003c/a\u003e on our website.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started-with-singularity\" class=\"anchor\" href=\"#getting-started-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started with Singularity\u003c/h2\u003e\n\u003cp\u003eTo install Singularity from source, see the \u003ca href=\"INSTALL.md\"\u003einstallation instructions\u003c/a\u003e. For other installation options, see \u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/installation.html\" rel=\"nofollow\"\u003eour website\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor system administrators, see the \u003ca href=\"https://www.sylabs.io/guides/3.0/admin-guide/\" rel=\"nofollow\"\u003eadministrator documentation\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor users, see the \u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/\" rel=\"nofollow\"\u003euser documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing-to-singularity\" class=\"anchor\" href=\"#contributing-to-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing to Singularity\u003c/h2\u003e\n\u003cp\u003eCommunity contributions are always greatly appreciated. To start developing Singularity, check out the \u003ca href=\"CONTRIBUTING.md\"\u003eguidelines for contributing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe also welcome contributions to our \u003ca href=\"https://github.com/sylabs/singularity-userdocs\"\u003euser docs\u003c/a\u003e and \u003ca href=\"https://github.com/sylabs/singularity-admindocs\"\u003eadmin docs\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h2\u003e\n\u003cp\u003eTo get help with Singularity, check out the \u003ca href=\"https://www.sylabs.io/singularity/community/\" rel=\"nofollow\"\u003eCommunity Portal\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor additional support, \u003ca href=\"https://www.sylabs.io/contact/\" rel=\"nofollow\"\u003econtact us\u003c/a\u003e to receive more information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cite-as\" class=\"anchor\" href=\"#cite-as\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCite as:\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe also have a Zenodo citation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer, Gregory M.. (2016). Singularity 2.1.2 - Linux application and environment\ncontainers for science. 10.5281/zenodo.60736\n\nhttps://doi.org/10.5281/zenodo.60736\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eUnless otherwise noted, this project is licensed under a 3-clause BSD license found in the \u003ca href=\"LICENSE.md\"\u003elicense file\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1558040154.0
  },
  {
    "data_format": 2,
    "description": "Singularity images to run on the cluster",
    "filenames": [
      "Singularity.py3_tf112",
      "Singularity.py3_tf114",
      "Singularity.py3_tf112_plus",
      "Singularity.py3_astro",
      "Singularity.py3_tf114_lls",
      "Singularity.py3_tf115",
      "Singularity.py3_tf113"
    ],
    "full_name": "joaocaldeira/singularity_imgs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_imgs\" class=\"anchor\" href=\"#singularity_imgs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_imgs\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2968\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity images to run on the cluster\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1590440777.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v4.2.0"
    ],
    "full_name": "baxpr/fmriqa",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-functional-mri-qa-pipeline\" class=\"anchor\" href=\"#functional-mri-qa-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFunctional MRI QA pipeline\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eTest the matlab code before compiling: \u003ccode\u003esrc/testmatlab.m\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eCompile: \u003ccode\u003ecompile_matlab.sh\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eTest the compiled runtime: \u003ccode\u003ebin/test_compiled_matlab.sh\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eBuild the Singularity container: \u003ccode\u003eSingularity.v4.2.0\u003c/code\u003e, \u003ca href=\"https://www.singularity-hub.org/collections/2945\" rel=\"nofollow\"\u003ehttps://www.singularity-hub.org/collections/2945\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSee \u003ccode\u003etest_sing_container.sh\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003cp\u003eThe inputs must all be provided, in the correct order. Paths are with respect to the container root.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eName of the output directory\u003c/li\u003e\n\u003cli\u003eFilename of the T1 structural image (.nii.gz)\u003c/li\u003e\n\u003cli\u003eFilename of the segmented T1 image (.nii.gz), typically the SEG output of a MultiAtlas or SLANT pipeline\u003c/li\u003e\n\u003cli\u003eFilename of the 4D fMRI (.nii.gz)\u003c/li\u003e\n\u003cli\u003eXNAT project label\u003c/li\u003e\n\u003cli\u003eXNAT subject label\u003c/li\u003e\n\u003cli\u003eXNAT session label\u003c/li\u003e\n\u003cli\u003eXNAT scan label (of the fMRI)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-processing\" class=\"anchor\" href=\"#processing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eProcessing\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eMotion realignment and creation of mean fMRI\u003c/li\u003e\n\u003cli\u003eCoregister T1 to mean fMRI\u003c/li\u003e\n\u003cli\u003eCompute SNR and quality metrics\u003c/li\u003e\n\u003cli\u003eCarpet plots, graphical report\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003efmriqa.pdf                               PDF report\nrp_fmri.txt                              Realignment parameters (SPM12 style)\nfmriqa_stats.csv                         Summary stats\nfmriqa_stats_wide.csv                    Summary stats in wide format (XNAT/REDCap compatible)\nFD.txt                                   Framewise displacement time series\nDVARS.txt                                DVARS time series\nglobal.txt                               Global mean time series\nmeanfmri.nii.gz                          Mean fMRI image after realignment\nmedian_voxel_displacement_mm.txt         Framewise displacement, median over voxels\ntemporal_snr.nii.gz                      Temporal signal-to-noise ratio image\nvoxel_displacement_mm_95prctile.nii.gz   Framewise displacement image (95th percentile over time)\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1558037991.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for SqueezeMeta (https://github.com/jtamames/SqueezeMeta)",
    "filenames": [
      "Singularity",
      "Singularity.0.4.4",
      "Singularity.1.0.0-beta"
    ],
    "full_name": "powerPlant/squeezemeta-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2930\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the SqueezeMeta fully automated metagenomics pipeline\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1557458055.0
  },
  {
    "data_format": 2,
    "description": "Docker and Singularity images for Scanpy",
    "filenames": [
      "Singularity"
    ],
    "full_name": "VIB-CBD/scanpy-images",
    "latest_release": null,
    "readme": "\u003cp\u003eDependency full Scanpy Docker and Scanpy images based on Alpine.\u003c/p\u003e\n\u003cp\u003eIncludes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLoompy\u003c/li\u003e\n\u003cli\u003eLouvain\u003c/li\u003e\n\u003cli\u003eigraph\u003c/li\u003e\n\u003cli\u003eipython\u003c/li\u003e\n\u003cli\u003eJupyter\u003c/li\u003e\n\u003cli\u003eCython\u003c/li\u003e\n\u003cli\u003eMulticoreTSNE\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1556798801.0
  },
  {
    "data_format": 2,
    "description": "Nextflow workflow for assembling large, diploid, eukaryotic genomes (2 gigabases haploid size or bigger)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "czbiohub/nf-large-assembly",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-czbiohubnf-large-assembly\" class=\"anchor\" href=\"#czbiohubnf-large-assembly\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eczbiohub/nf-large-assembly\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAssemble large diploid eukaryotic genomes (2 gigabases haploid size or bigger)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/czbiohub/nf-large-assembly\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8d428dc306e8c519b4952b8239ab3eace188860f1c5dfabe1a4059c42c067a1e/68747470733a2f2f7472617669732d63692e6f72672f637a62696f6875622f6e662d6c617267652d617373656d626c792e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/czbiohub/nf-large-assembly.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/nf-large-assembly\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/767f13dee3d8a1039b493b285b876f4ef216154825cb6401031b09e8d959b916/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f6e662d6c617267652d617373656d626c792e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/nf-large-assembly.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe czbiohub/nf-large-assembly pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1556036860.0
  },
  {
    "data_format": 2,
    "description": "Theano Singularity container scripts",
    "filenames": [
      "Singularity.1.0.4-py36"
    ],
    "full_name": "arcsUVA/theano",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-theano\" class=\"anchor\" href=\"#theano\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etheano\u003c/h1\u003e\n\u003cp\u003eTheano Singularity container scripts\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1554499739.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "andquintero/singularity_builds",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_builds\" class=\"anchor\" href=\"#singularity_builds\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_builds\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1554218133.0
  },
  {
    "data_format": 2,
    "description": "test of nf-core create",
    "filenames": [
      "Singularity"
    ],
    "full_name": "czbiohub/nf-core-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-coretest\" class=\"anchor\" href=\"#nf-coretest\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-core/test\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003etest of nf-core\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nf-core/test\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5656ec3ca80ae8775904761dfc7b47e3357d325de15a8d013edd4a0093630611/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f746573742e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nf-core/test.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/test\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8a74c7ad053a343b2d1b30e0ef0f86afe191999cfc823635773862aefd840fd2/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f746573742e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/test.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe nf-core/test pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1554245021.0
  },
  {
    "data_format": 2,
    "description": "Batch Connect - OSC RStudio Server - Pitzer",
    "filenames": [
      "Singularity"
    ],
    "full_name": "OSC/bc_osc_rstudio_server_pitzer",
    "latest_release": "v0.1.5",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-batch-connect---osc-rstudio-server\" class=\"anchor\" href=\"#batch-connect---osc-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBatch Connect - OSC RStudio Server\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667\" alt=\"GitHub Release\" data-canonical-src=\"https://img.shields.io/github/release/osc/bc_osc_rstudio_server_pitzer.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn interactive app designed for OSC OnDemand that launches an RStudio Server\nwithin an Pitzer batch job.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deprecated-application-warning\" class=\"anchor\" href=\"#deprecated-application-warning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeprecated application warning\u003c/h2\u003e\n\u003cp\u003eThis application no longer works.  It raises an exception when users attempt to submit jobs.\nThis is because we now have functionality to submit to multiple clusters and\n\u003ca href=\"https://github.com/OSC/bc_osc_rstudio_server\"\u003ethe generic application\u003c/a\u003e now submits\nto pitzer rendering this application useless.\u003c/p\u003e\n\u003cp\u003eFor historic versions, see the last released you can still view\n\u003ca href=\"https://github.com/OSC/bc_osc_rstudio_server_pitzer/tree/v0.3.0\"\u003ev0.3.0\u003c/a\u003e as it was the last\nworking version of this application.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1598640242.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "chenhongluo/horovord",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\" class=\"anchor\" href=\"#singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container for molecular electrostatic calculations using PDB2PQR/APBS and Brownian dynamics with BrownDye.\u003c/h1\u003e\n\u003cp\u003eThis singularity image contains a complete software environment for running \u003ca href=\"http://browndye.ucsd.edu/\" rel=\"nofollow\"\u003eBrownDye (version 1 and 2)\u003c/a\u003e simulations. It also includes \u003ca href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\"\u003ePDB2PQR\u003c/a\u003e and \u003ca href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\"\u003eAPBS\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePlease \u003ca href=\"http://eepurl.com/by4eQr\" rel=\"nofollow\"\u003eregister\u003c/a\u003e your use of APBS and PDB2PQR.\u003c/p\u003e\n\u003cp\u003eThe image has been verified to work on XSEDE \u003ca href=\"https://portal.xsede.org/sdsc-comet\" rel=\"nofollow\"\u003ecomet\u003c/a\u003e and \u003ca href=\"https://www.sdsc.edu/support/user_guides/tscc-quick-start.html\" rel=\"nofollow\"\u003eTSCC\u003c/a\u003e shared cluster at SDSC. It will automatically bind \u003ccode\u003e/cvmfs\u003c/code\u003e \u003ccode\u003e/oasis\u003c/code\u003e \u003ccode\u003e/projects\u003c/code\u003e \u003ccode\u003e/scratch\u003c/code\u003e directories, if available on the host.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-the-container\" class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the container\u003c/h2\u003e\n\u003cp\u003ePull the singularity image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://nbcrrolls/electrostatics-singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart bash shell in the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell nbcrrolls-electrostatics-singularity-master-latest.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow the container is running and we can start a BrownDye2 job (using the Thrombin example):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load browndye2\ncp -ai $BD2_PATH/examples/thrombin .\ncd thrombin\nsed -i \u0027s/-PE0//g\u0027 *\nsed -i \u0027s/\u0026lt;n_trajectories\u0026gt; 10000 /\u0026lt;n_trajectories\u0026gt; 1000 /\u0027 t_m_simulation.xml.bak\nmake all # takes about min to run\nmodule unload browndye2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd if you want to use BrownDye version 1:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load browndye1\ncp -ai $BD1_PATH/thrombin-example .\ncd thrombin-example\nsed -i \u0027s/-PE0//g\u0027 *\nsed -i \u0027s/\u0026lt;n-trajectories\u0026gt; 10000 /\u0026lt;n-trajectories\u0026gt; 1000 /\u0027 input.xml.bak # limit the number of calculated trajectories\nmake all\nbd_top input.xml\nnam_simulation t-m-simulation.xml # this takes about 3 min to run\ncat results.xml\nmodule unload browndye1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter we are finished we can quit the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also access individual applications from the electrostatics container.\u003c/p\u003e\n\u003cp\u003eTo list available applications:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity apps nbcrrolls-electrostatics-singularity-master-latest.simg \napbs\npdb2pqr\nnam_simulation\nwe_simulation\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run, for example, apbs calculation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec nbcrrolls-electrostatics-singularity-master-latest.simg apbs input.in\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app apbs nbcrrolls-electrostatics-singularity-master-latest.simg input.in\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis Singularity image is hosted on Singularity Hub: \u003ca href=\"https://singularity-hub.org/collections/2497\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-this-project-is-supported-by-nbcr\" class=\"anchor\" href=\"#this-project-is-supported-by-nbcr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThis project is supported by \u003ca href=\"http://nbcr.ucsd.edu\" rel=\"nofollow\"\u003eNBCR\u003c/a\u003e.\u003c/h6\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1553181936.0
  },
  {
    "data_format": 2,
    "description": "Molecular electrostatics singularity image",
    "filenames": [
      "Singularity"
    ],
    "full_name": "nbcrrolls/electrostatics-singularity",
    "latest_release": "v2.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\" class=\"anchor\" href=\"#singularity-container-for-molecular-electrostatic-calculations-using-pdb2pqrapbs-and-brownian-dynamics-with-browndye\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container for molecular electrostatic calculations using PDB2PQR/APBS and Brownian dynamics with BrownDye.\u003c/h1\u003e\n\u003cp\u003eThis singularity image contains a complete software environment for running \u003ca href=\"http://browndye.ucsd.edu/\" rel=\"nofollow\"\u003eBrownDye (version 1 and 2)\u003c/a\u003e simulations. It also includes \u003ca href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\"\u003ePDB2PQR\u003c/a\u003e and \u003ca href=\"http://www.poissonboltzmann.org/\" rel=\"nofollow\"\u003eAPBS\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePlease \u003ca href=\"http://eepurl.com/by4eQr\" rel=\"nofollow\"\u003eregister\u003c/a\u003e your use of APBS and PDB2PQR.\u003c/p\u003e\n\u003cp\u003eThe image has been verified to work on XSEDE \u003ca href=\"https://portal.xsede.org/sdsc-comet\" rel=\"nofollow\"\u003ecomet\u003c/a\u003e and \u003ca href=\"https://www.sdsc.edu/support/user_guides/tscc-quick-start.html\" rel=\"nofollow\"\u003eTSCC\u003c/a\u003e shared cluster at SDSC. It will automatically bind \u003ccode\u003e/cvmfs\u003c/code\u003e \u003ccode\u003e/oasis\u003c/code\u003e \u003ccode\u003e/projects\u003c/code\u003e \u003ccode\u003e/scratch\u003c/code\u003e directories, if available on the host.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-the-container\" class=\"anchor\" href=\"#using-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the container\u003c/h2\u003e\n\u003cp\u003ePull the singularity image:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://nbcrrolls/electrostatics-singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eStart bash shell in the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell nbcrrolls-electrostatics-singularity-master-latest.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow the container is running and we can start a BrownDye2 job (using the Thrombin example):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load browndye2\ncp -ai $BD2_PATH/examples/thrombin .\ncd thrombin\nsed -i \u0027s/-PE0//g\u0027 *\nsed -i \u0027s/\u0026lt;n_trajectories\u0026gt; 10000 /\u0026lt;n_trajectories\u0026gt; 1000 /\u0027 t_m_simulation.xml.bak\nmake all # takes about min to run\nmodule unload browndye2\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd if you want to use BrownDye version 1:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load browndye1\ncp -ai $BD1_PATH/thrombin-example .\ncd thrombin-example\nsed -i \u0027s/-PE0//g\u0027 *\nsed -i \u0027s/\u0026lt;n-trajectories\u0026gt; 10000 /\u0026lt;n-trajectories\u0026gt; 1000 /\u0027 input.xml.bak # limit the number of calculated trajectories\nmake all\nbd_top input.xml\nnam_simulation t-m-simulation.xml # this takes about 3 min to run\ncat results.xml\nmodule unload browndye1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter we are finished we can quit the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also access individual applications from the electrostatics container.\u003c/p\u003e\n\u003cp\u003eTo list available applications:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity apps nbcrrolls-electrostatics-singularity-master-latest.simg \napbs\npdb2pqr\nnam_simulation\nwe_simulation\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run, for example, apbs calculation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec nbcrrolls-electrostatics-singularity-master-latest.simg apbs input.in\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app apbs nbcrrolls-electrostatics-singularity-master-latest.simg input.in\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis Singularity image is hosted on Singularity Hub: \u003ca href=\"https://singularity-hub.org/collections/2497\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch6\u003e\n\u003ca id=\"user-content-this-project-is-supported-by-nbcr\" class=\"anchor\" href=\"#this-project-is-supported-by-nbcr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThis project is supported by \u003ca href=\"http://nbcr.ucsd.edu\" rel=\"nofollow\"\u003eNBCR\u003c/a\u003e.\u003c/h6\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1556048171.0
  },
  {
    "data_format": 2,
    "description": "Singularity container script for 10x Genomics SuperNova software",
    "filenames": [
      "Singularity.2.0.0"
    ],
    "full_name": "arcsUVA/supernova",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-supernova\" class=\"anchor\" href=\"#supernova\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esupernova\u003c/h1\u003e\n\u003cp\u003eSingularity container script for 10x Genomics SuperNova software\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1551891095.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "store_pw/Singularity.4.2.5",
      "store_pw/Singularity.pw_embed",
      "store_pw/Singularity.python-4.2.5",
      "store_pw/Singularity.base-4.2.5",
      "store_pw/Singularity.pw_encrypt",
      "docs/Singularity.3_0.debian9",
      "os_recipes/Singularity.archive.debian",
      "os_recipes/Singularity.4.2.5",
      "os_recipes/Singularity.SuSE",
      "os_recipes/Singularity.centos6",
      "os_recipes/Singularity.base-4.2.5",
      "os_recipes/Singularity.centos7",
      "os_recipes/Singularity.usmirror.debian",
      "os_recipes/Singularity.deboot.ubuntu"
    ],
    "full_name": "d-w-moore/new_d2c",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-installing-and-running-slurm-on-ubuntu-16-or-18\" class=\"anchor\" href=\"#installing-and-running-slurm-on-ubuntu-16-or-18\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling and Running SLURM on ubuntu 16 or 18\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-slurm\" class=\"anchor\" href=\"#install-slurm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall SLURM\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt install slurm-wlm\ngit clone http://github.com/d-w-moore/new_d2c\ncd new_d2c\nperl process_slurm_template.pl  | sudo dd of=/etc/slurm-llnl/slurm.conf\nsudo systemctl restart slurmctld slurmd\nsudo systemctl enable  slurmctld slurmd\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto test:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esudo apt install bc\u003c/li\u003e\n\u003cli\u003elocate command file slurm_install_test.sh containing:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e  #!/bin/bash\n  bc -l \u0026lt;\u0026lt;\u0026lt;\"scale=4000;a(1)*4\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003erun the above mentioned test script using : \u003ccode\u003esbatch \u0026lt;script\u0026gt;\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003etype: \u003ccode\u003esqueue\u003c/code\u003e and note the job present (most likely running)\u003c/li\u003e\n\u003cli\u003ewhen it disappears from queue (\u003ccode\u003ewatch -n1 squeue\u003c/code\u003e), look for \u003ccode\u003eslurm-\u0026lt;JOBNUM\u0026gt;.out\u003c/code\u003e\ncontaining the job\u0027s output\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-datacompute-automated-setup---install-irods-hook-scripts-for-slurm-prolog--epilog\" class=\"anchor\" href=\"#datacompute-automated-setup---install-irods-hook-scripts-for-slurm-prolog--epilog\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData/Compute automated setup - install iRODS hook scripts for slurm prolog / epilog\u003c/h2\u003e\n\u003cp\u003eThe following command will setup prolog and epilog scripts to be run (pre- and post-,\nrespectively) for each job executed by SLURM:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo ./slurm_hook_setup.sh\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1561308424.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for checkm (http://ecogenomics.github.io/CheckM)",
    "filenames": [
      "Singularity.1.0.13",
      "Singularity.1.0.11",
      "Singularity",
      "Singularity.1.1.3",
      "Singularity.1.0.12",
      "Singularity.1.0.10",
      "Singularity.1.0.7",
      "Singularity.1.0.8"
    ],
    "full_name": "powerPlant/checkm-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2464\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the CheckM set of tools for assessing the quality of genomes recovered from isolates, single cells, or metagenomes\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1598504920.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for Pblat (http://icebert.github.io/pblat/)",
    "filenames": [
      "Singularity",
      "Singularity.2.0",
      "Singularity.2.1"
    ],
    "full_name": "powerPlant/pblat-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2380\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for Pblat, the parallelized blat with multi-threads support\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550562816.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity",
      "Singularity.3.6.3"
    ],
    "full_name": "tpall/singularity-stan",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos7_aci\" class=\"anchor\" href=\"#centos7_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos7_aci\u003c/h1\u003e\n\u003cp\u003eCentos 7 base image for ACI Singualarity recipe\u003cbr\u003e\nThis recipe may include unnecessary packages for certain software installation.\u003cbr\u003e\nSize of CPU-only container: ~1 GB\u003cbr\u003e\nSize of GPU container: ~2.6 GB\u003c/p\u003e\n\u003cp\u003eMore packages will be added in the future\u003c/p\u003e\n\u003cp\u003e2019/2/17\n\u003cstrong\u003eCentos 7\u003c/strong\u003e with \u003cstrong\u003eGCC 8\u003c/strong\u003e\u003cbr\u003e\nTo enable GCC 8,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; source /opt/rh/devtoolset-8/enable\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2019/3/1\u003cbr\u003e\nOpenMPI is added to \u003ccode\u003e$PATH\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e2019/3/11\u003cbr\u003e\nOpenMPI is updated to version 2.1.6\u003c/p\u003e\n\u003cp\u003e2019/4/12\u003cbr\u003e\nBoost 1.70.0 in added\u003c/p\u003e\n\u003cp\u003e2019/7/19\u003cbr\u003e\n\u003cdel\u003ePython 2 and 3 are updated to version 2.7.16 and version 3.7.4\u003c/del\u003e\u003cbr\u003e\nOpenMPI is updated to version 4.0.1\u003c/p\u003e\n\u003cp\u003e2019/7/21\u003cbr\u003e\n\u003cdel\u003eFew Python packages are added\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e2019/7/22\u003cbr\u003e\n\u003cdel\u003eFew corrections are made including Python\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e2019/7/23\u003cbr\u003e\nPythons are replaced with packages\u003cbr\u003e\nTo enable Python 2.7.16,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; source /opt/rh/python27/enable\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSystem version of python is 3.6.8\u003c/p\u003e\n\u003cp\u003e2019/7/30\u003cbr\u003e\ndevtoolset-7 GCC is added (some software can\u0027t be built with GCC 8)\u003c/p\u003e\n\u003cp\u003e2019/11/9\u003cbr\u003e\nCMake 3.15.5 is added\u003c/p\u003e\n\u003cp\u003e2019/11/22\u003cbr\u003e\nOpenMPI is downgraded to 1.10.1 to match version on ACI\u003c/p\u003e\n\u003cp\u003e2020/2/12\u003cbr\u003e\nBoost is upgraded to 1.72.0 and CMake is upgraded to 3.16.4\u003c/p\u003e\n\u003cp\u003e2020/3/2\u003cbr\u003e\nGPU version is added\u003c/p\u003e\n\u003cp\u003e2020/9/21\u003cbr\u003e\nMinor updates are made (regarding libxkb)\u003c/p\u003e\n\u003cp\u003e2020/9/28\u003cbr\u003e\nRecipe for CUDA 9.1 is added (for FSL with CUDA)\u003c/p\u003e\n\u003cp\u003e2020/10/11\u003cbr\u003e\nBoost is upgraded to 1.74.0 and CMake is upgraded to 3.18.4\u003cbr\u003e\nR 4.0.3 is added (Curl 7.72.0 and XZ 5.2.5 are added for R)\u003cbr\u003e\nVirtualGL is downgraded to 2.5.2 to match system version\u003c/p\u003e\n\u003cp\u003e2020/10/18\u003cbr\u003e\nUDUNITS 2.2.26 is added\u003c/p\u003e\n\u003cp\u003e2020/10/20\u003cbr\u003e\nTix-devel, Tx-devel, TkInter-devel, LAPACK-devel, and BLAS-devel are added\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1603529584.0
  },
  {
    "data_format": 2,
    "description": "Centos 7 base image for ACI",
    "filenames": [
      "Singularity.gpu",
      "Singularity.test",
      "Singularity",
      "Singularity.cuda9.1"
    ],
    "full_name": "willgpaik/centos7_aci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos7_aci\" class=\"anchor\" href=\"#centos7_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos7_aci\u003c/h1\u003e\n\u003cp\u003eCentos 7 base image for ACI Singualarity recipe\u003cbr\u003e\nThis recipe may include unnecessary packages for certain software installation.\u003cbr\u003e\nSize of CPU-only container: ~1 GB\u003cbr\u003e\nSize of GPU container: ~2.6 GB\u003c/p\u003e\n\u003cp\u003eMore packages will be added in the future\u003c/p\u003e\n\u003cp\u003e2019/2/17\n\u003cstrong\u003eCentos 7\u003c/strong\u003e with \u003cstrong\u003eGCC 8\u003c/strong\u003e\u003cbr\u003e\nTo enable GCC 8,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; source /opt/rh/devtoolset-8/enable\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2019/3/1\u003cbr\u003e\nOpenMPI is added to \u003ccode\u003e$PATH\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e2019/3/11\u003cbr\u003e\nOpenMPI is updated to version 2.1.6\u003c/p\u003e\n\u003cp\u003e2019/4/12\u003cbr\u003e\nBoost 1.70.0 in added\u003c/p\u003e\n\u003cp\u003e2019/7/19\u003cbr\u003e\n\u003cdel\u003ePython 2 and 3 are updated to version 2.7.16 and version 3.7.4\u003c/del\u003e\u003cbr\u003e\nOpenMPI is updated to version 4.0.1\u003c/p\u003e\n\u003cp\u003e2019/7/21\u003cbr\u003e\n\u003cdel\u003eFew Python packages are added\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e2019/7/22\u003cbr\u003e\n\u003cdel\u003eFew corrections are made including Python\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e2019/7/23\u003cbr\u003e\nPythons are replaced with packages\u003cbr\u003e\nTo enable Python 2.7.16,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; source /opt/rh/python27/enable\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSystem version of python is 3.6.8\u003c/p\u003e\n\u003cp\u003e2019/7/30\u003cbr\u003e\ndevtoolset-7 GCC is added (some software can\u0027t be built with GCC 8)\u003c/p\u003e\n\u003cp\u003e2019/11/9\u003cbr\u003e\nCMake 3.15.5 is added\u003c/p\u003e\n\u003cp\u003e2019/11/22\u003cbr\u003e\nOpenMPI is downgraded to 1.10.1 to match version on ACI\u003c/p\u003e\n\u003cp\u003e2020/2/12\u003cbr\u003e\nBoost is upgraded to 1.72.0 and CMake is upgraded to 3.16.4\u003c/p\u003e\n\u003cp\u003e2020/3/2\u003cbr\u003e\nGPU version is added\u003c/p\u003e\n\u003cp\u003e2020/9/21\u003cbr\u003e\nMinor updates are made (regarding libxkb)\u003c/p\u003e\n\u003cp\u003e2020/9/28\u003cbr\u003e\nRecipe for CUDA 9.1 is added (for FSL with CUDA)\u003c/p\u003e\n\u003cp\u003e2020/10/11\u003cbr\u003e\nBoost is upgraded to 1.74.0 and CMake is upgraded to 3.18.4\u003cbr\u003e\nR 4.0.3 is added (Curl 7.72.0 and XZ 5.2.5 are added for R)\u003cbr\u003e\nVirtualGL is downgraded to 2.5.2 to match system version\u003c/p\u003e\n\u003cp\u003e2020/10/18\u003cbr\u003e\nUDUNITS 2.2.26 is added\u003c/p\u003e\n\u003cp\u003e2020/10/20\u003cbr\u003e\nTix-devel, Tx-devel, TkInter-devel, LAPACK-devel, and BLAS-devel are added\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1603227322.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Deformetrica on Centos 7",
    "filenames": [
      "Singularity"
    ],
    "full_name": "willgpaik/deformetrica_aci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deformetrica_aci\" class=\"anchor\" href=\"#deformetrica_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edeformetrica_aci\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for Deformetrica on Centos 7 for ACI-ICS clusters\u003c/p\u003e\n\u003cp\u003e2019/2/14\u003cbr\u003e\nAnaconda3 ver. 2018.12\u003cbr\u003e\nDeformetrica 4.1\u003cbr\u003e\nGUI can be used through EoD\u003c/p\u003e\n\u003cp\u003eCommands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; source activate deformetrica  \n\u0026gt; deformetrica  \nOr,  \n\u0026gt; deformetrica gui\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2020/9/21\u003cbr\u003e\nGPU support is added\u003cbr\u003e\nAnaconda, Python, and Deformetrica are updated\u003c/p\u003e\n\u003cp\u003e2020/10/9\u003cbr\u003e\nPyTorch and PyKeOps are added\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1602342657.0
  },
  {
    "data_format": 2,
    "description": "Singularity image running R tidyverse + some other libraries",
    "filenames": [
      "Singularity",
      "Singularity.3.6.3"
    ],
    "full_name": "tpall/singularity-tidyverse",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2366\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-tidyverse\" class=\"anchor\" href=\"#singularity-tidyverse\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity tidyverse\u003c/h2\u003e\n\u003cp\u003eThis will run R tidyverse + some other packages, like \u003cem\u003ehere\u003c/em\u003e, \u003cem\u003ereadxl\u003c/em\u003e, \u003cem\u003elubridate\u003c/em\u003e, \u003cem\u003ebookdown\u003c/em\u003e, etc.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1608284812.0
  },
  {
    "data_format": 2,
    "description": "w2l",
    "filenames": [
      "Singularity.gpu",
      "Singularity"
    ],
    "full_name": "klm122/w2l",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-w2l\" class=\"anchor\" href=\"#w2l\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ew2l\u003c/h1\u003e\n\u003cp\u003ew2l\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583870496.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for RaGOO (https://github.com/malonge/RaGOO)",
    "filenames": [
      "Singularity.1.01",
      "Singularity.1.02",
      "Singularity"
    ],
    "full_name": "powerPlant/ragoo-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2341\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the RaGOO tool to order and orient genome assembly contigs via Minimap2 alignments to a reference genome\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550774761.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for SWAN (http://bitbucket.org/charade/swan)",
    "filenames": [
      "Singularity.3516c2f"
    ],
    "full_name": "powerPlant/swan-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2354\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the SWAN tool for SV detection\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550114140.0
  },
  {
    "data_format": 2,
    "description": "A thin Singularity image used as an alternative to Proot to wrap applications in an arbitrary file system.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "OSC/centos7-launcher",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos7-launcher\" class=\"anchor\" href=\"#centos7-launcher\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos7-launcher\u003c/h1\u003e\n\u003cp\u003eA Singularity image used wrap applications RStudio \u003ccode\u003erserver\u003c/code\u003e instances in an arbitrary file system for use with \u003ca href=\"http://openondemand.org/\" rel=\"nofollow\"\u003eOnDemand\u003c/a\u003e. Tested as compatible with Singularity 2.x and 3.x.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage:\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-2x\" class=\"anchor\" href=\"#singularity-2x\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity 2.x\u003c/h3\u003e\n\u003cp\u003eTODO...\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-3x\" class=\"anchor\" href=\"#singularity-3x\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity 3.x\u003c/h3\u003e\n\u003cp\u003eTODO...\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1550176998.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for MAPGD (https://github.com/LynchLab/MAPGD)",
    "filenames": [
      "Singularity",
      "Singularity.0.4.38-d3edee2"
    ],
    "full_name": "powerPlant/mapgd-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2319\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the MAPGD series of related programs for the analysis of low coverage population genomic data or for the analysis of pooled data\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1549853299.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for DIAMOND (https://github.com/bbuchfink/diamond)",
    "filenames": [
      "Singularity.v0.9.21",
      "Singularity",
      "Singularity.v0.9.20",
      "Singularity.v0.9.22",
      "Singularity.v0.9.17",
      "Singularity.v0.9.15",
      "Singularity.v0.9.19",
      "Singularity.v0.9.18",
      "Singularity.v0.9.24",
      "Singularity.v0.9.16",
      "Singularity.v0.9.23"
    ],
    "full_name": "powerPlant/diamond-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2322\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the DIAMOND Accelerated BLAST compatible local sequence aligner\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1549857110.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for pcl (https://github.com/PointCloudLibrary/pcl)",
    "filenames": [
      "Singularity.1.9.1",
      "Singularity",
      "Singularity.1.9.0",
      "Singularity.1.8.1"
    ],
    "full_name": "powerPlant/pcl-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2329\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the pcl Point Cloud Library\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550093426.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for crema (https://github.com/gbgolding/crema)",
    "filenames": [
      "Singularity",
      "Singularity.fe4cf7a"
    ],
    "full_name": "powerPlant/crema-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2320\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the crema tool to classify RNAs by Ensemble Machine learning Algorithms\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550200930.0
  },
  {
    "data_format": 2,
    "description": "Snakemake pipeline to run analysis for the Illumina vs. Nanopore comparison.",
    "filenames": [
      "analysis/assembly/containers/Singularity.canu"
    ],
    "full_name": "mbhall88/head_to_head_pipeline",
    "latest_release": null,
    "readme": "\u003cp\u003eThis repository holds the pipelines/scripts used for our project analysing Illumina\nand Nanopore for Mtb drug resistance calling and epidemiological clustering.\u003c/p\u003e\n\u003cp\u003eIt is currently in progress.\u003c/p\u003e\n\u003cp\u003eSee subdirectories for more specific information about different pipelines.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"analysis/assembly\"\u003eAssembly\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/mbhall88/tubby\"\u003eBasecall training\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"data/H37Rv_PRG\"\u003eH37Rv PRG construction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"data/QC\"\u003eQuality Control\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"analysis/baseline_variants\"\u003eBaseline variant analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"analysis/pandora_variants\"\u003ePandora variant analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"analysis/transmission_clustering\"\u003eTransmission clustering\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"analysis/resistance_prediction\"\u003eDrug Resistance Prediction\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626135389.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for vg (https://github.com/vgteam/vg)",
    "filenames": [
      "Singularity.1.10.0",
      "Singularity.1.13.0",
      "Singularity",
      "Singularity.1.12.1",
      "Singularity.1.9.0",
      "Singularity.1.11.0",
      "Singularity.1.8.0",
      "Singularity.1.12.0"
    ],
    "full_name": "powerPlant/vg-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2311\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the vg tools for working with genome variation graphs\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1549578706.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for angsd (https://github.com/ANGSD/angsd)",
    "filenames": [
      "Singularity.0.923",
      "Singularity",
      "Singularity.0.925",
      "Singularity.0.919",
      "Singularity.0.917",
      "Singularity.0.921",
      "Singularity.0.922",
      "Singularity.0.918"
    ],
    "full_name": "powerPlant/angsd-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2300\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the angsd program for analysing NGS data\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549507443.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for 3D DNA (https://github.com/theaidenlab/3d-dna)",
    "filenames": [
      "Singularity",
      "Singularity.180922"
    ],
    "full_name": "powerPlant/3d-dna-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2286\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the 3D de novo assembly (3D DNA) pipeline\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549335251.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.1.0.0-py36",
      "Singularity.1.3.1-py36"
    ],
    "full_name": "arcsUVA/pytorch",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pytorch\" class=\"anchor\" href=\"#pytorch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epytorch\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1573410610.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.0.8.0"
    ],
    "full_name": "arcsUVA/caffe2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-caffe2\" class=\"anchor\" href=\"#caffe2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecaffe2\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1550983020.0
  },
  {
    "data_format": 2,
    "description": "singularity scripts for cellprofiler",
    "filenames": [
      "Singularity.2.2.0",
      "Singularity.3.1.8",
      "Singularity.3.0.0"
    ],
    "full_name": "arcsUVA/cellprofiler",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2270\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the Stacks software pipeline for building loci from short-read sequences\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1556734065.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for OpenDroneMap (https://www.opendronemap.org/)",
    "filenames": [
      "Singularity",
      "Singularity.0.4.1",
      "Singularity.0.4.0"
    ],
    "full_name": "powerPlant/opendronemap-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2266\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the OpenDroneMap Drone Mapping Software\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549336324.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for Racon (https://github.com/isovic/racon/)",
    "filenames": [
      "Singularity.1.3.0",
      "Singularity",
      "Singularity.1.4.7",
      "Singularity.1.3.2",
      "Singularity.1.4.3",
      "Singularity.1.4.2",
      "Singularity.1.3.1",
      "Singularity.1.3.3",
      "Singularity.1.4.0"
    ],
    "full_name": "powerPlant/racon-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2269\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the Racon consensus module for raw de novo DNA assembly of long uncorrected reads\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1590711591.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for EddyPro Engine (https://github.com/LI-COR/eddypro-engine)",
    "filenames": [
      "Singularity",
      "Singularity.5.2.1",
      "Singularity.6.2.1",
      "Singularity.5.1.1",
      "Singularity.6.0.0",
      "Singularity.6.1.0",
      "Singularity.6.2.0",
      "Singularity.5.2.0"
    ],
    "full_name": "powerPlant/eddypro-engine-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2272\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the EddyPro eddy covariance data processing software\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549923689.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for QIIME 2 (https://docs.qiime2.org/)",
    "filenames": [
      "Singularity.2019.10",
      "Singularity.2020.8",
      "Singularity.2020.2",
      "Singularity.2019.4",
      "Singularity.2018.11",
      "Singularity.2019.7",
      "Singularity",
      "Singularity.2019.1-picrust2",
      "Singularity.2019.7-picrust2",
      "Singularity.2019.1",
      "Singularity.2018.2",
      "Singularity.2020.6"
    ],
    "full_name": "powerPlant/qiime2-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2268\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the QIIME 2 microbiome analysis package\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1603334435.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for Portcullis (https://github.com/maplesond/portcullis)",
    "filenames": [
      "Singularity",
      "Singularity.1.1.0",
      "Singularity.1.1.1",
      "Singularity.1.1.2"
    ],
    "full_name": "powerPlant/portcullis-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2267\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for Portcullis, a program for PORTable CULLing of Invalid Splice junctions from pre-aligned RNA-seq data\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549336366.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for cdo (https://www.mpimet.mpg.de/cdo/)",
    "filenames": [
      "Singularity",
      "Singularity.1.7.0",
      "Singularity.1.9.5",
      "Singularity.1.9.3"
    ],
    "full_name": "powerPlant/cdo-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2262\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the Climate Data Operators toolset\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549335527.0
  },
  {
    "data_format": 2,
    "description": "sparkle planning challenge",
    "filenames": [
      "Singularity"
    ],
    "full_name": "hejm37/sysu-planner",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sysu-planner\" class=\"anchor\" href=\"#sysu-planner\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esysu-planner\u003c/h1\u003e\n\u003cp\u003eThe SYSU-Planner is a two-stage planner designed to solve classical planning problems. It first performs the 1-BFWS (\u003ca href=\"https://people.eng.unimelb.edu.au/nlipovetzky/papers/aaai17-BFWS-novelty-exploration.pdf\" rel=\"nofollow\"\u003eNir and Hector 2017\u003c/a\u003e) with very fast speed. If it fails to find a solution, it will then perform a modified online refinement algorithm named \u003ca href=\"http://ada.liacs.nl/events/sparkle-planning-19/documents/solver_description/SYSU-planner-description.pdf\" rel=\"nofollow\"\u003eForward-RHC\u003c/a\u003e (see also \u003ca href=\"https://ipc2018-classical.bitbucket.io/planner-abstracts/team8.pdf\" rel=\"nofollow\"\u003eMaximilian and Jorg 2018\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-and-run-with-container\" class=\"anchor\" href=\"#build-and-run-with-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild and run with container\u003c/h2\u003e\n\u003cp\u003eUsing the planner with \u003ca href=\"https://sylabs.io/docs/#singularity\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e is rather simple. First install Singularity following \u003ca href=\"https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003ethis guide\u003c/a\u003e. Then run the following script in CLI and you will have the plan file \u003cem\u003esas_plan\u003c/em\u003e under \u003cem\u003e$RUNDIR\u003c/em\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build planner.img sysu-planner/Singularity\nmkdir rundir\ncp path/to/domain.pddl rundir\ncp path/to/problem.pddl rundir\nRUNDIR=\"$(pwd)/rundir\"\nDOMAIN=\"$RUNDIR/domain.pddl\"\nPROBLEM=\"$RUNDIR/problem.pddl\"\nPLANFILE=\"$RUNDIR/sas_plan\"\nsingularity run -C -H $RUNDIR planner.img $DOMAIN $PROBLEM $PLANFILE $COSTBOUND\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-supported-problems\" class=\"anchor\" href=\"#supported-problems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupported problems\u003c/h3\u003e\n\u003cp\u003eThe formulation of supported problems is the same as \u003ca href=\"https://ipc2018-classical.bitbucket.io/#pddl\" rel=\"nofollow\"\u003eIPC 2018\u003c/a\u003e. We also provide a set of supported domains and problems in \u003ca href=\"https://github.com/hejm37/benchmark-domains\"\u003ebenchmark-domains\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes-on-playing-with-the-source-code\" class=\"anchor\" href=\"#notes-on-playing-with-the-source-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes on playing with the source code\u003c/h2\u003e\n\u003cp\u003eThe source code of the planner contains two part:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBFWS-public and its dependency, LAPKT-public\u003c/li\u003e\n\u003cli\u003efast-downward-conjunctions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen planner should be invoked in the fast-downward-conjunctions part (using --dual option and it will call BFWS-public/fd-version/bfws.py to perform 1-BFWS, see \u003ca href=\"https://github.com/hejm37/sysu-planner/blob/master/Singularity\"\u003ethe Singularity script\u003c/a\u003e for more details).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-potential-failures\" class=\"anchor\" href=\"#potential-failures\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePotential Failures\u003c/h3\u003e\n\u003cp\u003eIf the above build has failed, it may appears to be a cmake cache fail. In this case, remove the \u003cem\u003ebuilds\u003c/em\u003e (if it exists) directory under fast-downward-conjunctions and rerun the singularity command shall solve the problem.\u003c/p\u003e\n\u003cp\u003eOr it may appears to be a scons build fail. In this case, remove all the \u003cem\u003e.sconsign.dblite\u003c/em\u003e files under the directory shall solve the problem.\u003c/p\u003e\n\u003cp\u003eBoth cases would occur if the planner was built outside a container.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1563536767.0
  },
  {
    "data_format": 2,
    "description": "TensorFlow Singularity recipes.",
    "filenames": [
      "Singularity.1.12.0-py27",
      "Singularity.1.14.0-py36",
      "Singularity.1.13.0-py36",
      "Singularity.1.6.0-py36",
      "Singularity.1.12.0-py36",
      "Singularity.1.13.1-py36",
      "Singularity.1.6.0-py27"
    ],
    "full_name": "arcsUVA/tensorflow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tensorflow\" class=\"anchor\" href=\"#tensorflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etensorflow\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2235\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003cbr\u003e\nTensorFlow Singularity recipes.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1567631554.0
  },
  {
    "data_format": 2,
    "description": "Omero client Singularity recipes.",
    "filenames": [
      "Singularity.5.4.10",
      "Singularity.5.4.0"
    ],
    "full_name": "arcsUVA/omero-client",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-omero-client\" class=\"anchor\" href=\"#omero-client\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eomero-client\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2227\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003cbr\u003e\nOmero client Singularity recipes\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1557760203.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for Qt5 on Centos 7 and Ubuntu 16.04",
    "filenames": [
      "Singularity.ubuntu",
      "Singularity",
      "Singularity.qt5",
      "dsistudio_mrtrix3/Singularity.dsi_mrtrix3_ants",
      "dsistudio_mrtrix3/Singularity.dsi_mrtrix3_fsl",
      "dsistudio_mrtrix3/Singularity.dsi_mrtrix3_centos8",
      "dsistudio_mrtrix3/Singularity.dsi_mrtrix3_ants_fsl_fmriprep",
      "dsistudio_mrtrix3/Singularity.dsi_mrtrix3"
    ],
    "full_name": "willgpaik/qt5_aci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-qt5_aci\" class=\"anchor\" href=\"#qt5_aci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eqt5_aci\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for Qt5 on Centos 7 and Ubuntu 16.04 For ICS\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE: DO NOT rebuild \"Singularity.dsi_mrtrix3\" image.\u003c/strong\u003e\u003cbr\u003e\n(Last successful build was Mar 12 2019)\u003c/p\u003e\n\u003cp\u003eSingularity recipe for DSI Studio and MRtrix3 is updated on \u003cstrong\u003edsistudio_mrtrix3\u003c/strong\u003e folder\u003c/p\u003e\n\u003cp\u003eIf you want to install DSI Studio and MRtrix3 on Basic Qt5 Container,\u003cbr\u003e\ndownlaod \"dsistudio_mrtrix3_install.sh\" to preferred location\nand follow commands inside Singularity environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; chmod +x dsistudio_mrtrix3_install.sh  \n\u0026gt; ./dsistudio_mrtrix3_install.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e2019/2/21\u003cbr\u003e\nUnable to use \u003cstrong\u003eGCC 8.2.1\u003c/strong\u003e due to build failure =\u0026gt; Going back to \u003cstrong\u003eGCC 7.3.1\u003c/strong\u003e\u003cbr\u003e\n(Failed to resolve the issue at this moment)\u003c/p\u003e\n\u003cp\u003e\u003cdel\u003e2019/5/13\u003cbr\u003e\nUpdated dsistudio_mrtrix3_install.sh due to Qt version issue\u003cbr\u003e\n(Requires Qt 5.12.2 or above: \u003ca href=\"https://github.com/frankyeh/DSI-Studio/issues/34\"\u003ehttps://github.com/frankyeh/DSI-Studio/issues/34\u003c/a\u003e)\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e2019/5/24\u003cbr\u003e\nReverted changes made on 2019/5/13\u003c/p\u003e\n\u003cp\u003e2019/6/24\u003cbr\u003e\n\u003cdel\u003eNewer version qt5 installation recipe added (in progress)\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e2019/7/22\u003cbr\u003e\nQt is updated to 5.12 with Qt Charts (for DSI Studio)\u003c/p\u003e\n\u003cp\u003e2019/7/24\u003cbr\u003e\nQt SVG is added (for MRtrix 3)\u003cbr\u003e\n32-bit EoD graphics libraries are disable (to aviod warnings)\u003c/p\u003e\n\u003cp\u003e2019/7/29\u003cbr\u003e\nNVIDIA driver is added to DSI Studio MRtrix3 container\u003c/p\u003e\n\u003cp\u003e2019/11/10\u003cbr\u003e\nQt version 5.12.5 is used\u003c/p\u003e\n\u003cp\u003e2020/4/24\u003cbr\u003e\nUbuntu 16.04 version added with Qt 5.14.2\u003c/p\u003e\n\u003cp\u003e2020/6/20\u003cbr\u003e\nQt5 container is updated to have nvidia driver\u003c/p\u003e\n\u003cp\u003e2020/7/27\u003cbr\u003e\nUbuntu container is updated to have NVIDIA driver (Ubuntu 16.04 based)\u003c/p\u003e\n\u003cp\u003e2020/9/28\u003cbr\u003e\nQt5 container is updated to use CUDA 9.1 version (for FSL with CUDA)\u003cbr\u003e\n(Reference: \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GPU\" rel=\"nofollow\"\u003ehttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/GPU\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e2020/10/20\u003cbr\u003e\nQt5X11Extras is added to the Qt5 recipe\u003cbr\u003e\n(Ubuntu container will not be updated unless necessary)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1618004326.0
  },
  {
    "data_format": 2,
    "description": "Container to run various Game AI workloads",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sbutcher/minigym-container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-minigym-container\" class=\"anchor\" href=\"#minigym-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eminigym-container\u003c/h1\u003e\n\u003cp\u003eContainer to run various Game AI workloads\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1548062559.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "djarecka/tmp_nipype_tut",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nipype-tutorial-notebooks\" class=\"anchor\" href=\"#nipype-tutorial-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNipype Tutorial Notebooks\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/miykael/nipype_tutorial/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64\" alt=\"CircleCi\" data-canonical-src=\"https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/issues/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub issues\" data-canonical-src=\"https://img.shields.io/github/issues/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/pulls/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub pull-requests\" data-canonical-src=\"https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub contributors\" data-canonical-src=\"https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/commits/master\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub Commits\" data-canonical-src=\"https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/archive/master.zip\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub size\" data-canonical-src=\"https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/miykael/nipype_tutorial/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://hits.dwyl.io/miykael/nipype_tutorial\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub HitCount\" data-canonical-src=\"http://hits.dwyl.io/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is the Nipype Tutorial in Jupyter Notebook format. You can access the tutorial in two ways:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"https://miykael.github.io/nipype_tutorial/\" rel=\"nofollow\"\u003eNipype Tutorial Homepage\u003c/a\u003e: This website contains a static, read-only version of all the notebooks.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html\" rel=\"nofollow\"\u003eNipype Tutorial Docker Image\u003c/a\u003e: This guide explains how to use Docker to run the notebooks interactively on your own computer. The nipype tutorial docker image is the best interactive way to learn Nipype.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-feedback-help--support\" class=\"anchor\" href=\"#feedback-help--support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeedback, Help \u0026amp; Support\u003c/h1\u003e\n\u003cp\u003eIf you want to help with this tutorial or have any questions, feel free to fork the repo of the \u003ca href=\"https://github.com/miykael/nipype_tutorial\"\u003eNotebooks\u003c/a\u003e or interact with other contributors on the slack channel \u003ca href=\"https://brainhack.slack.com/messages/nipype/\" rel=\"nofollow\"\u003ebrainhack.slack.com/messages/nipype/\u003c/a\u003e. If you have any questions or found a problem, open a new \u003ca href=\"https://github.com/miykael/nipype_tutorial/issues\"\u003eissue on github\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-thanks-and-acknowledgment\" class=\"anchor\" href=\"#thanks-and-acknowledgment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks and Acknowledgment\u003c/h1\u003e\n\u003cp\u003eA huge thanks to \u003ca href=\"https://github.com/mwaskom\"\u003eMichael Waskom\u003c/a\u003e, \u003ca href=\"https://github.com/oesteban\"\u003eOscar Esteban\u003c/a\u003e, \u003ca href=\"https://github.com/chrisfilo\"\u003eChris Gorgolewski\u003c/a\u003e and \u003ca href=\"https://github.com/satra\"\u003eSatrajit Ghosh\u003c/a\u003e for their input to this tutorial! And a huge thanks to \u003ca href=\"https://github.com/djarecka/\"\u003eDorota Jarecka\u003c/a\u003e who updated this tutorial to Python 3 and is helping me with keeping this tutorial updated and running!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1547566090.0
  },
  {
    "data_format": 2,
    "description": "Bioinformatic tools in a singularity container",
    "filenames": [
      "containers/Singularity.etoki",
      "containers/Singularity",
      "containers/Singularity.lyveset"
    ],
    "full_name": "EnriqueDoster/sing_biotools",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sing_biotools\" class=\"anchor\" href=\"#sing_biotools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esing_biotools\u003c/h1\u003e\n\u003cp\u003eBioinformatic tools in a singularity container\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1606287922.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for Scanfold",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ResearchIT/Scanfold",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\" alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b7af09ab5d3e54feb3acda4c7b70aef9718f2928a49a50c92ea6ce95e96b2f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e657874666c6f772d254532253839254135302e32352e312d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/Nextflow-%E2%89%A50.25.1-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe goal of many metagenomics studies is to characterize the content and relative abundance of sequences of interest from the DNA of a given sample or set of samples. You may want to know what is contained within your sample or how abundant a given sequence is relative to another.\u003c/p\u003e\n\u003cp\u003eOften, metagenomics is performed when the answer to these questions must be obtained for a large number of targets where techniques like multiplex PCR and other targeted methods would be too cumbersome to perform. AmrPlusPlus can process the raw data from the sequencer, identify the fragments of DNA, and count them. It also provides a count of the polymorphisms that occur in each DNA fragment with respect to the reference database.\u003c/p\u003e\n\u003cp\u003eAdditionally, you may want to know if the depth of your sequencing (how many reads you obtain that are on target) is high enough to identify rare organisms (organisms with low abundance relative to others) in your population. This is referred to as rarefaction and is calculated by randomly subsampling your sequence data at intervals between 0% and 100% in order to determine how many targets are found at each depth. AmrPlusPlus can perform this analysis as well.\u003c/p\u003e\n\u003cp\u003eWith AmrPlusPlus, you will obtain count files for each sample that can be combined into a count matrix and analyzed using any statistical and mathematical techniques that can operate on a matrix of observations.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-more-information\" class=\"anchor\" href=\"#more-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMore Information\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/requirements.md\"\u003eSoftware Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/usage.md\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/configuration.md\"\u003eConfiguration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/output.md\"\u003eOutput\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/dependencies.md\"\u003eDependencies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/contact.md\"\u003eContact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description-of-scripts\" class=\"anchor\" href=\"#description-of-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription of scripts\u003c/h2\u003e\n\u003cp\u003emain_qiime2.nf\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main_qiime2.nf --reads \"/s/angus/index/projs/mega_tylan/concat_16S_LN/raw_data/*_{1,2}.fq\" --output XIT_LN_qiime2 -profile local --metadata /media/AngusWorkspace/run_Jake/LN_metadata.tsv --classifier /media/AngusWorkspace/run_Jake/bioinformatic-nextflow-pipelines/gg-13-8-99-515-806-nb-classifier.qza -resume --threads 25\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1570729149.0
  },
  {
    "data_format": 2,
    "description": "Collection of bioinformatic pipelines written in nextflow",
    "filenames": [
      "containers/Singularity",
      "containers/Singularity.RGI",
      "containers/Singularity.qiime2",
      "containers/Singularity.cfsansnp"
    ],
    "full_name": "EnriqueDoster/bioinformatic-nextflow-pipelines",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\" alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b7af09ab5d3e54feb3acda4c7b70aef9718f2928a49a50c92ea6ce95e96b2f7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4e657874666c6f772d254532253839254135302e32352e312d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/Nextflow-%E2%89%A50.25.1-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe goal of many metagenomics studies is to characterize the content and relative abundance of sequences of interest from the DNA of a given sample or set of samples. You may want to know what is contained within your sample or how abundant a given sequence is relative to another.\u003c/p\u003e\n\u003cp\u003eOften, metagenomics is performed when the answer to these questions must be obtained for a large number of targets where techniques like multiplex PCR and other targeted methods would be too cumbersome to perform. AmrPlusPlus can process the raw data from the sequencer, identify the fragments of DNA, and count them. It also provides a count of the polymorphisms that occur in each DNA fragment with respect to the reference database.\u003c/p\u003e\n\u003cp\u003eAdditionally, you may want to know if the depth of your sequencing (how many reads you obtain that are on target) is high enough to identify rare organisms (organisms with low abundance relative to others) in your population. This is referred to as rarefaction and is calculated by randomly subsampling your sequence data at intervals between 0% and 100% in order to determine how many targets are found at each depth. AmrPlusPlus can perform this analysis as well.\u003c/p\u003e\n\u003cp\u003eWith AmrPlusPlus, you will obtain count files for each sample that can be combined into a count matrix and analyzed using any statistical and mathematical techniques that can operate on a matrix of observations.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-more-information\" class=\"anchor\" href=\"#more-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMore Information\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/requirements.md\"\u003eSoftware Requirements\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/usage.md\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/configuration.md\"\u003eConfiguration\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/output.md\"\u003eOutput\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/dependencies.md\"\u003eDependencies\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/EnriqueDoster/bioinformatic-nextflow-pipelines/blob/master/docs/contact.md\"\u003eContact\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description-of-scripts\" class=\"anchor\" href=\"#description-of-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription of scripts\u003c/h2\u003e\n\u003cp\u003emain_qiime2.nf\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main_qiime2.nf --reads \"/s/angus/index/projs/mega_tylan/concat_16S_LN/raw_data/*_{1,2}.fq\" --output XIT_LN_qiime2 -profile local --metadata /media/AngusWorkspace/run_Jake/LN_metadata.tsv --classifier /media/AngusWorkspace/run_Jake/bioinformatic-nextflow-pipelines/gg-13-8-99-515-806-nb-classifier.qza -resume --threads 25\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619390181.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity"
    ],
    "full_name": "stevekm/bwa-bench",
    "latest_release": null,
    "readme": "",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1549319905.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "QE/Singularity.QuantumESPRESSO-6.3-intel-2018b-unrrc"
    ],
    "full_name": "UNR-HPC/singularity-recipes",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipes\" class=\"anchor\" href=\"#singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-recipes\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1544465938.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "dfornika/nf-core-cpo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-corecpo\" class=\"anchor\" href=\"#nf-corecpo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-core/cpo\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGenomic Analysis of Carbapenem Resistant Organisms\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nf-core/cpo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b4a4d26450e93f9c13ce85f059bb61ebe27051414d40e4f4ba81966ca0029a4/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f63706f2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nf-core/cpo.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/cpo\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4bc4e99ea4ca2a2f9b15fda9e4d3855153c0fd74431b920ed885080d46e0cc73/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f63706f2e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/cpo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe nf-core/cpo pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003enf-core/cpo was originally written by Dan Fornika.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1544054866.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity.petibm0.4.2-xenial",
      "singularity/Singularity.petibm0.5-xenial",
      "singularity/Singularity.petibm0.5.1-xenial"
    ],
    "full_name": "mesnardo/petibm-decoupledibpm",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-decoupled-immersed-boundary-projection-method-with-petibm\" class=\"anchor\" href=\"#decoupled-immersed-boundary-projection-method-with-petibm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDecoupled Immersed Boundary Projection Method with PetIBM\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/mesnardo/petibm-decoupledibpm/raw/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://cloud.docker.com/u/mesnardo/repository/docker/mesnardo/petibm-decoupledibpm\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b8d9674ae17bb539afa71ecc4169a1ee5a6a9242d8f9e12a10f4583093ba57c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d646f636b65722d2d6875622d696e666f726d6174696f6e616c2e737667\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/badge/hosted-docker--hub-informational.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3171\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity Hub\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-flow-over-a-stationary-circular-cylinder-re40-and-100\" class=\"anchor\" href=\"#flow-over-a-stationary-circular-cylinder-re40-and-100\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlow over a stationary circular cylinder ($Re=40$ and $100$)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"runs/cylinder2dRe40/189_markers/figures/wz_0005000.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/cylinder2dRe40/189_markers/figures/wz_0005000.png\" alt=\"cylinderRe40_vorticity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Vorticity contours around the cylinder at Reynolds number $40$. (Contour levels between $-3D/U_\\infty$ and $3D/U_\\infty$ with increments of $0.4$.)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/cylinder2dRe100/189_markers/figures/wz_0020000.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/cylinder2dRe100/189_markers/figures/wz_0020000.png\" alt=\"cylinderRe100_vorticity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Vorticity contours around the cylinder at Reynolds number $100$ after $200$ time units of flow simulation. (Contour levels between $-3D/U_\\infty$ and $3D/U_\\infty$ with increments of $0.4$.)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/cylinder2dRe40/189_markers/figures/cp_0005000.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/cylinder2dRe40/189_markers/figures/cp_0005000.png\" alt=\"cylinderRe40_pressure_coefficient\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Pressure coefficient along the upper and lower surfaces of the cylinder at Reynolds number $40$. We compare with the results from Li et al. (2016).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/cylinder2dRe100/189_markers/figures/pressure_coefficient.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/cylinder2dRe100/189_markers/figures/pressure_coefficient.png\" alt=\"cylinderRe100_pressure_coefficient\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Pressure coefficient along the upper and lower surfaces of the cylinder at Reynolds number $100$. We compare with the results from Li et al. (2016).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-flow-around-an-inline-oscillating-circular-cylinder-re100\" class=\"anchor\" href=\"#flow-around-an-inline-oscillating-circular-cylinder-re100\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlow around an inline oscillating circular cylinder ($Re=100$)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/algo1/figures/vorticity.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/algo1/figures/vorticity.png\" alt=\"oscillatingcylinderRe100_vorticity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Contours of the vorticity field around an inline oscillating cylinder at different phase angles ($\\phi = 2 \\pi f t$): $\\phi = 0^o$ (left) and $\\phi = 288^o$ (right). (Contour levels between $-20 U_m / D$ and $20 U_m / D$ using $30$ increments.)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/algo1/figures/pressure.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/algo1/figures/pressure.png\" alt=\"oscillatingcylinderRe100_pressure\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Contours of the pressure field around an inline oscillating cylinder at different phase angles ($\\phi = 2 \\pi f t$): $\\phi = 0^o$ (left) and $\\phi = 288^o$ (right). (Contour levels between $-1 \\rho U_m^2$ and $1 \\rho U_m^2$ using $50$ increments.)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/algo1/figures/velocity_profiles.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/algo1/figures/velocity_profiles.png\" alt=\"oscillatingcylinderRe100_velocity\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Profile of the velocity components ($u$: left, $v$: right) at four locations along the centerline for various phase angles $\\phi$.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/figures/drag_coefficient.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/figures/drag_coefficient.png\" alt=\"oscillatingcylinderRe100_drag_coefficient\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e History of the drag coefficient of the inline oscillating cylinder obtained using different algorithms. We also show zooms at early and developed stages.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/figures/drag_coefficient_dt.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/figures/drag_coefficient_dt.png\" alt=\"oscillatingcylinderRe100_drag_coefficient_dt\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"runs/oscillatingcylinderRe100/figures/drag_coefficient_dx.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/figures/drag_coefficient_dx.png\" alt=\"oscillatingcylinderRe100_drag_coefficient_dx\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e History of the drag coefficient obtained with Algorithm 1 for different time-step sizes and different grid sizes.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/figures/temporal_error.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/figures/temporal_error.png\" alt=\"oscillatingcylinderRe100_temporal_error\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Variations of the $L_\\infty$ and $L_2$ norm errors of the streamwise velocity as a function of the computational time-step size.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/figures/spatial_error.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/figures/spatial_error.png\" alt=\"oscillatingcylinderRe100_temporal_error\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Variations of the $L_\\infty$ and $L_2$ norm errors of the streamwise velocity as a function of the computational grid spacing.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingcylinderRe100/figures/drag_coefficient_lag.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingcylinderRe100/figures/drag_coefficient_lag.png\" alt=\"oscillatingcylinderRe100_cd_lag\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e History of the drag coefficient using Algorithm 3 with force-prediction scheme 3. We compared the history obtained with different Lagrangian mesh resolutions: $N_b = 500$ Lagrangian markers on the boundary and $N_b = 202$ markers (the latter one corresponding to the same resolution as the Eulerian background grid).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-flow-around-an-impulsively-started-circular-cylinder-re40\" class=\"anchor\" href=\"#flow-around-an-impulsively-started-circular-cylinder-re40\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFlow around an impulsively started circular cylinder (Re=40)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"runs/translatingcylinder2dRe40/figures/drag_coefficients.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/translatingcylinder2dRe40/figures/drag_coefficients.png\" alt=\"translatingcylinder2dRe40_cd\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e History of the drag coefficient of the impulsively started cylinder. Comparison with the analytical solution of Bar-Lev \u0026amp; Yang (1997) and the numerical results from Taira \u0026amp; Colonius (2007).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/translatingcylinder2dRe40/dt=0.0005/figures/vorticity.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/translatingcylinder2dRe40/dt=0.0005/figures/vorticity.png\" alt=\"translatingcylinder2dRe40_wz\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Vorticity contours around the impulsively started circular cylinder at $t=1.0$ (left) and $t=3.5$ (right). Contour levels between $-3 \\omega_z D / U_o$ and $3 \\omega_z D / U_o$ with increments of $0.4$.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"runs/translatingcylinder2dRe40/figures/recirculation_lengths.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/translatingcylinder2dRe40/figures/recirculation_lengths.png\" alt=\"translatingcylinder2dRe40_lw\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e History of the recirculation length measured in the reference frame of the impulsively start cylinder at Reynolds number 40 and for different time-step sizes.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-three-dimensional-flow-around-an-inline-oscillating-sphere-re7854\" class=\"anchor\" href=\"#three-dimensional-flow-around-an-inline-oscillating-sphere-re7854\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThree-dimensional flow around an inline oscillating sphere ($Re=78.54$)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"runs/oscillatingsphere/figures/pressure.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"runs/oscillatingsphere/figures/pressure.png\" alt=\"sphere_pressure\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cstrong\u003eFigure:\u003c/strong\u003e Contours of the pressure field in the $x$/$y$ at $z=0$ at three phase angles. Contour levels between $-2 p / \\rho U_m^2$ and $2 p / \\rho U_m^2$ with $30$ increments.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1581529613.0
  },
  {
    "data_format": 2,
    "description": "Snakemake workflow for analysis and assembly of viral genomes from IonTorrent AmpliSeq data.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "peterk87/viral-ampliseq-assembly",
    "latest_release": "v1.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-snakemake-workflow-viral-ampliseq-assembly\" class=\"anchor\" href=\"#snakemake-workflow-viral-ampliseq-assembly\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSnakemake workflow: viral-ampliseq-assembly\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://snakemake.bitbucket.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/de7b3ae9d2ddd7970750ed14a267d738217987e5635a19380de6f3b2ec3216e6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e352e342d627269676874677265656e2e737667\" alt=\"Snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.5.4-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.com/peterk87/viral-ampliseq-assembly\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9ca62ba99cb6a38032432759aa450c99bf81b9671bab9e21e2492c47bf7cf065/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f766972616c2d616d706c697365712d617373656d626c792e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/viral-ampliseq-assembly.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3359\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://snakemake.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003eSnakemake\u003c/a\u003e workflow for analysis and assembly of viral genomes such as Classical Swine Fever Virus (\u003ca href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=11096\" rel=\"nofollow\"\u003eCSFV\u003c/a\u003e) from IonTorrent AmpliSeq data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePreprocessing\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDuplicate reads were removed using \u003ca href=\"https://broadinstitute.github.io/picard/\" rel=\"nofollow\"\u003ePicard\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReads were trimmed with \u003ca href=\"http://www.usadellab.org/cms/?page=trimmomatic\" rel=\"nofollow\"\u003eTrimmomatic\u003c/a\u003e prior to \u003ca href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\u003eSPAdes\u003c/a\u003e assembly\u003c/li\u003e\n\u003cli\u003eBAM file stats computed using \u003ca href=\"https://samtools.github.io/\" rel=\"nofollow\"\u003eSamtools\u003c/a\u003e (coverage depth, extent, extent per genome, # of reads mapped)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eReference Genome Selection\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDownloading of all Classical swine fever virus (\u003ca href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=11096\" rel=\"nofollow\"\u003eCSFV\u003c/a\u003e) (or FMDV, Ebola, Zika) virus genomes from \u003ca href=\"https://www.ncbi.nlm.nih.gov/books/NBK25501/\" rel=\"nofollow\"\u003eNCBI Entrez API\u003c/a\u003e using \u003ca href=\"https://biopython.org/\" rel=\"nofollow\"\u003eBioPython\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://mash.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003eMash\u003c/a\u003e screen of deduplicated reads against all reference genomes with sketch size of 10000 and sketch k-mer size of 16, sorting by Mash screen identity to find top reference genome for read mapping and variant calling\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRead Mapping \u0026amp; Variant Calling\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRead mapping with \u003ca href=\"https://github.com/lh3/bwa\"\u003eBWA MEM\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRemoval of duplicate reads with \u003ca href=\"https://samtools.github.io/\" rel=\"nofollow\"\u003eSamtools\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eVariant calling with \u003ca href=\"https://github.com/ekg/freebayes\"\u003eFreeBayes\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://snpeff.sourceforge.net/SnpEff.html\" rel=\"nofollow\"\u003eSnpEff\u003c/a\u003e was used to predict and report variant effects using reference genome annotation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDe Novo Assembly\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\u003eSPAdes\u003c/a\u003e de novo assembly of trimmed deduplicated reads.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://quast.sourceforge.net/quast.html\" rel=\"nofollow\"\u003eQUAST\u003c/a\u003e quality assessment of assemblies\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eQuality Control\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://multiqc.info/\" rel=\"nofollow\"\u003eMultiQC\u003c/a\u003e interactive report of \u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\u003eFastQC\u003c/a\u003e, \u003ca href=\"https://samtools.github.io/\" rel=\"nofollow\"\u003eSamtools\u003c/a\u003e, \u003ca href=\"http://quast.sourceforge.net/quast.html\" rel=\"nofollow\"\u003eQUAST\u003c/a\u003e, \u003ca href=\"http://snpeff.sourceforge.net/SnpEff.html\" rel=\"nofollow\"\u003eSnpEff\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePhylogenetic Tree\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePhylogenetic tree constructed with \u003ca href=\"http://www.iqtree.org/\" rel=\"nofollow\"\u003eIQ-TREE\u003c/a\u003e (or \u003ca href=\"http://bioinformatics.hungry.com/clearcut/\" rel=\"nofollow\"\u003eClearcut\u003c/a\u003e if a quick and dirty tree is okay)\u003c/li\u003e\n\u003cli\u003eInteractive HTML phylogenetic tree visualization with \u003ca href=\"http://phylocanvas.org/\" rel=\"nofollow\"\u003ePhyloCanvas\u003c/a\u003e using \u003ca href=\"https://github.com/peterk87/shiptv\"\u003eshiptv\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors\" class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePeter Kruczkiewicz (@peterk87)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-0-install-pre-requisites\" class=\"anchor\" href=\"#step-0-install-pre-requisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 0: Install pre-requisites\u003c/h3\u003e\n\u003cp\u003eRunning this workflow with \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e is recommended, but you can use \u003ca href=\"https://conda.io/en/latest/\" rel=\"nofollow\"\u003eConda\u003c/a\u003e if you prefer. The Singularity image will come with all the dependencies bundled together in a single file.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-install-singularity-recommended\" class=\"anchor\" href=\"#install-singularity-recommended\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e (recommended)\u003c/h4\u003e\n\u003cp\u003eFollow the instructions for installing Singularity \u003ca href=\"https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-start\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-setup-and-activate-the-conda-environment-if-not-using-singularity-optional\" class=\"anchor\" href=\"#setup-and-activate-the-conda-environment-if-not-using-singularity-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup and activate the \u003ca href=\"https://conda.io/en/latest/\" rel=\"nofollow\"\u003eConda\u003c/a\u003e environment if not using \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e (optional)\u003c/h4\u003e\n\u003cp\u003eInstall \u003ca href=\"https://conda.io/en/latest/\" rel=\"nofollow\"\u003eConda\u003c/a\u003e if you haven\u0027t already following \u003ca href=\"https://conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003ethese instructions\u003c/a\u003e and setup the \u003ca href=\"https://bioconda.github.io/user/install.html#set-up-channels\" rel=\"nofollow\"\u003eBioConda channel\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eDownload or \u003ccode\u003egit clone\u003c/code\u003e this repo\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/peterk87/viral-ampliseq-assembly.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e viral-ampliseq-assembly\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e create a conda environment named \"viral-ampliseq-assembly-1.0.0\"\u003c/span\u003e\nconda env create -f environment.yml\nconda activate viral-ampliseq-assembly-1.0.0\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e install snakemake into this env\u003c/span\u003e\nconda install -y snakemake\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e run Snakemake on the test directory\u003c/span\u003e\nsnakemake --directory test/\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1-install-workflow\" class=\"anchor\" href=\"#step-1-install-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1: Install workflow\u003c/h3\u003e\n\u003cp\u003eIf you simply want to use this workflow, download and extract the \u003ca href=\"https://github.com/peterk87/viral-ampliseq-assembly/releases\"\u003elatest release\u003c/a\u003e.\nIf you intend to modify and further develop this workflow, fork this repository. Please consider providing any generally applicable modifications via a pull request.\u003c/p\u003e\n\u003cp\u003eIn any case, if you use this workflow in a paper, don\u0027t forget to give credits to the authors by citing the URL of this repository and, if available, its DOI (see above).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2-configure-workflow\" class=\"anchor\" href=\"#step-2-configure-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2: Configure workflow\u003c/h3\u003e\n\u003cp\u003eCreate an analysis directory, copy and modify the example \u003ccode\u003econfig.yaml\u003c/code\u003e and \u003ccode\u003esamples.tsv\u003c/code\u003e files to suit your needs.\u003c/p\u003e\n\u003cp\u003ee.g.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir ~/my-ampliseq-analysis\ncp viral-ampliseq-assembly/config.yaml ~/my-ampliseq-analysis/\ncp viral-ampliseq-assembly/samples.tsv ~/my-ampliseq-analysis/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEdit your \u003ccode\u003econfig.yaml\u003c/code\u003e as needed.\u003c/p\u003e\n\u003cp\u003eAdd sample entries to your \u003ccode\u003esamples.tsv\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esample  bam_file\nSample1 bams/Sample1.bam\nSample2 bams/Sample2.bam\nSample3 bams/Sample3.bam\n... \u0026lt;more sample entries\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003ccode\u003ebam_file\u003c/code\u003e can be the relative or absolute path to a sample\u0027s BAM file.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-iq-tree-maximum-likelihood-or-clearcut-rnj-tree\" class=\"anchor\" href=\"#iq-tree-maximum-likelihood-or-clearcut-rnj-tree\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://www.iqtree.org/\" rel=\"nofollow\"\u003eIQ-TREE\u003c/a\u003e maximum-likelihood or \u003ca href=\"http://bioinformatics.hungry.com/clearcut/\" rel=\"nofollow\"\u003eClearcut\u003c/a\u003e RNJ tree\u003c/h4\u003e\n\u003cp\u003eIn your \u003ccode\u003econfig.yaml\u003c/code\u003e the \u003ccode\u003efast_tree\u003c/code\u003e parameter controls which method (ML or RNJ) is used for phylogenetic tree construction.\u003c/p\u003e\n\u003cp\u003eIf you want a quick and dirty tree, set\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003efast_tree\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003etrue\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ein your \u003ccode\u003econfig.yaml\u003c/code\u003e to generate a Relaxed Neighbor Joining (RNJ) tree.\u003c/p\u003e\n\u003cp\u003eOtherwise, if you want a high accuracy phylogenetic tree and are willing to wait for it, then set\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003efast_tree\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003efalse\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eto use \u003ca href=\"http://www.iqtree.org/\" rel=\"nofollow\"\u003eIQ-TREE\u003c/a\u003e to generate a maximum-likelihood phylogenetic tree with 1000 ultrafast bootstraps (UFBoot) (see \u003ca href=\"http://dx.doi.org/10.1093/molbev/mst024\" rel=\"nofollow\"\u003eMinh et al., 2016\u003c/a\u003e for more info on UFBoot).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-3-execute-workflow\" class=\"anchor\" href=\"#step-3-execute-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3: Execute workflow\u003c/h3\u003e\n\u003cp\u003e\u003cem\u003eIf you do not have \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e installed then remove the \u003ccode\u003e--use-singularity\u003c/code\u003e flag\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eTest your configuration by performing a dry-run via\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --use-singularity -n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExecute the workflow locally via\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --use-singularity --cores $N\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eusing \u003ccode\u003e$N\u003c/code\u003e cores.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-cluster-execution\" class=\"anchor\" href=\"#cluster-execution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCluster execution\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eNote: You may need to install the \u003ccode\u003edrmaa\u003c/code\u003e Python library (\u003ccode\u003epip install drmaa\u003c/code\u003e)\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eYou can execute the workflow on a SLURM/DRMAA cluster environment with\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --directory test --use-singularity --drmaa \" -c 4 -p YourClusterQueueName --mem=4096 \" -j 8 -w 60\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will run the workflow on the test data in the \u003ccode\u003etest/\u003c/code\u003e directory with 4 CPUs and 4G memory per job and 8 jobs at once (\u003ccode\u003e-j 8\u003c/code\u003e) while waiting 60 seconds for output files to appear on the shared filesystem (\u003ccode\u003e-w 60\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eThe cluster partition or queue to schedule jobs to is specified with \u003ccode\u003e-p YourClusterQueueName\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe above will run each rule or job with 4 CPUs and 4GB memory each, which may be way more than needed or not enough so you could create a YAML (or JSON) file to specify default and specific resource requirements for some steps:\u003c/p\u003e\n\u003cp\u003eExample \u003ccode\u003ecluster-config.yaml\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003e__default__\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ecpu\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003epartition\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eYourClusterQueueName\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e1024\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003esamtools_index_bam_initial\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ecpu\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e32\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e16384\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003espades_assembly\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ecpu\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e32\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e16384\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003ebwa_mem\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ecpu\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e32\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4096\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003emafft_msa\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ecpu\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e32\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4096\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003eiqtree\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ecpu\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e8\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4096\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003esnpeff\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003ememory\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4096\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWith the \u003ccode\u003ecluster-config.yaml\u003c/code\u003e, run the workflow in a cluster environment via\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --directory test --use-singularity --cluster-config cluster-config.yaml --drmaa \" -c {cluster.cpu} -p {cluster.partition} --mem={cluster.memory} \" -j 8 -w 60\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWith the above command and \u003ccode\u003ecluster-config.yaml\u003c/code\u003e, by default, a rule or step in the workflow will only use 1 CPU and request 1G of memory, while the rules like \u003ccode\u003eiqtree\u003c/code\u003e or \u003ccode\u003espades_assembly\u003c/code\u003e will request more CPUs and memory from the SLURM/DRMAA scheduler.\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href=\"https://snakemake.readthedocs.io\" rel=\"nofollow\"\u003eSnakemake documentation\u003c/a\u003e for further details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003eTests cases are in the subfolder \u003ccode\u003etest\u003c/code\u003e. They should be executed via continuous integration with Travis CI.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003cp\u003eIf you were to copy the files in \u003ccode\u003etest\u003c/code\u003e (\u003ccode\u003esamples.tsv\u003c/code\u003e, \u003ccode\u003ebam/\u003c/code\u003e and \u003ccode\u003econfig.yaml\u003c/code\u003e) to a new directory \u003ccode\u003emy-analysis-directory\u003c/code\u003e and run the workflow on that directory, i.e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --directory my-analysis-directory/ \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e other args\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe contents of \u003ccode\u003emy-analysis-directory\u003c/code\u003e should look like:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emy-analysis-directory\n\u251c\u2500\u2500 phylogeny \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Phylogenetic Tree Output\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 genome-metadata.tsv\n\u2502   \u2514\u2500\u2500 tree.html\n\u251c\u2500\u2500 config.yaml \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e INPUT: Workflow Execution Config File \u003c/span\u003e\n\u251c\u2500\u2500 qc \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Quality Control Output\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 multiqc.html \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e MultiQC report file\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 fastqc \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e FastQC Output\u003c/span\u003e\n\u2502   \u2502   \u251c\u2500\u2500 Sample1.html\n\u2502   \u2502   \u2514\u2500\u2500 Sample1_fastqc.zip\n\u2502   \u251c\u2500\u2500 multiqc_data\n\u2502   \u2502   \u251c\u2500\u2500 [Text files]\n\u2502   \u2514\u2500\u2500 quast \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e QUAST Output\u003c/span\u003e\n\u2502       \u251c\u2500\u2500 report.tex\n\u2502       \u251c\u2500\u2500 icarus_viewers\n\u2502       \u2502   \u2514\u2500\u2500 contig_size_viewer.html\n\u2502       \u251c\u2500\u2500 report.html\n\u2502       \u251c\u2500\u2500 basic_stats\n\u2502       \u2502   \u251c\u2500\u2500 [QUAST PDFs]\n\u2502       \u251c\u2500\u2500 icarus.html\n\u2502       \u251c\u2500\u2500 transposed_report.tex\n\u2502       \u251c\u2500\u2500 quast.log\n\u2502       \u251c\u2500\u2500 report.pdf\n\u2502       \u251c\u2500\u2500 report.txt\n\u2502       \u251c\u2500\u2500 .snakemake_timestamp\n\u2502       \u251c\u2500\u2500 report.tsv\n\u2502       \u251c\u2500\u2500 transposed_report.tsv\n\u2502       \u2514\u2500\u2500 transposed_report.txt\n\u251c\u2500\u2500 variant_calling \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Variant Calling Output\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 Sample1-filtered.vcf \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Filtered variants for Sample1 in VCF format\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 Sample1.vcf \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Unfiltered variants for Sample1 in VCF format\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 snpeff \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e SnpEff Output\u003c/span\u003e\n\u2502   \u2502   \u251c\u2500\u2500 Sample1\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 [SnpEff specific files]\n\u2502   \u2502   \u251c\u2500\u2500 Sample1.vcf\n\u2502   \u2502   \u251c\u2500\u2500 Sample1.csv\n\u2502   \u2502   \u251c\u2500\u2500 Sample1.html \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e SnpEff report for Sample1\u003c/span\u003e\n\u2502   \u2502   \u2514\u2500\u2500 Sample1.genes.txt\n\u2502   \u2514\u2500\u2500 Sample1-vcf.tsv \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e SnpEff annotated variants in a tab-delimited table\u003c/span\u003e\n\u251c\u2500\u2500 mapping \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Read Mapping Output\u003c/span\u003e\n\u2502   \u2514\u2500\u2500 Sample1 \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Read mapping output and summary files for Sample1\u003c/span\u003e\n\u2502       \u251c\u2500\u2500 Sample1-extent.tsv\n\u2502       \u251c\u2500\u2500 Sample1-genome_extent.tsv\n\u2502       \u251c\u2500\u2500 Sample1-idxstats.tsv\n\u2502       \u251c\u2500\u2500 Sample1.bam\n\u2502       \u251c\u2500\u2500 Sample1-depth.tsv\n\u2502       \u251c\u2500\u2500 Sample1-idxstats-sorted.tsv\n\u2502       \u251c\u2500\u2500 Sample1-idxstats-top_mapped.txt\n\u2502       \u2514\u2500\u2500 Sample1.bam.bai\n\u251c\u2500\u2500 bam \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Input directory with Sample1 BAM file specified in config.yaml\u003c/span\u003e\n\u2502   \u2514\u2500\u2500 a.bam\n\u251c\u2500\u2500 consensus \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Consensus Sequence Output\u003c/span\u003e\n\u2502   \u2514\u2500\u2500 Sample1.fasta \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Consensus sequence for Sample1 from reference mapping and variant calling\u003c/span\u003e\n\u251c\u2500\u2500 logs \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Log files for various tools\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etool name\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u2502   \u2502   \u2514\u2500\u2500 Sample1.log\n\u251c\u2500\u2500 samples.tsv \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e INPUT: tab-delimited table with 2 fields: \"sample\" and \"bam_file\"\u003c/span\u003e\n\u251c\u2500\u2500 references \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Reference Genomes Downloaded From NCBI\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 Sample1 \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Top Reference Genome\u003c/span\u003e\n\u2502   \u2502   \u251c\u2500\u2500 reference.gff\n\u2502   \u2502   \u251c\u2500\u2500 reference-no_ambig.fasta.bwt\n\u2502   \u2502   \u251c\u2500\u2500 reference-no_ambig.fasta.pac\n\u2502   \u2502   \u251c\u2500\u2500 reference.genbank\n\u2502   \u2502   \u251c\u2500\u2500 reference-no_ambig.fasta.amb\n\u2502   \u2502   \u251c\u2500\u2500 reference-no_ambig.fasta.ann\n\u2502   \u2502   \u251c\u2500\u2500 reference-no_ambig.fasta\n\u2502   \u2502   \u251c\u2500\u2500 reference-no_ambig.fasta.sa\n\u2502   \u2502   \u251c\u2500\u2500 reference.fasta\n\u2502   \u2502   \u2514\u2500\u2500 reference-no_ambig.fasta.fai\n\u2502   \u251c\u2500\u2500 csf.msh \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Mash sketch database from \"csf.fasta\"\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 csf.genbank \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e CSFV genomes downloaded from NCBI in GenBank format\u003c/span\u003e\n\u2502   \u2514\u2500\u2500 csf.fasta \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e CSFV genomes downloaded from NCBI in FASTA format\u003c/span\u003e\n\u251c\u2500\u2500 assembly \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Assembly Output\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 spades \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e SPAdes assembly outputs for each input sample\u003c/span\u003e\n\u2502   \u2502   \u2514\u2500\u2500 Sample1 \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e SPAdes assembly output for Sample1\u003c/span\u003e\n\u2502   \u2502       \u251c\u2500\u2500 before_rr.fasta\n\u2502   \u2502       \u251c\u2500\u2500 params.txt\n\u2502   \u2502       \u251c\u2500\u2500 contigs.paths\n\u2502   \u2502       \u251c\u2500\u2500 input_dataset.yaml\n\u2502   \u2502       \u251c\u2500\u2500 \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eSPAdes specific output directories\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u2502   \u2502       \u251c\u2500\u2500 scaffolds.paths\n\u2502   \u2502       \u251c\u2500\u2500 contigs.fasta\n\u2502   \u2502       \u251c\u2500\u2500 spades.log\n\u2502   \u2502       \u251c\u2500\u2500 assembly_graph.fastg\n\u2502   \u2502       \u251c\u2500\u2500 dataset.info\n\u2502   \u2502       \u251c\u2500\u2500 scaffolds.fasta\n\u2502   \u2502       \u2514\u2500\u2500 assembly_graph_with_scaffolds.gfa\n\u2502   \u2514\u2500\u2500 spades-Sample1.fasta\n\u251c\u2500\u2500 benchmarks \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Benchmark runtime info for tools in workflow\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ebenchmark tab-delimited files \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003evarious tools\u003c/span\u003e \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e workflow\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u251c\u2500\u2500 msa \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Multiple sequence alignment (MSA) output and IQ-TREE/Clearcut phylogenetic tree\u003c/span\u003e\n\u2502   \u251c\u2500\u2500 alignment.fasta\n\u2502   \u251c\u2500\u2500 samples-pre-aln.fasta\n\u2502   \u2514\u2500\u2500 alignment.fasta.treefile\n\u2514\u2500\u2500 preprocess \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Preprocessing Output of Input BAM Files \u003c/span\u003e\n    \u251c\u2500\u2500 samtools \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Initial BAM file stats output\u003c/span\u003e\n    \u2502   \u251c\u2500\u2500 depth\n    \u2502   \u2502   \u251c\u2500\u2500 Sample1-extent.tsv\n    \u2502   \u2502   \u251c\u2500\u2500 Sample1-genome_extent.tsv\n    \u2502   \u2502   \u2514\u2500\u2500 Sample1.tsv\n    \u2502   \u251c\u2500\u2500 flagstat\n    \u2502   \u2502   \u2514\u2500\u2500 Sample1.flagstat\n    \u2502   \u251c\u2500\u2500 index\n    \u2502   \u2502   \u2514\u2500\u2500 Sample1.done\n    \u2502   \u2514\u2500\u2500 idxstats\n    \u2502       \u251c\u2500\u2500 Sample1-top_mapped.txt\n    \u2502       \u251c\u2500\u2500 Sample1.tsv\n    \u2502       \u2514\u2500\u2500 Sample1-sorted.tsv\n    \u251c\u2500\u2500 fastqs \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Deduplicated reads in FASTQ format\u003c/span\u003e\n    \u2502   \u2514\u2500\u2500 Sample1.fastq\n    \u251c\u2500\u2500 mash \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Mash Screen results\u003c/span\u003e\n    \u2502   \u251c\u2500\u2500 Sample1-screen_references-sorted.tsv\n    \u2502   \u2514\u2500\u2500 Sample1-screen_references.tsv\n    \u251c\u2500\u2500 trimmed_fastqs \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Trimmomatic trimmed reads\u003c/span\u003e\n    \u2502   \u2514\u2500\u2500 Sample1.fastq\n    \u2514\u2500\u2500 dedup \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Deduplicated BAM files\u003c/span\u003e\n        \u251c\u2500\u2500 Sample1.bam\n        \u251c\u2500\u2500 Sample1.metrics.txt\n        \u2514\u2500\u2500 Sample1.bam.bai\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1566573045.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "melnel000/Sarek_CBIO",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://sarek.scilifelab.se/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/Sarek_logo.png\" alt=\"Sarek\" title=\"Sarek\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h1\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-an-open-source-analysis-pipeline-to-detect-germline-or-somatic-variants-from-whole-genome-or-targeted-sequencing\" class=\"anchor\" href=\"#an-open-source-analysis-pipeline-to-detect-germline-or-somatic-variants-from-whole-genome-or-targeted-sequencing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAn open-source analysis pipeline to detect germline or somatic variants from whole genome or targeted sequencing\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.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\" alt=\"Nextflow version\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg?logo=data:image/svg+xml;base64,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\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.org/SciLifeLab/Sarek\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/63575514d0146dcd6226c111d1293fade62aa93f4946f2afcbb55718f3be5d86/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d747261766973\" alt=\"Travis build status\" data-canonical-src=\"https://img.shields.io/travis/SciLifeLab/Sarek.svg?logo=travis\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://gitter.im/SciLifeLab/Sarek\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/419eea0b1c13bbf50717b250a81b0de7616141e86a5fc88eee2360f003f4c4e6/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d676974746572266c6f676f436f6c6f723d776869746526636f6c6f72423d346662393961\" alt=\"Join the chat on https://gitter.im/SciLifeLab/Sarek\" data-canonical-src=\"https://img.shields.io/gitter/room/SciLifeLab/Sarek.svg?logo=gitter\u0026amp;logoColor=white\u0026amp;colorB=4fb99a\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e041bce107aca31eca6dd63a962556bf70d6cef2508c777604eedb99ca049c4b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f5363694c6966654c61622f536172656b2e737667\" alt=\"MIT License\" data-canonical-src=\"https://img.shields.io/github/license/SciLifeLab/Sarek.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/SciLifeLab/Sarek/releases/latest\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/720a0b93892db5c772d24eb7dc2fd6fefb2b556eff92ee7ae6a2963a40a8dd5a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f5363694c6966654c61622f536172656b2e7376673f6c6f676f3d676974687562266c6f676f436f6c6f723d7768697465\" alt=\"Sarek version\" data-canonical-src=\"https://img.shields.io/github/release/SciLifeLab/Sarek.svg?logo=github\u0026amp;logoColor=white\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://zenodo.org/badge/latestdoi/54024046\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2794ec0225017cde71e3ed51dd8393510fe23a950955ef03f7439d7c0f288f83/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f35343032343034362e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/54024046.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.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\" alt=\"Install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg?logo=data:image/png;base64,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\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/maxulysse/sarek\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bc3bec2ef3bf857d42e0bff8df09f0e81595bbd7dbc2681d0feadd729acb4bc0/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6d6178756c797373652f736172656b2e7376673f6c6f676f3d646f636b6572\" alt=\"Docker Container available\" data-canonical-src=\"https://img.shields.io/docker/automated/maxulysse/sarek.svg?logo=docker\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/CAW_logo.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg align=\"right\" title=\"CAW\" src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/CAW_logo.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePreviously known as the Cancer Analysis Workflow (CAW),\nSarek is a workflow designed to run analyses on WGS data from regular samples or tumour / normal pairs, including relapse samples if required.\u003c/p\u003e\n\u003cp\u003eIt\u0027s built using \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a domain specific language for workflow building.\nSoftware dependencies are handled using \u003ca href=\"https://www.docker.com\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e or \u003ca href=\"https://www.sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e - container technologies that provide excellent reproducibility and ease of use.\nSingularity has been designed specifically for high-performance computing environments.\nThis means that although Sarek has been primarily designed for use with the Swedish \u003ca href=\"https://www.uppmax.uu.se\" rel=\"nofollow\"\u003eUPPMAX HPC systems\u003c/a\u003e, it should be able to run on any system that supports these two tools.\u003c/p\u003e\n\u003cp\u003eSarek was developed at the \u003ca href=\"https://ngisweden.scilifelab.se/\" rel=\"nofollow\"\u003eNational Genomics Infastructure\u003c/a\u003e and \u003ca href=\"https://www.nbis.se/\" rel=\"nofollow\"\u003eNational Bioinformatics Infastructure Sweden\u003c/a\u003e which are both platforms at \u003ca href=\"https://www.scilifelab.se/\" rel=\"nofollow\"\u003eSciLifeLab\u003c/a\u003e.\nIt is listed on the \u003ca href=\"https://bio.tools/Sarek\" rel=\"nofollow\"\u003eElixir - Tools and Data Services Registry\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-workflow-steps\" class=\"anchor\" href=\"#workflow-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow steps\u003c/h2\u003e\n\u003cp\u003eSarek is built with several workflow scripts.\nA wrapper script contained within the repository makes it easy to run the different workflow scripts as a single job.\nTo test your installation, follow the \u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/TESTS.md\"\u003etests documentation.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRaw FastQ files or aligned BAM files (with or without realignment \u0026amp; recalibration) can be used as inputs.\nYou can choose which variant callers to use, plus the pipeline is capable of accommodating additional variant calling software or CNV callers if required.\u003c/p\u003e\n\u003cp\u003eThe worflow steps and tools used are as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cstrong\u003ePreprocessing\u003c/strong\u003e - \u003ccode\u003emain.nf\u003c/code\u003e \u003cem\u003e(based on \u003ca href=\"https://software.broadinstitute.org/gatk/best-practices/\" rel=\"nofollow\"\u003eGATK best practices\u003c/a\u003e)\u003c/em\u003e\n\u003cul\u003e\n\u003cli\u003eMap reads to Reference\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://bio-bwa.sourceforge.net/\" rel=\"nofollow\"\u003eBWA\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMark Duplicates\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broadinstitute/gatk\"\u003eGATK MarkDuplicates\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eBase (Quality Score) Recalibration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broadinstitute/gatk\"\u003eGATK BaseRecalibrator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broadinstitute/gatk\"\u003eGATK ApplyBQSR\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eGermline variant calling\u003c/strong\u003e - \u003ccode\u003egermlineVC.nf\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eSNVs and small indels\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broadinstitute/gatk\"\u003eGATK HaplotyeCaller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Illumina/strelka\"\u003eStrelka2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStructural variants\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Illumina/manta\"\u003eManta\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eSomatic variant calling\u003c/strong\u003e - \u003ccode\u003esomaticVC.nf\u003c/code\u003e \u003cem\u003e(optional)\u003c/em\u003e\n\u003cul\u003e\n\u003cli\u003eSNVs and small indels\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/broadinstitute/gatk\"\u003eMuTect2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ekg/freebayes\"\u003eFreebayes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Illumina/strelka\"\u003eStrelka2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStructural variants\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Illumina/manta\"\u003eManta\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSample heterogeneity, ploidy and CNVs\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Crick-CancerGenomics/ascat\"\u003eASCAT\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eAnnotation\u003c/strong\u003e - \u003ccode\u003eannotate.nf\u003c/code\u003e \u003cem\u003e(optional)\u003c/em\u003e\n\u003cul\u003e\n\u003cli\u003eVariant annotation\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://snpeff.sourceforge.net/\" rel=\"nofollow\"\u003eSnpEff\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.ensembl.org/info/docs/tools/vep/index.html\" rel=\"nofollow\"\u003eVEP (Variant Effect Predictor)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eReporting\u003c/strong\u003e - \u003ccode\u003erunMultiQC.nf\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eReporting\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://multiqc.info\" rel=\"nofollow\"\u003eMultiQC\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eThe Sarek pipeline comes with documentation in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL.md\"\u003eInstallation documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL_RACKHAM.md\"\u003eInstallation documentation specific for UPPMAX \u003ccode\u003erackham\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INSTALL_BIANCA.md\"\u003eInstallation documentation specific for UPPMAX \u003ccode\u003ebianca\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/TESTS.md\"\u003eTests documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/REFERENCES.md\"\u003eReference files documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/CONFIG.md\"\u003eConfiguration and profiles documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INTERVALS.md\"\u003eIntervals documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/USAGE.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/PARAMETERS.md\"\u003eCommand line parameters\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/USE_CASES.md\"\u003eExamples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/INPUT.md\"\u003eInput files documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/PROCESS.md\"\u003eProcesses documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/CONTAINERS.md\"\u003eDocumentation about containers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/ASCAT.md\"\u003eMore information about ASCAT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/docs/OUTPUT.md\"\u003eOutput documentation structure\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributions--support\" class=\"anchor\" href=\"#contributions--support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributions \u0026amp; Support\u003c/h2\u003e\n\u003cp\u003eIf you would like to contribute to this pipeline, please see the \u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/.github/CONTRIBUTING.md\"\u003econtributing guidelines\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor further information or help, don\u0027t hesitate to get in touch on \u003ca href=\"https://gitter.im/SciLifeLab/Sarek\" rel=\"nofollow\"\u003eGitter\u003c/a\u003e or contact us: \u003ca href=\"mailto:maxime.garcia@scilifelab.se\"\u003emaxime.garcia@scilifelab.se\u003c/a\u003e, \u003ca href=\"mailto:szilveszter.juhos@scilifelab.se\"\u003eszilveszter.juhos@scilifelab.se\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-changelog\" class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCHANGELOG\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/SciLifeLab/Sarek/blob/master/CHANGELOG.md\"\u003eCHANGELOG\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eMain authors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/MaxUlysse\"\u003eMaxime Garcia\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/szilvajuhos\"\u003eSzilveszter Juhos\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHelpful contributors:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/alneberg\"\u003eJohannes Alneberg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Sebastian-D\"\u003eSebastian DiLorenzo\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/J35P312\"\u003eJesper Eisfeldt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ewels\"\u003ePhil Ewels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/gulfshores\"\u003eMax K\u00e4ller\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/malinlarsson\"\u003eMalin Larsson\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/marcelm\"\u003eMarcel Martin\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/bjornnystedt\"\u003eBj\u00f6rn Nystedt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/pallolason\"\u003ePall Olason\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/arontommi\"\u003eAron Skaftason\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://www.scilifelab.se/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/SciLifeLab_logo.png\" alt=\"SciLifeLab\" title=\"SciLifeLab\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://ngisweden.scilifelab.se/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/NGI_logo.png\" alt=\"NGI\" title=\"NGI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nbis.se/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/SciLifeLab/Sarek/master/docs/images/NBIS_logo.png\" alt=\"NBIS\" title=\"NBIS\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1541579046.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v2.0.0"
    ],
    "full_name": "baxpr/fmri_modularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fmri_conncalc\" class=\"anchor\" href=\"#fmri_conncalc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efmri_conncalc\u003c/h1\u003e\n\u003cp\u003ePreprocessing and functional connectivity computation for fMRI\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h2\u003e\n\u003cp\u003eHere is an example for the \"jsins\" version of the processor, as described in\n\u003ca href=\"conncalc_jsins_v1.0.0.yaml\"\u003econncalc_jsins_v1.0.0.yaml\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity\n  run\n  --bind \u0026lt;INDIR\u0026gt;:/INPUTS\n  --bind \u0026lt;OUTDIR\u0026gt;:/OUTPUTS\n  baxpr-fmri_conncalc-master-v1.0.0.simg\n  magick_path /usr/bin\n  param_file params_JSins.csv\n  wroi_file rois_JSins.nii.gz\n  roi_file \u0027\u0027\n  roiinfo_file rois_JSins.csv\n  coregmat_file /INPUTS/coreg_mat.txt \\\n  deffwd_file /INPUTS/y_deffwd.nii.gz \\\n  ct1_file /INPUTS/ct1.nii.gz \\\n  wgm_file /INPUTS/wgm.nii.gz \\\n  wcseg_file /INPUTS/wcseg.nii.gz \\\n  func_file /INPUTS/fmri.nii.gz \\\n  project PROJECT_LABEL \\\n  subject SUBJECT_LABEL \\\n  session SESSION_LABEL \\\n  scan SCAN_LABEL \\\n  out_dir /OUTPUTS\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe inputs \u003ccode\u003ecoregmat_file\u003c/code\u003e, \u003ccode\u003edeffwd_file\u003c/code\u003e, \u003ccode\u003ect1_file\u003c/code\u003e, \u003ccode\u003ewgm_file\u003c/code\u003e, \u003ccode\u003ewcseg_file\u003c/code\u003e would typically be obtained from the outputs of the \u003ccode\u003eMAGM_Coreg_Normalize_v2\u003c/code\u003e spider.\u003c/p\u003e\n\u003cp\u003eThe outputs are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efmri_conncalc.pdf    Report\nparams.csv           Parameters used in the analysis\nFD.txt               Framewise displacement time series\nDVARS.txt            Framewise variance time series\nbadvols.txt          Scrubbed volumes indicator time series\nrp_adfunc.txt        Realignment (motion) values\nwmeanadfunc.nii.gz   Mean functional image in standard space\nwadfunc.nii.gz       Slice time corrected and realigned functional images in standard space\nrroi_labels.nii.gz   Region of interest label image\nroi_snr.nii.gz       ROI SNR image\nroi_info.csv         ROI info\nroi_labels.csv       ROI names (if available)\n\nSeries of results repeated for each of the four processing streams\n(keep or remove mean gray matter; scrub or no scrub):\n\n  confounds_removegm_noscrub.txt               Confound (filter) matrix\n  connectivity_matrix_R_removegm_noscrub.csv   Connectivity matrix\n  filtered_removegm_noscrub.nii.gz             Filtered functional images\n  roi_timeseries_removegm_noscrub.csv          Filtered ROI time series\n  stats_removegm_noscrub.txt                   Various statistics\n  Zmap_removegm_noscrub.nii.gz                 Unsmoothed ROI connectivity maps\n  sZmap_removegm_noscrub.nii.gz                Smoothed ROI connectivity maps\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eThe built singularity container \u003ccode\u003ebaxpr-fmri_conncalc-master-v1.0.0.simg\u003c/code\u003e (URL is shub://baxpr/fmri_conncalc:v1.0.0) is stand-alone with no external dependencies. The compiled matlab \u003ca href=\"bin/run_fmri_conncalc.sh\"\u003erun_fmri_conncalc.sh\u003c/a\u003e requires only the appropriate MATLAB Runtime to execute. To build these there are two stages:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCompile the MATLAB code into a stand-alone executable, using \u003ca href=\"compile_matlab.sh\"\u003ecompile_matlab.sh\u003c/a\u003e. This requires a full MATLAB installation (R2017a, v92) and SPM12 (\u003ca href=\"https://www.fil.ion.ucl.ac.uk/spm/\" rel=\"nofollow\"\u003ehttps://www.fil.ion.ucl.ac.uk/spm/\u003c/a\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the singularity container. In addition to a few specific OS packages, this requires the MATLAB Compiled Runtime. All are specified to be downloaded during the build in the singularity recipe \u003ca href=\"Singularity.v1.0.0\"\u003eSingularity.v1.0.0\u003c/a\u003e. The container help text gives build instructions. Alternatively the built container can be obtained from singularity-hub:\n\u003ccode\u003esingularity pull shub://baxpr/fmri_conncalc:v1.0.0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-peculiarities-of-specific-pipelines\" class=\"anchor\" href=\"#peculiarities-of-specific-pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePeculiarities of specific pipelines\u003c/h2\u003e\n\u003cp\u003eSome critical analysis parameters are specified in the \u003ccode\u003eparam_file\u003c/code\u003e, e.g. \u003ccode\u003eparams_JSins.csv\u003c/code\u003e. This is a reference to a file that\u0027s in the built container, but these can also be viewed in the code repository e.g. \u003ca href=\"src/params/params_JSins.csv\"\u003esrc/params/params_JSins.csv\u003c/a\u003e. The parameters get as detailed as the repetition time of the fMRI scans. If the needed parameter file is not in the container already:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdd the new parameter file in \u003ccode\u003esrc/params\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eUpdate the matlab compilation code to include it with \u003ccode\u003e-a\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRecompile the matlab\u003c/li\u003e\n\u003cli\u003eCommit to github. Note that the compiled matlab executable is stored using LFS\u003c/li\u003e\n\u003cli\u003eRebuild the container (increment the patch number, e.g. 1.0.0 to 1.0.1)\u003c/li\u003e\n\u003cli\u003eCreate an updated YAML file appropriate for the parameter set\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jsins-version\" class=\"anchor\" href=\"#jsins-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ejsins version\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"conncalc_jsins_v1.0.0.yaml\"\u003econncalc_jsins_v1.0.0.yaml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStandard space regions of interest are used, \u003ca href=\"src/params/JS_insula/rois_JSins.nii.gz\"\u003erois_JSins.nii.gz\u003c/a\u003e, identical for every subject.\u003c/p\u003e\n\u003cp\u003eConnectivity matrix is computed (Pearson bivariate correlation R). A connectivity map is computed for each ROI (Fisher Z transform applied to Pearson bivariate correlation). Spatial smoothing is applied to the connectivity maps only.\u003c/p\u003e\n\u003cp\u003eParameter settings in \u003ca href=\"src/params/params_JSins.csv\"\u003eparams_JSins.csv\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFMRI repetition time (TR) is assumed to be 2.000 sec\u003c/li\u003e\n\u003cli\u003eUse all fMRI volumes (none dropped)\u003c/li\u003e\n\u003cli\u003eNo slice timing correction\u003c/li\u003e\n\u003cli\u003e6mm FWHM Gaussian spatial smoothing applied to connectivity maps\u003c/li\u003e\n\u003cli\u003eFilter settings (confound regressor matrix):\n\u003cul\u003e\n\u003cli\u003e0.01 Hz - 0.10 Hz bandpass filter (Fourier basis)\u003c/li\u003e\n\u003cli\u003e6 motion parameters (translation and rotation)\u003c/li\u003e\n\u003cli\u003e6 first differences of motion parameters\u003c/li\u003e\n\u003cli\u003eFirst 6 principal components of voxel time series from the eroded white matter/CSF compartment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFor scrubbed results, volumes before and after an excursion of FD \u0026gt; 0.5 are removed. DVARS is not used for scrubbing.\u003c/li\u003e\n\u003cli\u003eConnectivity maps are saved for each ROI.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-szhab-version\" class=\"anchor\" href=\"#szhab-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eszhab version\u003c/h3\u003e\n\u003cp\u003eNo YAML available yet.\u003c/p\u003e\n\u003cp\u003eSubject-specific regions of interest are used, as described in the native space ROI image supplied as input. This image must be in the same space as the subject\u0027s native space structural.\u003c/p\u003e\n\u003cp\u003eConnectivity matrix is computed (Pearson bivariate correlation R of filtered time series). Spatial smoothing is not used.\u003c/p\u003e\n\u003cp\u003eParameter settings in \u003ccode\u003eparams_SZhab.csv\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFMRI repetition time (TR) is assumed to be 2.000 sec\u003c/li\u003e\n\u003cli\u003e5 initial volumes are dropped, and the following 60 volumes are used for the analysis\u003c/li\u003e\n\u003cli\u003eNo slice timing correction\u003c/li\u003e\n\u003cli\u003eFilter settings (confound regressor matrix):\n\u003cul\u003e\n\u003cli\u003e0.01 Hz - 0.15 Hz bandpass filter (Fourier basis)\u003c/li\u003e\n\u003cli\u003e6 motion parameters (translation and rotation)\u003c/li\u003e\n\u003cli\u003eFirst 3 principal components of voxel time series from the eroded white matter/CSF compartment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFor scrubbed results, volumes before and after an excursion of FD \u0026gt; 0.5 are removed. DVARS is not used for scrubbing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-general-pipeline\" class=\"anchor\" href=\"#general-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeneral pipeline\u003c/h2\u003e\n\u003cp\u003eOther than the above, processing proceeds as follows.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDrop functional volumes as specified.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePerform slice timing correction as specified. (SPM12 slice timing correction)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePerform motion realignment: two-stage alignment to mean image. (SPM12 realignment)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCoregister the mean functional image to the T1 weighted structural using a rigid body transform. The structural is first skull-stripped by zeroing all voxels that were not labeled by the multiatlas segmentation. The transformation is then applied to all functional volumes. (SPM12 coregistration)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eQuality parameters are computed: framewise displacement FD and framewise signal variance DVARS. Volumes exceeding scrubbing criteria are marked (\"badvols\").\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe functional and structural images are warped to standard space using the supplied nonlinear transform (forward deformation image). (SPM12 deformation tools)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe supplied standard space ROI image file is resampled to match the standard space fMRI geometry. (SPM12 reslice)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConnectivity computation. All filtering is done in a single step: a design matrix of confounds is created (see lists above), it is fit to each voxel time series, and the residuals are extracted. Then bivariate Pearson correlation is computed between ROI residual time series to produce the connectivity matrix. Fisher transformed correlation between ROIs/voxel residual time series is used to produce connectivity maps if that option is selected.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550158474.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v1.0.0"
    ],
    "full_name": "baxpr/fmri_conncalc",
    "latest_release": "v1.0.0-rc0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fmri_conncalc\" class=\"anchor\" href=\"#fmri_conncalc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efmri_conncalc\u003c/h1\u003e\n\u003cp\u003ePreprocessing and functional connectivity computation for fMRI\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h2\u003e\n\u003cp\u003eHere is an example for the \"jsins\" version of the processor, as described in\n\u003ca href=\"conncalc_jsins_v1.0.0.yaml\"\u003econncalc_jsins_v1.0.0.yaml\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity\n  run\n  --bind \u0026lt;INDIR\u0026gt;:/INPUTS\n  --bind \u0026lt;OUTDIR\u0026gt;:/OUTPUTS\n  baxpr-fmri_conncalc-master-v1.0.0.simg\n  magick_path /usr/bin\n  param_file params_JSins.csv\n  wroi_file rois_JSins.nii.gz\n  roi_file \u0027\u0027\n  roiinfo_file rois_JSins.csv\n  coregmat_file /INPUTS/coreg_mat.txt \\\n  deffwd_file /INPUTS/y_deffwd.nii.gz \\\n  ct1_file /INPUTS/ct1.nii.gz \\\n  wgm_file /INPUTS/wgm.nii.gz \\\n  wcseg_file /INPUTS/wcseg.nii.gz \\\n  func_file /INPUTS/fmri.nii.gz \\\n  project PROJECT_LABEL \\\n  subject SUBJECT_LABEL \\\n  session SESSION_LABEL \\\n  scan SCAN_LABEL \\\n  out_dir /OUTPUTS\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe inputs \u003ccode\u003ecoregmat_file\u003c/code\u003e, \u003ccode\u003edeffwd_file\u003c/code\u003e, \u003ccode\u003ect1_file\u003c/code\u003e, \u003ccode\u003ewgm_file\u003c/code\u003e, \u003ccode\u003ewcseg_file\u003c/code\u003e would typically be obtained from the outputs of the \u003ccode\u003eMAGM_Coreg_Normalize_v2\u003c/code\u003e spider.\u003c/p\u003e\n\u003cp\u003eThe outputs are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efmri_conncalc.pdf    Report\nparams.csv           Parameters used in the analysis\nFD.txt               Framewise displacement time series\nDVARS.txt            Framewise variance time series\nbadvols.txt          Scrubbed volumes indicator time series\nrp_adfunc.txt        Realignment (motion) values\nwmeanadfunc.nii.gz   Mean functional image in standard space\nwadfunc.nii.gz       Slice time corrected and realigned functional images in standard space\nrroi_labels.nii.gz   Region of interest label image\nroi_snr.nii.gz       ROI SNR image\nroi_info.csv         ROI info\nroi_labels.csv       ROI names (if available)\n\nSeries of results repeated for each of the four processing streams\n(keep or remove mean gray matter; scrub or no scrub):\n\n  confounds_removegm_noscrub.txt               Confound (filter) matrix\n  connectivity_matrix_R_removegm_noscrub.csv   Connectivity matrix\n  filtered_removegm_noscrub.nii.gz             Filtered functional images\n  roi_timeseries_removegm_noscrub.csv          Filtered ROI time series\n  stats_removegm_noscrub.txt                   Various statistics\n  Zmap_removegm_noscrub.nii.gz                 Unsmoothed ROI connectivity maps\n  sZmap_removegm_noscrub.nii.gz                Smoothed ROI connectivity maps\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eThe built singularity container \u003ccode\u003ebaxpr-fmri_conncalc-master-v1.0.0.simg\u003c/code\u003e (URL is shub://baxpr/fmri_conncalc:v1.0.0) is stand-alone with no external dependencies. The compiled matlab \u003ca href=\"bin/run_fmri_conncalc.sh\"\u003erun_fmri_conncalc.sh\u003c/a\u003e requires only the appropriate MATLAB Runtime to execute. To build these there are two stages:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCompile the MATLAB code into a stand-alone executable, using \u003ca href=\"compile_matlab.sh\"\u003ecompile_matlab.sh\u003c/a\u003e. This requires a full MATLAB installation (R2017a, v92) and SPM12 (\u003ca href=\"https://www.fil.ion.ucl.ac.uk/spm/\" rel=\"nofollow\"\u003ehttps://www.fil.ion.ucl.ac.uk/spm/\u003c/a\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the singularity container. In addition to a few specific OS packages, this requires the MATLAB Compiled Runtime. All are specified to be downloaded during the build in the singularity recipe \u003ca href=\"Singularity.v1.0.0\"\u003eSingularity.v1.0.0\u003c/a\u003e. The container help text gives build instructions. Alternatively the built container can be obtained from singularity-hub:\n\u003ccode\u003esingularity pull shub://baxpr/fmri_conncalc:v1.0.0\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-peculiarities-of-specific-pipelines\" class=\"anchor\" href=\"#peculiarities-of-specific-pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePeculiarities of specific pipelines\u003c/h2\u003e\n\u003cp\u003eSome critical analysis parameters are specified in the \u003ccode\u003eparam_file\u003c/code\u003e, e.g. \u003ccode\u003eparams_JSins.csv\u003c/code\u003e. This is a reference to a file that\u0027s in the built container, but these can also be viewed in the code repository e.g. \u003ca href=\"src/params/params_JSins.csv\"\u003esrc/params/params_JSins.csv\u003c/a\u003e. The parameters get as detailed as the repetition time of the fMRI scans. If the needed parameter file is not in the container already:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdd the new parameter file in \u003ccode\u003esrc/params\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eUpdate the matlab compilation code to include it with \u003ccode\u003e-a\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRecompile the matlab\u003c/li\u003e\n\u003cli\u003eCommit to github. Note that the compiled matlab executable is stored using LFS\u003c/li\u003e\n\u003cli\u003eRebuild the container (increment the patch number, e.g. 1.0.0 to 1.0.1)\u003c/li\u003e\n\u003cli\u003eCreate an updated YAML file appropriate for the parameter set\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jsins-version\" class=\"anchor\" href=\"#jsins-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ejsins version\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"conncalc_jsins_v1.0.0.yaml\"\u003econncalc_jsins_v1.0.0.yaml\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStandard space regions of interest are used, \u003ca href=\"src/params/JS_insula/rois_JSins.nii.gz\"\u003erois_JSins.nii.gz\u003c/a\u003e, identical for every subject.\u003c/p\u003e\n\u003cp\u003eConnectivity matrix is computed (Pearson bivariate correlation R). A connectivity map is computed for each ROI (Fisher Z transform applied to Pearson bivariate correlation). Spatial smoothing is applied to the connectivity maps only.\u003c/p\u003e\n\u003cp\u003eParameter settings in \u003ca href=\"src/params/params_JSins.csv\"\u003eparams_JSins.csv\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFMRI repetition time (TR) is assumed to be 2.000 sec\u003c/li\u003e\n\u003cli\u003eUse all fMRI volumes (none dropped)\u003c/li\u003e\n\u003cli\u003eNo slice timing correction\u003c/li\u003e\n\u003cli\u003e6mm FWHM Gaussian spatial smoothing applied to connectivity maps\u003c/li\u003e\n\u003cli\u003eFilter settings (confound regressor matrix):\n\u003cul\u003e\n\u003cli\u003e0.01 Hz - 0.10 Hz bandpass filter (Fourier basis)\u003c/li\u003e\n\u003cli\u003e6 motion parameters (translation and rotation)\u003c/li\u003e\n\u003cli\u003e6 first differences of motion parameters\u003c/li\u003e\n\u003cli\u003eFirst 6 principal components of voxel time series from the eroded white matter/CSF compartment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFor scrubbed results, volumes before and after an excursion of FD \u0026gt; 0.5 are removed. DVARS is not used for scrubbing.\u003c/li\u003e\n\u003cli\u003eConnectivity maps are saved for each ROI.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-szhab-version\" class=\"anchor\" href=\"#szhab-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eszhab version\u003c/h3\u003e\n\u003cp\u003eNo YAML available yet.\u003c/p\u003e\n\u003cp\u003eSubject-specific regions of interest are used, as described in the native space ROI image supplied as input. This image must be in the same space as the subject\u0027s native space structural.\u003c/p\u003e\n\u003cp\u003eConnectivity matrix is computed (Pearson bivariate correlation R of filtered time series). Spatial smoothing is not used.\u003c/p\u003e\n\u003cp\u003eParameter settings in \u003ccode\u003eparams_SZhab.csv\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFMRI repetition time (TR) is assumed to be 2.000 sec\u003c/li\u003e\n\u003cli\u003e5 initial volumes are dropped, and the following 60 volumes are used for the analysis\u003c/li\u003e\n\u003cli\u003eNo slice timing correction\u003c/li\u003e\n\u003cli\u003eFilter settings (confound regressor matrix):\n\u003cul\u003e\n\u003cli\u003e0.01 Hz - 0.15 Hz bandpass filter (Fourier basis)\u003c/li\u003e\n\u003cli\u003e6 motion parameters (translation and rotation)\u003c/li\u003e\n\u003cli\u003eFirst 3 principal components of voxel time series from the eroded white matter/CSF compartment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eFor scrubbed results, volumes before and after an excursion of FD \u0026gt; 0.5 are removed. DVARS is not used for scrubbing.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-general-pipeline\" class=\"anchor\" href=\"#general-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeneral pipeline\u003c/h2\u003e\n\u003cp\u003eOther than the above, processing proceeds as follows.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDrop functional volumes as specified.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePerform slice timing correction as specified. (SPM12 slice timing correction)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePerform motion realignment: two-stage alignment to mean image. (SPM12 realignment)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCoregister the mean functional image to the T1 weighted structural using a rigid body transform. The structural is first skull-stripped by zeroing all voxels that were not labeled by the multiatlas segmentation. The transformation is then applied to all functional volumes. (SPM12 coregistration)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eQuality parameters are computed: framewise displacement FD and framewise signal variance DVARS. Volumes exceeding scrubbing criteria are marked (\"badvols\").\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe functional and structural images are warped to standard space using the supplied nonlinear transform (forward deformation image). (SPM12 deformation tools)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe supplied standard space ROI image file is resampled to match the standard space fMRI geometry. (SPM12 reslice)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConnectivity computation. All filtering is done in a single step: a design matrix of confounds is created (see lists above), it is fit to each voxel time series, and the residuals are extracted. Then bivariate Pearson correlation is computed between ROI residual time series to produce the connectivity matrix. Fisher transformed correlation between ROIs/voxel residual time series is used to produce connectivity maps if that option is selected.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1543615331.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v2.1.0"
    ],
    "full_name": "baxpr/mp2rage",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mp2rage\" class=\"anchor\" href=\"#mp2rage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emp2rage\u003c/h1\u003e\n\u003cp\u003eReconstructs a T1-weighted image from images at multiple inversion times following Marques et al. 2009. The robust adjustment (beta factor) of O\u0027Brien 2014 is also implemented.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMarques JP, Kober T, Krueger G, van der Zwaag W, Van de Moortele PF, Gruetter  R. MP2RAGE, a self bias-field corrected sequence for improved segmentation and T1-mapping at high field. Neuroimage. 2010 Jan 15;49(2):1271-81. doi:10.1016/j.neuroimage.2009.10.002. PMID: 19819338.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.ncbi.nlm.nih.gov/pubmed/19819338\" rel=\"nofollow\"\u003ehttps://www.ncbi.nlm.nih.gov/pubmed/19819338\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe large spatial inhomogeneity in transmit B(1) field (B(1)(+)) observable in human MR images at high static magnetic fields (B(0)) severely impairs image quality. To overcome this effect in brain T(1)-weighted images, the MPRAGE sequence was modified to generate two different images at different inversion times, MP2RAGE. By combining the two images in a novel fashion, it was possible to create T(1)-weighted images where the result image was free of proton density contrast, T(2) contrast, reception bias field, and, to first order, transmit field inhomogeneity. MP2RAGE sequence parameters were optimized using Bloch equations to maximize contrast-to-noise ratio per unit of time between brain tissues and minimize the effect of B(1)(+) variations through space. Images of high anatomical quality and excellent brain tissue differentiation suitable for applications such as segmentation and voxel-based morphometry were obtained at 3 and 7 T. From such T(1)-weighted images, acquired within 12 min, high-resolution 3D T(1) maps were routinely calculated at 7 T with sub-millimeter voxel resolution (0.65-0.85 mm isotropic). T(1) maps were validated in phantom experiments. In humans, the T(1) values obtained at 7 T were 1.15+/-0.06 s for white matter (WM) and 1.92+/-0.16 s for grey matter (GM), in good agreement with literature values obtained at lower spatial resolution. At 3 T, where whole-brain acquisitions with 1 mm isotropic voxels were acquired in 8 min, the T(1) values obtained (0.81+/-0.03 s for WM and 1.35+/-0.05 for GM) were once again found to be in very good agreement with values in the literature.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eO\u0027Brien KR, Kober T, Hagmann P, et al. Robust T1-weighted Structural Brain Imaging and Morphometry at 7T Using MP2RAGE PLoS One. 2014;9(6):e99676. Published 2014 Jun 16. doi:10.1371/journal.pone.0099676\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://pubmed.ncbi.nlm.nih.gov/24932514/\" rel=\"nofollow\"\u003ehttps://pubmed.ncbi.nlm.nih.gov/24932514/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePurpose: To suppress the noise, by sacrificing some of the signal homogeneity for numerical stability, in uniform T1 weighted (T1w) images obtained with the magnetization prepared 2 rapid gradient echoes sequence (MP2RAGE) and to compare the clinical utility of these robust T1w images against the uniform T1w images.\u003c/p\u003e\n\u003cp\u003eMaterials and methods: 8 healthy subjects (29.0 \u00b1 4.1 years; 6 Male), who provided written consent, underwent two scan sessions within a 24 hour period on a 7T head-only scanner. The uniform and robust T1w image volumes were calculated inline on the scanner. Two experienced radiologists qualitatively rated the images for: general image quality; 7T specific artefacts; and, local structure definition. Voxel-based and volume-based morphometry packages were used to compare the segmentation quality between the uniform and robust images. Statistical differences were evaluated by using a positive sided Wilcoxon rank test.\u003c/p\u003e\n\u003cp\u003eResults: The robust image suppresses background noise inside and outside the skull. The inhomogeneity introduced was ranked as mild. The robust image was significantly ranked higher than the uniform image for both observers (observer 1/2, p-value = 0.0006/0.0004). In particular, an improved delineation of the pituitary gland, cerebellar lobes was observed in the robust versus uniform T1w image. The reproducibility of the segmentation results between repeat scans improved (p-value = 0.0004) from an average volumetric difference across structures of \u2248 6.6% to \u2248 2.4% for the uniform image and robust T1w image respectively.\u003c/p\u003e\n\u003cp\u003eConclusions: The robust T1w image enables MP2RAGE to produce, clinically familiar T1w images, in addition to T1 maps, which can be readily used in uniform morphometry packages.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1595274818.0
  },
  {
    "data_format": 2,
    "description": "Singularity Container Recipes",
    "filenames": [
      "images/Singularity.lsf",
      "images/protomo/Singularity",
      "images/resmap/Singularity",
      "images/amira/6.7.0/Singularity",
      "images/appion-protomo/Singularity",
      "images/openmbir/2.3.5/Singularity",
      "images/motioncor2/Singularity",
      "images/motioncor2/1.2.2/Singularity",
      "images/motioncor2/1.2.3-intpix/Singularity",
      "images/motioncor2/1.2.1/Singularity",
      "images/motioncor2/1.2.3/Singularity",
      "images/motioncor2/1.2.6/Singularity",
      "images/motioncor2/1.3.0/Singularity",
      "images/motioncor2/1.3.2/Singularity",
      "images/cdms-jupyterlab/Singularity",
      "images/pymol/Singularity",
      "images/git/Singularity",
      "images/phenix/Singularity",
      "images/cryosparc/2.14.2/Singularity",
      "images/cryosparc/2.13.2/Singularity",
      "images/cryosparc/2.12.4/Singularity",
      "images/openmpi/Singularity",
      "images/openmpi/Singularity.ubuntu1810",
      "images/openmpi/Singularity.ubuntu1804",
      "images/openmpi/Singularity.centos7",
      "images/relion/Singularity",
      "images/relion/Singularity.old",
      "images/relion/Singularity.docker",
      "images/relion/3.0.8/Singularity.docker",
      "images/relion/3.0.2/Singularity",
      "images/relion/3.0.2/Singularity.docker",
      "images/relion/3.0.7/Singularity",
      "images/relion/3.0.7/Singularity.orig",
      "images/relion/3.0.6/Singularity",
      "images/relion/3.0.6/Singularity.docker",
      "images/relion/ver3.1/Singularity.docker",
      "images/relion/3.1.0-beta/Singularity",
      "images/relion/3.1.0-beta/Singularity.docker",
      "images/relion/2.1/Singularity",
      "images/relion/2.1/Singularity.docker",
      "images/relion/3.0.4/Singularity",
      "images/relion/3.0.4/Singularity.docker",
      "images/cryolo/1.5.4/Singularity",
      "images/rclone/Singularity",
      "images/xds/Singularity",
      "images/fah/7.5.1/Singularity",
      "images/rosetta/Singularity",
      "images/rosetta/2018.48/Singularity",
      "images/emClarity/Singularity",
      "images/slac-ml/Singularity",
      "images/slac-ml/20190712.2/Singularity",
      "images/slac-ml/20200618.0/Singularity",
      "images/slac-ml/20200227.0/Singularity",
      "images/slac-ml/20200211.0/Singularity",
      "images/scipion/Singularity",
      "images/matlab/R2020a/Singularityfile",
      "images/chimera/Singularity",
      "images/topaz/0.2.2/Singularity",
      "images/topaz/0.2.4/Singularity",
      "images/imagemagick/Singularity",
      "images/icon-gpu/Singularity",
      "images/ctffind/Singularity",
      "images/ctffind/4.1.10/Singularity",
      "images/ctffind/4.1.13/Singularity",
      "images/ctffind/4.1.12/Singularity",
      "images/eman2/Singularity",
      "images/eman2/2.31/Singularity",
      "images/eman2/20200330/Singularity",
      "images/eman2/20190805/Singularity",
      "images/eman2/20190324/Singularity",
      "images/eman2/20200419/Singularity",
      "images/eman2/20190917/Singularity",
      "images/eman2/20200319.0/Singularity",
      "images/eman2/20190603/Singularity",
      "images/eman2/20190418/Singularity",
      "images/imod/Singularity",
      "images/imod/4.10.38/Singularity",
      "images/imod/4.9.10/Singularity",
      "images/imod/4.9.11/Singularity",
      "images/imod/4.9.12/Singularity",
      "images/imod/4.10.42/Singularity",
      "images/tem-simulator/Singularity"
    ],
    "full_name": "slaclab/singularity-modules",
    "latest_release": null,
    "readme": "\u003ch3\u003e\n\u003ca id=\"user-content-simnibs-singularity-recipe\" class=\"anchor\" href=\"#simnibs-singularity-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimNIBS singularity recipe\u003c/h3\u003e\n\u003cp\u003eBefore building, place the SimNIBS source tarball in the /tmp directory. (recipe version 2.1.1)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1592808314.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ResearchIT/SimNIBS",
    "latest_release": null,
    "readme": "\u003ch3\u003e\n\u003ca id=\"user-content-simnibs-singularity-recipe\" class=\"anchor\" href=\"#simnibs-singularity-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimNIBS singularity recipe\u003c/h3\u003e\n\u003cp\u003eBefore building, place the SimNIBS source tarball in the /tmp directory. (recipe version 2.1.1)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1546981375.0
  },
  {
    "data_format": 2,
    "description": "This is singularity 2.6.0 image for PHEnix -1.4a",
    "filenames": [
      "Singularity-2.6.0"
    ],
    "full_name": "Amjadhpc/PHEnix",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-game-container\" class=\"anchor\" href=\"#game-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egame-container\u003c/h1\u003e\n\u003cp\u003eContainers for game AI\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1539686949.0
  },
  {
    "data_format": 2,
    "description": "Containers for game AI",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sbutcher/game-container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-game-container\" class=\"anchor\" href=\"#game-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egame-container\u003c/h1\u003e\n\u003cp\u003eContainers for game AI\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1547647598.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ResearchIT/scanindel",
    "latest_release": null,
    "readme": "\u003ch3\u003e\n\u003ca id=\"user-content-scanindel-singularity-recipe\" class=\"anchor\" href=\"#scanindel-singularity-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eScanIndel Singularity recipe\u003c/h3\u003e\n\u003cp\u003eScanIndel is a python program to detect indels (insertions and deletions) from NGS data by re-align and de novo assemble soft clipped reads.\u003c/p\u003e\n\u003cp\u003eOriginal repository \u003ca href=\"https://github.com/cauyrd/ScanIndel\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1539032220.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for freesurfer",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ResearchIT/singularity-freesurfer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-uresnet-tomo-seg\" class=\"anchor\" href=\"#uresnet-tomo-seg\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003euresnet-tomo-seg\u003c/h1\u003e\n\u003cp\u003euresnet based deep neutral network for the segmentation of high resolution cryo-EM tomographs\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1603915556.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "sbutcher/container-setc",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-container-setc\" class=\"anchor\" href=\"#container-setc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainer-setc\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1538491698.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "cmaumet/nipype_tutorial",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nipype-tutorial-notebooks\" class=\"anchor\" href=\"#nipype-tutorial-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNipype Tutorial Notebooks\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/miykael/nipype_tutorial/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64\" alt=\"CircleCi\" data-canonical-src=\"https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/issues/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub issues\" data-canonical-src=\"https://img.shields.io/github/issues/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/pulls/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub pull-requests\" data-canonical-src=\"https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub contributors\" data-canonical-src=\"https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/commits/master\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub Commits\" data-canonical-src=\"https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/archive/master.zip\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub size\" data-canonical-src=\"https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/miykael/nipype_tutorial/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://hits.dwyl.io/miykael/nipype_tutorial\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub HitCount\" data-canonical-src=\"http://hits.dwyl.io/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is the Nipype Tutorial in Jupyter Notebook format. You can access the tutorial in two ways:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"https://miykael.github.io/nipype_tutorial/\" rel=\"nofollow\"\u003eNipype Tutorial Homepage\u003c/a\u003e: This website contains a static, read-only version of all the notebooks.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html\" rel=\"nofollow\"\u003eNipype Tutorial Docker Image\u003c/a\u003e: This guide explains how to use Docker to run the notebooks interactively on your own computer. The nipype tutorial docker image is the best interactive way to learn Nipype.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-feedback-help--support\" class=\"anchor\" href=\"#feedback-help--support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeedback, Help \u0026amp; Support\u003c/h1\u003e\n\u003cp\u003eIf you want to help with this tutorial or have any questions, feel free to fork the repo of the \u003ca href=\"https://github.com/miykael/nipype_tutorial\"\u003eNotebooks\u003c/a\u003e or interact with other contributors on the slack channel \u003ca href=\"https://brainhack.slack.com/messages/nipype/\" rel=\"nofollow\"\u003ebrainhack.slack.com/messages/nipype/\u003c/a\u003e. If you have any questions or found a problem, open a new \u003ca href=\"https://github.com/miykael/nipype_tutorial/issues\"\u003eissue on github\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-thanks-and-acknowledgment\" class=\"anchor\" href=\"#thanks-and-acknowledgment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks and Acknowledgment\u003c/h1\u003e\n\u003cp\u003eA huge thanks to \u003ca href=\"https://github.com/mwaskom\"\u003eMichael Waskom\u003c/a\u003e, \u003ca href=\"https://github.com/oesteban\"\u003eOscar Esteban\u003c/a\u003e, \u003ca href=\"https://github.com/chrisfilo\"\u003eChris Gorgolewski\u003c/a\u003e and \u003ca href=\"https://github.com/satra\"\u003eSatrajit Ghosh\u003c/a\u003e for their input to this tutorial! And a huge thanks to \u003ca href=\"https://github.com/djarecka/\"\u003eDorota Jarecka\u003c/a\u003e who updated this tutorial to Python 3 and is helping me with keeping this tutorial updated and running!\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1538064645.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.ubuntu"
    ],
    "full_name": "UNM-CARC/heudiconv",
    "latest_release": null,
    "readme": "\u003cp\u003eNot much\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1536784012.0
  },
  {
    "data_format": 2,
    "description": "Python repository of code for preprocessing and extracting metrics of the volume fraction and size of the gamma double prime and gamma prime in a nickel-based superalloy microstructure",
    "filenames": [
      "Singularity"
    ],
    "full_name": "CWRU-MSL/GammaDoublePrime",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-characterization-of-nanoscale-precipitates-in-superalloy-718-using-high-resolution-sem-imaging\" class=\"anchor\" href=\"#characterization-of-nanoscale-precipitates-in-superalloy-718-using-high-resolution-sem-imaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCharacterization of nanoscale precipitates in superalloy 718 using high resolution SEM imaging\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tm-smith-a-nm-senanayake-b-ck-sudbrack-c-p-bonacuse-a-rb-rogers-a-p-chao-d-j-carter-b\" class=\"anchor\" href=\"#tm-smith-a-nm-senanayake-b-ck-sudbrack-c-p-bonacuse-a-rb-rogers-a-p-chao-d-j-carter-b\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eT.M. Smith a*, N.M. Senanayake b, C.K. Sudbrack c, P. Bonacuse a, R.B. Rogers a, P. Chao d, J. Carter b\u003c/h2\u003e\n\u003cp\u003ea NASA Glenn Research Center, Materials and Structures Division, Cleveland, OH 44135, United States of America\nb Case Western Reserve University, Department of Materials Science and Engineering, Cleveland, OH 44106, United States of America\nc QuesTek Innovations LLC, Evanston, IL 60201, United States of America\nd Carnegie Mellon University, Department of Materials Science and Engineering, Pittsburgh, PA 15213, United States of America\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-materials-characterization-212019-v148-p-178-197\" class=\"anchor\" href=\"#materials-characterization-212019-v148-p-178-197\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaterials Characterization, (2/1/2019) V148, p 178-197\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-httpwwwsciencedirectcomsciencearticlepiis1044580318328444\" class=\"anchor\" href=\"#httpwwwsciencedirectcomsciencearticlepiis1044580318328444\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"http://www.sciencedirect.com/science/article/pii/S1044580318328444\" rel=\"nofollow\"\u003ehttp://www.sciencedirect.com/science/article/pii/S1044580318328444\u003c/a\u003e\n\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-doi-101016jmatchar201812018\" class=\"anchor\" href=\"#doi-101016jmatchar201812018\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDOI: 10.1016/j.matchar.2018.12.018\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-repo-information\" class=\"anchor\" href=\"#repo-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRepo Information\u003c/h3\u003e\n\u003cp\u003eThis repository contains the codes necessary to utilize the algorthims presented in the paper below. When implimented, the can be used to obtain accurate volume fraction and size measurements of gamma double prime, and gamma prime, precipitates in Superalloy\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-clone-a-repository\" class=\"anchor\" href=\"#clone-a-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClone a repository\u003c/h2\u003e\n\u003cp\u003eUse these steps to clone from SourceTree, our client for using the repository command-line free. Cloning allows you to work on your files locally. If you don\u0027t yet have SourceTree, \u003ca href=\"https://www.sourcetreeapp.com/\" rel=\"nofollow\"\u003edownload and install first\u003c/a\u003e. If you prefer to clone from the command line, see \u003ca href=\"https://confluence.atlassian.com/x/4whODQ\" rel=\"nofollow\"\u003eClone a repository\u003c/a\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eYou\u2019ll see the clone button under the \u003cstrong\u003eSource\u003c/strong\u003e heading. Click that button.\u003c/li\u003e\n\u003cli\u003eNow click \u003cstrong\u003eCheck out in SourceTree\u003c/strong\u003e. You may need to create a SourceTree account or log in.\u003c/li\u003e\n\u003cli\u003eWhen you see the \u003cstrong\u003eClone New\u003c/strong\u003e dialog in SourceTree, update the destination path and name if you\u2019d like to and then click \u003cstrong\u003eClone\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eOpen the directory you just created to see your repository\u2019s files.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNow that you\u0027re more familiar with your Bitbucket repository, go ahead and add a new file locally. You can \u003ca href=\"https://confluence.atlassian.com/x/iqyBMg\" rel=\"nofollow\"\u003epush your change back to Bitbucket with SourceTree\u003c/a\u003e, or you can \u003ca href=\"https://confluence.atlassian.com/x/8QhODQ\" rel=\"nofollow\"\u003eadd, commit,\u003c/a\u003e and \u003ca href=\"https://confluence.atlassian.com/x/NQ0zDQ\" rel=\"nofollow\"\u003epush from the command line\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1613426965.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "mosoriob/pegasus_montage-workflow-v2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-montage-workflow-v2\" class=\"anchor\" href=\"#montage-workflow-v2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emontage-workflow-v2\u003c/h1\u003e\n\u003cp\u003eA new Python DAX generator version of the classic Montage workflow. This workflow uses the \u003ca href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\"\u003eMontage\ntoolkit\u003c/a\u003e to re-project, background correct and add astronomical\nimages into custom mosaics.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://montage.ipac.caltech.edu\" rel=\"nofollow\"\u003eMontage\u003c/a\u003e - version 4.0 or later\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.astropy.org/\" rel=\"nofollow\"\u003eAstroPy\u003c/a\u003e - version 1.0 or later\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-plan-a-montage-workflow\" class=\"anchor\" href=\"#plan-a-montage-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePlan a Montage Workflow\u003c/h2\u003e\n\u003cp\u003eThe \u003cem\u003e./montage-workflow.py\u003c/em\u003e Python script sets up a \u003cem\u003edata/\u003c/em\u003e directory with a Pegasus DAX,\nimage tables and region headers. For example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./montage-workflow.py --center \"56.7 24.0\" --degrees 2.0 \\\n          --band dss:DSS2B:blue --band dss:DSS2R:green --band dss:DSS2IR:red\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will create a 2x2 degree mosaic centered on 56.7 24.0, with 3 bands making up the\nred, green, and blue channels for the final JPEG output. A 2 degree workflow has a lot\nof input images and thus the workflow becomes wide. I simplified version of the workflow\nlooks like:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"docs/images/dax1.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/dax1.png?raw=true\" alt=\"DAX 1\" title=\"DAX 1\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eThe quickest way to get started is to use the \u003cem\u003e./example-dss.sh\u003c/em\u003e\nscript. It shows how to use the \u003cem\u003emontage-workflow.py\u003c/em\u003e DAX generator to set up and plan\n2 degree workflows as described above. Example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ./example-dss.sh \n\nAdding band 1 (dss DSS2B -\u0026gt; blue)\nRunning sub command: mArchiveList dss DSS2B \"56.7 24.00\" 2.2 2.2 data/1-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mDAGTbls 1-images.tbl region-oversized.hdr 1-raw.tbl 1-projected.tbl 1-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mOverlaps 1-raw.tbl 1-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 2 (dss DSS2R -\u0026gt; green)\nRunning sub command: mArchiveList dss DSS2R \"56.7 24.00\" 2.2 2.2 data/2-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mDAGTbls 2-images.tbl region-oversized.hdr 2-raw.tbl 2-projected.tbl 2-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mOverlaps 2-raw.tbl 2-diffs.tbl\n[struct stat=\"OK\", count=120]\n\nAdding band 3 (dss DSS2IR -\u0026gt; red)\nRunning sub command: mArchiveList dss DSS2IR \"56.7 24.00\" 2.2 2.2 data/3-images.tbl\n[struct stat=\"OK\", count=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mDAGTbls 3-images.tbl region-oversized.hdr 3-raw.tbl 3-projected.tbl 3-corrected.tbl\n[struct stat=\"OK\", count=\"16\", total=\"16\"]\nRunning sub command: cd data \u0026amp;\u0026amp; mOverlaps 3-raw.tbl 3-diffs.tbl\n[struct stat=\"OK\", count=120]\n2016.06.02 21:46:32.455 PDT:    \n2016.06.02 21:46:32.461 PDT:   ----------------------------------------------------------------------- \n2016.06.02 21:46:32.466 PDT:   File for submitting this DAG to HTCondor           : montage-0.dag.condor.sub \n2016.06.02 21:46:32.471 PDT:   Log of DAGMan debugging messages                 : montage-0.dag.dagman.out \n2016.06.02 21:46:32.476 PDT:   Log of HTCondor library output                     : montage-0.dag.lib.out \n2016.06.02 21:46:32.481 PDT:   Log of HTCondor library error messages             : montage-0.dag.lib.err \n2016.06.02 21:46:32.487 PDT:   Log of the life of condor_dagman itself          : montage-0.dag.dagman.log \n2016.06.02 21:46:32.492 PDT:    \n2016.06.02 21:46:32.497 PDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: \n2016.06.02 21:46:32.507 PDT:   ----------------------------------------------------------------------- \n2016.06.02 21:46:33.387 PDT:   Your database is compatible with Pegasus version: 4.6.1 \n2016.06.02 21:46:33.392 PDT:   \n\nI have concretized your abstract workflow. The workflow has been entered \ninto the workflow database with a state of \"planned\". The next step is \nto start or execute your workflow. The invocation required is\n\npegasus-run  /data/scratch/rynge/montage2/montage-workflow-v2/work/1464929190\n\n2016.06.02 21:46:33.419 PDT:   Time taken to execute is 2.961 seconds \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning the workflow produces fits and jpeg mosaics for each band, as well as a combined color one:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"docs/images/pleiades.jpg?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/pleiades.jpg?raw=true\" alt=\"Pleiades\" title=\"Pleiades\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1535257330.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "ext/Singularity"
    ],
    "full_name": "OSC/shiny_launcher",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wip-batch-connect---osc-shiny-app-launcher\" class=\"anchor\" href=\"#wip-batch-connect---osc-shiny-app-launcher\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e[WIP] Batch Connect - OSC Shiny App Launcher\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/5e71a5a07e8e6e15f922edb76b5fb68e7ee6087e336807c38095861b8c7f5fc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f7368696e795f6c61756e636865722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5e71a5a07e8e6e15f922edb76b5fb68e7ee6087e336807c38095861b8c7f5fc2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f7368696e795f6c61756e636865722e737667\" alt=\"GitHub Release\" data-canonical-src=\"https://img.shields.io/github/release/osc/shiny_launcher.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA Batch Connect app designed for OSC OnDemand that launches a Shiny App within\nan Owens batch job.\u003c/p\u003e\n\u003cp\u003eThe Shiny app is included a submodule and each deployment can modified which\nShiny app to deploy using git config to specify the URL for the submodule. This\nway the launcher code can be reused for multiple apps but the launcher and the\napp itself can be managed separately.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eThis Batch Connect app requires the following software be installed on the\n\u003cstrong\u003ecompute nodes\u003c/strong\u003e that the batch job is intended to run on (\u003cstrong\u003eNOT\u003c/strong\u003e the\nOnDemand node):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://shiny.rstudio.com/\" rel=\"nofollow\"\u003eShiny\u003c/a\u003e x.y.z+\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.tacc.utexas.edu/research-development/tacc-projects/lmod\" rel=\"nofollow\"\u003eLmod\u003c/a\u003e 6.0.1+ or any other \u003ccode\u003emodule purge\u003c/code\u003e and \u003ccode\u003emodule load \u0026lt;modules\u0026gt;\u003c/code\u003e based\nCLI used to load appropriate environments within the batch job\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eTODO\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAgain, you do not need to restart the app as it isn\u0027t a Passenger app.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eFork it ( \u003ca href=\"https://github.com/OSC/bc_osc_example_shiny/fork\"\u003ehttps://github.com/OSC/bc_osc_example_shiny/fork\u003c/a\u003e )\u003c/li\u003e\n\u003cli\u003eCreate your feature branch (\u003ccode\u003egit checkout -b my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCommit your changes (\u003ccode\u003egit commit -am \u0027Add some feature\u0027\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003ePush to the branch (\u003ccode\u003egit push origin my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCreate a new Pull Request\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1569007230.0
  },
  {
    "data_format": 2,
    "description": "Quality Control plots and data normalisation for Microarray data",
    "filenames": [
      "Singularity"
    ],
    "full_name": "qbicsoftware-archive/microarray-qc-workflow",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-work-in-progress\" class=\"anchor\" href=\"#work-in-progress\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWork in progress\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-microarray-qc-workflow\" class=\"anchor\" href=\"#microarray-qc-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emicroarray-qc-workflow\u003c/h1\u003e\n\u003cp\u003eTakes .cel files and creates qc plots as well as normalising the data\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/qbicsoftware/microarray-qc-workflow\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/16f4a8e65783fd7014126125a1c351cf1ac4d34d672b6cb4e6cab27706d0c85f/68747470733a2f2f7472617669732d63692e6f72672f71626963736f6674776172652f6d6963726f61727261792d71632d776f726b666c6f772e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/qbicsoftware/microarray-qc-workflow.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/17df893666bfa5cad487466d4476ab773ea5def560c04c50f45795017865e81c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33302e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.30.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/microarray-qc-workflow\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8af3143d98534fd47758128af0ea9ed413467e1151e5ab095c16968f578fcdd3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6d6963726f61727261792d71632d776f726b666c6f772e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/microarray-qc-workflow.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003emicroarray-qc-workflow: Takes .cel files and creates qc plots as well as normalising the data\u003c/p\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe microarray-qc-workflow pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003eThis pipeline was written by Timo Lucas (\u003ca href=\"https://github.com/lucass122\"\u003elucass122\u003c/a\u003e) at \u003ca href=\"http://www.qbic.uni-tuebingen.de/\" rel=\"nofollow\"\u003eQBiC T\u00fcbingen\u003c/a\u003e.\nR script based on script by Stefan Czemmel [qbicStefanC]:\n\u003ca href=\"https://github.com/qbicsoftware/qbic-wf-microarrayQC\"\u003ehttps://github.com/qbicsoftware/qbic-wf-microarrayQC\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1600940492.0
  },
  {
    "data_format": 2,
    "description": "Nextflow workflow for automated IGV snapshots",
    "filenames": [
      "containers/IGV/Singularity.IGV"
    ],
    "full_name": "stevekm/IGV-snapshot-nf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-igv-snapshot-nf\" class=\"anchor\" href=\"#igv-snapshot-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIGV-snapshot-nf\u003c/h1\u003e\n\u003cp\u003eAn example Nextflow workflow for creating automated IGV snapshots of .bam files based on a list of target regions.\u003c/p\u003e\n\u003cp\u003eThis workflow is designed to show how to integrate \u003ca href=\"https://github.com/stevekm/IGV-snapshot-automator\"\u003eautomated IGV snapshotting\u003c/a\u003e into a Nextflow workflow.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eFirst, clone this repository:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/stevekm/IGV-snapshot-automator.git\ncd IGV-snapshot-automator\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h3\u003e\n\u003cp\u003eDocker and/or Singularity containers are used to package IGV, X11, and \u003ccode\u003exvfb\u003c/code\u003e required for functionality. Docker is required to build Singularity containers\u003c/p\u003e\n\u003cp\u003eTo build the Docker container for IGV:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd containers\nmake docker-build VAR=IGV\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo test out the IGV Docker container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake docker-test VAR=IGV\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(optional) To build a Singuarity container for IGV, first build the Singularity Docker container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake docker-build VAR=Singularity-2.4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eThis container is used to build Singularity containers\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo build the Singularity container for IGV:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake singularity-build VAR=IGV\n\n# test the container:\nmake singularity-test VAR=IGV\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eThe Singularity container will be saved to \u003ccode\u003econtainers/IGV/IGV.simg\u003c/code\u003e, which you can upload to your remote server for usage\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003eRun the included demo workflow (from the parent repo directory):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eShould look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eIGV-snapshot-nf$ make run\n./nextflow run main.nf -profile \"docker\"\nN E X T F L O W  ~  version 19.04.1\nLaunching `main.nf` [kickass_cray] - revision: 1823b32e4f\n~~~~~~~ IGV Pipeline ~~~~~~~\n* Project dir:        /Users/steve/projects/IGV-snapshot-nf\n* Launch dir:         /Users/steve/projects/IGV-snapshot-nf\n* Work dir:           /Users/steve/projects/IGV-snapshot-nf/work\n* Profile:            docker\n* Script name:        main.nf\n* Script ID:          1823b32e4f4fbc1caa63b0c12b2d4340\n* Container engine:   docker\n* Workflow session:   843f9541-9cc2-46c8-9005-89659c67ed80\n* Nextflow run name:  kickass_cray\n* Nextflow version:   19.04.1, build 5072 (03-05-2019 12:29 UTC)\n* Launch command:\nnextflow run main.nf -profile docker\n[warm up] executor \u0026gt; local\nexecutor \u0026gt;  local (1)\n[91/852794] process \u0026gt; run_IGV [100%] 1 of 1 \u2714\nCompleted at: 22-May-2019 15:27:46\nDuration    : 1m 20s\nCPU hours   : (a few seconds)\nSucceeded   : 1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExample snapshot output:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://raw.githubusercontent.com/stevekm/IGV-snapshot-nf/output/output/snapshots/chr13_113976596_113976736.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://raw.githubusercontent.com/stevekm/IGV-snapshot-nf/output/output/snapshots/chr13_113976596_113976736.png\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-software\" class=\"anchor\" href=\"#software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTested with macOS 10.12.6 and RHEL 7\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow (requires Java 8+ and \u003ccode\u003ebash\u003c/code\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIGV 2.4.10\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePython\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "igv",
      "nextflow"
    ],
    "updated_at": 1558554996.0
  },
  {
    "data_format": 2,
    "description": "Adapt the BEaST skull stripping method for 7T MRI as a BIDS app",
    "filenames": [
      "Singularity.v0.0.1a"
    ],
    "full_name": "Martybird/7TBEaST",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-7tbeast\" class=\"anchor\" href=\"#7tbeast\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e7TBEaST\u003c/h1\u003e\n\u003cp\u003eAdapt the BEaST skull stripping method for 7T MRI as a BIDS app\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1530840788.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.prc-0_8_0"
    ],
    "full_name": "d-w-moore/singularity-python-irodsclient",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNews\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2019-12-20\" class=\"anchor\" href=\"#2019-12-20\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cem\u003e\u003cstrong\u003e2019-12-20\u003c/strong\u003e\u003c/em\u003e:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecisTopic v3: Faster topic modelling based on WarpLDA (see vignettes for details).\u003c/li\u003e\n\u003cli\u003eThe function runModels() is deprecated. Use runCGSModels() for modelling based on Collapsed Gibbs Sampling (equivalent to runModels()), or runWarpLDAModels() (for modelling based on WarpLDA).\u003c/li\u003e\n\u003cli\u003eVersion 2 objects (with or without models) can be used and analyzed with version 3.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data\" class=\"anchor\" href=\"#cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecisTopic: Probabilistic modelling of cis-regulatory topics from single cell epigenomics data\u003c/h1\u003e\n\u003cp\u003ecisTopic is an R-package to simultaneously identify cell states and cis-regulatory topics from single cell epigenomics data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies-for-r--35\" class=\"anchor\" href=\"#dependencies-for-r--35\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies (for R \u0026lt; 3.5)\u003c/h2\u003e\n\u003cp\u003eThe following packages have to be installed manually before installing cisTopic:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaertslab/RcisTarget\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaertslab/AUCell\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eFor installing and loading cisTopic, run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaertslab/cisTopic\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\nlibrary(\u003cspan class=\"pl-smi\"\u003ecisTopic\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-databases\" class=\"anchor\" href=\"#databases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDatabases\u003c/h2\u003e\n\u003cp\u003eRcisTarget feather databases are available at \u003ca href=\"https://resources.aertslab.org/cistarget/\" rel=\"nofollow\"\u003ehttps://resources.aertslab.org/cistarget/\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tutorials\" class=\"anchor\" href=\"#tutorials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTutorials\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version-2\" class=\"anchor\" href=\"#version-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion 2\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/CompleteAnalysis.html\" rel=\"nofollow\"\u003eBasic tutorial on simulated single cell epigenomes from melanoma cell lines\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/10X_workflow.html\" rel=\"nofollow\"\u003e10X tutorial on the 5k PBMC data set from 10X\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/Runningwithmm10andhg38.html\" rel=\"nofollow\"\u003eRunning GREAT and motif enrichment with the mm10 and hg38 genome assemblies\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version-3\" class=\"anchor\" href=\"#version-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion 3\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_CompleteAnalysis.html\" rel=\"nofollow\"\u003eBasic tutorial on simulated single cell epigenomes from melanoma cell lines\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_10X_workflow.html\" rel=\"nofollow\"\u003e10X tutorial on the 5k PBMC data set from 10X\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1530133683.0
  },
  {
    "data_format": 2,
    "description": "for singularity biuld",
    "filenames": [
      "Singularity"
    ],
    "full_name": "d-w-moore/singularity-icommands-4.2.1",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNews\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2019-12-20\" class=\"anchor\" href=\"#2019-12-20\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cem\u003e\u003cstrong\u003e2019-12-20\u003c/strong\u003e\u003c/em\u003e:\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003ecisTopic v3: Faster topic modelling based on WarpLDA (see vignettes for details).\u003c/li\u003e\n\u003cli\u003eThe function runModels() is deprecated. Use runCGSModels() for modelling based on Collapsed Gibbs Sampling (equivalent to runModels()), or runWarpLDAModels() (for modelling based on WarpLDA).\u003c/li\u003e\n\u003cli\u003eVersion 2 objects (with or without models) can be used and analyzed with version 3.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data\" class=\"anchor\" href=\"#cistopic-probabilistic-modelling-of-cis-regulatory-topics-from-single-cell-epigenomics-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecisTopic: Probabilistic modelling of cis-regulatory topics from single cell epigenomics data\u003c/h1\u003e\n\u003cp\u003ecisTopic is an R-package to simultaneously identify cell states and cis-regulatory topics from single cell epigenomics data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies-for-r--35\" class=\"anchor\" href=\"#dependencies-for-r--35\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies (for R \u0026lt; 3.5)\u003c/h2\u003e\n\u003cp\u003eThe following packages have to be installed manually before installing cisTopic:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaertslab/RcisTarget\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\n\u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaertslab/AUCell\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eFor installing and loading cisTopic, run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-r\"\u003e\u003cpre\u003e\u003cspan class=\"pl-e\"\u003edevtools\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e::\u003c/span\u003einstall_github(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaertslab/cisTopic\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e)\nlibrary(\u003cspan class=\"pl-smi\"\u003ecisTopic\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-databases\" class=\"anchor\" href=\"#databases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDatabases\u003c/h2\u003e\n\u003cp\u003eRcisTarget feather databases are available at \u003ca href=\"https://resources.aertslab.org/cistarget/\" rel=\"nofollow\"\u003ehttps://resources.aertslab.org/cistarget/\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tutorials\" class=\"anchor\" href=\"#tutorials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTutorials\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version-2\" class=\"anchor\" href=\"#version-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion 2\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/CompleteAnalysis.html\" rel=\"nofollow\"\u003eBasic tutorial on simulated single cell epigenomes from melanoma cell lines\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/10X_workflow.html\" rel=\"nofollow\"\u003e10X tutorial on the 5k PBMC data set from 10X\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/Runningwithmm10andhg38.html\" rel=\"nofollow\"\u003eRunning GREAT and motif enrichment with the mm10 and hg38 genome assemblies\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-version-3\" class=\"anchor\" href=\"#version-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion 3\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_CompleteAnalysis.html\" rel=\"nofollow\"\u003eBasic tutorial on simulated single cell epigenomes from melanoma cell lines\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/18ETGIKgXkILo3Xfv9KuysOMqchmSfFX2?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://htmlpreview.github.io/?https://github.com/aertslab/cisTopic/blob/master/vignettes/WarpLDA_10X_workflow.html\" rel=\"nofollow\"\u003e10X tutorial on the 5k PBMC data set from 10X\u003c/a\u003e. Data available \u003ca href=\"https://drive.google.com/drive/folders/1QORpLPsXejva3oFhECLrnAh5a7FVmJF1?usp=sharing\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1527027070.0
  },
  {
    "data_format": 2,
    "description": "Batch Connect - Example Shiny App that runs on OSC OnDemand",
    "filenames": [
      "ext/Singularity"
    ],
    "full_name": "OSC/bc_osc_example_shiny",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wip-batch-connect---osc-example-shiny-app\" class=\"anchor\" href=\"#wip-batch-connect---osc-example-shiny-app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e[WIP] Batch Connect - OSC Example Shiny App\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/a8152a2780451d58acdca1e79b03f771d6e84ae12087e6e7a824b6759b715dc1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f6578616d706c655f7368696e792e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a8152a2780451d58acdca1e79b03f771d6e84ae12087e6e7a824b6759b715dc1/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f6578616d706c655f7368696e792e737667\" alt=\"GitHub Release\" data-canonical-src=\"https://img.shields.io/github/release/osc/bc_osc_example_shiny.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA Batch Connect app designed for OSC OnDemand that launches a Shiny App within\nan Owens batch job.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eThis Batch Connect app requires the following software be installed on the\n\u003cstrong\u003ecompute nodes\u003c/strong\u003e that the batch job is intended to run on (\u003cstrong\u003eNOT\u003c/strong\u003e the\nOnDemand node):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://shiny.rstudio.com/\" rel=\"nofollow\"\u003eShiny\u003c/a\u003e x.y.z+\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.tacc.utexas.edu/research-development/tacc-projects/lmod\" rel=\"nofollow\"\u003eLmod\u003c/a\u003e 6.0.1+ or any other \u003ccode\u003emodule purge\u003c/code\u003e and \u003ccode\u003emodule load \u0026lt;modules\u0026gt;\u003c/code\u003e based\nCLI used to load appropriate environments within the batch job\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eUse git to clone this app and checkout the desired branch/version you want to\nuse:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003escl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git clone \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003erepo\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git checkout \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etag/branch\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou will not need to do anything beyond this as all necessary assets are\ninstalled. You will also not need to restart this app as it isn\u0027t a Passenger\napp.\u003c/p\u003e\n\u003cp\u003eTo update the app you would:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git fetch\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git checkout \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etag/branch\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAgain, you do not need to restart the app as it isn\u0027t a Passenger app.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eFork it ( \u003ca href=\"https://github.com/OSC/bc_osc_example_shiny/fork\"\u003ehttps://github.com/OSC/bc_osc_example_shiny/fork\u003c/a\u003e )\u003c/li\u003e\n\u003cli\u003eCreate your feature branch (\u003ccode\u003egit checkout -b my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCommit your changes (\u003ccode\u003egit commit -am \u0027Add some feature\u0027\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003ePush to the branch (\u003ccode\u003egit push origin my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCreate a new Pull Request\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1527005209.0
  },
  {
    "data_format": 2,
    "description": "Singularity images for deep learning software",
    "filenames": [
      "Singularity.py2_tf17",
      "Singularity.py3_trch",
      "Singularity.py3_dmda",
      "Singularity.py3_tf2gnt",
      "Singularity.py3_tf",
      "Singularity.py2_tf110",
      "Singularity.py3_tf1gnt",
      "Singularity.py3_fast2"
    ],
    "full_name": "gnperdue/singularity_imgs",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity containers (with inspiration from J. Simone, \u003ca href=\"https://github.com/TomaszGolan/mlmpr\"\u003eT. Golan\u003c/a\u003e, and \u003ca href=\"https://github.com/DeepLearnPhysics/larcv2-singularity\"\u003eK. Terao\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/998\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePull, e.g. \u003ccode\u003e$ singularity pull shub://gnperdue/singularity_imgs:py2_tf17\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSingularity.py2_tf110\u003c/code\u003e - See \u003ca href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel-gpu\"\u003eTF\u003c/a\u003e for base package definition.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "singularity-hub",
      "singularity-container"
    ],
    "updated_at": 1593117348.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for High-Performance GEOS-Chem (GCHP)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "geoschem/Singularity_GCHP",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-apsim\" class=\"anchor\" href=\"#apsim\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAPSIM\u003c/h1\u003e\n\u003cp\u003eThe Agricultural Production Systems sIMulator (APSIM) is internationally recognised as a highly advanced simulator of agricultural systems. It contains a suite of modules which enable the simulation of systems that cover a range of plant, animal, soil, climate and management interactions. APSIM is undergoing continual development, with new capability added to regular releases of official versions. Its development and maintenance is underpinned by rigorous science and software engineering standards. The APSIM Initiative has been established to promote the development and use of the science modules and infrastructure software of APSIM.\u003c/p\u003e\n\u003cp\u003eCI builds of this repository can be found \u003ca href=\"https://apsimdev.apsim.info/APSIM.Builds.Portal/Bob.aspx\" rel=\"nofollow\"\u003eHere\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1531155094.0
  },
  {
    "data_format": 2,
    "description": " Build for docker and singularity containers for FMRIQA",
    "filenames": [
      "Singularity",
      "Singularity.4.0.0"
    ],
    "full_name": "VUIIS/FMRIQA_app",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fmriqa_app\" class=\"anchor\" href=\"#fmriqa_app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFMRIQA_app\u003c/h1\u003e\n\u003cp\u003eThis includes everything required (except for the \"spm12r6225_with_vbm8r435_compiled\" directory and \"FMRIQA_v4_0_0\" compiled MATLAB executable, which are too large to commit) to build a docker and corresponding singularity container for the FMRIQA pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/vuiiscci/fmriqa/tags/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.singularity-hub.org/collections/920\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-instructions\" class=\"anchor\" href=\"#build-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Instructions:\u003c/h1\u003e\n\u003cp\u003eJust clone and run \u003ccode\u003ebuild.sh\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/vuiiscci/FMRIQA_app.git\ncd FMRIQA_app/\n./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE that you must have \"spm12r6225_with_vbm8r435_compiled\" directory and \"FMRIQA_v4_0_0\" compiled MATLAB executable.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-instructions\" class=\"anchor\" href=\"#run-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Instructions:\u003c/h1\u003e\n\u003cp\u003eFor docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/ \\\n--user $(id -u):$(id -g) \\\nvuiiscci/fmriqa\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\nshub://vuiiscci/FMRIQA_app\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1592512932.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "weatherlab/metview",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-metview\" class=\"anchor\" href=\"#metview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emetview\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1523286570.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for NMRPipe",
    "filenames": [
      "Singularity",
      "Singularity.212_64"
    ],
    "full_name": "ResearchIT/NMRPipe",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-nmrpipe\" class=\"anchor\" href=\"#singularity-recipe-for-nmrpipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipe for NMRPipe\u003c/h1\u003e\n\u003cp\u003eThis repo contains the recipe to run \u003ca href=\"https://www.ibbr.umd.edu/nmrpipe/\" rel=\"nofollow\"\u003eNMRPipe\u003c/a\u003e\nwithin a \u003ca href=\"https://singularity.lbl.gov\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container, which can be built using \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eVersions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e212_64 - NMRPipe linux212_64 built on centos7.4\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1523030864.0
  },
  {
    "data_format": 2,
    "description": " Build for docker and singularity containers for temporal lobe segmentation",
    "filenames": [
      "Singularity",
      "Singularity.3.1.0"
    ],
    "full_name": "VUIIS/Temporal_Lobe_app",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-temporal_lobe_app\" class=\"anchor\" href=\"#temporal_lobe_app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTemporal_Lobe_app\u003c/h1\u003e\n\u003cp\u003eThis includes everything required to build a docker and corresponding singularity container for the Temporal Lobe pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/vuiiscci/temporal_lobe/tags/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.singularity-hub.org/collections/828\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-instructions\" class=\"anchor\" href=\"#build-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Instructions:\u003c/h1\u003e\n\u003cp\u003eJust clone and run \u003ccode\u003ebuild.sh\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/vuiiscci/Temporal_Lobe_app.git\ncd Temporal_Lobe_app/\n./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-instructions\" class=\"anchor\" href=\"#run-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Instructions:\u003c/h1\u003e\n\u003cp\u003eFor docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/ \\\n--user $(id -u):$(id -g) \\\nvuiiscci/temporal_lobe\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\nshub://vuiiscci/Temporal_Lobe_app\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1592512741.0
  },
  {
    "data_format": 2,
    "description": "PreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "soudabeh19/centos7-reprozip.fslbuild-centos5",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos7-reprozipfslbuild-centos5\" class=\"anchor\" href=\"#centos7-reprozipfslbuild-centos5\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos7-reprozip.fslbuild-centos5\u003c/h1\u003e\n\u003cp\u003ePreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1521572666.0
  },
  {
    "data_format": 2,
    "description": "Nextflow + Singularity/Docker demo for CentOS 6.8 without OverlayFS",
    "filenames": [
      "containers/demo1/Singularity.demo1",
      "containers/base/Singularity.base"
    ],
    "full_name": "stevekm/NYU-phoenix-docker-singularity-nextflow-demo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nyu-phoenix-hpc-dockersingularity-nextflow-demo\" class=\"anchor\" href=\"#nyu-phoenix-hpc-dockersingularity-nextflow-demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNYU phoenix HPC Docker/Singularity Nextflow Demo\u003c/h1\u003e\n\u003cp\u003eDemo on how to run a Nextflow pipeline on the HPC using Singularity containers built from Docker.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/stevekm/NYU-phoenix-docker-singularity-nextflow-demo.git\ncd NYU-phoenix-docker-singularity-nextflow-demo\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-remote-hpc-phoenix\" class=\"anchor\" href=\"#remote-hpc-phoenix\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemote HPC (phoenix)\u003c/h2\u003e\n\u003cp\u003eTo run this workflow on the NYU phoenix HPC system, use the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run-p\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003einstall Nextflow to the current directory\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eextract a pre-built demo Singularity image from this repository\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erun the Nextflow pipeline using the Singularity image\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local\" class=\"anchor\" href=\"#local\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal\u003c/h2\u003e\n\u003cp\u003eTo run this workflow on your local computer (Docker required), use the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run-l\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003einstall Nextflow to the current directory\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ebuild the Docker containers included in this repository\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erun the Nextflow pipeline using the Docker containers\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eMakefile\u003c/code\u003e: shortcuts to common actions used in the demo\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003emain.nf\u003c/code\u003e: main Nextflow pipeline file\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003enextflow.config\u003c/code\u003e: Nextflow configuration file\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ebin\u003c/code\u003e: directory for scripts to use inside the Nextflow pipeline; its contents will be prepended to your \u003ccode\u003ePATH\u003c/code\u003e when pipeline tasks are executed\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003econtainers\u003c/code\u003e: directory containing Docker and Singularity container files, along with documentation on their setup \u0026amp; usage\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-software-requirements\" class=\"anchor\" href=\"#software-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware Requirements\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local--remote-hpc-server\" class=\"anchor\" href=\"#local--remote-hpc-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elocal \u0026amp; remote HPC server\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eJava 8 (for Nextflow)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGraphViz Dot (to compile flowchart)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local-only\" class=\"anchor\" href=\"#local-only\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elocal only\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDocker version 17.12.0-ce, build c97c6d6\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eVagrant version 2.0.1 (for tesing Singularity containers)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-remote-hpc-server-only\" class=\"anchor\" href=\"#remote-hpc-server-only\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eremote HPC server only\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity version 2.4.2\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1521145930.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.add_g2gtools",
      "Singularity_recipev1.0",
      "Singularity_recipe_R.3.4.1",
      "Singularity_recipev1.0_addR.3.4.3",
      "Singularity_hicpro_v1",
      "Singularity_add.R_packages",
      "Singularity_recipev1.R-3-4-3",
      "Singularity.add_python_packages",
      "Singularity_recipe0_part1",
      "Singularity_recipe_MMARGE"
    ],
    "full_name": "pranithavangala/singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-multi_atlas_app\" class=\"anchor\" href=\"#multi_atlas_app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMulti_Atlas_app\u003c/h1\u003e\n\u003cp\u003eThis includes everything required (except for the \"full-multi-atlas\" directory) to build a docker and corresponding singularity container for the Multi Atlas pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/vuiiscci/multi_atlas/tags/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/734\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-instructions\" class=\"anchor\" href=\"#build-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Instructions:\u003c/h1\u003e\n\u003cp\u003eJust clone and run \u003ccode\u003ebuild.sh\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/vuiiscci/Multi_Atlas_app.git\ncd Multi_Atlas_app/\n./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE that you must have full-multi-atlas directory which contains atlases.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-instructions\" class=\"anchor\" href=\"#run-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Instructions:\u003c/h1\u003e\n\u003cp\u003eFor docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/ \\\n--user $(id -u):$(id -g) \\\nvuiiscci/multi_atlas\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\nshub://vuiiscci/Multi_Atlas_app\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1609299433.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for GEOS-Chem",
    "filenames": [
      "Singularity"
    ],
    "full_name": "geoschem/Singularity_GC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-multi_atlas_app\" class=\"anchor\" href=\"#multi_atlas_app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMulti_Atlas_app\u003c/h1\u003e\n\u003cp\u003eThis includes everything required (except for the \"full-multi-atlas\" directory) to build a docker and corresponding singularity container for the Multi Atlas pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/vuiiscci/multi_atlas/tags/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/734\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-instructions\" class=\"anchor\" href=\"#build-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Instructions:\u003c/h1\u003e\n\u003cp\u003eJust clone and run \u003ccode\u003ebuild.sh\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/vuiiscci/Multi_Atlas_app.git\ncd Multi_Atlas_app/\n./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE that you must have full-multi-atlas directory which contains atlases.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-instructions\" class=\"anchor\" href=\"#run-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Instructions:\u003c/h1\u003e\n\u003cp\u003eFor docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/ \\\n--user $(id -u):$(id -g) \\\nvuiiscci/multi_atlas\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\nshub://vuiiscci/Multi_Atlas_app\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [
      "geos-chem",
      "singularity-container",
      "docker-image"
    ],
    "updated_at": 1564082038.0
  },
  {
    "data_format": 2,
    "description": " Build for docker and singularity containers for Multi Atlas",
    "filenames": [
      "Singularity",
      "Singularity.2.1.0"
    ],
    "full_name": "VUIIS/Multi_Atlas_app",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-multi_atlas_app\" class=\"anchor\" href=\"#multi_atlas_app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMulti_Atlas_app\u003c/h1\u003e\n\u003cp\u003eThis includes everything required (except for the \"full-multi-atlas\" directory) to build a docker and corresponding singularity container for the Multi Atlas pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/vuiiscci/multi_atlas/tags/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/734\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-instructions\" class=\"anchor\" href=\"#build-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Instructions:\u003c/h1\u003e\n\u003cp\u003eJust clone and run \u003ccode\u003ebuild.sh\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/vuiiscci/Multi_Atlas_app.git\ncd Multi_Atlas_app/\n./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE that you must have full-multi-atlas directory which contains atlases.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-instructions\" class=\"anchor\" href=\"#run-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Instructions:\u003c/h1\u003e\n\u003cp\u003eFor docker:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run --rm \\\n-v $(pwd)/INPUTS/:/INPUTS/ \\\n-v $(pwd)/OUTPUTS:/OUTPUTS/ \\\n--user $(id -u):$(id -g) \\\nvuiiscci/multi_atlas\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor singularity:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -e \\\n-B INPUTS/:/INPUTS \\\n-B OUTPUTS/:/OUTPUTS \\\nshub://vuiiscci/Multi_Atlas_app\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1592512956.0
  },
  {
    "data_format": 2,
    "description": "Virtual Research Environment for Sara Server - container build scripts",
    "filenames": [
      "Singularity"
    ],
    "full_name": "54r4/sara-server-vre",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-sara-server-vre\" class=\"anchor\" href=\"#sara-server-vre\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esara-server-vre\u003c/h1\u003e\n\u003cp\u003eVirtual Research Environment for Sara Server - container build scripts\u003c/p\u003e\n\u003cp\u003eThis is the VRE main spec containing a Java Runtime Environment plus Eclipse\nused for the development of the SARA service.\nA local postgres database is integrated, too. The source is a docker repo\nwhich is being pulled on build time and used to locally run a postgresql\nserver using udocker.\nThis VRE has no external requirements whatsoever once the image has been built.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-use-prebuild-image\" class=\"anchor\" href=\"#use-prebuild-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse prebuild image\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e   cd /tmp\n   singularity pull --name \"sara-server-vre.img\" shub://c1t4r/sara-server-vre\n   ./sara-server-vre.img\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-local-image-singularity-23\" class=\"anchor\" href=\"#build-local-image-singularity-23\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild local image (Singularity 2.3)\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003ecd /tmp\nsingularity create -s 2048 sara-server-vre.img\nsingularity bootstrap sara-server-vre.img ./Singularity\n./sara-server-vre.img\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-local-image-singularity-24\" class=\"anchor\" href=\"#build-local-image-singularity-24\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild local image (Singularity 2.4)\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build sara-server-vre.simg ./Singularity\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1546985098.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for scipion",
    "filenames": [
      "Singularity.2.0.cuda",
      "Singularity.2.0",
      "Singularity.1.1",
      "Singularity.1.1.cuda"
    ],
    "full_name": "ResearchIT/scipion",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://zenodo.org/badge/latestdoi/124456755\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4641d504e79e577f2add43b190e60f3910a1688ac8f26f972d799fd6f3f4b213/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3132343435363735352e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/124456755.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/ambv/black\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\" alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium\" class=\"anchor\" href=\"#ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAncient hybridization and adaptive introgression of an invadolysin gene in \u003cem\u003eSchistosoma haematobium\u003c/em\u003e.\u003c/h1\u003e\n\u003cp\u003eRoy N. Platt II, Marina McDew-White, Winka Le Clec\u0027h, Frederic D. Chevalier, Fiona Allan, Aidan M. Emery, Amadou Garba, Shaali M. Ame, Joanne P. Webster, David Rollinson, Bonnie L. Webster, Timothy J. C. Anderson.\u003c/p\u003e\n\u003cp\u003eThe parasitic blood fluke \u003cem\u003eSchistosoma\u003c/em\u003e \u003cem\u003ehaematobium\u003c/em\u003e causes urogenital schistosomiasis in humans and is a major cause of morbidity and mortality across sub-Saharan Africa. \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e can hybridize with closely-related livestock schistosomes, including \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ebovis\u003c/em\u003e, however the frequency, direction, age and genomic consequences of hybridization in nature are unknown. We sequenced 96 \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e exomes from Niger and the Zanzibar archipelago. We found evidence of an ancient, adaptive introgression event between Nigerien \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e and \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ebovis\u003c/em\u003e occurring 108-613 generations ago. Introgressed S. bovis alleles constitute 3.3-8.2% of Nigerien \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e genomes. Some \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ebovis\u003c/em\u003e alleles have reached high frequency and show signatures of directional selection; the strongest signal spans a single gene in the invadolysin gene family, an M8 metalloprotease associated with parasitic life-history traits.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-biorxiv-pre-print\" class=\"anchor\" href=\"#biorxiv-pre-print\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://doi.org/10.1101/539353\" rel=\"nofollow\"\u003ebioRxiv pre-print\u003c/a\u003e\n\u003c/h4\u003e\n\u003chr\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNOTES:\u003c/h3\u003e\n\u003cp\u003eAll analyses were conducted on a HPCC in a \u003ccode\u003esingularity\u003c/code\u003e container or in a \u003ccode\u003econda\u003c/code\u003e managed environment. The singularity recipe and conda environmental yaml are in the \u003ccode\u003econfig\u003c/code\u003e dir.\u003c/p\u003e\n\u003cp\u003eRaw code is found in the \u003ccode\u003escripts\u003c/code\u003e dir\u003c/p\u003e\n\u003cp\u003eData that is not readily available through the SRA is in the \u003ccode\u003edata\u003c/code\u003e dir.  These will be housed in an online repository (ex. Dryad), but provided here for documentation purposes.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [
      "singularity",
      "scipion"
    ],
    "updated_at": 1592515044.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "markxiao/freesurfer",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-freesurfer\" class=\"anchor\" href=\"#freesurfer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efreesurfer\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1618603672.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "markxiao/fsl",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fsl\" class=\"anchor\" href=\"#fsl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efsl\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618603672.0
  },
  {
    "data_format": 2,
    "description": "R wrapper for bamdb",
    "filenames": [
      "src/bamdb/Singularity.bamdb"
    ],
    "full_name": "D-Lo/bambi",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://travis-ci.org/mskilab/bambi\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/47c82ab2d405aa684f3a5004ed8fc79887c025105127effda9ce1d35b5568974/68747470733a2f2f7472617669732d63692e6f72672f6d736b696c61622f62616d62692e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/mskilab/bambi.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/github/mskilab/bambi?branch=master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ccb3814df2f3f1c65e518dd49a10732518ba754f251e50546a0d42ec9fd9cdab/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f6d736b696c61622f62616d62692e737667\" alt=\"codecov.io\" data-canonical-src=\"https://img.shields.io/codecov/c/github/mskilab/bambi.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-bambi\" class=\"anchor\" href=\"#bambi\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebambi\u003c/h1\u003e\n\u003cp\u003eR package for querying 10x WGS and single-cell BAMs\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cpre lang=\"{r}\"\u003e\u003ccode\u003edevtools::install_github(\u0027mskilab/gUtils\u0027)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre lang=\"{r}\"\u003e\u003ccode\u003edevtools::install_github(\u0027mskilab/bamUtils\u0027)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bambi-commands\" class=\"anchor\" href=\"#bambi-commands\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebambi commands\u003c/h2\u003e\n\u003cp\u003eInstantiate a bambi object:\u003c/p\u003e\n\u003cp\u003eMethods:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003egrab_bx()\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egrab_bx(barcodes, query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003egrab_cb()\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egrab_cb(barcodes, query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003egrab_ub()\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egrab_ub(barcodes, query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003efetch_by_tag()\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003efetch_by_tag(tag, tag_queries, query=NULL, data.table = FALSE, verbose = FALSE, mc.cores = 1)\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-demo\" class=\"anchor\" href=\"#demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eInstantiate a \u003ccode\u003ebambi\u003c/code\u003e object\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{r}\"\u003e\u003ccode\u003elibrary(bambi)\n\n\u0026gt; hcc1143_subset = bambi$new(bam_file = \"subsetHCC1143_phased_possorted0001.bam\", bamdb_path=\"subsetHCC1143_phased_possorted0001_lmdb\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eCall methods\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{r}\"\u003e\u003ccode\u003e\u0026gt; hcc1143_subset$grab_bx(\u0027CGACGTGTCCTCTAGC-1\u0027)\nGRanges object with 2 ranges and 11 metadata columns:\n      seqnames                 ranges strand |\n         \u0026lt;Rle\u0026gt;              \u0026lt;IRanges\u0026gt;  \u0026lt;Rle\u0026gt; |\n  [1]     chr1 [147975454, 147975580]      + |\n  [2]     chr1 [147975675, 147975824]      - |\n                                         qname      flag      mapq       cigar\n                                   \u0026lt;character\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;character\u0026gt;\n  [1] ST-K00126:3:H5TL3BBXX:2:2109:25926:37800        99        16        127M\n  [2] ST-K00126:3:H5TL3BBXX:2:2109:25926:37800       147        16        150M\n            rnext     pnext      tlen\n      \u0026lt;character\u0026gt; \u0026lt;numeric\u0026gt; \u0026lt;numeric\u0026gt;\n  [1]           = 147975676       371\n  [2]           = 147975455      -371\n                                                                                                                                                         seq\n                                                                                                                                                 \u0026lt;character\u0026gt;\n  [1]                        ATGTCTTCTTCCTCATTATCTGGCACTGGTTAGGAAGCACTCATCTCCATGAAGTCATCTTTTGTTAATTCCTCTGGTGTGGTGTGTATTAGCTCTTAAATTCCTCCAAGATCCATATCTTGCAACC\n  [2] ATCTGGACACAAATTGTACTTTTGTCCAGCACGAATTTATTGTTTTGAGTTTCATGGTTTTCTATATCAACTGATGACATCTTGAAAGGTGTAAGCCTTCCAGACTTCCATGATGTTCTCTCTATTGGGTTTCTCTTTTGCAATGTTGAC\n                                                                                                                                                        qual\n                                                                                                                                                 \u0026lt;character\u0026gt;\n  [1]                        JJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJJFJJJJJJJJJJJAJFJJJJJJJJJFJJJJJJJJJJFJJJJFFFJJJFJJJJJJAAJFJJJFAFAFFFJAA\u0026lt;7F\u0026lt;\n  [2] A\u0026lt;7FFFJFFFAJJAAAJJF\u0026lt;F\u0026lt;7A-\u0026lt;AA-\u0026lt;\u0026lt;\u0026lt;AFFJJJJJJJJFFJAFFAAFJFJJJAFFJJJJJJJJJJFJFAJJJJJJFJJJJJJ\u0026lt;FFJJJFJJJFJJJJJJJJJJJJJFJJJJFFJ7JJJJF\u0026lt;JJJJJJJJJJJJJJJJJJJFFAA\u0026lt;\n                      BX    qwidth\n             \u0026lt;character\u0026gt; \u0026lt;integer\u0026gt;\n  [1] CGACGTGTCCTCTAGC-1       127\n  [2] CGACGTGTCCTCTAGC-1       150\n  -------\n  seqinfo: 1 sequence from an unspecified genome; no seqlengths\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1531085438.0
  },
  {
    "data_format": 2,
    "description": "A container for PyMultinest",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sysmso/singularity-multinest",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-multinest\" class=\"anchor\" href=\"#singularity-multinest\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-multinest\u003c/h1\u003e\n\u003cp\u003eA container for PyMultinest\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1602594100.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for Tofu2",
    "filenames": [
      "Singularity",
      "Singularity.v17"
    ],
    "full_name": "ResearchIT/tofu2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-tofu2\" class=\"anchor\" href=\"#singularity-recipe-for-tofu2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipe for Tofu2\u003c/h1\u003e\n\u003cp\u003eThis repo contains recipes to run \u003ca href=\"https://github.com/PacificBiosciences/IsoSeq_SA3nUP/wiki/%5BBeta%5D-ToFU2:-running-and-installing-ToFU2#install\"\u003eTofu2\u003c/a\u003e\nwithin a \u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container, which can be built\nusing \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eVersions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ev17 - Tofu2 installed on Ubuntu\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to Use:\u003c/h2\u003e\n\u003cp\u003eRun example:\u003c/p\u003e\n\u003cp\u003esingularity run shub://ResearchIT/tofu2 run_preCluster.py --cpus=4\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-alternative-method\" class=\"anchor\" href=\"#alternative-method\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlternative method:\u003c/h2\u003e\n\u003cp\u003euse the provided bash wrapper and module file to use the tofu2 singularity container like a standard module\n(this assumes you have a singularity/2.4 module)\u003c/p\u003e\n\u003cp\u003ee.g.\u003c/p\u003e\n\u003cp\u003emodule load tofu2/v17\ntofu2 run_preCluster.py --cpus=4\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 6,
    "topics": [
      "tofu",
      "pacbio",
      "singularity"
    ],
    "updated_at": 1522255502.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "tanhnhn/singularityhub-sregistry",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-registry\" class=\"anchor\" href=\"#singularity-registry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Registry\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"http://joss.theoj.org/papers/050362b7e7691d2a5d0ebed8251bc01e\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4cb65855144c475cbe5584c579404a17e3e6984f958da24427dbe46b6202eb3c/687474703a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f30353033363262376537363931643261356430656265643832353162633031652f7374617475732e737667\" alt=\"status\" data-canonical-src=\"http://joss.theoj.org/papers/050362b7e7691d2a5d0ebed8251bc01e/status.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://doi.org/10.5281/zenodo.1012531\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/411f713db9ba01edfcb60386aaa1dff3e4ed4464707b95d889900a88d8f54936/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313031323533312e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.1012531.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://singularityhub.github.io/sregistry\" rel=\"nofollow\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-what-is-singularity-registry\" class=\"anchor\" href=\"#what-is-singularity-registry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat is Singularity Registry\u003c/h2\u003e\n\u003cp\u003eSingularity Registry is a management and storage of Singularity images for an institution or user to deploy locally. It does not manage building, but serves endpoints to obtain and save containers. The Registry is expected to be available for use in the Fall.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-images-included\" class=\"anchor\" href=\"#images-included\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImages Included\u003c/h2\u003e\n\u003cp\u003eSingularity Registry consists of several Docker images, and they are integrated to work together using \u003ca href=\"docker-compose.yml\"\u003edocker-compose.yml\u003c/a\u003e. The images are the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003evanessa/sregistry\u003c/strong\u003e: is the main uwsgi application, which serves a Django (python-based) application.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003enginx\u003c/strong\u003e: pronounced (engine-X) is the webserver. The starter application is configured for http, however you should follow the instructions to set up https properly.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eworker\u003c/strong\u003e: is the same uwsgi image, but with a running command that is specialized to perform tasks. The tasks are run via \u003ca href=\"http://www.celeryproject.org/\" rel=\"nofollow\"\u003ecelery\u003c/a\u003e, a distributed job queue that fits nicely into Django. The celery worker uses a\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eredis\u003c/strong\u003e: database to organize the jobs themselves.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor more information about Singularity Registry, please reference the \u003ca href=\"https://singularityhub.github.io/sregistry\" rel=\"nofollow\"\u003edocs\u003c/a\u003e. If you have any issues, please \u003ca href=\"https://github.com/singularityhub/sregistry/issues\"\u003elet me know\u003c/a\u003e!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis code is licensed under the Affero GPL, version 3.0 or later \u003ca href=\"LICENSE\"\u003eLICENSE\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1513562903.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for samtools ",
    "filenames": [
      "Singularity",
      "old/Singularity.v1.6"
    ],
    "full_name": "stevekm/singularity-samtools-demo",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h1\u003e\n\u003cp\u003eThis assumes you are building a Singularity container locally on a Mac\u003c/p\u003e\n\u003cp\u003eMake sure you\u0027ve already installed Vagrant, since its needed to run Singularity on a Mac\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebrew cask install virtualbox\nbrew cask install vagrant\nbrew cask install vagrant-manager\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have trouble install Vagrant with homebrew, try using \u003ca href=\"https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg\" rel=\"nofollow\"\u003ethis\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-creating-the-container\" class=\"anchor\" href=\"#creating-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreating the Container\u003c/h1\u003e\n\u003cp\u003eThe workflow for creating a Singularity container on a Mac through Vagrant is saved in the included \u003ccode\u003eMakefile\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eMake the container by running:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake container\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd run a test on the created container with\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake \u003cspan class=\"pl-c1\"\u003etest\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h2\u003e\n\u003cp\u003eIf everything worked, the following files should be created:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esingularity-vm/image/singularity-container-samtools\u003c/code\u003e: the Singularity container file for samtools\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esingularity-vm/image/samtools-version.txt\u003c/code\u003e: the output from running samtools inside the container, should look like this:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esamtools 1.6\nUsing htslib 1.6\nCopyright (C) 2017 Genome Research Ltd.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-resources\" class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResources\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"http://singularity.lbl.gov/install-mac\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/install-mac\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4\" rel=\"nofollow\"\u003ehttps://app.vagrantup.com/singularityware/boxes/singularity-2.4\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg\" rel=\"nofollow\"\u003ehttps://releases.hashicorp.com/vagrant/2.0.1/vagrant_2.0.1_x86_64.dmg\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://singularity.lbl.gov/docs-build-container\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/docs-build-container\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://singularity.lbl.gov/docs-recipes\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/docs-recipes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/qbicsoftware/qbic-singularity-samtools\"\u003ehttps://github.com/qbicsoftware/qbic-singularity-samtools\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "singularity-container"
    ],
    "updated_at": 1521728818.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "tgac-vumc/QDNAseq.snakemake",
    "latest_release": null,
    "readme": "\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://github.com/tgac-vumc/QDNAseq.snakemake/blob/master/DAG_all.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"100%\" height=\"100%\" src=\"https://github.com/tgac-vumc/QDNAseq.snakemake/raw/master/DAG_all.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eFor the installation of this pipeline any Python install compatable Conda is required.\u003c/p\u003e\n\u003cp\u003eThe pipeline itself will run on Python 3.8.5 and R 3.6.3. For exact dependencies view \u003ccode\u003eenvironment.yaml\u003c/code\u003e and \u003ccode\u003er-dependencies.R\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-condamamba\" class=\"anchor\" href=\"#using-condamamba\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Conda/Mamba\u003c/h3\u003e\n\u003cp\u003efor easy installation you need (Mini)Conda.\u003c/p\u003e\n\u003cp\u003eMiniconda installation from folder where you want to install Miniconda:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd \u0026lt;/path/to/files/dir/\u0026gt;\nwget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003efollow the instructions of the installation process, give the location where you want Miniconda to be installed and answer YES to add Miniconda to your path.\u003c/p\u003e\n\u003cp\u003ego to the directory where the analysis need to be performed\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd \u0026lt;/path/to/analysis/dir\u0026gt;\ngit clone https://github.com/tgac-vumc/QDNAseq.snakemake/\ncd QDNAseq.snakemake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003einstall Mamba as drop-in replacement for Conda with Mamba\u0027s improved installation-performance:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda install -c conda-forge mamba\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecreate  the environment using Mamba:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emamba env create --name QDNAseq-snakemake --file environment.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eactivate the environment by:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate QDNAseq-snakemake\n\u003c/code\u003e\u003c/pre\u003e\n\n\u003cp\u003eThen run the R-script r-dependencies.R in the terminal to install the non-conda R dependencies in the environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eRscript r-dependencies.R\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Singularity\u003c/h3\u003e\n\u003cp\u003eUnder development\u003c/p\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-preparing-analysis\" class=\"anchor\" href=\"#preparing-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreparing analysis\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prepare-the-data\" class=\"anchor\" href=\"#prepare-the-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare the data\u003c/h3\u003e\n\u003cp\u003ego to analysis dir and prepare analysis by copy or create links to fastq.gz files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd \u0026lt;/path/to/analysis/dir\u0026gt;\n\nmkdir fastq\ncd fastq\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto link a single file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eln -s \u0026lt;path/to/file\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto link all files from a folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003efor file in \u0026lt;path/to/fastq/files\u0026gt;/*.fastq.gz\ndo ln -s $file\ndone\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prepare-the-snakemake-settings\" class=\"anchor\" href=\"#prepare-the-snakemake-settings\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare the snakemake settings\u003c/h3\u003e\n\u003cp\u003eOpen the configuration file \u003ccode\u003econfig.yaml\u003c/code\u003e to check the settings that snakemake will use and change according to your needs.\nFor providing service-analysis, set \u003ccode\u003esetting\u003c/code\u003e to \u003ccode\u003e\u0027service\u0027\u003c/code\u003e. For research purposes, set \u003ccode\u003esetting\u003c/code\u003e to \u003ccode\u003e\u0027research\u0027\u003c/code\u003e. For all settings set \u003ccode\u003esetting\u003c/code\u003e to \u003ccode\u003e\u0027all\u0027\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOne of the options in the configfile is \u003ccode\u003edewaving\u003c/code\u003e, if set to \u003ccode\u003e\u0027true\u0027\u003c/code\u003e QNDAseq objects will be dewaved before segmentation.\u003c/p\u003e\n\u003cp\u003eThese options change the rules performed in the pipeline, see the rule-graph in the next section.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-analysis\" class=\"anchor\" href=\"#running-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning analysis\u003c/h2\u003e\n\u003cp\u003eMake sure that snakemake is able to find the excecutive file Snakefile by performing a dry-run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd ../QDNAseq.snakemake\nsnakemake -n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck the rules that are planned to be performed, conform the rule-graph.\u003c/p\u003e\n\u003cp\u003eAn visualization of the order of rules to be performed can be viewed by running the following command and opening the DAG-file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --forceall --rulegraph | dot -Tsvg \u0026gt; DAG.svg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRulegraphs for the intial settings \u003ccode\u003e\u0027service\u0027\u003c/code\u003e, \u003ccode\u003e\u0027research\u0027\u003c/code\u003e and \u003ccode\u003e\u0027all\u0027\u003c/code\u003e are commited to this repro in the files \u003ccode\u003eDAG_\u0026lt;setting\u0026gt;.svg\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eWhen ready, run the analysis\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUseful snakemake options\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-j , --cores, --jobs\u003c/code\u003e : Use at most N cores in parallel (default: 1). If N is omitted, the limit is set to the number of available cores.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-n , --dryrun\u003c/code\u003e : Do not execute anything. but show rules which are planned to be performed.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-k , --keep-going\u003c/code\u003e : Go on with independent jobs if a job fails.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-f , --force\u003c/code\u003e : Force the execution of the selected target or the first rule regardless of already created output.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-R , --forcerun\u003c/code\u003e : Force the re-execution or creation of the given rules or files. Use this option if you changed a rule and want to have all its output in your workflow updated.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-U , --until\u003c/code\u003e : Runs the pipeline until it reaches the specified rules or files. Only runs jobs that are dependencies of the specified rule or files, does not run sibling DAGs.\u003c/p\u003e\n\u003cp\u003efor all options go to \u003ca href=\"https://snakemake.readthedocs.io/en/v5.31.1/executing/cli.html#all-options\" rel=\"nofollow\"\u003ehttps://snakemake.readthedocs.io/en/v5.31.1/executing/cli.html#all-options\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1622800091.0
  },
  {
    "data_format": 2,
    "description": "FEniCS containers for CARC systems",
    "filenames": [
      "Singularity.ubuntu",
      "Singularity.docker"
    ],
    "full_name": "UNM-CARC/FEniCS",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fenics\" class=\"anchor\" href=\"#fenics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFEniCS\u003c/h1\u003e\n\u003cp\u003eThis repository contains a FEniCS container for UNM CARC high performance systems\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity.docker - Singularity container built from the standard FEniCS docker container\u003c/li\u003e\n\u003cli\u003eSingularity.ubuntu - Singularity container built from the FEniCS ubuntu packages\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1511832970.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipes for CARC systems",
    "filenames": [
      "Singularity.ubuntu-mpich",
      "Singularity.ubuntu-ompi",
      "Singularity.centos"
    ],
    "full_name": "UNM-CARC/singularity-test",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-tests\" class=\"anchor\" href=\"#singularity-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Tests\u003c/h1\u003e\n\u003cp\u003eThis repository contains test singularity recipes for Ubuntu and CentOS repository builds for\nHPC systems at the UNM Center for Advanced Research Computing. These recipes are generally built\nusing Singularity Hub, which links to this repository, and are meant for debugging basic\ncontainer setups that are then used to develop other more complex recipes.\u003c/p\u003e\n\u003cp\u003eNote that these containers pull the CARC modules //into// the containers when they run so that\ncode compiled outside the container can run inside the container. That\u0027s rarely something you want to\ndo, as one of the main point of containers is that they\u0027re stable and reproducible.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1536783389.0
  },
  {
    "data_format": 2,
    "description": "dot and other graphviz executable in a simple singularity container",
    "filenames": [
      "singularity/Singularity.v1"
    ],
    "full_name": "cokelaer/graphviz4all",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-graphviz4all\" class=\"anchor\" href=\"#graphviz4all\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egraphviz4all\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eDEPRECATED, Aug 2020\u003c/strong\u003e: This is now part of \u003ca href=\"https://damona.readthedocs.io\" rel=\"nofollow\"\u003ehttps://damona.readthedocs.io\u003c/a\u003e project.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edamona install graphviz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA container with graphviz (\u003ca href=\"http://www.graphviz.org/\" rel=\"nofollow\"\u003ehttp://www.graphviz.org/\u003c/a\u003e) executables (dot, circo, etc).\u003c/p\u003e\n\u003cp\u003eThis is for Singularity 2.4 at least and is available on singularity-hub\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name graphviz.img shub://cokelaer/graphviz4all:v1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConversion of the dot file into SVG conterpart:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./graphviz.img dot -Tsvg test.dot -o test.svg\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "dot",
      "circo",
      "graphviz",
      "singularity"
    ],
    "updated_at": 1597173467.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "thehyve/singularity-jupyter",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-jupyter\" class=\"anchor\" href=\"#singularity-jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-jupyter\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1611165393.0
  },
  {
    "data_format": 2,
    "description": "pacbio tools",
    "filenames": [
      "singularity/Singularity.v2",
      "singularity/Singularity.v3",
      "singularity/Singularity.v1"
    ],
    "full_name": "cokelaer/pacbio4all",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pacbio4all\" class=\"anchor\" href=\"#pacbio4all\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epacbio4all\u003c/h1\u003e\n\u003cp\u003eA container with some of the pacbio tools. This is for Singularity 2.4 at least !\u003c/p\u003e\n\u003cp\u003e::\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name pacbio.img shub://cokelaer/pacbio4all:v2\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1508516491.0
  },
  {
    "data_format": 2,
    "description": "This is a github MIRROR of the main ocellaris repo on bitbucket (https://bitbucket.org/ocellarisproject/ocellaris). NO pull request or issues should go to this repo, please! This repository is only here to support Singularity Hub which lacks bitbucket support. The code in this repository may be severely out of date! It is synced with bitbucket manually and may be months or years behind!",
    "filenames": [
      "containers/Singularity"
    ],
    "full_name": "TormodLandet/Ocellaris",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNews\u003c/h1\u003e\n\u003cp\u003eFirst release of gemBS-rs, a complete rewrite of the gemBS pipeline (apart from the mapper) in Rust bringing increased\nstability while maintaining the high performance of gemBS: \u003ca href=\"https://github.com/heathsc/gemBS-rs.git\"\u003ehttps://github.com/heathsc/gemBS-rs.git\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-gembs\" class=\"anchor\" href=\"#gembs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egemBS\u003c/h1\u003e\n\u003cp\u003egemBS is a high performance bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3, a high performance read aligner and\nbs_call, a high performance variant and methyation caller, into a streamlined and efficient pipeline for\nbisulfite sueqnce analysis.\u003c/p\u003e\n\u003cp\u003eThe manuscript describing the pipeline is available \u003ca href=\"https://www.biorxiv.org/content/early/2017/10/11/201988\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicensing\u003c/h2\u003e\n\u003cp\u003egemBS is licensed under GPL.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h2\u003e\n\u003cp\u003eUse \u003ccode\u003egit clone --recursive\u003c/code\u003e to retrieve the complete source code including the code from external projects such as \u003ccode\u003ebs_call\u003c/code\u003e and \u003ccode\u003egem3-mapper\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/heathsc/gemBS.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eBefore starting the installation of gemBS, you will need to install\nor check the installation of several packages.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ea) gcc with development libraries\nb) python3, pip3, matplotlib, multiprocess\nc) zlib, lzma, openssl, libcurl, libncurses, wget, pigz\u003c/p\u003e\n\u003cp\u003eIf you are working on a clean (fairly recent) Ubuntu installation, you\ncan install everything required with the followiwg commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get update\nsudo apt-get install -y python3 build-essential git python3-pip wget pigz\nsudo apt-get install -y zlib1g-dev libbz2-dev\nsudo apt-get install -y libncurses5-dev liblzma-dev libssl-dev libcurl4-openssl-dev\npip3 install matplotlib multiprocess\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003eDownload the gemBS distribution if you haven\u0027t already done so:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egit clone --recursive https://github.com/heathsc/gemBS.git\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse python install command:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo install to the standard system location (i.e., so that all users\ncan use gemBS):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e``python3 setup.py install``\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo install to the user\u0027s home directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e``python3 setup.py install --user``\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-check-your-installation\" class=\"anchor\" href=\"#check-your-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheck your installation\u003c/h2\u003e\n\u003cp\u003eFor checking your installation follow this\n\u003ca href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/example.html\" rel=\"nofollow\"\u003eworked example\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eDocumentation can be found at\n\u003ca href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/index.html\" rel=\"nofollow\"\u003egemBS\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-changelog\" class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChangelog:\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e3.5.5 Fix logging bug caused by trimming change in 3.5.3\n3.5.4 Fix bug in the output of strand specific cpg txt files (not\n      encode Bed files) where the \u0027C\u0027 entry was not being printed\n3.5.3 Allow for read end specific trimming in bs_call\n3.5.3 Enable range checks and asserts in bs_call all target; add bs_call debug target\n3.5.2 Correct problems with gcc10.  Move to htslib/samtools/bcftools version 1.11\n3.5.1 Check if C compiler requires --std=c99 flag for standards conformant behaviour\n3.5.1 Make sure bgzip is copied correctly during installation\n3.5.0 Make bs_call process contig pools from largest to smallest (this change alters the sqlite db format so\n      if you have a previously started gemBS run you should (a) remove the .gemBS directory, (b) redo the\n      \u0027gemBS prepare\u0027 step to recreate the db file and (3) run \u0027gemBS db-sync\u0027. \n3.5.0 Switch bs_call and snpxtr to use the new dbSNP index format\n3.5.0 Add ability of dbSNP to read the new JSON and VCF  dbSNP format files\n      that are now used for human and non-human species respectively\n3.5.0 Add multithreading to dbSNP_idx\n3.5.0 Change format of dbSNP index to allow (a) efficient loading\n      of SNP data for individual contigs and (b) parallel index creation \n3.5.0 Rewrite mextr and snpxtr as standalone tools rather than\n      bcftools plugins.  Now multithreaded and (relatively) memoryefficient\n3.5.0 Replace bedToBigBed and wigToBigWig to reduce memory usage\n      and improve speed\n3.4.5 Fix crash when using the -k (keep-mismatch) flag, and fix rare hangs at end of processing\n3.4.4 Sort input bcf files to bcftools concat stage to ensure reproducibility.\n3.4.4 Add extra sort keys when generating pools to ensure stability of pool membership in the event of multiple contigs\n      having the same size\n3.4.3 Remove calculation of the goodness of filter (GOF) as this is expensive, non-standard and unreliable.  Removing this\n      removes the dependency on GSL.\n3.4.3 Add autodetection of output format to bs_call (unless explicitly specified on the command line)\n3.4.2 Add CRAM support (via make_cram option in configuration file)\n3.4.1 Add benchmark-mode that does not write date or program version numbers into SAM/BAM or VCF/BCF files\n      Switch to samtools, bcftools and htslib v1.10\n3.4.0 Move to new bs_call version (2.1.0) which is more efficient\n      in memory use and can read BAMs and write BCFs natively.\n      The new bs_call requires a faidx indexed reference, so gemBS\n      no creates this during indexing.\n3.4.0 Add switches to give more control to threads and memory\n      usage in mapping and calling stages\n3.3.3 Remove legacy pathway for config files with no header line (fix issue \u0027error in gemBS index #65)\n3.3.2 Fix error where header line for wig files could be omitted\n3.3.2 Fix generation of non_cpg files\n3.3.1 Fix Attribute error bug due to not checking if conversion is a list\n3.3.0 Make new release for IHEC\n3.3.0 Switch conversion default in IHEC_standard configuration to 0.01,0.05 rather than auto, which can give odd results if conversion controls not present or not working correctly\n3.3.0 Fix bug where conversion parameters could be ignored\n3.2.13 Fix formatting bug in mextr with multiple samples (not triggered in normal gemBS use)\n3.2.12 Ensure that conversion statistics are correctly calculated for non-stranded or reverse conversion protocols\n3.2.11 Introduce reverse_conversion option for mapping where read 1 is G2A converted and read 2 is C2T converted\n3.2.10 Correct regex patch for single end reads\n3.2.9 Update Singularity and Dockerfile recipes to allow kemp utils to be built correctly\n3.2.9 Make setup.py and gemBS/commands.py read the version information from gemBS/version.py, so ensuring consistency\n3.2.9 Fix bug added in last version where options in config file were not being taken into account\n3.2.8 Fix mis specification errors in long options for mextr. \n3.2.8 Fix bug where mextr (methyl extract plugin for bcftools) would crash if cpg output  option was not set.\n3.2.7 Apply patches for bugs in handling single end reads (suggested by I. Moghul)\n3.2.7 Changed regex for filenames to make it more general (suggested by I. Moghul)\n3.2.7 Fixed bug (reported by chhylp123) where zero arguments to some options were being ignored\n3.2.6 Cleaned up compilation and cleaning of gemBS tools\n3.2.6 Fixed python error if either the over conversion reference sequence was not defined\n3.2.6 Added check in bs_call that conversion parameters are valid (between 0 and 1)\n3.2.6 Perform more stringent sanity checking on conversion vaalues when autocomputed by gemBS\n3.2.6 Use --diasble-lzma configuration flag for samtools and bcftools as we don\u0027t need it and it removes an unneccesary dependency\n3.2.6 Add install options --disable-cuda (on by default) and --enable-cuda that affect GEM3 comppilation\n3.2.6 Bug fix with incorrect handling of duplicate reads\n3.2.5 Minor bug fix - correct error with non-paired end non-bisulfite reads\n3.2.4 Modify the bisulfite processing in gem-mapper to be more efficient (in particular for the non-stranded option)\n3.2.4 Modify gemBS to use the new conversion options for gem-mapper\n3.2.4 Switch gem-mapper to use option --underconversion-sequence and --overconversion-sequence rather than --underconversion_sequence to be consistent with other options\n3.2.3 Fixed bug if conversion parameters were not set\n3.2.2 Rework non-stranded mode so that both possible conversions are tried and the results merged\n3.2.2 Fix bug where non-stranded flag was not being passed to mapper in paired end mode\n3.2.1 Move warning message from bscall from stdout to stderr\n3.2.1 Switch Singularity build to use Ubuntu 16.04 rather than 18.04 to allow the image to work in CentOS 6 (Docker build changed as well to keep the two in sync)\n3.2.1 Fix undeclared variable bugs and missing --ignore-deps option in merge-bcfs\n3.2.1 Add default for dbSNP_index if dbSNP_files is set\n3.2.1 Add gsl-path install option\n3.2.0 Make new release\n3.1.0 Make installation process more modular.  Allow for sub-installs\n3.1.0 Add support for reading config from ${index_dir}/gemBS.json if it exists\n3.1.0 Add --reference-bias option to mextr and gemBS extract\n3.1.0 Add support for non-bisulfite mapping of individual datasets\n3.1.0 Allow white space in variable values\n3.1.0 Allow fallback to gzip if pigz not present\n3.1.0 Add --dry-run, --json, --ignore-db and --ignore-dep to extract command\n3.1.0 Add --ignore-dep option to call and merge-bcfs commands\n3.1.0 Add SNP extraction function to extract command\n3.0 Make v3.0 release\n3.0 Merge with master branch.\n3.0 Bump samtools sort memory limit to 2G\n3.0 Add extra_references option for reference generation\n3.0 Allow input files to mapping to be shell commands\n3.0 Add links to documentation\n3.0 Upload new yeast example and add documentation\n3.0 Add --dir option to gemBS\n3.0 Add --ignore-db options for --dry-run / --json\n3.0 Add --json output option for dry runs\n3.0 Update help text to match new functions\n3.0 Introduce standard analysis configurations stored within distribution\n3.0 Switch gem3-mapper distribution to gembs branch on official gem3-mapper repo\n3.0 Removal of incomplete files and roll back of db in the event of pipeline failure\n3.0 Automatic removal of individual BAMs and BCFs after successful merging\n3.0 Prevent pipelines hanging in event of failure\n3.0 Generate ENCODE bed and bigbed files\n3.0 Switch to python 3\n3.0 Switch to mextr for BCF filtering\n3.0 Include fetch and build of samtools / bcftools during build process\n3.0 Add dry-run capability to map and call commands\n3.0 Introduce contig pools to automatically group small contigs\n3.0 Automatic generation of contig.size files from index build\n3.0 Allow use of in memory sqlite3 db as an option\n3.0 Allow multiple instances of gemBS (possible on different hosts) to work \n    simultaneously on the same analysis\n3.0 Reduce and simply commands\n3.0 Add Dockerfile\n3.0 Add multi-threading and multi-processing options for most commands\n3.0 Use sqlite3 to track progress of analyses, file paths etc.\n3.0 Added more flexible configuration options (new csv format + new configuration file)\n3.0 Remove test dataset from distribution (distribute from web site)\n2.1.0 Ensure commands run during pipeline come from installation\n2.1.0 Added Singularity build recipe\n2.1.0 Add new command gemBS direct-mapping\n2.1.0 Fixed Makefile clean in tools\n2.0.2 Fixed bug related with the percentage of High Quality Variant in Variants summary report.\n2.0.2 Check temporary directory existence.\n2.0.2 Fixed QualityNonRefCpg sample name in png image.\n2.0.2 Fixed mapper issues related with aligning performace.\n2.0.2 Fixed arguments for Under/Over Conversion sequence name in gem3-mapper\n2.0.1 On bscall repository, fixed argument -k about discarded reads that do not form proper pairs.\n2.0 Check tmp folder before starting mapping process.\n2.0 Added Left and Right Trimming optional arguments to gemBS bscall.\n2.0 Added GC Coverage correlation value to BS Call Stats Summary.\n2.0 Fixed error when reporting complete path to not found bam files.\n2.0 Fixed iteration over sampleBams dictionary in MergeAll method.\n2.0 Updated: Avoid redo indexing when merging just one file.\n2.0 Changed conversion formula.\n2.0 Added parameter for dbSNP.\n2.0 Added threads to bscall.\n2.0 Removed CpGs reports. Already done from bscall report.\n2.0 Fixed bs_call makefile for the gcc to be used.\n2.0 New bscall version. Generates JSON report.\n2.0 Removed gemBS options snp-stats,cpg-report,cpg-stats.\n2.0 Added summary report from the bs_call json stats\n2.0 New BSCall Report. From bscall son file generates three types of reports:\n    Mapping and Coverage Report\n    Bs-Genotypes Calls Report\n    Methylation Statistics report\n1.7 Added non stranded read conversion parameter\n1.7 Fixed SE crash when estimating overlapped bases.\n1.7 Fixed gem-index (gem3) to follow fastq and SAM specifications. \n    Modified gem3-mapper repository external module.\n    New external module https://github.com/heathsc/gem3-mapper.git\n1.7 Fixed threads parameter to samtools merge\n1.7 Fixed threads parameter to gem-mapper\n1.7 Removed Indels Field on Variants Report.\n1.7 Added Overlapping Bases at Mapping Report\n1.7 Modified Base Counts Overall, removed Base Counts general and Base Counts Overall\n1.7 New Dinucleotide CpGs Report\n    New table dinucleotide stats\n    New plots for Informative Reads and CpGs\n    Methylation levels plots for different types of CpGs\n    Summary Table\n1.7 New Readme file to inform about report test\n1.7 New basic statis table for Variants Report\n1.7 Removed parameter -r (reference length) parameter for mapping reports command (gemBS bsMap).\n1.6 New CpGs Density plot, include box plos, bar plot and fitting curve\n1.6 Change name at CpG report:\n    \"Heterozygous\" for \"Alternative CX\"\n    \"De Novo CpGs Methylation Status\" for \"Non Reference CpGs\"\n    \"CpGs with SNP\" for \"SNPs (CX) at Reference CpGs\"\n1.6 CpGs Report Simplified to Q\u0026gt;20\n1.6 BigWig Default parameters for filtering CpG per a given quality and a total number of supported informative reads   \n1.5 Initial Release  \n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-developers\" class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopers\u003c/h2\u003e\n\u003cp\u003egemBS:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMarcos Fernandez-Callejo - \u003ca href=\"mailto:marcos.fernandez@cnag.crg.eu\"\u003emarcos.fernandez@cnag.crg.eu\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSimon Heath - \u003ca href=\"mailto:simon.heath@gmail.com\"\u003esimon.heath@gmail.com\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003egem mapper:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSantiago Marco-Sola - \u003ca href=\"mailto:santiagomsola@gmail.com\"\u003esantiagomsola@gmail.com\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ebisulfite caller and filtering:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSimon Heath - \u003ca href=\"mailto:simon.heath@gmail.com\"\u003esimon.heath@gmail.com\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1553974960.0
  },
  {
    "data_format": 2,
    "description": "Repository used to build Singularity containers of HD software",
    "filenames": [
      "Singularity"
    ],
    "full_name": "faustus123/hdsingularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hdsingularity\" class=\"anchor\" href=\"#hdsingularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehdsingularity\u003c/h1\u003e\n\u003cp\u003eRepository used to build Singularity containers of HD software\u003c/p\u003e\n\u003cp\u003eCheckout singularity-hub.org for details\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1501591637.0
  },
  {
    "data_format": 2,
    "description": "Python from source for use with singularity",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sbutcher/container-python",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-container-python\" class=\"anchor\" href=\"#container-python\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainer-python\u003c/h1\u003e\n\u003cp\u003ePython from source for use with singularity\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1525427896.0
  },
  {
    "data_format": 2,
    "description": "singularity container for use with singularity hub",
    "filenames": [
      "Singularity"
    ],
    "full_name": "sbutcher/container-R",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-container-r\" class=\"anchor\" href=\"#container-r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainer-R\u003c/h1\u003e\n\u003cp\u003esingularity container for use with singularity hub\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1525440620.0
  },
  {
    "data_format": 2,
    "description": "Code used to generate summaries, models and figures for article \"A field-wide assessment of differential high throughput sequencing reveals widespread bias\".",
    "filenames": [
      "Singularity"
    ],
    "full_name": "tpall/geo-htseq-paper",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/tpall/geo-htseq-paper/workflows/CI/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/tpall/geo-htseq-paper/workflows/CI/badge.svg\" alt=\"CI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-geo-htseq-paper\" class=\"anchor\" href=\"#geo-htseq-paper\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeo-htseq-paper\u003c/h1\u003e\n\u003cp\u003eWe analyzed the field of expression profiling by high throughput sequencing, or RNA-seq, in terms of replicability and reproducibility, using data from the GEO (Gene Expression Omnibus) repository. Our work puts an upper bound of 56% to field-wide reproducibility, based on the types of files submitted to GEO.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-data\" class=\"anchor\" href=\"#getting-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting data\u003c/h2\u003e\n\u003cp\u003eGot to \u003ca href=\"https://zenodo.org/record/3754095\" rel=\"nofollow\"\u003ehttps://zenodo.org/record/3754095\u003c/a\u003e and download data archive, let\u0027s say, to your Downloads folder.\u003c/p\u003e\n\u003cp\u003eThen create new folder, e.g. \"geo-htseq\" and enter this folder\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir geo-htseq\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e geo-htseq\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eCopy downloaded dataset to your working directory and uncompress:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ecp \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/Downloads/geo-htseq-until-2019-12-31.tar.gz \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\ntar -xzvf geo-htseq-until-2019-12-31.tar.gz\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eRemove tar.gz archive from working directory:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003erm geo-htseq-until-2019-12-31.tar.gz\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow you should have dataset in \"output\" subdirectory ready for analysis.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-workflow-graph\" class=\"anchor\" href=\"#workflow-graph\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow graph\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"images/rulegraph.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/rulegraph.svg\" alt=\"rulegraph\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619076381.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for edta (https://github.com/oushujun/EDTA)",
    "filenames": [
      "Singularity",
      "Singularity.1.9.0",
      "Singularity.1.8.3"
    ],
    "full_name": "powerPlant/edta-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for the Extensive de novo TE Annotator tool\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603071842.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "3.11.0.5/Singularity"
    ],
    "full_name": "pscedu/singularity-aspera-connect",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-downward\" class=\"anchor\" href=\"#fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast Downward\u003c/h1\u003e\n\u003cp\u003eFast Downward is a domain-independent classical planning system.\u003c/p\u003e\n\u003cp\u003eCopyright 2003-2020 Fast Downward contributors (see below).\u003c/p\u003e\n\u003cp\u003eFor further information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast Downward website: \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eReport a bug or file an issue: \u003ca href=\"http://issues.fast-downward.org\" rel=\"nofollow\"\u003ehttp://issues.fast-downward.org\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward mailing list: \u003ca href=\"https://groups.google.com/forum/#!forum/fast-downward\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/fast-downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFast Downward main repository: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tested-software-versions\" class=\"anchor\" href=\"#tested-software-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTested software versions\u003c/h2\u003e\n\u003cp\u003eThis version of Fast Downward has been tested with the following software versions:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOS\u003c/th\u003e\n\u003cth\u003ePython\u003c/th\u003e\n\u003cth\u003eC++ compiler\u003c/th\u003e\n\u003cth\u003eCMake\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 20.04\u003c/td\u003e\n\u003ctd\u003e3.8\u003c/td\u003e\n\u003ctd\u003eGCC 9, GCC 10, Clang 10, Clang 11\u003c/td\u003e\n\u003ctd\u003e3.16\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eUbuntu 18.04\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eGCC 7, Clang 6\u003c/td\u003e\n\u003ctd\u003e3.10\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003emacOS 10.15\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eAppleClang 12\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWindows 10\u003c/td\u003e\n\u003ctd\u003e3.6\u003c/td\u003e\n\u003ctd\u003eVisual Studio Enterprise 2017 (MSVC 19.16) and 2019 (MSVC 19.28)\u003c/td\u003e\n\u003ctd\u003e3.19\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eWe test LP support with CPLEX 12.9, SoPlex 3.1.1 and Osi 0.107.9.\nOn Ubuntu, we test both CPLEX and SoPlex. On Windows, we currently\nonly test CPLEX, and on macOS, we do not test LP solvers (yet).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003eThe following list includes all people that actively contributed to\nFast Downward, i.e. all people that appear in some commits in Fast\nDownward\u0027s history (see below for a history on how Fast Downward\nemerged) or people that influenced the development of such commits.\nCurrently, this list is sorted by the last year the person has been\nactive, and in case of ties, by the earliest year the person started\ncontributing, and finally by last name.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e2003-2020 Malte Helmert\u003c/li\u003e\n\u003cli\u003e2008-2016, 2018-2020 Gabriele Roeger\u003c/li\u003e\n\u003cli\u003e2010-2020 Jendrik Seipp\u003c/li\u003e\n\u003cli\u003e2010-2011, 2013-2020 Silvan Sievers\u003c/li\u003e\n\u003cli\u003e2012-2020 Florian Pommerening\u003c/li\u003e\n\u003cli\u003e2013, 2015-2020 Salome Eriksson\u003c/li\u003e\n\u003cli\u003e2016-2020 Cedric Geissmann\u003c/li\u003e\n\u003cli\u003e2017-2020 Guillem Franc\u00e8s\u003c/li\u003e\n\u003cli\u003e2018-2020 Augusto B. Corr\u00eaa\u003c/li\u003e\n\u003cli\u003e2018-2020 Patrick Ferber\u003c/li\u003e\n\u003cli\u003e2015-2019 Manuel Heusner\u003c/li\u003e\n\u003cli\u003e2017 Daniel Killenberger\u003c/li\u003e\n\u003cli\u003e2016 Yusra Alkhazraji\u003c/li\u003e\n\u003cli\u003e2016 Martin Wehrle\u003c/li\u003e\n\u003cli\u003e2014-2015 Patrick von Reth\u003c/li\u003e\n\u003cli\u003e2015 Thomas Keller\u003c/li\u003e\n\u003cli\u003e2009-2014 Erez Karpas\u003c/li\u003e\n\u003cli\u003e2014 Robert P. Goldman\u003c/li\u003e\n\u003cli\u003e2010-2012 Andrew Coles\u003c/li\u003e\n\u003cli\u003e2010, 2012 Patrik Haslum\u003c/li\u003e\n\u003cli\u003e2003-2011 Silvia Richter\u003c/li\u003e\n\u003cli\u003e2009-2011 Emil Keyder\u003c/li\u003e\n\u003cli\u003e2010-2011 Moritz Gronbach\u003c/li\u003e\n\u003cli\u003e2010-2011 Manuela Ortlieb\u003c/li\u003e\n\u003cli\u003e2011 Vidal Alc\u00e1zar Saiz\u003c/li\u003e\n\u003cli\u003e2011 Michael Katz\u003c/li\u003e\n\u003cli\u003e2011 Raz Nissim\u003c/li\u003e\n\u003cli\u003e2010 Moritz Goebelbecker\u003c/li\u003e\n\u003cli\u003e2007-2009 Matthias Westphal\u003c/li\u003e\n\u003cli\u003e2009 Christian Muise\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cp\u003eThe current version of Fast Downward is the merger of three different\nprojects:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe original version of Fast Downward developed by Malte Helmert\nand Silvia Richter\u003c/li\u003e\n\u003cli\u003eLAMA, developed by Silvia Richter and Matthias Westphal based on\nthe original Fast Downward\u003c/li\u003e\n\u003cli\u003eFD-Tech, a modified version of Fast Downward developed by Erez\nKarpas and Michael Katz based on the original code\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn addition to these three main sources, the codebase incorporates\ncode and features from numerous branches of the Fast Downward codebase\ndeveloped for various research papers. The main contributors to these\nbranches are Malte Helmert, Gabi R\u00f6ger and Silvia Richter.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe following directory is not part of Fast Downward as covered by\nthis license:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or (at\nyour option) any later version.\n\nFast Downward is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program. If not, see \u0026lt;https://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1625246132.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/demo-singularity-matlab-fsl",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demo-singularity-container-for-matlab-plus-fsl\" class=\"anchor\" href=\"#demo-singularity-container-for-matlab-plus-fsl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo singularity container for Matlab plus FSL\u003c/h1\u003e\n\u003cp\u003eThis example container takes a Nifti image as input, zeroes out a hole in it of\nthe specified diameter, and saves the result to a new Nifti file. Quick,\npointless, and easy to tell whether it worked right.\u003c/p\u003e\n\u003cp\u003eThis is one way to organize a Matlab-based Singularity container -\nperhaps most easily conceived of as a series of wrappers around the main\ncodebase. Done this way, it\u0027s fairly easy to work on each piece in isolation,\nproblem-solving from the inside out.\u003c/p\u003e\n\u003cp\u003eThis container also includes an installation of FSL, which has a lot of handy\ntools including fsleyes to make the QA PDF. The FSL parts could be removed from\nthe Singularity file if FSL isn\u0027t used, to end up with a smaller container.\nContrariwise, all the Matlab parts could be removed to end up with an FSL-only\ncontainer.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSingularity container\n|   Primary entrypoint (shell script)\n|   |   X11 wrapper\n|   |   |   Shell script preprocessing\n|   |   |   Matlab processing (compiled)\n|   |   |   |   Matlab entrypoint\n|   |   |   |       Matlab main function\n|   |   |   \\           Matlab sub-functions / codebase\n\\   \\   \\   Shell script postprocessing\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDependencies in terms of the actual files:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eSingularity\n    src/pipeline_entrypoint.sh\n        src/pipeline_main.sh\n            src/copy_inputs.sh\n            src/preprocessing.sh\n            matlab/bin/run_matlab_entrypoint.sh\n                matlab/bin/matlab_entrypoint\n                    / matlab/src/matlab_entrypoint.m \\  Used for compilation,\n                    |     matlab/src/matlab_main.m   |  but not at container\n                    \\         matlab/src/*           /  runtime\n            src/postprocessing.sh\n            src/make_pdf.sh\n            src/finalize.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe process of putting it together is described below. The scripts and code in\nthis repository are extensively commented, so if something isn\u0027t clear here,\nit\u0027s probably explained in the Singularity file or the example code.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-matlab-part\" class=\"anchor\" href=\"#matlab-part\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMatlab part\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-write-the-basic-matlab-code\" class=\"anchor\" href=\"#write-the-basic-matlab-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWrite the basic Matlab code\u003c/h3\u003e\n\u003cp\u003eWrite Matlab code that does what\u0027s needed. Put it in \u003ccode\u003ematlab/src\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eA popular toolbox for reading and writing Nifti files that\u0027s available on Matlab\nCentral has a lot of insidious bugs and is not being maintained. Matlab\u0027s own\nfunctions for Nifti files are quite limited. Here is an alternative, which is\nused in this example:\n\u003ca href=\"https://github.com/VUIIS/spm_readwrite_nii\"\u003ehttps://github.com/VUIIS/spm_readwrite_nii\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-write-the-matlab-entrypoint\" class=\"anchor\" href=\"#write-the-matlab-entrypoint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWrite the Matlab entrypoint\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003ematlab/src/matlab_entrypoint.m\u003c/code\u003e exists to take command line arguments, parse\nthem, and call the main code. A convenient way to set things up is to write a\nmain function that takes a structure as its sole input, with the structure\ncontaining whatever inputs are needed. See \u003ccode\u003ematlab/src/matlab_main.m\u003c/code\u003e for an\nexample of this.\u003c/p\u003e\n\u003cp\u003eCouple of things to note in the entrypoint code are the quit/exit sections at\nbeginning and end. The bit at the beginning allows the executable to run during\nthe container build, without actually doing anything - this is needed to extract\nthe CTF archive into the container at the only time the container is writeable\n(h/t \u003ca href=\"https://twitter.com/annash128\" rel=\"nofollow\"\u003ehttps://twitter.com/annash128\u003c/a\u003e for figuring that one out). The bit at the\nend exits matlab when the function is finished. Without it, the running Matlab\nprocess won\u0027t release execution back to the calling script when it\u0027s done.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-test-the-matlab-entrypoint\" class=\"anchor\" href=\"#test-the-matlab-entrypoint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest the Matlab entrypoint\u003c/h3\u003e\n\u003cp\u003eThe script \u003ccode\u003ematlab/src/test_matlab_entrypoint.m\u003c/code\u003e is an example of how to do\nthis. The appropriate Matlab must be installed on the testing computer.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-compile-the-matlab-code\" class=\"anchor\" href=\"#compile-the-matlab-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCompile the Matlab code\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003ematlab/compile_matlab.sh\u003c/code\u003e shows how. Many compiled executables are likely to be\ntoo large to store on github. Git LFS may be a solution.\n\u003ca href=\"https://docs.github.com/en/github/managing-large-files/working-with-large-files\"\u003ehttps://docs.github.com/en/github/managing-large-files/working-with-large-files\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-test-the-compiled-matlab-code\" class=\"anchor\" href=\"#test-the-compiled-matlab-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest the compiled Matlab code\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003ematlab/test_compiled_matlab.sh\u003c/code\u003e. The appropriate Matlab Runtime must be\ninstalled on the testing computer.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-shell-script-part\" class=\"anchor\" href=\"#shell-script-part\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eShell script part\u003c/h2\u003e\n\u003cp\u003eAll of the below procedures could be done in the matlab part, if desired,\ninstead of in shell script. If so, parsing inputs should be done following the\nexample in \u003ccode\u003ematlab/src/matlab_entrypoint.m\u003c/code\u003e. But it\u0027s often easier to move\nfiles, create the QA PDF, etc using shell script and FSL. So that\u0027s what we are\ndoing in this example. All this code is in the \u003ccode\u003esrc\u003c/code\u003e directory.\u003c/p\u003e\n\u003cp\u003eAll the shell scripts called from \u003ccode\u003esrc/pipeline_entrypoint.sh\u003c/code\u003e \"know\" the\nenvironment variables that are exported there. This is a very convenient way to\npass along the input arguments, although it isn\u0027t entirely transparent, because\nthere\u0027s no hint in the shell scripts where the variables\u0027 values are coming from\nunless we explain it in the comments.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-main-entrypoint\" class=\"anchor\" href=\"#main-entrypoint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMain entrypoint\u003c/h3\u003e\n\u003cp\u003eThis is \u003ccode\u003esrc/pipeline_entrypoint.sh\u003c/code\u003e. It uses bash to parse the command line\ninputs and export them to environment variables so they\u0027re accessible. Then it\ncalls the primary shell script \u003ccode\u003esrc/pipeline_main.sh\u003c/code\u003e which in turn calls\neverything else. The main script is run in xvfb to provide a virtual display,\noften needed by matlab and required for fsleyes.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-copy-inputs\" class=\"anchor\" href=\"#copy-inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopy inputs\u003c/h3\u003e\n\u003cp\u003eWe copy input files to the output/working directory so we don\u0027t mess them up.\nThis also is an opportunity to rename them to something consistent. It\u0027s very\nconvenient to hard-code the filenames so we don\u0027t have to store and manipulate\nfilenames in environment variables or the like. Also, this makes it easy to\nproduce output files with consistent names - outputs of one pipeline may serve\nas inputs to another, and it\u0027s much easier to manage this if filenames are the\nsame for every run, or at least consistent.\u003c/p\u003e\n\u003cp\u003eWe generally assume the output directory starts out empty and will not be\ninterfered with by any other processes - this is true for XNAT/DAX, but\nimportant to be aware of in other contexts.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-preprocessing\" class=\"anchor\" href=\"#preprocessing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreprocessing\u003c/h3\u003e\n\u003cp\u003eFor this example, there is no preprocessing before the matlab part. But initial\nFSL steps or similar could be put here: \u003ccode\u003esrc/preprocessing.sh\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-postprocessing\" class=\"anchor\" href=\"#postprocessing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePostprocessing\u003c/h3\u003e\n\u003cp\u003eThere isn\u0027t any postprocessing for this example either, but there could be:\n\u003ccode\u003esrc/postprocessing.sh\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pdf-creation\" class=\"anchor\" href=\"#pdf-creation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePDF creation\u003c/h3\u003e\n\u003cp\u003eAll assessors on VUIIS XNAT require a PDF QA report of some sort. For this\nexample, a display of the segmented ROIs overlaid on the T1 is created using\nfsleyes and ImageMagick, \u003ccode\u003esrc/make_pdf.sh\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003ePDF creation can be done in Matlab instead. It\u0027s hard to make these look good.\nAn example with some tricks, including a \u003ccode\u003e.fig\u003c/code\u003e file painstakingly made with\nMatlab\u0027s GUIDE, is\n\u003ca href=\"https://github.com/baxpr/connprep/blob/855dadc/src/connectivity_filter.m#L271\"\u003ehttps://github.com/baxpr/connprep/blob/855dadc/src/connectivity_filter.m#L271\u003c/a\u003e\nA way to show slices of functional images with a nice red/blue colormap is\n\u003ca href=\"https://github.com/baxpr/connprep/blob/855dadc/src/make_network_maps.m\"\u003ehttps://github.com/baxpr/connprep/blob/855dadc/src/make_network_maps.m\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-finalizing-the-output\" class=\"anchor\" href=\"#finalizing-the-output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFinalizing the output\u003c/h3\u003e\n\u003cp\u003eAll Niftis must be compressed for storage on XNAT, and outputs can be organized\nin an easily understandable way: \u003ccode\u003esrc/finalize.sh\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eWrite an informative README - so tedious, yet so helpful. Include the\nappropriate citations for all the methods and software you have used. Even\nessentially write the methods section for a paper that uses the pipeline. Here\u0027s\nan excellent example: \u003ca href=\"https://github.com/MASILab/PreQual\"\u003ehttps://github.com/MASILab/PreQual\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAlternatively, git-ify some documentation like this:\n\u003ca href=\"https://github.com/VUIIS/dax/tree/main/docs\"\u003ehttps://github.com/VUIIS/dax/tree/main/docs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eto get something like this:\n\u003ca href=\"https://dax.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttps://dax.readthedocs.io/en/latest/\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-container\" class=\"anchor\" href=\"#building-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the container\u003c/h2\u003e\n\u003cp\u003eBe sure the Matlab code is newly compiled, see above. You can run\n\u003ccode\u003ematlab/check_for_compilation.sh\u003c/code\u003e first to make sure there\u0027s no source code\nnewer than the compiled executable.\u003c/p\u003e\n\u003cp\u003eThen from the root directory of the working copy of the repo, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build \u0026lt;container_name\u0026gt;.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGood practice: before you build, create a release on github (if using github) or\nat least tag the commit you are about to build. Give the container a versioned\nname like \u003ccode\u003edemo-singularity-matlab-fsl_v1.0.0.simg\u003c/code\u003e that matches the repository\nname and release version/tag.\u003c/p\u003e\n\u003cp\u003eExternal binaries such as Matlab Runtime and FSL can be included by copying\nlocal copies into the container in the Singularity file\u0027s \u003ccode\u003e%files\u003c/code\u003e section. This\ntends to be a little faster when multiple builds are needed during debugging,\nor necessary for files that are not available to download, and this is what\u0027s\nbeing done in the example Singularity file. Alternatively, binaries or install\nfiles can be downloaded from their source at build time - there are some\ncommented-out sections in the Singularity file showing how that is done. (Thanks\n\u003ca href=\"https://github.com/praitayini\"\u003ehttps://github.com/praitayini\u003c/a\u003e for exploring this in detail)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-container\" class=\"anchor\" href=\"#running-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the container\u003c/h2\u003e\n\u003cp\u003eSee \u003ccode\u003etest_singularity_container.sh\u003c/code\u003e for an example run command and some\nimportant info.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h3\u003e\n\u003cp\u003ePaths to files are relative to the container.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--t1_niigz        A T1 image\n--seg_niigz       Its corresponding segmentation from e.g. slant pipeline\n--diameter_mm     Diameter of the hole to zero out, in mm (default 30)\n\n--project         Labels from XNAT, used only to annotate the QA PDF\n--subject         (default UNK_*)\n--session\n--scan\n\n--out_dir         Where outputs will be stored (default /OUTPUTS)\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003ePDF/holed_image.pdf           QA report\nHOLED_T1/holed_t1.nii.gz      T1 image with a hole in it\nHOLED_SEG/holed_seg.nii.gz    Segmentation with a hole in it\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625430481.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for bat",
    "filenames": [
      "0.17.1/Singularity"
    ],
    "full_name": "icaoberg/singularity-bat",
    "latest_release": "v0.17.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bat\" class=\"anchor\" href=\"#singularity-bat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bat\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7b7c397acc5b91b4c4cf7756015185fe3c5f700f70d256a212de51294a0cf673/68747470733a2f2f696d6775722e636f6d2f724773646e44652e706e67\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sharkdp/bat\"\u003ebat\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ebat\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bat/0.17.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/bat\u003c/code\u003e as \u003ccode\u003e0.17.1\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1622870361.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "granek/jupyter-MIC-2021",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hts-jupyter-notebook-container\" class=\"anchor\" href=\"#hts-jupyter-notebook-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHTS Jupyter notebook container\u003c/h1\u003e\n\u003cp\u003eWe are offering a series of 6 workshops on biological assays and data analysis for HIV researchers.\nThis series is funded by an R25 grand from the National Institute of Allergies and Infectious Disease (NIAID).\nOur goal is to provide educational enrichment for HIV researchers on current assay technologies and the statistical and bioinformatic analysis techniques necessary to process such data.\u003c/p\u003e\n\u003cp\u003eThis is the source for the Docker container used to run the course Jupyter\nnotebooks.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-the-image\" class=\"anchor\" href=\"#using-the-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the image\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-docker\" class=\"anchor\" href=\"#install-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall docker\u003c/h2\u003e\n\u003cp\u003eTo run a container on your local machine or laptop, download the docker program from \u003ca href=\"https://www.docker.com\" rel=\"nofollow\"\u003ehttps://www.docker.com\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-image-on-your-local-computer\" class=\"anchor\" href=\"#run-image-on-your-local-computer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun image on your local computer\u003c/h2\u003e\n\u003cp\u003eOnce you have the docker program installed, open the program (you should get a terminal screen with command line). Enter the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull dukehtscourse/jupyter-hts-2019\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will pull down the course docker image from dockerhub. It may take a few minutes. Next, run the command to start a container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --name hts-course -v YOUR_DIRECTORY_WITH_COURSE_MATERIAL:/home/jovyan/work \\\n-d -p 127.0.0.1\\:9999\\:8888 \\\n-e PASSWORD=\"YOUR_CHOSEN_NOTEBOOK_PASSWORD\" \\\n-e NB_UID=1000 \\\n-t dukehtscourse/jupyter-hts-2019\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe most important parts of this verbiage are the \u003ccode\u003eYOUR_DIRECTORY_WITH_COURSE_MATERIALS\u003c/code\u003e and \u003ccode\u003eYOUR_CHOSEN_NOTEBOOK_PASSWORD\u003c/code\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eYOUR_DIRECTORY_WITH_COURSE_MATERIALS\u003c/code\u003e (Bind mounting): The directory name is the one you extracted your course materials into. So, if you put them in your home directory, it might look something like: \u003ccode\u003e-v /home/janice/HTS2019-notebooks:/home/jovyan/work\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eYOUR_CHOSEN_NOTEBOOK_PASSWORD\u003c/code\u003e: The password is whatever you want to use to password protect your notebook. Now, this command is running the notebook so that it is only \u0027seen\u0027 by your local computer - no one else on the internet can access it, and you cannot access it remotely, so the password is a bit of overkill. Use it anyway. An example might be: \u003ccode\u003e-e PASSWORD=\"Pssst_this_is_Secret\"\u003c/code\u003e except that this is a terrible password and you should follow standard rules of not using words, include a mix of capital and lowercase and special symbols. etc.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-d -p 127.0.0.1\\:9999\\:8888\u003c/code\u003e part of the command is telling docker to run the notebook so that it is only visible to the local machine. It is absolutely possible to run it as a server to be accessed across the web - but there are some security risks associated, so if you want to do this proceed with great caution and get help.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOf course, it would be better either configure HTTPS (see the options section below) or run an Nginx proxy in front of the container instance so you get https (encryption) instead of http.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-open-the-jupyter-in-your-browser\" class=\"anchor\" href=\"#open-the-jupyter-in-your-browser\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpen the Jupyter in your browser\u003c/h3\u003e\n\u003cp\u003eOpen a browser and point it to \u003ca href=\"http://127.0.0.1:9999\" rel=\"nofollow\"\u003ehttp://127.0.0.1:9999\u003c/a\u003e\nYou should get to a Jupyter screen asking for a password. This is the password you created in the docker run command.\nNow, you should be able to run anything you like from the course. Depending on your laptop\u0027s resources (RAM, cores), this might be slow, so be aware and start by testing only one file (vs the entire course data set).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-stopping-docker\" class=\"anchor\" href=\"#stopping-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStopping Docker\u003c/h3\u003e\n\u003cp\u003eThe container will continue running, even if you do not have Jupyter open in a web browser.  If you don\u0027t plan to use it for a while, you might want to shut it down so it isn\u0027t using resources on your computer.  Here are two ways to do that:\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-kitematic\" class=\"anchor\" href=\"#kitematic\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKitematic\u003c/h4\u003e\n\u003cp\u003eIncluded in the \u003ca href=\"https://docs.docker.com/docker-for-mac/\" rel=\"nofollow\"\u003eDocker for Mac\u003c/a\u003e and the \u003ca href=\"https://docs.docker.com/docker-for-windows/\" rel=\"nofollow\"\u003eDocker for Windows\u003c/a\u003e installations.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-commandline\" class=\"anchor\" href=\"#commandline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommandline\u003c/h4\u003e\n\u003cp\u003eYou may want to familiarize yourself with the following Docker commands.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003edocker stop\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edocker rm\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edocker ps -a\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edocker images\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edocker rmi\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-windows-note\" class=\"anchor\" href=\"#windows-note\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWindows Note\u003c/h3\u003e\n\u003cp\u003eThese instructions have not been tested in a Windows environment.  If you have problems with them, please give us feedback\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-image-on-a-server\" class=\"anchor\" href=\"#run-image-on-a-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun image on a server\u003c/h2\u003e\n\u003cp\u003eTo run on a remote server you will want to use a slightly different command from above, because you \u003cem\u003ewill need to connect remotely\u003c/em\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --name hts-course \\\n-v YOUR_DIRECTORY_WITH_COURSE_MATERIAL:/home/jovyan/work \\\n-d -p 8888:8888 \\\n-e USE_HTTPS=\"yes\" \\\n-e PASSWORD=\"YOUR_CHOSEN_NOTEBOOK_PASSWORD\" \\\n-e NB_UID=1000 \\\n-t dukehtscourse/jupyter-hts-2019\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptions\u003c/h2\u003e\n\u003cp\u003eYou may customize the execution of the Docker container and the Notebook server it contains with the following optional arguments.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-e PASSWORD=\"YOURPASS\"\u003c/code\u003e - Configures Jupyter Notebook to require the given password. Should be conbined with \u003ccode\u003eUSE_HTTPS\u003c/code\u003e on untrusted networks.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e USE_HTTPS=yes\u003c/code\u003e - Configures Jupyter Notebook to accept encrypted HTTPS connections. If a \u003ccode\u003epem\u003c/code\u003e file containing a SSL certificate and key is not provided (see below), the container will generate a self-signed certificate for you.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e(v4.0.x)\u003c/strong\u003e \u003ccode\u003e-e NB_UID=1000\u003c/code\u003e - Specify the uid of the \u003ccode\u003ejovyan\u003c/code\u003e user. Useful to mount host volumes with specific file ownership.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e GRANT_SUDO=yes\u003c/code\u003e - Gives the \u003ccode\u003ejovyan\u003c/code\u003e user passwordless \u003ccode\u003esudo\u003c/code\u003e capability. Useful for installing OS packages. \u003cstrong\u003eYou should only enable \u003ccode\u003esudo\u003c/code\u003e if you trust the user or if the container is running on an isolated host.\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-v /some/host/folder/for/work:/home/jovyan/work\u003c/code\u003e - Host mounts the default working directory on the host to preserve work even when the container is destroyed and recreated (e.g., during an upgrade).\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e(v3.2.x)\u003c/strong\u003e \u003ccode\u003e-v /some/host/folder/for/server.pem:/home/jovyan/.ipython/profile_default/security/notebook.pem\u003c/code\u003e - Mounts a SSL certificate plus key for \u003ccode\u003eUSE_HTTPS\u003c/code\u003e. Useful if you have a real certificate for the domain under which you are running the Notebook server.\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e(v4.0.x)\u003c/strong\u003e \u003ccode\u003e-v /some/host/folder/for/server.pem:/home/jovyan/.local/share/jupyter/notebook.pem\u003c/code\u003e - Mounts a SSL certificate plus key for \u003ccode\u003eUSE_HTTPS\u003c/code\u003e. Useful if you have a real certificate for the domain under which you are running the Notebook server.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e INTERFACE=10.10.10.10\u003c/code\u003e - Configures Jupyter Notebook to listen on the given interface. Defaults to \u0027*\u0027, all interfaces, which is appropriate when running using default bridged Docker networking. When using Docker\u0027s \u003ccode\u003e--net=host\u003c/code\u003e, you may wish to use this option to specify a particular network interface.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e PORT=8888\u003c/code\u003e - Configures Jupyter Notebook to listen on the given port. Defaults to 8888, which is the port exposed within the Dockerfile for the image. When using Docker\u0027s \u003ccode\u003e--net=host\u003c/code\u003e, you may wish to use this option to specify a particular port.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-course-image-with-singularity\" class=\"anchor\" href=\"#running-the-course-image-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the Course Image with Singularity\u003c/h2\u003e\n\u003cp\u003eDocker requires root permissions to run, so you are unlikely to be able to run Docker on a computer that you are not fully in control of.  As an alternative you can run the course image with \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e, another container system. Singularity is similar to Docker, and can run Docker images, but you do not need special permissions to run Singularity images \u003cem\u003eor\u003c/em\u003e Docker images with Singularity (as long as Singularity is actually installed on the computer).\u003c/p\u003e\n\u003cp\u003eThe following command uses Singularity to start up a container from the course Jupyter image.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-the-course-image-on-a-slurm-cluster\" class=\"anchor\" href=\"#running-the-course-image-on-a-slurm-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the Course Image on a SLURM cluster\u003c/h3\u003e\n\u003cp\u003eWe will use the example of the Duke Computer Cluster, but these instructions should be easily adaptable to other clusters\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFrom your computer run this to connect to DCC:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003essh NetID@dcc-login-03.oit.duke.edu\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eOnce you are connected run this to start a tmux session:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003etmux new -s jupyter\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eOnce you have started a tmux session you can start up Jupyter with this command:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003esrun singularity exec docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: the first time you run this, it might take a VERY long time to download the Docker image and build the Singularity image from it\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eRunning this command will print a bunch of stuff. You can ignore everything except the last two lines, which will say something like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehttp://dcc-chsi-01:8889/?token=08172007896ad29bb5fbd92f6f3f516a8b2f7303ed7f1df3\nor http://127.0.0.1:8889/?token=08172007896ad29bb5fbd92f6f3f516a8b2f7303ed7f1df3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou need this information for the next few steps. For the next step you need the \u201cdcc-chsi-01:8889\u201d part.\n\u201cdcc-chsi-01\u201d is the compute node that Jupyter is running on and \u201c8889\u201d is the port it is listening on. You may get a different value every time you start the container.\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eYou want to run the following command in another terminal on your computer to set up port forwarding.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003essh -L PORT:NODE.rc.duke.edu:PORT NetID@dcc-login-03.oit.duke.edu\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn this command you want to replace \u201cPORT\u201d with the value you got for port from the srun command and replace \u201cNODE\u201d with the compute node that was printed by the srun command. So for the example above, the ssh port forwarding command would be:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -L 8889:dcc-chsi-01.rc.duke.edu:8889 NetID@dcc-login-03.oit.duke.edu\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003eNow you can put the last line that the srun command printed in your web browser and it should open your Jupyter instance running on DCC.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThe Jupyter session keeps running until you explicitly shut it down.  If the port forwarding SSH connection drops you will need to restart SSH with the same command, but you don\u2019t need to restart Jupyter.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThere are two ways to explicitly shut down Jupyter:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWithin Jupyter, click on the \u003cem\u003eJupyter\u003c/em\u003e logo in the top left to go to the main Jupyter page, then click \"Quit\" in the top right\u003c/li\u003e\n\u003cli\u003eDo control-C twice in the terminal where you started Jupyter. If this connection dropped, you can reconnect to it with:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003essh NetID@dcc-login-03.oit.duke.edu\ntmux a -t jupyter\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter shutting down the Jupyter session you can type \u003ccode\u003eexit\u003c/code\u003e at the terminal to close the tmux session.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you need more memory or more cpus you can use the \u003ccode\u003e--mem\u003c/code\u003e and/or \u003ccode\u003e--cpus-per-task\u003c/code\u003e arguments to in the \u201csrun\u201d, for example to request 4 CPUs and 10GB of RAM:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003esrun --cpus-per-task=4 --mem=10G singularity exec docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eIf you have high priority access to a partition you can request that partition be used with the \u003ccode\u003e-A\u003c/code\u003e and \u003ccode\u003e-p\u003c/code\u003e arguments to \u003ccode\u003esrun\u003c/code\u003e:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003esrun -A chsi -p chsi singularity exec docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003eYou might want to access files that are outside of your home directory. Within a singularity container your access to the host computer is\nlimited: by default, from inside the container you can only access your home directory. If you want to access directories that are outside your home\ndirectory, you have to tell \u003cem\u003eSingularity\u003c/em\u003e when you start the container with the \u003ccode\u003e--bind\u003c/code\u003e command line argument. For example:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003esrun singularity --bind /work/josh:/work/josh exec docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eYou can combine several of these command line flags:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003esrun -A chsi -p chsi --cpus-per-task=4 --mem=10G singularity  exec --bind /work/josh:/work/josh docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eIt is strongly recommended to set the \u003ccode\u003eSINGULARITY_CACHEDIR\u003c/code\u003e environment variables in your .bashrc or when running \u003ccode\u003esrun\u003c/code\u003e. This environment variable specifies where the Docker image (and the Singularity image built from it) are saved. If this variable is not specified, singularity will cache images in \u003ccode\u003e$HOME/.singularity/cache\u003c/code\u003e, which can fill up quickly. This is discussed in the \u003ca href=\"https://sylabs.io/guides/3.7/user-guide/build_env.html#cache-folders\" rel=\"nofollow\"\u003eSingularity Documentation\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eexport SINGULARITY_CACHEDIR=\"/work/josh/singularity_cache\"; srun -A chsi -p chsi --cpus-per-task=4 --mem=10G singularity  exec --bind /work/josh:/work/josh docker://dukehtscourse/jupyter-hts-2019 /usr/local/bin/start.sh jupyter notebook --ip=0.0.0.0 --no-browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-install-singularity\" class=\"anchor\" href=\"#install-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Singularity\u003c/h3\u003e\n\u003cp\u003eHere are instructions for installing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/2.6/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003eSingularity version 2.6\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.2/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003eSingularity version 3.2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/singularity-desktop-macos/\" rel=\"nofollow\"\u003eSingularity Desktop for macOS (Alpha Preview)\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623704048.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.2.1/Singularity"
    ],
    "full_name": "icaoberg/singularity-hisat2",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-hisat2/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ea61f9228ca14a66e58889a560447e0b7c8ba73ddbaa594055242ace96eb0a84/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4796300b08f76b423ee0574c15e8bd8ad15b0a389a36fd3f58549b8bb5df8690/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/25eeccddaeb5bf9a3f7053b646f9b7bf540d33b801c371db1850d745da46fc95/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f14a5cb988478f36746bb17a364f669ee4d02a32a6f6fddfbccaff9dc50a8379/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d686973617432\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hisat2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hisat\" class=\"anchor\" href=\"#singularity-hisat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hisat\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://daehwankimlab.github.io/hisat2/\" rel=\"nofollow\"\u003ehisat2\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the scripts \u003ccode\u003ehisat2*\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/hisat2/2.2.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/hisat2\u003c/code\u003e as \u003ccode\u003e2.2.1.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-singularity-definition-file\" class=\"anchor\" href=\"#building-the-image-using-the-singularity-definition-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the Singularity definition file\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing\nCenter\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624060173.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for graphviz",
    "filenames": [
      "2.44.0/Singularity"
    ],
    "full_name": "icaoberg/singularity-graphviz",
    "latest_release": "v2.44.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-graphviz\" class=\"anchor\" href=\"#singularity-graphviz\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-graphviz\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/960789693fa68a8f442f9c6cc7d6a117639f1a792ec84c96648ad4764c385fcf/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f656e2f342f34382f477261706876697a4c6f676f2e706e67\" alt=\"Logo\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/en/4/48/GraphvizLogo.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://graphviz.org/\" rel=\"nofollow\"\u003egraphviz \u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003egraphviz \u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/graphviz/2.44.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modules/graphviz \u003c/code\u003e as \u003ccode\u003e 2.44.0\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1622859227.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for dust",
    "filenames": [
      "0.5.4/Singularity"
    ],
    "full_name": "icaoberg/singularity-dust",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-dust\" class=\"anchor\" href=\"#singularity-dust\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-dust\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/bootandy/dust/raw/master/media/snap.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/bootandy/dust/raw/master/media/snap.png\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/bootandy/dust\"\u003edust\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003edust\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/dust/0.5.4\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/dust\u003c/code\u003e as \u003ccode\u003e0.5.4\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1622860644.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for hyperfine",
    "filenames": [
      "1.11.0/Singularity"
    ],
    "full_name": "icaoberg/singularity-hyperfine",
    "latest_release": "v1.11.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hyperfine\" class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hyperfine\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sharkdp/hyperfine\"\u003ehyperfine\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ehyperfine\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/hyperfine/1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/hyperfine\u003c/code\u003e as \u003ccode\u003e1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1624059711.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for methylpy",
    "filenames": [
      "1.4.3/Singularity"
    ],
    "full_name": "icaoberg/singularity-methylpy",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hyperfine\" class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hyperfine\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sharkdp/hyperfine\"\u003ehyperfine\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ehyperfine\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/hyperfine/1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/hyperfine\u003c/code\u003e as \u003ccode\u003e1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622730662.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for ABySS",
    "filenames": [
      "2.1.5/Singularity"
    ],
    "full_name": "icaoberg/singularity-abyss",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/icaoberg/singularity-hyperfine/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aa6ccae89f711313efdead7c3be0b796360740e33514a985a0f4968fa01cb0f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3251ba672b3bf023faefb928be7900afe28d7d2ebe6ae3a775c7e820687c6571/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5c452c9170d570d496414a1d624b5d3731e36ca355843b447512b991c91ee546/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a734b24e30b49dc758633fa5394475d80c0cfe02dc89ed582ec651d630a3f19c/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6963616f626572672f73696e67756c61726974792d687970657266696e65\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/icaoberg/singularity-hyperfine\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-hyperfine\" class=\"anchor\" href=\"#singularity-hyperfine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-hyperfine\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88a0cb35f42e02e28b0433d4b5e0029e52e723d8feb8df753e1ed06a5161db56/68747470733a2f2f692e696d6775722e636f6d2f7a31394f5978452e676966\" alt=\"Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/sharkdp/hyperfine\"\u003ehyperfine\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ehyperfine\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/hyperfine/1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/hyperfine\u003c/code\u003e as \u003ccode\u003e1.11.0\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622730467.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe for shellcheck",
    "filenames": [
      "0.5.0/Singularity"
    ],
    "full_name": "icaoberg/singularity-shellcheck",
    "latest_release": null,
    "readme": "\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\" width=\"300\" align=\"left\" data-canonical-src=\"https://i.paste.pics/870189fadf668a958c8aac83f38e799c.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pema\" class=\"anchor\" href=\"#pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA:\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\" class=\"anchor\" href=\"#a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ea flexible Pipeline for Environmental DNA Metabarcoding Analysis of the 16S/18S rRNA, ITS and COI marker genes\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003ePEMA is reposited in\u003c/em\u003e \u003ca href=\"https://hub.docker.com/r/hariszaf/pema\" rel=\"nofollow\"\u003e\u003cem\u003eDocker Hub\u003c/em\u003e\u003c/a\u003e \u003cem\u003eas well as in\u003c/em\u003e \u003ca href=\"https://singularity-hub.org/collections/2295\" rel=\"nofollow\"\u003e\u003cem\u003eSingularity Hub\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-a-pema-tutorial-can-be-found-here\" class=\"anchor\" href=\"#a-pema-tutorial-can-be-found-here\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA PEMA tutorial can be found \u003ca href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\" rel=\"nofollow\"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e.\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\" class=\"anchor\" href=\"#for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor any troubles you may have when running PEMA or for any potential improvevments you would like to suggest, please share on the \u003ca href=\"https://gitter.im/pema-helpdesk/community\" rel=\"nofollow\"\u003ePEMA Gitter community\u003c/a\u003e.\u003c/h4\u003e\n\n\u003cp\u003e\u003ca href=\"https://gitter.im/pema-helpdesk/community?utm_source=badge\u0026amp;utm_medium=badge\u0026amp;utm_campaign=pr-badge\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7385c04b449351f12fb57a4bd6f9791ebd68a483493399e50a8f096fadde4246/68747470733a2f2f6261646765732e6769747465722e696d2f70656d612d68656c706465736b2f636f6d6d756e6974792e737667\" alt=\"Gitter\" data-canonical-src=\"https://badges.gitter.im/pema-helpdesk/community.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.gnu.org/licenses/gpl-3.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\" alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#pema-biodiversity-in-all-its-different-levels\"\u003ePEMA: biodiversity in all its different levels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#a-container-based-tool\"\u003e A container-based tool\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#how-to-run-pema\"\u003eHow to run PEMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#parameters-file\"\u003eParameters\u0027 file\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#pema-on-hpc\"\u003ePEMA on HPC\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#prerequisites-1\"\u003ePrerequisites\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installing-1\"\u003eInstalling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-pema-1\"\u003eRunning PEMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#example\"\u003eExample\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#pema-on-a-simple-pc\"\u003ePEMA on a simple PC\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#prerequisites\"\u003ePrerequisites\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installing\"\u003eInstalling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-pema\"\u003eRunning PEMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#step-1---build-a-docker-container\"\u003eStep 1 - Build a Docker container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#step-2---run-pema\"\u003eStep 2 - Run PEMA\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#the-phyloseq-r-package\"\u003ephyloseq - for a downstream ecological analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#acknowledgments\"\u003eAcknowledgments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#license\"\u003eLicense\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-diff\"\u003e\u003cpre\u003e\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e convertion of the Illumina raw data is now implemented in the framework of PEMA\u003c/span\u003e\n\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e PEMA now supports 2 extra marker genes, 18S rRNA and ITS. \u003c/span\u003e\n\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e PEMA is now available for macOS!\u003c/span\u003e\n\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e for anything feel free to contact me at: haris-zaf@hcmr.gr\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-pema-biodiversity-in-all-its-different-levels\" class=\"anchor\" href=\"#pema-biodiversity-in-all-its-different-levels\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA: biodiversity in all its different levels\u003c/h1\u003e\n\u003cp\u003ePEMA supports the metabarcoding analysis of four marker genes, \u003cstrong\u003e16S rRNA\u003c/strong\u003e (Bacteria), \u003cstrong\u003eITS\u003c/strong\u003e (Fungi) as well as \u003cstrong\u003eCOI\u003c/strong\u003e and \u003cstrong\u003e18S rRNA\u003c/strong\u003e (metazoa). As input, PEMA accepts .fastq.gz files as returned by Illumina sequencing platforms.\u003c/p\u003e\n\u003cp\u003ePEMA processes the reads from each sample and \u003cstrong\u003ereturns an OTU- or an ASV-table with the taxonomies\u003c/strong\u003e of the taxa found and their abundances in each sample. It also returns statistics and a FASTQC diagram about the quality of the reads for each sample. Finally, PEMA supports \u003cstrong\u003edownstream ecological analysis\u003c/strong\u003e of the profiles retrieved, facilitated by the \u003ca href=\"http://joey711.github.io/phyloseq/index.html\" rel=\"nofollow\"\u003ephyloseq\u003c/a\u003e R package.\u003c/p\u003e\n\u003cp\u003ePEMA supports both OTU clustering (thanks to VSEARCH and CROP algorithms) and ASV inference (via SWARM) for all four marker genes.\u003c/p\u003e\n\u003cp\u003eFor the case of the 16S rRNA marker gene, PEMA includes two separate approaches for taxonomy assignment: alignment-based and phylogenetic-based. For the latter, a reference tree of 1000 taxa was created using SILVA_132_SSURef, EPA-ng and RaxML-ng.\u003c/p\u003e\n\u003cp\u003ePEMA has been implemented in \u003ca href=\"https://pcingola.github.io/BigDataScript/\" rel=\"nofollow\"\u003eBigDataScript\u003c/a\u003e programming language. BDS\u2019s ad hoc task parallelism and task synchronization, supports heavyweight computation. Thus, PEMA inherits such features and it also supports roll-back checkpoints and on-demand partial pipeline execution. In addition, PEMA takes advantage of all the computational power available on a specific machine; for example, if PEMA is executed on a personal laptop with 4 cores, it is going to use all four of them.\u003c/p\u003e\n\u003cp\u003eFinally, container-based technologies such as Docker and Singularity, make PEMA easy accessible for all operating systems.\nAs you can see in the \u003ca href=\"https://github.com/hariszaf/pema/blob/master/help_files/GitHub%20tutorial.pdf\"\u003ePEMA_tutorial.pdf\u003c/a\u003e, once you have either Docker or Singularity on your computational environment (see below which suits your case better), running PEMA is cakewalk. You can also find the \u003ca href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?usp=sharing\" rel=\"nofollow\"\u003e\u003cstrong\u003ePEMA tutorial\u003c/strong\u003e\u003c/a\u003e as a Google Slides file.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-a-container-based-tool\" class=\"anchor\" href=\"#a-container-based-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA container-based tool\u003c/h1\u003e\n\u003cp\u003ePEMA can run either on a HPC environment (server, cluster etc) or on a simple PC. However, we definitely suggest to run it on an HPC environment to exploit the full potential of PEMA. Running on a powerful server or a cluster can be time-saving since it would require significantly less computational time than in a common PC. However, for analyses with a small number of samples, a common PC can suffice.\u003c/p\u003e\n\u003cp\u003eThere is one \u003cstrong\u003emajor difference\u003c/strong\u003e between running PEMA on a common PC than running it on a HPC environment. In the first case, PEMA runs through \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003e\u003cstrong\u003eDocker\u003c/strong\u003e\u003c/a\u003e, while in the latter one, it runs through \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOn the following chapters, you can find how to install PEMA both in Docker and Singlularity including examples.\u003c/p\u003e\n\u003cp\u003eRunning PEMA is exactly \u003cstrong\u003ethe same\u003c/strong\u003e procedure in both of those cases.\u003c/p\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run-pema\" class=\"anchor\" href=\"#how-to-run-pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run PEMA\u003c/h2\u003e\n\u003cp\u003eAssuming you have either Docker or Singularity on your system (see below how to get them).\nYou need to create a directory where you will have everything PEMA needs - we will call it \u003cem\u003e\u003cstrong\u003eanalysis directory\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn this directory, you need to add the following \u003cstrong\u003emandatory\u003c/strong\u003e files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\u003e\u003cem\u003e\u003cstrong\u003eparameters.tsv\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e file (you can download it from this repository and then \u003cstrong\u003ecomplete it\u003c/strong\u003e according to the needs of your analysis)\u003c/li\u003e\n\u003cli\u003ea subdirectory called \u003cem\u003e\u003cstrong\u003emydata\u003c/strong\u003e\u003c/em\u003e where your .fastq.gz files will be located \u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf your need to perform phyloseq, in the analysis directory you also need to add the following \u003cstrong\u003eoptionally\u003c/strong\u003e files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\u003e\u003cem\u003e\u003cstrong\u003ephyloseq_in_PEMA.R\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e which you can also download from this repository and set it the way you want (that is an R script which we have implemented and has some main features that need to stay always the same in order to be executed as part of PEMA and some parts where the user can set what exactly needs to get from the phyloseq package)\u003c/li\u003e\n\u003cli\u003ethe \u003ca href=\"https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\" rel=\"nofollow\"\u003e\u003cem\u003e\u003cstrong\u003emetadata.csv\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e file which has to be in a \u003cstrong\u003ecomma separated\u003c/strong\u003e format (you can find an example of this file on PEMA\u0027s GitHub repository).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-attention--\" class=\"anchor\" href=\"#attention--\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eAttention!\u003c/strong\u003e  \u003cbr\u003e\n\u003c/h3\u003e\n\u003cp\u003ePEMA will \u003cstrong\u003efail\u003c/strong\u003e unless you name the aforementioned files and directories \u003cstrong\u003eexactly\u003c/strong\u003e as described above.\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example of how your \u003cem\u003eanalysis directory\u003c/em\u003e should be in case you do want a phyloseq analysis:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euser@home-PC:~/Desktop/analysis_directory$ ls\nmydata  parameters.tsv  phyloseq_in_PEMA.R  metadata.csv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand in case you do not:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euser@home-PC:~/Desktop/analysis_directory$ ls\nmydata  parameters.tsv \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/hariszaf/pema/tree/master/analysis_directory\"\u003e\u003cstrong\u003eHere\u003c/strong\u003e\u003c/a\u003e you can find an example of an \u003cem\u003eanalysis directory\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eAfter you have prepared this \u003cem\u003eanalysis directory\u003c/em\u003e you are ready to run PEMA (see below).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAn extended list with PEMA\u0027s ouput can be found \u003ca href=\"https://github.com/hariszaf/pema/blob/master/help_files/PEMA\u0027s%20output%20files.md\"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e.\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-parameters-file\" class=\"anchor\" href=\"#parameters-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u0027 file\u003c/h1\u003e\n\u003cp\u003eThe most crucial component in running PEMA is the parameters file. This file must be located \u003cstrong\u003ein\u003c/strong\u003e the \u003cem\u003eanalysis directory\u003c/em\u003e and the user needs to fill it \u003cstrong\u003eevery time\u003c/strong\u003e PEMA is about to be called. If you need more than one analyses to run, then you need to make copies of the parameters\u0027 file and have one of those in eah of the analysis directrories you create.\u003c/p\u003e\n\u003cp\u003eSo, here is the \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\u003e\u003cem\u003e\u003cstrong\u003eparameters.tsv\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e file as it looks like, in a study case of our own.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-pema-on-hpc\" class=\"anchor\" href=\"#pema-on-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA on HPC\u003c/h1\u003e\n\u003cp\u003ePEMA is best to run on HPC (server, cluster, cloud). Usually environmental data are quite large and the whole process has huge computational demands. To get PEMA running on your HPC you (actually your system administrator) need to install Singularity as described below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/strong\u003e  is a free, cross-platform and open-source computer program that performs operating-system-level virtualization also known as containerization. One of the main uses of Singularity is to bring containers and reproducibility to scientific computing and the high-performance computing (HPC) world.\u003c/p\u003e\n\u003cp\u003eSingularity needs a Linux/Unix system to run.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing\" class=\"anchor\" href=\"#installing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling\u003c/h2\u003e\n\u003cp\u003eAfter you install Singularity in your environment and open it, you need to download PEMA\u0027s image from Singularity Hub, by running the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e singularity pull shub://hariszaf/pema:v.1.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow you have PEMA on your environment. But there is still one really \u003cstrong\u003eimportant\u003c/strong\u003e thing that you need to do! Please \u003cstrong\u003edownload\u003c/strong\u003e the \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\u003e\u003cem\u003eparameters.tsv\u003c/em\u003e\u003c/a\u003e file and move it or copy it to the same directory with your raw data.\u003c/p\u003e\n\u003cp\u003eNow you are ready to go!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-pema\" class=\"anchor\" href=\"#running-pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning PEMA\u003c/h2\u003e\n\u003cp\u003eSingularity permits the use of a job scheduler that allocates computional resources on clusters and at the same time, works as a queuing system, as \u003cstrong\u003e\u003ca href=\"https://slurm.schedmd.com/overview.html\" rel=\"nofollow\"\u003eSlurm\u003c/a\u003e\u003c/strong\u003e. This way you are able to create a job as you usually do in your system and after editing the parameters file as needed, run PEMA as a job on your cluster.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e#SBATCH --partition=batch\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20\n#SBATCH --mem=\n# Memory per node specification is in MB. It is optional.\n# The default limit is 3000MB per core.\n#SBATCH --job-name=\"testPema\"\n#SBATCH --output=PEMA.output\n#SBATCH --mail-user=haris-zafr@hcmr.gr\n#SBATCH --mail-type=ALL\n#SBATCH --requeue\n\n\nsingularity run -B /\u0026lt;path\u0026gt;/\u0026lt;of\u0026gt;/\u0026lt;input\u0026gt;/\u0026lt;directory\u0026gt;/:/mnt/analysis /\u0026lt;path\u0026gt;/\u0026lt;of\u0026gt;/\u0026lt;PEMA_container\u0026gt;\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the above example, we set the cluster \"Zorba\", to run PEMA in 1 node, with 20 cores.\u003c/p\u003e\n\u003cp\u003eFor further information, you can always check \u003ca href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\" rel=\"nofollow\"\u003ePEMA\u0027s tutorial\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-pema-on-a-simple-pc\" class=\"anchor\" href=\"#pema-on-a-simple-pc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA on a simple PC\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eTo run PEMA in a simple PC on your own environment, you first need to install \u003ca href=\"https://docs.docker.com/install/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e, in case you do not already have it.\u003c/p\u003e\n\u003cp\u003eYou should check your software version. A version of Docker is avalable for all Windows, Mac and Linux. If you have Windows 10 Pro or your Mac\u0027s hardware in after 2010, then you can insall Docker straightforward. Otherwise, you need to install the \u003ca href=\"https://docs.docker.com/toolbox/\" rel=\"nofollow\"\u003eDocker toolbox\u003c/a\u003e instead. You can check if your System Requirements are according to the ones mentioned below in order to be sure what you need to do.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSystem Requirements\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e**__Windows 10 64bit__**:\nPro, Enterprise or Education (1607 Anniversary Update, Build 14393 or later).\nVirtualization is enabled in BIOS. Typically, virtualization is enabled by default.\nThis is different from having Hyper-V enabled. For more detail see Virtualization must be enabled in Troubleshooting.\nCPU SLAT-capable feature.\nAt least 4GB of RAM.\n\n**__Mac__**\nMac hardware must be a 2010 or newer model, with Intel\u2019s hardware support for memory management unit (MMU)\nvirtualization, including Extended Page Tables (EPT) and Unrestricted Mode. You can check to see if your machine\nhas this support by running the following command in a terminal:\nsysctl kern.hv_support macOS El Capitan 10.11 and newer macOS releases are supported.\nWe recommend upgrading to the latest version of macOS.\nAt least 4GB of RAM\nVirtualBox prior to version 4.3.30 must NOT be installed (it is incompatible with Docker for Mac).\nIf you have a newer version of VirtualBox installed, it\u2019s fine.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-1\" class=\"anchor\" href=\"#installing-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling\u003c/h2\u003e\n\u003cp\u003eAfter you install Docker in your environment and run it, the only thing you need to do, is to download PEMA\u0027s image, by running the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull hariszaf/pema\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe PEMA image file is a quite large (~3Gb), so it will take a while until it is downloaded in your computer system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-pema-1\" class=\"anchor\" href=\"#running-pema-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning PEMA\u003c/h2\u003e\n\u003cp\u003eRunning PEMA has two discrete steps.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1---build-a-docker-container\" class=\"anchor\" href=\"#step-1---build-a-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1 - Build a Docker container\u003c/h3\u003e\n\u003cp\u003eAt first, you need to let Docker have access in your dataset. To provide access you need to run the following command and specifying the path to where your data is stored, i.e. changing the \u0026lt;path_to_analysis_directory\u0026gt; accordingly:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -it -v /\u0026lt;path_to_analysis_directory\u0026gt;/:/mnt/analysis hariszaf/pema\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter you run the command above, you have now built a Docker container, in which you can run PEMA!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2---run-pema\" class=\"anchor\" href=\"#step-2---run-pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2 - Run PEMA\u003c/h3\u003e\n\u003cp\u003eNow, being inside the PEMA container, the only thing remaining to do, is to run PEMA\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./PEMA_v1.bds\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePEMA is now running. The runtime of PEMA depends on the computational features of your environment, on the size of your data, as well as the parameters you chose.\u003c/p\u003e\n\u003cp\u003ePlease, keep in mind that when you need to copy a whole directory, then you always have to put \"/\" in the end of the path that describes where the directory is located.\u003c/p\u003e\n\u003cp\u003eFinally, you will find the PEMA output in the analysis directory on your computer. \u003cbr\u003e\nAs the output directory is mounted into the built Docker container, you can copy its contents wherever you want. However, in case you want to remove it permanently, you need to do this as a sudo user.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-the-phyloseq-r-package\" class=\"anchor\" href=\"#the-phyloseq-r-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe \"phyloseq\" R package\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003efor a downstream ecological analysis of OTUs/ASVs retrieved\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePEMA performs all the basic functions of the \"phyloseq\" R package. In addition, it performs certain functions of the \u003ca href=\"https://cran.r-project.org/web/packages/vegan/index.html\" rel=\"nofollow\"\u003e\u003cem\u003e\u003cstrong\u003evegan\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e R package.\u003c/p\u003e\n\u003cp\u003eWhen the user asks for a downstream analysis using the \"phyloseq\" R package, then an extra input file called \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\u003e\u003cem\u003e\u003cstrong\u003e\"phyloseq_script.R\"\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e needs to be imported in the \"analysis_directory\". In PEMA\u0027s main repository, you can find a template of this file; this file needs to be as it would run on your own computer, as you would run \u003cem\u003ephyloseq\u003c/em\u003e in any case. PEMA will create the \u003cem\u003e\"phyloseq object\"\u003c/em\u003e automatically and then it will perform the analysis as asked. The output will be placed in an extra subfolder in the main output directory of PEMA called \u003cem\u003ephyloseq_analysis\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn addition, the \u003cem\u003e\u003cstrong\u003emetadata.tsv\u003c/strong\u003e\u003c/em\u003e file is also required when the phyloseq option has been selected. An example of this file you can find \u003ca href=\"https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h1\u003e\n\u003cp\u003ePEMA uses a series of tools, datasets as well as Big Data Script language. We thank all the groups that developed them.\nThe tools \u0026amp; databases that PEMA uses are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBigDataScript programming language - \u003ca href=\"https://pcingola.github.io/BigDataScript/\" rel=\"nofollow\"\u003ehttps://pcingola.github.io/BigDataScript/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFASTQC - \u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\u003ehttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u03a4rimmomatic - \u003ca href=\"http://www.usadellab.org/cms/?page=trimmomatic\" rel=\"nofollow\"\u003ehttp://www.usadellab.org/cms/?page=trimmomatic\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCutadapt - \u003ca href=\"https://cutadapt.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003ehttps://cutadapt.readthedocs.io/en/stable/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eBayesHammer - included in SPAdes - \u003ca href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\u003ehttp://cab.spbu.ru/software/spades/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePANDAseq - \u003ca href=\"https://github.com/neufeld/pandaseq\"\u003ehttps://github.com/neufeld/pandaseq\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eOBITools - \u003ca href=\"https://pythonhosted.org/OBITools/welcome.html\" rel=\"nofollow\"\u003ehttps://pythonhosted.org/OBITools/welcome.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eBLAST Command Line Applications - \u003ca href=\"https://www.ncbi.nlm.nih.gov/books/NBK52640/\" rel=\"nofollow\"\u003ehttps://www.ncbi.nlm.nih.gov/books/NBK52640/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eVSEARCH-2.9.1 - \u003ca href=\"https://github.com/torognes/vsearch/releases/tag/v2.9.1\"\u003ehttps://github.com/torognes/vsearch/releases/tag/v2.9.1\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSWARM - \u003ca href=\"https://github.com/torognes/swarm\"\u003ehttps://github.com/torognes/swarm\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCROP - \u003ca href=\"https://github.com/tingchenlab/CROP\"\u003ehttps://github.com/tingchenlab/CROP\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCREST - \u003ca href=\"https://github.com/lanzen/CREST\"\u003ehttps://github.com/lanzen/CREST\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRDPClassifier - \u003ca href=\"https://github.com/rdpstaff/classifier\"\u003ehttps://github.com/rdpstaff/classifier\u003c/a\u003e\n(RPDtools are required in order to execute RDPClassifier)\u003c/li\u003e\n\u003cli\u003eSILVA db - \u003ca href=\"https://www.arb-silva.de/no_cache/download/archive/current/Exports/\" rel=\"nofollow\"\u003ehttps://www.arb-silva.de/no_cache/download/archive/current/Exports/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMIDORI db - \u003ca href=\"http://reference-midori.info/index.html\" rel=\"nofollow\"\u003ehttp://reference-midori.info/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\"phat\" algorithm, from the \"gappa\" package - \u003ca href=\"https://github.com/lczech/gappa/wiki/Subcommand:-phat\"\u003ehttps://github.com/lczech/gappa/wiki/Subcommand:-phat\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMAFFT - \u003ca href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\u003ehttps://mafft.cbrc.jp/alignment/software/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRAxML -ng - \u003ca href=\"https://github.com/amkozlov/raxml-ng\"\u003ehttps://github.com/amkozlov/raxml-ng\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePaPaRa - \u003ca href=\"https://cme.h-its.org/exelixis/web/software/papara/index.html\" rel=\"nofollow\"\u003ehttps://cme.h-its.org/exelixis/web/software/papara/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eEPA-ng - \u003ca href=\"https://github.com/Pbdas/epa-ng\"\u003ehttps://github.com/Pbdas/epa-ng\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ephyloseq R package - \u003ca href=\"http://joey711.github.io/phyloseq/index.html\" rel=\"nofollow\"\u003ehttp://joey711.github.io/phyloseq/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003evegan R package - \u003ca href=\"https://cran.r-project.org/web/packages/vegan/index.html\" rel=\"nofollow\"\u003ehttps://cran.r-project.org/web/packages/vegan/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd of course the container-based technologies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocker - \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003ehttps://www.docker.com/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity - \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003ehttps://sylabs.io/singularity/\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h1\u003e\n\u003cp\u003ePEMA is under the GNU GPLv3 license (for 3rd party components separate licenses apply).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h1\u003e\n\u003cp\u003eHaris Zafeiropoulos, Ha Quoc Viet, Katerina Vasileiadou, Antonis Potirakis, Christos Arvanitidis, Pantelis Topalis, Christina Pavloudi, Evangelos Pafilis, PEMA: a flexible Pipeline for Environmental DNA Metabarcoding Analysis of the 16S/18S ribosomal RNA, ITS, and COI marker genes, GigaScience, Volume 9, Issue 3, March 2020, giaa022, \u003ca href=\"https://doi.org/10.1093/gigascience/giaa022\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/gigascience/giaa022\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622859451.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "baxpr/demo-singularity-spm-freeview",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demo-singularity-container-with-spm12-and-freeview\" class=\"anchor\" href=\"#demo-singularity-container-with-spm12-and-freeview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo singularity container with SPM12 and Freeview\u003c/h1\u003e\n\u003cp\u003eSPM12-based pipelines require a little extra work to get them compiled and working in a\ncontainer. Freesurfer\u0027s Freeview is also included here, as it\u0027s very handy for creating\nthe PDF QA report.\u003c/p\u003e\n\u003cp\u003eSee \u003ca href=\"https://github.com/baxpr/demo-singularity-matlab-fsl\"\u003ehttps://github.com/baxpr/demo-singularity-matlab-fsl\u003c/a\u003e for a lot of detailed info about\nputting Matlab code into singularity containers. This example uses the same structure.\u003c/p\u003e\n\u003cp\u003eA licensed Matlab installation is required to compile the Matlab code, but is not needed\nto run the compiled executable in the container.\u003c/p\u003e\n\u003cp\u003eSPM12 (\u003ca href=\"https://www.fil.ion.ucl.ac.uk/spm/software/spm12/\" rel=\"nofollow\"\u003ehttps://www.fil.ion.ucl.ac.uk/spm/software/spm12/\u003c/a\u003e) is not in this repository and must\nbe installed separately on the compilation host. Edit \u003ccode\u003ematlab/compile_matlab.sh\u003c/code\u003e to point\nto it.\u003c/p\u003e\n\u003cp\u003eSPM requires jumping an extra hurdle at the compilation step - we use a modified version\nof SPM\u0027s own compiler function \u003ccode\u003espm_make_standalone.m\u003c/code\u003e, found at\n\u003ccode\u003ematlab/spm_make_standalone_local.m\u003c/code\u003e. This process captures a lot of dependencies that\ncould otherwise easily be left out of the executable, with the resulting symptom that it\ncompiles just fine but fails at run time with various cryptic error messages. In addition\nto SPM12, everything in the \u003ccode\u003ematlab/src\u003c/code\u003e directory is included in the path at compile time.\nIf Matlab toolboxes are used, they will need to be added to the list of included toolboxes\nin \u003ccode\u003ematlab/spm_make_standalone_local.m\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe compiled Matlab executable is stored on github using git LFS. A regular git clone will\ndownload a pointer text file instead of the executable binary. The result of building a\ncontainer from that will be a cryptic error message - so, compile it yourself. Or, if\nstoring on github, download it manually and replace the pointer text file, or include this\nstep in the Singularity file if helpful - example here:\n\u003ca href=\"https://github.com/baxpr/gf-fmri/blob/master/Singularity.v1.3.4#L65\"\u003ehttps://github.com/baxpr/gf-fmri/blob/master/Singularity.v1.3.4#L65\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFreesurfer requires a license to run:\n\u003ca href=\"https://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License\" rel=\"nofollow\"\u003ehttps://surfer.nmr.mgh.harvard.edu/fswiki/DownloadAndInstall#License\u003c/a\u003e\nBest practice is to store your license file on the host that will run the container, and\nbind it to the container at runtime - NOT to include your own license file in the\ncontainer itself.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625615789.0
  },
  {
    "data_format": 2,
    "description": "Hosts DockerFiles to build MRtrix3 containers",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MRtrix3/containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers-for-mrtrix3\" class=\"anchor\" href=\"#containers-for-mrtrix3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers for \u003cem\u003eMRtrix3\u003c/em\u003e\n\u003c/h1\u003e\n\u003cp\u003eHosts recipe files to build \u003cem\u003eMRtrix3\u003c/em\u003e containers\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-docker\" class=\"anchor\" href=\"#using-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Docker\u003c/h2\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-run-terminal-command\" class=\"anchor\" href=\"#run-terminal-command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun terminal command\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --rm -it mrtrix3 \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf not built locally, \u003ccode\u003edocker\u003c/code\u003e will download the latest image from DockerHub.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-run-gui\" class=\"anchor\" href=\"#run-gui\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun GUI\u003c/h4\u003e\n\u003cp\u003eThese instructions are for Linux.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003exhost +local:root\ndocker run --rm -it -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=$DISPLAY mrtrix3 mrview\nxhost -local:root  # Run this when finished.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-locally-build-docker-image\" class=\"anchor\" href=\"#locally-build-docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocally build Docker image\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003edocker build --tag mrtrix3 .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSet \u003ccode\u003eDOCKER_BUILDKIT=1\u003c/code\u003e to build parts of the Docker image in parallel, which can speed up build time.\nUse \u003ccode\u003e--build-arg MAKE_JOBS=4\u003c/code\u003e to build \u003cem\u003eMRtrix3\u003c/em\u003e with 4 processors (can substitute this with any number of processors \u0026gt; 0); if omitted, \u003cem\u003eMRtrix3\u003c/em\u003e will be built using a single thread only.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Singularity\u003c/h2\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-build-container-natively\" class=\"anchor\" href=\"#build-container-natively\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild container natively\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build MRtrix3_\u0026lt;version\u0026gt;.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-convert-from-docker-container\" class=\"anchor\" href=\"#convert-from-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConvert from Docker container\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build MRtrix3_\u0026lt;version\u0026gt;.sif docker://mrtrix/mrtrix3:\u0026lt;version\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-run-terminal-command-1\" class=\"anchor\" href=\"#run-terminal-command-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun terminal command\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003eMRtrix3_\u0026lt;version\u0026gt;.sif \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-run-gui-1\" class=\"anchor\" href=\"#run-gui-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun GUI\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec -B /run MRtrix3_\u0026lt;version\u0026gt;.sif mrview\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-developers-update-minified-external-dependencies\" class=\"anchor\" href=\"#developers-update-minified-external-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopers: Update minified external dependencies\u003c/h2\u003e\n\u003cp\u003eThis process can only be completed by those with write access to the \u003ca href=\"https://osf.io/5rwp3/\" rel=\"nofollow\"\u003e\"\u003cem\u003eMRtrix3\u003c/em\u003e container dependencies\" OSF project\u003c/a\u003e.\nThese files contain \"minified\" versions of external neuroimaging software package dependencies, containing only those components that are utilised by \u003cem\u003eMRtrix3\u003c/em\u003e scripts.\nThese files should only need to be updated if:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn \u003cem\u003eMRtrix3\u003c/em\u003e update introduces a new feature that invokes some new external software tool not previously utilised;\u003c/li\u003e\n\u003cli\u003eA requisite update occurs in one of these external softwares.\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the \u003ccode\u003edocker\u003c/code\u003e and \u003ccode\u003eneurodocker\u003c/code\u003e Python packages.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip install docker neurodocker\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDownload the ART ACPCdetect tool from NITRC into the working directory.\u003c/p\u003e\n\u003cp\u003eThis cannot be downloaded directly via e.g. \u003ccode\u003ewget\u003c/code\u003e, as it requires logging in to NITRC; instead, visit the following link with a web browser:\n\u003ca href=\"https://www.nitrc.org/frs/download.php/10595/acpcdetect_v2.0_LinuxCentOS6.7.tar.gz\" rel=\"nofollow\"\u003e\u003ccode\u003ehttps://www.nitrc.org/frs/download.php/10595/acpcdetect_v2.0_LinuxCentOS6.7.tar.gz\u003c/code\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDownload test data necessary for minification process.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl -fL -# https://github.com/MRtrix3/script_test_data/archive/master.tar.gz | tar xz\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUpdate file \u003ccode\u003eminify.Dockerfile\u003c/code\u003e to install the desired versions of external software packages.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Docker image for \u003ccode\u003eneurodocker-minify\u003c/code\u003e, with complete installations of external packages.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eDOCKER_BUILDKIT=1 docker build --tag mrtrix3:minify --file minify.Dockerfile --build-arg MAKE_JOBS=4 .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccode\u003eDOCKER_BUILDKIT=1\u003c/code\u003e enables BuildKit, which builds separate build stages in parallel.\nThis can speed up Docker build times in some circumstances.\nIn this case, ANTs and \u003cem\u003eMRtrix3\u003c/em\u003e will be compiled in parallel, and other downloads will be performed at the same time as well.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eMAKE_JOBS\u003c/code\u003e argument controls how many cores are used for compilation of ANTs and \u003cem\u003eMRtrix3\u003c/em\u003e.\nIf BuildKit is utilised, do not specify all of the available threads; specify half or fewer, so that threads are not unnecessarily split across jobs and RAM usage is not excessive.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a minified version of the Docker image.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --rm -itd --name mrtrix3 --security-opt=seccomp:unconfined --volume $(pwd)/script_test_data-master:/mnt mrtrix3:minify\nneurodocker-minify --dirs-to-prune /opt --container mrtrix3 --commands \"bash cmds-to-minify.sh\"\ndocker export mrtrix3 | docker import - mrtrix3:minified\ndocker stop mrtrix3\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGenerate tarballs for each of the utilised dependencies.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p tarballs\ndocker run --rm -itd --workdir /opt --name mrtrix3 \\\n    --volume $(pwd)/tarballs:/output mrtrix3:minified bash\ndocker exec mrtrix3 bash -c \"tar c art | pigz -9 \u0026gt; /output/acpcdetect_\u0026lt;version\u0026gt;.tar.gz\"\ndocker exec mrtrix3 bash -c \"tar c ants | pigz -9 \u0026gt; /output/ants_\u0026lt;version\u0026gt;.tar.gz\"\ndocker exec mrtrix3 bash -c \"tar c fsl | pigz -9 \u0026gt; /output/fsl_\u0026lt;version\u0026gt;.tar.gz\"\ndocker stop mrtrix3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor each tarball, manually replace text \"\u003ccode\u003e\u0026lt;version\u0026gt;\u003c/code\u003e\" with the version number of that particular software that was installed in the container.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUpload these files to \u003ca href=\"https://osf.io/nfx85/\" rel=\"nofollow\"\u003eOSF\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFile \u003ccode\u003eDockerfile\u003c/code\u003e can then be modified to download the desired versions of external software packages.\nAs OSF file download links do not contain file names, which would otherwise indicate the version of each software to be downloaded, please ensure that comments within that file are updated to indicate the version of that software within the tarball.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1612696118.0
  },
  {
    "data_format": 2,
    "description": "Simple terminal UI for git commands",
    "filenames": [
      "0.24.2/Singularity",
      "0.22.9/Singularity",
      "0.23.7/Singularity"
    ],
    "full_name": "pscedu/singularity-lazygit",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lazygit\" class=\"anchor\" href=\"#lazygit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elazygit\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec singularity-lazygit-0.22.9.sif lazygit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"/images/screenshot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/images/screenshot.png\" alt=\"Screenshot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ee7cb50ef5a31fe25a8e149b0a09b50a8523d5e11a27311af8e63f22a8c915b2/687474703a2f2f7777772e616e647265772e636d752e6564752f757365722f6963616f626572672f696d616765732f6c6f676f732f7073632e706e67\" alt=\"PSC\" data-canonical-src=\"http://www.andrew.cmu.edu/user/icaoberg/images/logos/psc.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [
      "singularity",
      "utilities"
    ],
    "updated_at": 1626307362.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "2.7.6a/Singularity"
    ],
    "full_name": "pscedu/singularity-star",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lazygit\" class=\"anchor\" href=\"#lazygit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elazygit\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec singularity-lazygit-0.22.9.sif lazygit\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"/images/screenshot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/images/screenshot.png\" alt=\"Screenshot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ee7cb50ef5a31fe25a8e149b0a09b50a8523d5e11a27311af8e63f22a8c915b2/687474703a2f2f7777772e616e647265772e636d752e6564752f757365722f6963616f626572672f696d616765732f6c6f676f732f7073632e706e67\" alt=\"PSC\" data-canonical-src=\"http://www.andrew.cmu.edu/user/icaoberg/images/logos/psc.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.andrew.cmu.edu/~icaoberg\" rel=\"nofollow\"\u003eicaoberg\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626306899.0
  },
  {
    "data_format": 2,
    "description": "GNU Octave is software featuring a high-level programming language, primarily intended for numerical computations",
    "filenames": [
      "6.2.0/Singularity"
    ],
    "full_name": "pscedu/singularity-octave",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-octave/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-octave/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/f0eef640ab704754409e5b3805cf764e1ae5c4a70f474b51e86e21ca7c7ce71a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6f6374617665\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f0eef640ab704754409e5b3805cf764e1ae5c4a70f474b51e86e21ca7c7ce71a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d6f6374617665\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-octave\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/af95265639886aed45e4f673056ff5d595df2b5d5760eb36abe563365c430bb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6f6374617665\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/af95265639886aed45e4f673056ff5d595df2b5d5760eb36abe563365c430bb7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d6f6374617665\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-octave\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/4f62994add717900b8861b5c16791ebf7c20c850bdd05ed86990f15629ef2696/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6f6374617665\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4f62994add717900b8861b5c16791ebf7c20c850bdd05ed86990f15629ef2696/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d6f6374617665\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-octave\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a17a4ce476d76eb74b11af4b5598c58d76785b012ef2ed56b3eb98106a68fdab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6f6374617665\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a17a4ce476d76eb74b11af4b5598c58d76785b012ef2ed56b3eb98106a68fdab/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d6f6374617665\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-octave\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-octave\" class=\"anchor\" href=\"#singularity-octave\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-octave\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/fb797545afaa696b8fee075f0170506292bd4e6d411bac5a8358b465a6d8a87d/68747470733a2f2f7777772e676e752e6f72672f736f6674776172652f6f63746176652f696d672f474e555f4f63746176655f342d342d305f73637265656e73686f745f31363030783930302e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fb797545afaa696b8fee075f0170506292bd4e6d411bac5a8358b465a6d8a87d/68747470733a2f2f7777772e676e752e6f72672f736f6674776172652f6f63746176652f696d672f474e555f4f63746176655f342d342d305f73637265656e73686f745f31363030783930302e706e67\" alt=\"Octave\" data-canonical-src=\"https://www.gnu.org/software/octave/img/GNU_Octave_4-4-0_screenshot_1600x900.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://www.gnu.org/software/octave/\" rel=\"nofollow\"\u003eoctave\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003eoctave\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/octave/6.2.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/octave\u003c/code\u003e as \u003ccode\u003e6.2.0.lua\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image remotely\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./rbuild.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "numerical-computation"
    ],
    "updated_at": 1625243548.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for NovoGraph (https://github.com/NCBI-Hackathons/NovoGraph)",
    "filenames": [
      "Singularity",
      "Singularity.1.0.0"
    ],
    "full_name": "powerPlant/novograph-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2342\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the NovoGraph tool to construct a genome graph representation of long-read-based de novo sequence assemblies\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1550014659.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v2.2.0"
    ],
    "full_name": "baxpr/connprep",
    "latest_release": "v2.2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-connprep\" class=\"anchor\" href=\"#connprep\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econnprep\u003c/h1\u003e\n\u003cp\u003eProduce preprocessed fMRI images ready for connectivity analysis.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eDrop initial or final volumes as specified. Default: Analyze all volumes.\u003c/li\u003e\n\u003cli\u003eGet the TR (volume acquisition time) from pixdim[4] field of the Nifti header.\u003c/li\u003e\n\u003cli\u003eSlice timing correction. Default: none.\u003c/li\u003e\n\u003cli\u003eHead motion realignment (SPM12 two-stage) and production of mean fMRI.\u003c/li\u003e\n\u003cli\u003eRigid body coregistration of mean fMRI to T1 structural.\u003c/li\u003e\n\u003cli\u003eCompute volume quality metrics FD, DVARS.\u003c/li\u003e\n\u003cli\u003eReslice realigned fMRI to native space, and also warp to MNI space using CAT12 transform.\u003c/li\u003e\n\u003cli\u003eRemove confounds from the native and MNI space fMRIs by simultaneous regression. Defaults:\n\u003cul\u003e\n\u003cli\u003e0.01 - 0.10 Hz bandpass filter\u003c/li\u003e\n\u003cli\u003e6 estimated motion parameters and their first differences\u003c/li\u003e\n\u003cli\u003e6 principal components from the white matter + CSF compartment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRepeat the confound removal, additionally removing the mean signal of the gray matter compartment.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003enum_initial_vols_to_drop      0       Number of initial volumes to drop\nnum_vols_to_analyze           all     Total number of volumes to analyze\nbandpasslo_hz                 0.01    Low edge of bandpass filter in Hz\nbandpasshi_hz                 0.10    High edge of bandpass filter\nmot_PCs                       6       Number of PCs of motion params to remove\nmotderiv_PCs                  6       Same for motion derivatives\nwmcsf_PCs                     6       Same for white matter/CSF compartment\nslorder                       none    Slice timing correction, SPM12 nomenclature \nfmri_niigz                            fMRI images, 4D Nifti\nmt1_niigz                             T1 structural\ndeffwd_niigz                          Forward deformation of T1 to MNI\ngray_niigz                            Gray matter volume fraction\nwhite_niigz                           White matter volume fraction\ncsf_niigz                             CSF volume fraction\nproject                               XNAT project label\nsubject                               XNAT subject label\nsession                               XNAT session label\nscan                                  XNAT scan label\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003econnprep.pdf                               Processing report\nrp_adfmri.txt                              Realignment parameters\nFD.txt                                     Framewise displacement\nDVARS.txt                                  Framewise noise\nfiltered_keepgm_noscrub_nadfmri.nii.gz     Filtered data, native space, gray matter signal retained\nfiltered_keepgm_noscrub_wadfmri.nii.gz     Filtered data, MNI space, gray matter signal retained\nfiltered_removegm_noscrub_nadfmri.nii.gz   Filtered data, native space, gray matter signal removed\nfiltered_removegm_noscrub_wadfmri.nii.gz   Filtered data, MNI space, gray matter signal removed\nmeanadfmri.nii.gz                          Mean fMRI, native space\nwmeanadfmri.nii.gz                         Mean fMRI, MNI space\nstats_keepgm_noscrub.txt                   Processing info when gray matter signal retained\nstats_removegm_noscrub.txt                 Processing info when gray matter signal removed\ngm_mask.nii.gz                             Native space gray matter mask\nwmcsf_mask.nii.gz                          Native space white matter/CSF mask\nconfounds_keepgm_noscrub.txt               Confounds matrix when gray matter signal retained\nconfounds_removegm_noscrub.txt             Confounds matrix  when gray matter signal removed\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1595372367.0
  },
  {
    "data_format": 2,
    "description": "Affinity Representing Instance Descriptors",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "funkelab/arid",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-connprep\" class=\"anchor\" href=\"#connprep\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econnprep\u003c/h1\u003e\n\u003cp\u003eProduce preprocessed fMRI images ready for connectivity analysis.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eDrop initial or final volumes as specified. Default: Analyze all volumes.\u003c/li\u003e\n\u003cli\u003eGet the TR (volume acquisition time) from pixdim[4] field of the Nifti header.\u003c/li\u003e\n\u003cli\u003eSlice timing correction. Default: none.\u003c/li\u003e\n\u003cli\u003eHead motion realignment (SPM12 two-stage) and production of mean fMRI.\u003c/li\u003e\n\u003cli\u003eRigid body coregistration of mean fMRI to T1 structural.\u003c/li\u003e\n\u003cli\u003eCompute volume quality metrics FD, DVARS.\u003c/li\u003e\n\u003cli\u003eReslice realigned fMRI to native space, and also warp to MNI space using CAT12 transform.\u003c/li\u003e\n\u003cli\u003eRemove confounds from the native and MNI space fMRIs by simultaneous regression. Defaults:\n\u003cul\u003e\n\u003cli\u003e0.01 - 0.10 Hz bandpass filter\u003c/li\u003e\n\u003cli\u003e6 estimated motion parameters and their first differences\u003c/li\u003e\n\u003cli\u003e6 principal components from the white matter + CSF compartment\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRepeat the confound removal, additionally removing the mean signal of the gray matter compartment.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003enum_initial_vols_to_drop      0       Number of initial volumes to drop\nnum_vols_to_analyze           all     Total number of volumes to analyze\nbandpasslo_hz                 0.01    Low edge of bandpass filter in Hz\nbandpasshi_hz                 0.10    High edge of bandpass filter\nmot_PCs                       6       Number of PCs of motion params to remove\nmotderiv_PCs                  6       Same for motion derivatives\nwmcsf_PCs                     6       Same for white matter/CSF compartment\nslorder                       none    Slice timing correction, SPM12 nomenclature \nfmri_niigz                            fMRI images, 4D Nifti\nmt1_niigz                             T1 structural\ndeffwd_niigz                          Forward deformation of T1 to MNI\ngray_niigz                            Gray matter volume fraction\nwhite_niigz                           White matter volume fraction\ncsf_niigz                             CSF volume fraction\nproject                               XNAT project label\nsubject                               XNAT subject label\nsession                               XNAT session label\nscan                                  XNAT scan label\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003econnprep.pdf                               Processing report\nrp_adfmri.txt                              Realignment parameters\nFD.txt                                     Framewise displacement\nDVARS.txt                                  Framewise noise\nfiltered_keepgm_noscrub_nadfmri.nii.gz     Filtered data, native space, gray matter signal retained\nfiltered_keepgm_noscrub_wadfmri.nii.gz     Filtered data, MNI space, gray matter signal retained\nfiltered_removegm_noscrub_nadfmri.nii.gz   Filtered data, native space, gray matter signal removed\nfiltered_removegm_noscrub_wadfmri.nii.gz   Filtered data, MNI space, gray matter signal removed\nmeanadfmri.nii.gz                          Mean fMRI, native space\nwmeanadfmri.nii.gz                         Mean fMRI, MNI space\nstats_keepgm_noscrub.txt                   Processing info when gray matter signal retained\nstats_removegm_noscrub.txt                 Processing info when gray matter signal removed\ngm_mask.nii.gz                             Native space gray matter mask\nwmcsf_mask.nii.gz                          Native space white matter/CSF mask\nconfounds_keepgm_noscrub.txt               Confounds matrix when gray matter signal retained\nconfounds_removegm_noscrub.txt             Confounds matrix  when gray matter signal removed\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1562764827.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity.BlendIt.def"
    ],
    "full_name": "housw/BlendIt",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-corebedtools-intersect\" class=\"anchor\" href=\"#nf-corebedtools-intersect\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-core/bedtools-intersect\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eIntersect lots of bed files with lots of other bed files\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nf-core/bedtools-intersect\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/811368779316af4f70b4dd35fc2c24cebcc4dc194cd63234e130384ec38ac89f/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f626564746f6f6c732d696e746572736563742e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nf-core/bedtools-intersect.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/bedtools-intersect\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca7e06b0d2929a9cba14da1892e90c6d4673a695806cb07ea82e89a1cbecef92/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f626564746f6f6c732d696e746572736563742e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/bedtools-intersect.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe nf-core/bedtools-intersect pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1623019661.0
  },
  {
    "data_format": 2,
    "description": "singularity lc builds",
    "filenames": [
      "Singularity"
    ],
    "full_name": "iapalm/lc-builds",
    "latest_release": null,
    "readme": "\u003cp\u003eThis repository provides some boiler plate scripts for running \u0027pangeo\u0027 python ecosystem using singularity containers.\u003c/p\u003e\n\u003cp\u003eSteps are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eObtain docker image curated at \u003ca href=\"https://github.com/pangeo-data/pangeo-stacks\"\u003ehttps://github.com/pangeo-data/pangeo-stacks\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull pangeo/pangeo-notebook\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe pangeo-notebook has a pretty diverse set of libraries for most cloud,\ndask, zarr, netCDF, analysis type tasks.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e(Optional) Obtain docker image curated at \u003ca href=\"https://github.com/pangeo-data/pangeo-stacks\"\u003ehttps://github.com/pangeo-data/pangeo-stacks\u003c/a\u003e\nIf you need to customise, see minimal example in Dockerfile and requirements.txt and description here:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e(Deprecated) \u003ca href=\"https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants\"\u003ehttps://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e(\u003cstrong\u003eUse this since 27-07-2020\u003c/strong\u003e) \u003ca href=\"https://github.com/pangeo-data/pangeo-docker-images\"\u003ehttps://github.com/pangeo-data/pangeo-docker-images\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThen you would build a custom image along the lines of:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake pangeo-notebook\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConvert docker image to singularity with a command such as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity -d build pangeo-latest.sif docker-daemon://pangeo/pangeo-notebook:master\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy the created \u003ccode\u003epangeo-latest.sif\u003c/code\u003e singularity image to somewhere accessible on the HPC filesystem and edit the \u003ccode\u003econtainer=\u003c/code\u003e and \u003ccode\u003escheduler_file=\u003c/code\u003e variables in the \u003ccode\u003estart_jupyter.slurm\u003c/code\u003e and \u003ccode\u003estart_worker.slurm\u003c/code\u003e scripts to point to the singularity image and the shared filesystem location to write the scheduler details, respectively.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStart the jupyter lab and dask-scheduler, the first parameter is the working path you want to use for jupyter lab:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch start_jupyter.slurm $MYGROUP\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis starts a scheduler and jupyterlab with 2 cores each and 8GB/core memory. These can be edited in the #SBATCH headers, also note you can set the default directory for jupyterlab with the notebook_dir which is the parameter passed to start_jupyter.slurm.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStart dask-workers (where n is the number of workers you want - these are configures for \u0026lt; 2 hour wall time limit so that they use the \u003ccode\u003eh2\u003c/code\u003e queue):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch -n 10 start_worker.slurm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ealso note that this input argument to dask-worker \u003ccode\u003e--local-directory $LOCALDIR\u003c/code\u003e tells the worker the path to local disk storage on the node which can be used for spilling data, but not all HPC nodes/centres have attached local storage. Currently this is disabled.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSee instruction printed to the slurm-######.out log file for connecting to the jupyter session running on the compute node, something like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -N -l pbranson -L 8888:compute-node123:8888 hpc-login.host.com\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand take note of the randomly generated token printed to the slurm-######.out log file. You will need that to login to Jupyterlab.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo connect to the dask-scheduler from a notebook use the following snippet:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\nfrom distributed import Client\nclient=Client(scheduler_file=os.environ[\u0027MYSCRATCH\u0027] + \u0027/scheduler.json\u0027)\nclient\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eView the scheduler bokeh dashboard at \u003ca href=\"http://localhost:8888/proxy/8787/status\" rel=\"nofollow\"\u003ehttp://localhost:8888/proxy/8787/status\u003c/a\u003e. This can also be entered into the Jupyterlab dask widget as \u003ccode\u003e/proxy/8787/status\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs a little cheat in jupyter lab I open up a terminal and then do\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh localhost\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto connect to the host running the jupyter container - this gives you access to the slurm job scheduler from that terminal and you can start workers  in there with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch start_worker.slurm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlso note that the dask worker specifications used in the \u003ccode\u003estart_worker.slurm\u003c/code\u003e script are based of the slurm environment variables, so you can alter the worker specification using the \u003ccode\u003e#SBATCH\u003c/code\u003e directives:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#SBATCH --ntasks=20\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=10G\n#SBATCH --time=0:30:00\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor at the command line when you submit the script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch -n 4 -c 4 --mem-per-cpu=16G start_worker.slurm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich would start 4 workers with 4 cores per worker and 16*4 = 64GB memory per dask-worker.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1560984106.0
  },
  {
    "data_format": 2,
    "description": "Python wrapper for submitting jobs via bsub with the option to do so in a container environment.",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "funkelab/funlib.run",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-funlibrun\" class=\"anchor\" href=\"#funlibrun\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efunlib.run\u003c/h1\u003e\n\u003cp\u003ePython wrapper for submitting jobs via bsub with the option to do so in a container environment.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003emake install-full\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis creates a funlib.run config file ~/.funlib.run\nthat contains default parameters that\ncan be overwritten for each specific run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enum_gpus = 1\nmemory = 25600\nworking_directory = .\nsingularity = \"\"\nhost = \"\"\nqueue = \"normal\"\nenvironment = \"\"\nbatch = False\nmount_dirs = \"\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eThere are three useful ways to use funlib.run:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eDirect usage via command line arguments (overwrites config file defaults):\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython run.py -p \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epython train.py\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e -c 5 -g 1 -q normal -s path-to-singularity-image\n\npython run_singularity.py -p \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epython mknet.py\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e -s path-to-singularity-image\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eIndirect call via another script:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003efunlib\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003erun\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erun\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erun_singularity\u003c/span\u003e\n\n\u003cspan class=\"pl-en\"\u003erun\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ecommand\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"python train.py\"\u003c/span\u003e,\n    \u003cspan class=\"pl-s1\"\u003enum_cpus\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e5\u003c/span\u003e,\n    \u003cspan class=\"pl-s1\"\u003enum_gpus\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n    \u003cspan class=\"pl-s1\"\u003equeue\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"normal\"\u003c/span\u003e,\n    \u003cspan class=\"pl-s1\"\u003eexecute\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"pl-en\"\u003erun_singularity\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ecommand\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"python mknet.py\"\u003c/span\u003e,\n                \u003cspan class=\"pl-s1\"\u003esingularity_image\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"path_to_image\"\u003c/span\u003e,\n                \u003cspan class=\"pl-s1\"\u003eexecute\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eCommand creation and subsequent call:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003efunlib\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003erun\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erun\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erun_singularity\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esubprocess\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003echeck_call\u003c/span\u003e\n\n\u003cspan class=\"pl-s1\"\u003erun_command\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erun\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ecommand\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"python train.py\"\u003c/span\u003e,\n                  \u003cspan class=\"pl-s1\"\u003enum_cpus\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e5\u003c/span\u003e,\n                  \u003cspan class=\"pl-s1\"\u003enum_gpus\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n                  \u003cspan class=\"pl-s1\"\u003equeue\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"normal\"\u003c/span\u003e,\n                  \u003cspan class=\"pl-s1\"\u003eexecute\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\n\n\u003cspan class=\"pl-en\"\u003echeck_call\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003erun_command\u003c/span\u003e,\n           \u003cspan class=\"pl-s1\"\u003eshell\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e)\n\n\u003cspan class=\"pl-s1\"\u003erun_singularity_command\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erun_singularity\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ecommand\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"python mknet.py\"\u003c/span\u003e,\n                                          \u003cspan class=\"pl-s1\"\u003esingularity_image\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"path_to_image\"\u003c/span\u003e,\n                                          \u003cspan class=\"pl-s1\"\u003eexecute\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\n\n\u003cspan class=\"pl-en\"\u003echeck_call\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003erun_singularity_command\u003c/span\u003e,\n           \u003cspan class=\"pl-s1\"\u003eshell\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage-with-daisy\" class=\"anchor\" href=\"#usage-with-daisy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage with Daisy\u003c/h2\u003e\n\u003cp\u003eWhen used with daisy.call do not expand the cmd to a string via setting expand=False:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s1\"\u003ecmd\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erun\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ecommand\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ebase_command\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003equeue\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003equeue\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003enum_gpus\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003enum_cpus\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003enum_cpus\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003esingularity_image\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003esingularity_container\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003emount_dirs\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003emount_dirs\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003eexecute\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e,\n          \u003cspan class=\"pl-s1\"\u003eexpand\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\n\n\u003cspan class=\"pl-s1\"\u003edaisy\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003ecall\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ecmd\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003elog_out\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003elog_out\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003elog_err\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003elog_err\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1594370552.0
  },
  {
    "data_format": 2,
    "description": "demo pipeline for testing different data chunking methods for MuTect2",
    "filenames": [
      "containers/variant-calling-0.0.2/Singularity.variant-calling-0.0.2",
      "containers/annovar-150617/Singularity.annovar-150617"
    ],
    "full_name": "stevekm/MuTect2_target_chunking",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mutect2-target-chunking\" class=\"anchor\" href=\"#mutect2-target-chunking\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMuTect2 Target Chunking\u003c/h1\u003e\n\u003cp\u003eDemo pipeline for testing different data chunking methods for MuTect2.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://software.broadinstitute.org/gatk/documentation/tooldocs/3.8-0/org_broadinstitute_gatk_tools_walkers_cancer_m2_MuTect2.php\" rel=\"nofollow\"\u003eMuTect2\u003c/a\u003e is a common tool used for variant calling of tumor-normal pairs. However, it is limited to running only in single-threaded mode, which can lead to extremely long execution times.\u003c/p\u003e\n\u003cp\u003eThis demo pipeline uses different techniques to chunk the included list of target regions (\u003ccode\u003etargets.bed\u003c/code\u003e) into smaller segments to run in parallel, then aggregate all results for comparison to ensure that variant calls are the same across all chunking methods.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003eThis pipeline comes pre-configured for usage on NYULMC\u0027s Big Purple HPC cluster using pre-built Singularity containers and pre-downloaded reference files.\u003c/p\u003e\n\u003cp\u003eIn order to use this pipeline on your system you will need to update the file paths saved in \u003ccode\u003enextflow.config\u003c/code\u003e for your system.\u003c/p\u003e\n\u003cp\u003eSingularity and Docker container recipes are included in the \u003ccode\u003econtainers\u003c/code\u003e directory.\u003c/p\u003e\n\u003cp\u003ePaths to input .bam files for tumor and normal samples are read from the file \u003ccode\u003esamples.analysis.tsv\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOnce correctly configured, the pipeline can be run with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [
      "nextflow",
      "mutect2",
      "variant-calling"
    ],
    "updated_at": 1562090008.0
  },
  {
    "data_format": 2,
    "description": "Example of deployment of a Galaxy Production Instance using CVMFS with Ansible",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MiguelJulia/GCC2019_GalaxyAnsibleDeplyoment_CVMFS",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-gcc2019_galaxyansibledeplyoment_cvmfs\" class=\"anchor\" href=\"#gcc2019_galaxyansibledeplyoment_cvmfs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGCC2019_GalaxyAnsibleDeplyoment_CVMFS\u003c/h1\u003e\n\u003cp\u003eExample of deployment of a Galaxy Production Instance using CVMFS with Ansible.\nFor more info, look into \u003ca href=\"https://galaxyproject.github.io/training-material/topics/admin/\" rel=\"nofollow\"\u003egalaxy admin training materials\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-deploying-a-galaxy-stance\" class=\"anchor\" href=\"#deploying-a-galaxy-stance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploying a galaxy stance\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003eansible-playbook -i host cvmfs_playbook.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-restart-galaxy\" class=\"anchor\" href=\"#restart-galaxy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRestart galaxy\u003c/h4\u003e\n\u003cpre\u003e\u003ccode\u003esudo su - galaxy\nsupervisorctl restart galaxy\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-variables-to-modify-for-quick-deployment\" class=\"anchor\" href=\"#variables-to-modify-for-quick-deployment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVariables to modify for quick deployment\u003c/h4\u003e\n\u003cp\u003eAdmin user name. This user is not created, still has to be registered the first time and it will automatically get admin permissions:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egalaxy_config:\n  galaxy:\n    admin_users: admin@example.com\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBrand: Whatever appears on the banner\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egalaxy_config:\n  galaxy:\n    brand: \"Freiburg GCC\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-welcomehtml\" class=\"anchor\" href=\"#welcomehtml\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ewelcome.html\u003c/h4\u003e\n\u003cp\u003eFrontpage is not created by default. You can find the template inside \u003ccode\u003egalaxy_root: /srv/galaxy\u003c/code\u003e, in \u003ccode\u003eserver/static/welcome.html.sample\u003c/code\u003e. Just create a \u003ccode\u003ewelcome.html\u003c/code\u003e page from this template in that same location and restart galaxy.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-deploying-your-ansible-managed-galaxy-into-a-container-not-working-yet\" class=\"anchor\" href=\"#deploying-your-ansible-managed-galaxy-into-a-container-not-working-yet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploying your ansible-managed galaxy into a container (not working yet!)\u003c/h4\u003e\n\u003cp\u003eWe will use \u003ca href=\"https://github.com/ansible-community/ansible-bender\"\u003eansible-bender\u003c/a\u003e for this task. Your playbook will have to be adapted to this plugging standars as described in their documentation, or compare the differences between my cvmfs_playbook.yml and ansible-bender-test.yml to have a quick idea of how it has to be done.\u003c/p\u003e\n\u003cp\u003eMake sure you are running the right version of ansible, as ansible-bender only works with python3. Still, playbooks designed for python2 can still be used. You will also need to install \u003ca href=\"https://github.com/containers/buildah/blob/master/install.md\"\u003ebuildah\u003c/a\u003e and \u003ca href=\"https://github.com/containers/libpod/blob/master/install.md\"\u003epodman\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFinally, you will need to configurate podman repo config file \u003ccode\u003e/etc/containers/registries.conf\u003c/code\u003e to tell it where to look for your containers. For example, to search in dokerhub add \u003ccode\u003e\u0027docker.io\u0027\u003c/code\u003e inside\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[registries.search]\nregistries = [\u0027docker.io\u0027]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe image is required to have python interpreter build in.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-building-galaxy-container-with-docker-idea---not-testet-yet\" class=\"anchor\" href=\"#building-galaxy-container-with-docker-idea---not-testet-yet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding galaxy container with Docker (idea - not testet yet)\u003c/h4\u003e\n\u003cp\u003eUse galaxy-container \u003ca href=\"https://github.com/bgruening/docker-galaxy-stable/blob/master/galaxy/Dockerfile\"\u003eDockerfile\u003c/a\u003e as template.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1562583598.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.openrefine"
    ],
    "full_name": "ternaustralia/coesra-singularity-openrefine",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-openrefine\" class=\"anchor\" href=\"#coesra-singularity-openrefine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-openrefine\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610426463.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.macroecodesktop"
    ],
    "full_name": "ternaustralia/coesra-singularity-macroecodesktop",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-openrefine\" class=\"anchor\" href=\"#coesra-singularity-openrefine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-openrefine\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610426323.0
  },
  {
    "data_format": 2,
    "description": "Knime",
    "filenames": [
      "Singularity.knime"
    ],
    "full_name": "ternaustralia/coesra-singularity-knime",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-knime\" class=\"anchor\" href=\"#coesra-singularity-knime\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-knime\u003c/h1\u003e\n\u003cp\u003eAuthor: Hoang Nguyen\nCreated: 22 July 2019\nThis will create a image with Singularity 2.5.1\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610426074.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.dropbox"
    ],
    "full_name": "ternaustralia/coesra-singularity-dropbox",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-dropbox\" class=\"anchor\" href=\"#coesra-singularity-dropbox\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-dropbox\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1610425054.0
  },
  {
    "data_format": 2,
    "description": "Owncloud",
    "filenames": [
      "Singularity.owncloud"
    ],
    "full_name": "ternaustralia/coesra-singularity-owncloud",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-owncloud\" class=\"anchor\" href=\"#coesra-singularity-owncloud\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-owncloud\u003c/h1\u003e\n\u003cp\u003eAuthor: Hoang Nguyen\nCreated: 22 July 2019\nThis will create a image with Singularity 2.5.1\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610426521.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.canopy"
    ],
    "full_name": "ternaustralia/coesra-singularity-canopy",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-canopy\" class=\"anchor\" href=\"#coesra-singularity-canopy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-canopy\u003c/h1\u003e\n\u003cp\u003eAuthor: Hoang Nguyen\nCreated: 22 July 2019\nThis will create a image with Singularity 2.5.1\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610425023.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.qgis"
    ],
    "full_name": "ternaustralia/coesra-singularity-qgis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-qgis\" class=\"anchor\" href=\"#coesra-singularity-qgis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-qgis\u003c/h1\u003e\n\u003cp\u003eHoang Nguyen 24 July 2019\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1610427940.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.jupyter"
    ],
    "full_name": "ternaustralia/coesra-singularity-jupyter",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-jupyter\" class=\"anchor\" href=\"#coesra-singularity-jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-jupyter\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1610425229.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.kepler"
    ],
    "full_name": "ternaustralia/coesra-singularity-kepler",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-kepler\" class=\"anchor\" href=\"#coesra-singularity-kepler\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-kepler\u003c/h1\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610425796.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.rstudio"
    ],
    "full_name": "ternaustralia/coesra-singularity-rstudio",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-rstudio\" class=\"anchor\" href=\"#coesra-singularity-rstudio\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-rstudio\u003c/h1\u003e\n\u003cp\u003eHoang Nguyen\n25 July 2019\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610424737.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.panoply"
    ],
    "full_name": "ternaustralia/coesra-singularity-panoply",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-panoply\" class=\"anchor\" href=\"#coesra-singularity-panoply\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-panoply\u003c/h1\u003e\n\u003cp\u003eHoang Nguyen\n25 July 2019\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [
      "coesra"
    ],
    "updated_at": 1610426866.0
  },
  {
    "data_format": 2,
    "description": "Repo for recipes to put on singularity hub",
    "filenames": [
      "Singularity.xenial",
      "Singularity.dbspype"
    ],
    "full_name": "hbraunDSP/containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainers\u003c/h1\u003e\n\u003cp\u003ePRIVATE repo for recipes to put on singularity hub.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1567609710.0
  },
  {
    "data_format": 2,
    "description": "Kraken 2 is the newest version of Kraken, a taxonomic classification system using exact k-mer matches to achieve high accuracy and fast classification speeds.",
    "filenames": [
      "2.1.2/Singularity"
    ],
    "full_name": "pscedu/singularity-kraken2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainers\u003c/h1\u003e\n\u003cp\u003ePRIVATE repo for recipes to put on singularity hub.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626988968.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ddbj/singularity_guacamole_mysql",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity_guacamole_mysql\" class=\"anchor\" href=\"#singularity_guacamole_mysql\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity_guacamole_mysql\u003c/h1\u003e\n\u003cp\u003eRemote Desktop \u3084 VNC \u306e\u63a5\u7d9a\u3092 HTTP \u306b\u5909\u63db\u3057\u3066 HTML5 \u30a6\u30a7\u30d6\u30d6\u30e9\u30a6\u30b6\u3067\u8868\u793a\u3059\u308b Apache Guacamole \u3092 singularity instance \u3067\u5b9f\u884c\u3059\u308b\u305f\u3081\u306e\u30ec\u30b7\u30d4\u30d5\u30a1\u30a4\u30eb\u30fb\u521d\u671f\u5316\u30b9\u30af\u30ea\u30d7\u30c8\u3067\u3059\u3002\u30e6\u30fc\u30b6\u30fc\u8a8d\u8a3c\u306bMySQL\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-image-\u306e\u30d3\u30eb\u30c9\" class=\"anchor\" href=\"#singularity-image-%E3%81%AE%E3%83%93%E3%83%AB%E3%83%89\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity image \u306e\u30d3\u30eb\u30c9\u003c/h2\u003e\n\u003cp\u003e\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067 singularity image \u3092\u30d3\u30eb\u30c9\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build guacamole.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u521d\u671f\u8a2d\u5b9a\" class=\"anchor\" href=\"#%E5%88%9D%E6%9C%9F%E8%A8%AD%E5%AE%9A\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u521d\u671f\u8a2d\u5b9a\u003c/h2\u003e\n\u003cp\u003e\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067 singularity isntance \u8d77\u52d5\u306e\u305f\u3081\u306e\u521d\u671f\u8a2d\u5b9a\u3092\u884c\u3044\u307e\u3059\u3002\u003c/p\u003e\n\u003cp\u003e\u5b9f\u884c\u524d\u306b init.sh \u5185\u306e MYSQL_ROOT_PASSWD, MYSQL_GUACAMOLE_USER_PASSWD, MYSQL_PORT, GUACAMOLE_PORT, TOMCAT_SHUTDOWN_PORT, TOMCAT_PORT \u306e\u5024\u3092\u9069\u5b9c\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003cp\u003e\"Enter current password for root (enter for none):\" \u3068\u8868\u793a\u3055\u308c\u305f\u3068\u3053\u308d\u3067\u51e6\u7406\u304c\u30a4\u30f3\u30bf\u30e9\u30af\u30c6\u30a3\u30d6\u306b\u306a\u308a\u307e\u3059\u3002\u3053\u3053\u3067\u306f\u30ea\u30bf\u30fc\u30f3\u30ad\u30fc\u3092\u62bc\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003cp\u003e\u6b21\u306b\u3001\"Set root password? [Y/n]\" \u3068\u8868\u793a\u3055\u308c\u305f\u3068\u3053\u308d\u3067Y\u3092\u5165\u529b\u3057\u3001MySQL\u306eroot\u30e6\u30fc\u30b6\u30fc\u306e\u30d1\u30b9\u30ef\u30fc\u30c9\u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002\u3053\u3053\u3067\u3001init.sh\u306eMYSQL_ROOT_PASSWD\u306b\u8a2d\u5b9a\u3057\u305f\u5024\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u4ee5\u964d\u306f\u3059\u3079\u3066Y\u3092\u5165\u529b\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003cp\u003e\u51e6\u7406\u304c\u5b8c\u4e86\u3059\u308b\u3068\u3001data\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u3068start_container.sh\u30d5\u30a1\u30a4\u30eb\u304c\u751f\u6210\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ bash init.sh\nexec init_mysql.sh\nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n\tLANGUAGE = \"ja_JP\",\n\tLC_ALL = (unset),\n\tLANG = \"ja_JP.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to the standard locale (\"C\").\nWARNING: Could not write to config file ./my.cnf: Read-only file system\n\nInstalling MySQL system tables...2021-03-17 18:46:46 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).\n2021-03-17 18:46:46 0 [Note] Ignoring --secure-file-priv value as server is running with --bootstrap.\n2021-03-17 18:46:46 0 [Note] ./bin/mysqld (mysqld 5.6.51) starting as process 18851 ...\n2021-03-17 18:46:46 18851 [Note] InnoDB: Using atomics to ref count buffer pool pages\n2021-03-17 18:46:46 18851 [Note] InnoDB: The InnoDB memory heap is disabled\n2021-03-17 18:46:46 18851 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins\n2021-03-17 18:46:46 18851 [Note] InnoDB: Memory barrier is not used\n2021-03-17 18:46:46 18851 [Note] InnoDB: Compressed tables use zlib 1.2.11\n2021-03-17 18:46:46 18851 [Note] InnoDB: Using CPU crc32 instructions\n2021-03-17 18:46:46 18851 [Note] InnoDB: Initializing buffer pool, size = 128.0M\n2021-03-17 18:46:46 18851 [Note] InnoDB: Completed initialization of buffer pool\n2021-03-17 18:46:46 18851 [Note] InnoDB: The first specified data file ./ibdata1 did not exist: a new database to be created!\n2021-03-17 18:46:46 18851 [Note] InnoDB: Setting file ./ibdata1 size to 12 MB\n2021-03-17 18:46:46 18851 [Note] InnoDB: Database physically writes the file full: wait...\n2021-03-17 18:46:46 18851 [Note] InnoDB: Setting log file ./ib_logfile101 size to 48 MB\n2021-03-17 18:46:46 18851 [Note] InnoDB: Setting log file ./ib_logfile1 size to 48 MB\n2021-03-17 18:46:47 18851 [Note] InnoDB: Renaming log file ./ib_logfile101 to ./ib_logfile0\n2021-03-17 18:46:47 18851 [Warning] InnoDB: New log files created, LSN=45781\n2021-03-17 18:46:47 18851 [Note] InnoDB: Doublewrite buffer not found: creating new\n2021-03-17 18:46:47 18851 [Note] InnoDB: Doublewrite buffer created\n2021-03-17 18:46:47 18851 [Note] InnoDB: 128 rollback segment(s) are active.\n2021-03-17 18:46:47 18851 [Warning] InnoDB: Creating foreign key constraint system tables.\n2021-03-17 18:46:47 18851 [Note] InnoDB: Foreign key constraint system tables created\n2021-03-17 18:46:47 18851 [Note] InnoDB: Creating tablespace and datafile system tables.\n2021-03-17 18:46:47 18851 [Note] InnoDB: Tablespace and datafile system tables created.\n2021-03-17 18:46:47 18851 [Note] InnoDB: Waiting for purge to start\n2021-03-17 18:46:47 18851 [Note] InnoDB: 5.6.51 started; log sequence number 0\n2021-03-17 18:46:47 18851 [Note] RSA private key file not found: /usr/local/mysql/data//private_key.pem. Some authentication plugins will not work.\n2021-03-17 18:46:47 18851 [Note] RSA public key file not found: /usr/local/mysql/data//public_key.pem. Some authentication plugins will not work.\n2021-03-17 18:46:53 18851 [Note] Binlog end\n2021-03-17 18:46:53 18851 [Note] InnoDB: FTS optimize thread exiting.\n2021-03-17 18:46:53 18851 [Note] InnoDB: Starting shutdown...\n2021-03-17 18:46:54 18851 [Note] InnoDB: Shutdown completed; log sequence number 1625977\nOK\n\nFilling help tables...2021-03-17 18:46:54 0 [Warning] TIMESTAMP with implicit DEFAULT value is deprecated. Please use --explicit_defaults_for_timestamp server option (see documentation for more details).\n2021-03-17 18:46:54 0 [Note] Ignoring --secure-file-priv value as server is running with --bootstrap.\n2021-03-17 18:46:54 0 [Note] ./bin/mysqld (mysqld 5.6.51) starting as process 18875 ...\n2021-03-17 18:46:54 18875 [Note] InnoDB: Using atomics to ref count buffer pool pages\n2021-03-17 18:46:54 18875 [Note] InnoDB: The InnoDB memory heap is disabled\n2021-03-17 18:46:54 18875 [Note] InnoDB: Mutexes and rw_locks use GCC atomic builtins\n2021-03-17 18:46:54 18875 [Note] InnoDB: Memory barrier is not used\n2021-03-17 18:46:54 18875 [Note] InnoDB: Compressed tables use zlib 1.2.11\n2021-03-17 18:46:54 18875 [Note] InnoDB: Using CPU crc32 instructions\n2021-03-17 18:46:54 18875 [Note] InnoDB: Initializing buffer pool, size = 128.0M\n2021-03-17 18:46:54 18875 [Note] InnoDB: Completed initialization of buffer pool\n2021-03-17 18:46:54 18875 [Note] InnoDB: Highest supported file format is Barracuda.\n2021-03-17 18:46:54 18875 [Note] InnoDB: 128 rollback segment(s) are active.\n2021-03-17 18:46:54 18875 [Note] InnoDB: Waiting for purge to start\n2021-03-17 18:46:55 18875 [Note] InnoDB: 5.6.51 started; log sequence number 1625977\n2021-03-17 18:46:55 18875 [Note] RSA private key file not found: /usr/local/mysql/data//private_key.pem. Some authentication plugins will not work.\n2021-03-17 18:46:55 18875 [Note] RSA public key file not found: /usr/local/mysql/data//public_key.pem. Some authentication plugins will not work.\n2021-03-17 18:46:55 18875 [Note] Binlog end\n2021-03-17 18:46:55 18875 [Note] InnoDB: FTS optimize thread exiting.\n2021-03-17 18:46:55 18875 [Note] InnoDB: Starting shutdown...\n2021-03-17 18:46:56 18875 [Note] InnoDB: Shutdown completed; log sequence number 1625987\nOK\n\nTo start mysqld at boot time you have to copy\nsupport-files/mysql.server to the right place for your system\n\nPLEASE REMEMBER TO SET A PASSWORD FOR THE MySQL root USER !\nTo do so, start the server, then issue the following commands:\n\n  ./bin/mysqladmin -u root password \u0027new-password\u0027\n  ./bin/mysqladmin -u root -h dbod04 password \u0027new-password\u0027\n\nAlternatively you can run:\n\n  ./bin/mysql_secure_installation\n\nwhich will also give you the option of removing the test\ndatabases and anonymous user created by default.  This is\nstrongly recommended for production servers.\n\nSee the manual for more instructions.\n\nYou can start the MySQL daemon with:\n\n  cd . ; ./bin/mysqld_safe \u0026amp;\n\nYou can test the MySQL daemon with mysql-test-run.pl\n\n  cd mysql-test ; perl mysql-test-run.pl\n\nPlease report any problems at http://bugs.mysql.com/\n\nThe latest information about MySQL is available on the web at\n\n  http://www.mysql.com\n\nSupport MySQL by buying support/licenses at http://shop.mysql.com\n\nWARNING: Could not copy config file template ./support-files/my-default.cnf to\n./my.cnf, may not have access rights to do so.\nYou may want to copy the file manually, or create your own,\nit will then be used by default by the server when you start it.\n\nexec mysql_secure_installation\nINFO:    instance started successfully\nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n\tLANGUAGE = \"ja_JP\",\n\tLC_ALL = (unset),\n\tLANG = \"ja_JP.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to the standard locale (\"C\").\n\n\n\nNOTE: RUNNING ALL PARTS OF THIS SCRIPT IS RECOMMENDED FOR ALL MySQL\n      SERVERS IN PRODUCTION USE!  PLEASE READ EACH STEP CAREFULLY!\n\nIn order to log into MySQL to secure it, we\u0027ll need the current\npassword for the root user.  If you\u0027ve just installed MySQL, and\nyou haven\u0027t set the root password yet, the password will be blank,\nso you should just press enter here.\n\nEnter current password for root (enter for none): \nOK, successfully used password, moving on...\n\nSetting the root password ensures that nobody can log into the MySQL\nroot user without the proper authorisation.\n\nSet root password? [Y/n] Y\nNew password: \nRe-enter new password: \nPassword updated successfully!\nReloading privilege tables..\n ... Success!\n\n\nBy default, a MySQL installation has an anonymous user, allowing anyone\nto log into MySQL without having to have a user account created for\nthem.  This is intended only for testing, and to make the installation\ngo a bit smoother.  You should remove them before moving into a\nproduction environment.\n\nRemove anonymous users? [Y/n] Y\n ... Success!\n\nNormally, root should only be allowed to connect from \u0027localhost\u0027.  This\nensures that someone cannot guess at the root password from the network.\n\nDisallow root login remotely? [Y/n] Y\n ... Success!\n\nBy default, MySQL comes with a database named \u0027test\u0027 that anyone can\naccess.  This is also intended only for testing, and should be removed\nbefore moving into a production environment.\n\nRemove test database and access to it? [Y/n] Y\n - Dropping test database...\n ... Success!\n - Removing privileges on test database...\n ... Success!\n\nReloading the privilege tables will ensure that all changes made so far\nwill take effect immediately.\n\nReload privilege tables now? [Y/n] Y\n ... Success!\n\n\n\n\nAll done!  If you\u0027ve completed all of the above steps, your MySQL\ninstallation should now be secure.\n\nThanks for using MySQL!\n\n\nCleaning up...\nsetup guacamole database\nWarning: Using a password on the command line interface can be insecure.\nWarning: Using a password on the command line interface can be insecure.\nWarning: Using a password on the command line interface can be insecure.\nWarning: Using a password on the command line interface can be insecure.\nWarning: Using a password on the command line interface can be insecure.\nINFO:    Stopping guacamole instance of /home/okuda/singularity/ubuntu-18.04-guacamole-1.3.0-mysql/guacamole.sif (PID=18915)\ncreate server.xml\ncreate guacamole_home\nINFO:    instance started successfully\nINFO:    Stopping guacamole instance of /home/okuda/singularity/ubuntu-18.04-guacamole-1.3.0-mysql/guacamole.sif (PID=19214)\ncreate guacamole.properties\ncreate start_container.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-instance-\u306e\u8d77\u52d5\" class=\"anchor\" href=\"#singularity-instance-%E3%81%AE%E8%B5%B7%E5%8B%95\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity instance \u306e\u8d77\u52d5\u003c/h2\u003e\n\u003cp\u003e\u4ee5\u4e0b\u306e\u30b3\u30de\u30f3\u30c9\u3067 singularity instance \u3092\u8d77\u52d5\u3057\u307e\u3059\u3002instance \u306e\u8d77\u52d5\u5f8c\u3001instance \u5185\u3067mysqld, guacd, tomcat\u3000\u304c\u8d77\u52d5\u3055\u308c\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ bash start_container.sh\nINFO:    instance started successfully\nguacd[22]: INFO:\tGuacamole proxy daemon (guacd) version 1.3.0 started\nUsing CATALINA_BASE:   /opt/tomcat\nUsing CATALINA_HOME:   /opt/tomcat\nUsing CATALINA_TMPDIR: /opt/tomcat/temp\nUsing JRE_HOME:        /usr\nUsing CLASSPATH:       /opt/tomcat/bin/bootstrap.jar:/opt/tomcat/bin/tomcat-juli.jar\nUsing CATALINA_OPTS:   \nTomcat started.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-guacamole-\u3078\u306e\u30a2\u30af\u30bb\u30b9\" class=\"anchor\" href=\"#guacamole-%E3%81%B8%E3%81%AE%E3%82%A2%E3%82%AF%E3%82%BB%E3%82%B9\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eguacamole \u3078\u306e\u30a2\u30af\u30bb\u30b9\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"http://localhost\" rel=\"nofollow\"\u003ehttp://localhost\u003c/a\u003e:\u0026lt;TOMCAT_PORT\u306e\u5024\u0026gt;/guacamole \u3092\u30a6\u30a7\u30d6\u30d6\u30e9\u30a6\u30b6\u3067\u958b\u3044\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003cp\u003e\u8d77\u52d5\u76f4\u5f8c\u306e\u30e6\u30fc\u30b6\u30fc\u540d\u3001\u30d1\u30b9\u30ef\u30fc\u30c9\u306f\u3044\u305a\u308c\u3082 guacadmin \u306b\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1627268816.0
  },
  {
    "data_format": 2,
    "description": "A Nextflow pipeline for automatically running QC on Nano runs",
    "filenames": [
      "environments/illumina/Singularity"
    ],
    "full_name": "WalesGenePark/NanoSeqQC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nanoseqqc\" class=\"anchor\" href=\"#nanoseqqc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNanoSeqQC\u003c/h1\u003e\n\u003cp\u003eA Nextflow pipeline for automatically running QC on Nano runs\u003c/p\u003e\n\u003cp\u003eWARNING - UNDER CURRENT DEVELOPMENT AND NOT FULLY FUNCTIONAL\u003c/p\u003e\n\u003cp\u003elarge sections of nextflow coding are based off the excellent ncov2019-artic-nf pipeline \u003ccode\u003econnor-lab/ncov2019-artic-nf\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h4\u003e\n\u003chr\u003e\n\u003cp\u003eThe running of this will automatically take fastq reads from a Nano sequencing read, run FastP read diagnostics and trimming before performing some comparative statistics based on library metadata such as RIN and concentration.\nAdditionally, reads will be run through Kraken2 to confirm species profile (and lack of contamination!)\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick-start\u003c/h4\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-illumina\" class=\"anchor\" href=\"#illumina\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIllumina\u003c/h5\u003e\n\u003cp\u003e\u003ccode\u003enextflow run WalesGenePark/NanoSeqQC --profile singularity,slurm --prefix \"job_output\" --directory /path/to/reads --outdir /path/to/outfile\u003c/code\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h4\u003e\n\u003cp\u003eAn up-to-date version of Nextflow is required because the pipeline is written in DSL2. Following the instructions at \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003ehttps://www.nextflow.io/\u003c/a\u003e to download and install Nextflow should get you a recent-enough version.\u003c/p\u003e\n\u003cp\u003e1: git clone the repository\u003cbr\u003e\n2: chmod +x the two scripts in NanoSeqQC/scripts/\u003cbr\u003e\n3: run the singularity build\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-executor\" class=\"anchor\" href=\"#executor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecutor\u003c/h4\u003e\n\u003cp\u003eBy default, the pipeline runs locally unless specifying \u003ccode\u003e-profile slurm\u003c/code\u003e to send to a SLURM cluster.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-config\" class=\"anchor\" href=\"#config\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfig\u003c/h4\u003e\n\u003cp\u003eCommon config options are set in \u0027conf/base.config\u0027.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1626943697.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "docker/Singularity.snowflake"
    ],
    "full_name": "nuKs/preprocessing",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-useful-utilities-for-single-cell-processing-with-alevin-fry\" class=\"anchor\" href=\"#useful-utilities-for-single-cell-processing-with-alevin-fry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful utilities for single-cell processing with alevin-fry\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/COMBINE-lab/alevin-fry\"\u003eAlevin-fry\u003c/a\u003e is a fast, accurate and memory-frugal tool for preprocessing single-cell and single-nucleus RNA-seq data.  You can read more about alevin-fry in \u003ca href=\"https://www.biorxiv.org/content/10.1101/2021.06.29.450377v1\" rel=\"nofollow\"\u003eits pre-print\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis respoistory contains scripts, functions and utilities that are useful for preparing data for processing with alevin-fry, as well as for reading alevin-fry data into other packages for downstream analysis.\u003c/p\u003e\n\u003cp\u003eThe different utilities are broken down in this repository by the language in which they are written (right now, Python, R and bash).  A brief listing of\nthe available utilities currently in the repository is:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-r-language\" class=\"anchor\" href=\"#r-language\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR language\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e \u2014 A script to build a spliced + intron (splici) ref for indexing and quantification with \u003ccode\u003ealevin-fry\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esplici.R\u003c/code\u003e \u2014 Contains the \u003ccode\u003emake_splici_txome\u003c/code\u003e function, which is the function called by the \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e wrapper script.  If you want to build a splici reference programatically in R code, you can use this function.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecellRangerLikeEmptyDrops.R\u003c/code\u003e \u2014 An implementation of the hybrid UMI count filtering and \u003ca href=\"https://github.com/MarioniLab/DropletUtils\"\u003e\u003ccode\u003eemptyDrops\u003c/code\u003e\u003c/a\u003e used by CellRanger (and subsequently by \u003ca href=\"https://github.com/alexdobin/STAR\"\u003eSTARsolo\u003c/a\u003e). This R implementation is a translation of the implemntation in STARsolo, which itself was reverse-engineered from CellRanger.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eload_fry.R\u003c/code\u003e \u2014 Contains a function to load \u003ccode\u003ealevin-fry\u003c/code\u003e output (including from USA mode quantification) into a \u003ca href=\"https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html\" rel=\"nofollow\"\u003e\u003ccode\u003eSingleCellExperiment\u003c/code\u003e\u003c/a\u003e object.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-language\" class=\"anchor\" href=\"#python-language\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython language\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eload_fry.py\u003c/code\u003e \u2014 Contains a Python function \u003ccode\u003eload_fry\u003c/code\u003e which is intended to load \u003ccode\u003ealevin-fry\u003c/code\u003e output (including from USA mode quantification) into a \u003ca href=\"https://github.com/theislab/scanpy\"\u003e\u003ccode\u003eScanpy\u003c/code\u003e\u003c/a\u003e object.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-bash\" class=\"anchor\" href=\"#bash\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBash\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eget_10x_permit_lists.sh\u003c/code\u003e \u2014 Provides a script to download the 10x chromium v2 or v3 permit lists.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esimpleaf\u003c/code\u003e \u2014 Provides a script to run the entire \u003ccode\u003esalmon -\u0026gt; alevin-fry (generate-permit-list \u0026gt; collate \u0026gt; quant)\u003c/code\u003e pipeline, though providing only a simplified set of options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-simpleaf\" class=\"anchor\" href=\"#using-simpleaf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing simpleaf\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003esimpleaf\u003c/code\u003e script that resides in the \u003ccode\u003ebash\u003c/code\u003e subdirectory is intended to simply the running of \u003ccode\u003ealevin-fry\u003c/code\u003e in common usage scenarios.  By limiting some of the different options that can be set, it provides a streamlined way to build the splici reference and index in a single command, as well as to process an experiment from raw FASTQ files to a count matrix in a single command.\u003c/p\u003e\n\u003cp\u003eTo work properly, \u003ccode\u003esimpleaf\u003c/code\u003e has a few requirements.  First, it should be run from \u003cem\u003ewithin\u003c/em\u003e the \u003ccode\u003ebash\u003c/code\u003e subdirectory of this repository.  This is because it currently makes assumptions about the relative paths of the scripts \u003ccode\u003eget_10x_permit_lists.sh\u003c/code\u003e and \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e.  Additionally, the following environment variables are used within \u003ccode\u003esimpleaf\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eALEVIN_FRY_HOME\u003c/code\u003e \u003cstrong\u003eREQUIRED\u003c/strong\u003e \u2014 This directory will be used for persistent configuration and small file (\u0026lt;1G) storage between runs.  If you provide a directory and it doesn\u0027t exist, it will be created.  It is easiest to just set this in your enviornment globally so that the same home can be used over many runs without you having to provide the variable explicitly each time.  A good choice for this variable might be something like \u003ccode\u003e~/.alevin_fry_home\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSALMON_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ccode\u003esalmon\u003c/code\u003e executable of version \u0026gt;= 1.5.1.  If not provided, the script will assume it can simply invoke \u003ccode\u003esalmon\u003c/code\u003e in the current enviornment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eFRY_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ccode\u003ealevin-fry\u003c/code\u003e executable of version \u0026gt;= 0.4.0.  If not provided, the script will assume it can simply invoke \u003ccode\u003ealevin-fry\u003c/code\u003e in the current enviornment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eTIME_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ca href=\"https://www.gnu.org/software/time/\" rel=\"nofollow\"\u003eGNU time\u003c/a\u003e executable; this is different from the shell \u003ccode\u003etime\u003c/code\u003e command, and on most linux systems exists at \u003ccode\u003e/usr/bin/time\u003c/code\u003e.  If this variable is not provided, the script will assume it can use \u003ccode\u003e/usr/bin/time\u003c/code\u003e.  On OSX systems, you should install GNU time explicitly.  This can be done with \u003ca href=\"https://anaconda.org/conda-forge/time\" rel=\"nofollow\"\u003econda\u003c/a\u003e or homebrew.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ccode\u003esimpleaf\u003c/code\u003e script has two sub-commands:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eindex\u003c/code\u003e \u2014 The \u003ccode\u003eindex\u003c/code\u003e command will take a reference genome FASTA and GTF as input, build a splici reference using the \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e script, and then build a sparse \u003ccode\u003esalmon\u003c/code\u003e index on the resulting reference. \u003cstrong\u003eNote\u003c/strong\u003e: The \u003ccode\u003eindex\u003c/code\u003e command requires the \u003ccode\u003eRscript\u003c/code\u003e executable to be in the path, as well as all of theR packages that are required by \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e. The relevant options (which you can obtain by running \u003ccode\u003e./simpleaf index -h\u003c/code\u003e) are:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{bash}\"\u003e\u003ccode\u003eUsage: ./simpleaf index [options]\n      options:\n       -f, --fasta REQUIRED genome reference FASTA file\n       -g, --gtf REQUIRED GTF file with gene annotations\n       -l, --rlen REQUIRED the target read length the index will be built for\n       -o, --output REQUIRED path to output directory (will be created if it doesn\u0027t exist)\n       -s, --spliced OPTIONAL path to FASTA file with extra spliced sequence to add to the index\n       -u, --unspliced OPTIONAL path to FASTA file with extra unspliced sequence to add to the index\n       -d, --dedup FLAG OPTIONAL deduplicate identical sequences inside the R script when building the splici reference\n       -t, --threads OPTIONAL number of threads to use when running [default: min(16, num cores)]\n       -h, --help display this help message\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003equant\u003c/code\u003e \u2014 The \u003ccode\u003equant\u003c/code\u003e command takes as input the index, reads, and relevant information about the experiment (e.g. chemistry), and runs all of the steps of the \u003ccode\u003ealevin-fry\u003c/code\u003e pipeline, from mapping with \u003ccode\u003esalmon\u003c/code\u003e through \u003ccode\u003equant\u003c/code\u003e with \u003ccode\u003ealevin-fry\u003c/code\u003e. The relevant options (which you can obtain by running \u003ccode\u003e./simpleaf quant -h\u003c/code\u003e) are:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{bash}\"\u003e\u003ccode\u003eUsage: ./simpleaf quant [options]\n      options:\n       -1, --r1 REQUIRED comma separated list of left reads\n       -2, --r2 REQUIRED comma separated list of right reads\n       -i, --index REQUIRED path to a (sparse or dense) salmon splici index\n       -o, --output REQUIRED path to output directory (will be created if it doesn\u0027t exist)\n       -f, --fmode REQUIRED permit list filter mode, one of {knee, k, unfilt, u}\n       -c, --chem REQUIRED chemistry of experiment, one of {v2, v3}\n       -r, --res REQUIRED resolution strategy for alevin-fry, one of {cr-like, cr-like-em}\n       -m, --t2g REQUIRED three-column txp-to-gene file to pass to alevin-fry quant command\n       -t, --threads OPTIONAL number of threads to use when running [default: min(16, num cores)]\n       -h, --help display this help message\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626495005.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "1.3.1/Singularity",
      "1.3.3/Singularity"
    ],
    "full_name": "yh549848/singularity-rsem",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/pscedu/singularity-bamtools/actions/workflows/main.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pscedu/singularity-bamtools/actions/workflows/main.yml/badge.svg\" alt=\"Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/43cc1554b9e51a28dfa82673f6a9629d4b3f4151419378b68631040a1a5f52a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/43cc1554b9e51a28dfa82673f6a9629d4b3f4151419378b68631040a1a5f52a2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"Issue\" data-canonical-src=\"https://img.shields.io/github/issues/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/bb25827e818656bc2a93d95c923b954c29792611568e2828cee62c6501555455/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bb25827e818656bc2a93d95c923b954c29792611568e2828cee62c6501555455/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"forks\" data-canonical-src=\"https://img.shields.io/github/forks/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/30cf5022df93f7c848ab5284b476648b2a424ac87e287144bfdbc9460bb75256/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/30cf5022df93f7c848ab5284b476648b2a424ac87e287144bfdbc9460bb75256/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/72066080adaa298a3f65d9ad3d27681498685739defe4d4061e06d30f8bf5277/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/72066080adaa298a3f65d9ad3d27681498685739defe4d4061e06d30f8bf5277/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f7073636564752f73696e67756c61726974792d62616d746f6f6c73\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/pscedu/singularity-bamtools\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-bamtools\" class=\"anchor\" href=\"#singularity-bamtools\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-bamtools\u003c/h1\u003e\n\u003cp\u003eSingularity recipe for \u003ca href=\"https://github.com/pezmaster31/bamtools\"\u003ebamtools\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-the-container-on-bridges-2\" class=\"anchor\" href=\"#installing-the-container-on-bridges-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling the container on Bridges 2\u003c/h2\u003e\n\u003cp\u003eCopy the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eSIF\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eand the \u003ccode\u003ebamtools\u003c/code\u003e script\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eto \u003ccode\u003e/opt/packages/bamtools/2.5.1\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eCopy the file \u003ccode\u003emodulefile.lua\u003c/code\u003e to \u003ccode\u003e/opt/modulefiles/bamtools\u003c/code\u003e as \u003ccode\u003e2.5.1\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-the-image-using-the-recipe\" class=\"anchor\" href=\"#building-the-image-using-the-recipe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding the image using the recipe\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-locally\" class=\"anchor\" href=\"#to-build-the-image-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image locally\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003ebuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-build-the-image-remotely\" class=\"anchor\" href=\"#to-build-the-image-remotely\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo build the image remotely\u003c/h3\u003e\n\u003cp\u003eRun the script \u003ccode\u003erbuild.sh\u003c/code\u003e to build image locally.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./build.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-to-run-tests\" class=\"anchor\" href=\"#to-run-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo run tests\u003c/h2\u003e\n\u003cp\u003eTo run the available tests, run the command\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebash ./test.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003cp\u003eCopyright \u00a9 2020-2021 Pittsburgh Supercomputing Center. All Rights Reserved.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.psc.edu/biomedical-applications/\" rel=\"nofollow\"\u003eBiomedical Applications Group\u003c/a\u003e at the \u003ca href=\"http://www.psc.edu\" rel=\"nofollow\"\u003ePittsburgh Supercomputing Center\u003c/a\u003e in the \u003ca href=\"https://www.cmu.edu/mcs/\" rel=\"nofollow\"\u003eMellon College of Science\u003c/a\u003e at \u003ca href=\"http://www.cmu.edu\" rel=\"nofollow\"\u003eCarnegie Mellon University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623772549.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Angora (https://github.com/AngoraFuzzer/Angora)",
    "filenames": [
      "Singularity.1804",
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/angora",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003eSingularity image for Angora (\u003ca href=\"https://github.com/AngoraFuzzer/Angora\"\u003ehttps://github.com/AngoraFuzzer/Angora\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/angora/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3645\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eusage:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name angora.sif https://github.com/shub-fuzz/angora/releases/download/0.0.2/shub-fuzz-angora.1604.sif\n\nsingularity shell angora.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003einteractive session:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell angora.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003estart fuzzing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec angora.sif /start_fuzzing [[ -n \u0026lt;# instances\u0026gt; ]  -t ] \u0026lt;target_path\u0026gt; \n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682691.0
  },
  {
    "data_format": 2,
    "description": "Singularity image for Eclipser (https://github.com/SoftSec-KAIST/Eclipser)",
    "filenames": [
      "Singularity.1604"
    ],
    "full_name": "shub-fuzz/eclipser",
    "latest_release": "0.0.2",
    "readme": "\u003cp\u003eSingularity image for Eclipser (\u003ca href=\"https://github.com/SoftSec-KAIST/Eclipser\"\u003ehttps://github.com/SoftSec-KAIST/Eclipser\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml\"\u003e\u003cimg src=\"https://github.com/shub-fuzz/eclipser/actions/workflows/builder.yml/badge.svg?branch=main\" alt=\"singularity-deploy\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhat\u003c/strong\u003e is \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e?\u003cbr\u003e\nA containerization system primarily used by the scientific community on high-performance computing (HPC).\nOn many University HPC systems, docker is not allowed, but singularity is availble because it runs with\nuser level permisions.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWhy\u003c/strong\u003e?\u003cbr\u003e\nFuzzing on HPC!\u003cbr\u003e\nUniversities have trememdous resources available in HPC clusters that can be used to support\nlarge-scale fuzzing evaluations.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eusage:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name eclipser.sif https://github.com/shub-fuzz/eclipser/releases/download/0.0.2/shub-fuzz-eclipser.1604.sif\n\nsingularity shell eclipser.sif\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623682705.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "bc3.10--rstudio125042r362/Singularity",
      "bc3.12--rstudio125042r405/Singularity"
    ],
    "full_name": "yh549848/singularity-rstudio-rnaseqde",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-running-rstudio-server-in-a-conda-environment\" class=\"anchor\" href=\"#running-rstudio-server-in-a-conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server in a Conda Environment\u003c/h1\u003e\n\u003cp\u003eI usually rely on the \u003ca href=\"https://docs.conda.io/en/latest/\" rel=\"nofollow\"\u003econda package manager\u003c/a\u003e to manage my environments during development. Thanks to \u003ca href=\"https://conda-forge.org/\" rel=\"nofollow\"\u003econda-forge\u003c/a\u003e and \u003ca href=\"https://bioconda.github.io/\" rel=\"nofollow\"\u003ebioconda\u003c/a\u003e most R packages are now also available through conda. For production,\nI \u003ca href=\"https://github.com/grst/containerize-conda\"\u003econvert them to containers\u003c/a\u003e as these are easier to share.\u003c/p\u003e\n\u003cp\u003eUnfortunately, there seems to be \u003ca href=\"https://community.rstudio.com/t/start-rstudio-server-session-in-conda-environment/12516/15\" rel=\"nofollow\"\u003eno straightforward way\u003c/a\u003e to use conda envs in Rstudio server. This repository provides three approaches to make rstudio server work with conda envs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#running-rstudio-server-with-singularity\"\u003eRunning Rstudio Server in a Singularity Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-rstudio-server-with-podmandocker\"\u003eRunning Rstudio Server in a Docker/Podman Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-locally\"\u003eRunning Rstudio Server locally\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-rstudio-server-with-singularity\" class=\"anchor\" href=\"#running-rstudio-server-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server with Singularity\u003c/h2\u003e\n\u003cp\u003eWith this approach Rstudio Server runs in a Singularity container (based on \u003ca href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\"\u003erocker/rstudio\u003c/a\u003e).\u003cbr\u003e\nThe conda environment gets mounted into the container - like that there\u0027s no need to rebuild the container to add a package and\n\u003ccode\u003einstall.packages\u003c/code\u003e can be used without issues. The container-based approach has the following benefits:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAuthentication works (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003e#3\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSeveral separate instances of Rstudio server can run in parallel, even without the \u003cem\u003ePro\u003c/em\u003e version.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/quick_start.html\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:grst/rstudio-server-conda.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e rstudio-server-conda/singularity\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eActivate the target conda env or set the environment variable \u003ccode\u003eCONDA_PREFIX\u003c/code\u003e\nto point to the location of the conda env.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the \u003ccode\u003erun_singularity.sh\u003c/code\u003e script. In particular, you may need to add additional bind mounts\n(e.g. a global data directory).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExecute the \u003ccode\u003erun_singularity.sh\u003c/code\u003e script. It will automatically build the container if it is not available.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ePORT=8787 PASSWORD=notsafe ./run_singularity.sh\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLog into Rstudio\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen rstudio server at \u003ccode\u003ehttp://localhost:8787\u003c/code\u003e (or whatever port you specified)\u003c/li\u003e\n\u003cli\u003elogin with your default username and the password you specified via the \u003ccode\u003ePASSWORD\u003c/code\u003e environment variable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-rstudio-server-with-podmandocker\" class=\"anchor\" href=\"#running-rstudio-server-with-podmandocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server with Podman/Docker\u003c/h2\u003e\n\u003cp\u003eThis approach is similar to \u003ca href=\"#running-rstudio-server-with-singularity\"\u003eSingularity\u003c/a\u003e, but uses\nDocker or Podman and a \u003ccode\u003edocker-compose.yml\u003c/code\u003e file instead.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNo access to shared group directories (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/14\"\u003e#14\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e or \u003ca href=\"https://podman.io/\" rel=\"nofollow\"\u003ePodman\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/docker/compose\"\u003edocker-compose\u003c/a\u003e or \u003ca href=\"https://github.com/containers/podman-compose\"\u003epodman-compose\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-1\" class=\"anchor\" href=\"#usage-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:grst/rstudio-server-conda.git\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the rstudio container (fetches the latest version of \u003ca href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\"\u003erocker/rstudio\u003c/a\u003e and adds some custom scripts)\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e rstudio-server-conda/docker\ndocker-compose build     \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or podman-compose\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy the docker-compose.yml file into your project directory and adjust the paths.\u003c/p\u003e\n\u003cp\u003eYou may want to add additional volumes with your data.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s\"\u003e[...]\u003c/span\u003e\n   \u003cspan class=\"pl-ent\"\u003eports\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e port on the host : port in the container (the latter is always 8787)\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e8889:8787\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003evolumes\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e mount conda env into exactely the same path as on the host system - some paths are hardcoded in the env.\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/anaconda3/envs/R400:/home/sturm/anaconda3/envs/R400\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Share settings between rstudio instances\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/.local/share/rstudio/monitored/user-settings:/root/.local/share/rstudio/monitored/user-settings\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e mount the working directory containing your R project.\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/projects:/projects\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003eenvironment\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e password used for authentication\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003ePASSWORD=notsafe\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e repeat the path of the conda environment (must be identical to the path in \"volumes\")\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003eCONDAENV=/home/sturm/anaconda3/envs/R400\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun your project-specific instance of Rstudio-server\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker-compose up \u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLog into Rstudio\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eOpen your server at \u003ccode\u003ehttp://localhost:8889\u003c/code\u003e (or whatever port you specified)\u003c/li\u003e\n\u003cli\u003eLogin with the user \u003ccode\u003erstudio\u003c/code\u003e (when using Docker) or \u003ccode\u003eroot\u003c/code\u003e (when using Podman) and the password you specified\nin the \u003ccode\u003edocker-compose.yml\u003c/code\u003e. If you are using Podman and login with \u003ccode\u003erstudio\u003c/code\u003e you won\u0027t have permissions to\naccess the mounted volumes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-locally\" class=\"anchor\" href=\"#running-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Locally\u003c/h2\u003e\n\u003cp\u003eWith this approach a locally installed Rstudio server is ran such that it uses the conda env.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-known-limitations-1\" class=\"anchor\" href=\"#known-limitations-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eno authentication (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003e#3\u003c/a\u003e). Use this approach only in a secure network!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites-2\" class=\"anchor\" href=\"#prerequisites-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.rstudio.com/products/rstudio/download-server/\" rel=\"nofollow\"\u003erstudio server\u003c/a\u003e installed locally\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-2\" class=\"anchor\" href=\"#usage-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repo\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/grst/rstudio-server-conda.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun rstudio server in the conda env\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd rstudio-server-conda/local\nconda activate my_project\n./start_rstudio_server.sh 8787  # use any free port number here. \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConnect to Rstudio\u003c/p\u003e\n\u003cp\u003eYou should now be able to connect to rstudio server on the port you specify.\n\u003cstrong\u003eIf an R Session has previously been running, you\u0027ll need to rstart the Rsession now\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eObviously, if your env does not have a version of \u003ccode\u003eR\u003c/code\u003e installed, this will either not\nwork at all, or fall back to the system-wide R installation.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-it-works\" class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow it works\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRstudio server, can be started in non-daemonized mode by each user individually on a custom port (similar to a jupyter notebook). This instance can then run in a conda environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; conda activate my_project\n\u0026gt; /usr/lib/rstudio-server/bin/rserver \\\n   --server-daemonize=0 \\\n   --www-port 8787 \\\n   --rsession-which-r=$(which R) \\\n   --rsession-ld-library-path=$CONDA_PREFIX/lib\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo avoid additional problems with library paths, also \u003ccode\u003ersession\u003c/code\u003e needs to run within the conda environment. This is achieved by wrapping \u003ccode\u003ersession\u003c/code\u003e into the \u003ca href=\"https://github.com/grst/rstudio-server-conda/blob/master/local/rsession.sh\"\u003ersession.sh\u003c/a\u003e script. The path to the wrapped \u003ccode\u003ersession\u003c/code\u003e executable can be passed to \u003ccode\u003erserver\u003c/code\u003e as command line argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erserver # ...\n    --rsession-path=rsession.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen using multiple users a unique \u003ccode\u003esecret-cookie-key\u003c/code\u003e has to be generated for each user. The path to the secret cookie key can be passed to \u003ccode\u003erserver\u003c/code\u003e as a command line parameter.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euuid \u0026gt; /tmp/rstudio-server/${USER}_secure-cookie-key\nrserver # ...\n  --secure-cookie-key-file /tmp/rstudio-server/${USER}_secure-cookie-key\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623388496.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "latest--rstudio1.2.5042r362/Singularity"
    ],
    "full_name": "yh549848/singularity-rstudio-methylseq",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-running-rstudio-server-in-a-conda-environment\" class=\"anchor\" href=\"#running-rstudio-server-in-a-conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server in a Conda Environment\u003c/h1\u003e\n\u003cp\u003eI usually rely on the \u003ca href=\"https://docs.conda.io/en/latest/\" rel=\"nofollow\"\u003econda package manager\u003c/a\u003e to manage my environments during development. Thanks to \u003ca href=\"https://conda-forge.org/\" rel=\"nofollow\"\u003econda-forge\u003c/a\u003e and \u003ca href=\"https://bioconda.github.io/\" rel=\"nofollow\"\u003ebioconda\u003c/a\u003e most R packages are now also available through conda. For production,\nI \u003ca href=\"https://github.com/grst/containerize-conda\"\u003econvert them to containers\u003c/a\u003e as these are easier to share.\u003c/p\u003e\n\u003cp\u003eUnfortunately, there seems to be \u003ca href=\"https://community.rstudio.com/t/start-rstudio-server-session-in-conda-environment/12516/15\" rel=\"nofollow\"\u003eno straightforward way\u003c/a\u003e to use conda envs in Rstudio server. This repository provides three approaches to make rstudio server work with conda envs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#running-rstudio-server-with-singularity\"\u003eRunning Rstudio Server in a Singularity Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-rstudio-server-with-podmandocker\"\u003eRunning Rstudio Server in a Docker/Podman Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-locally\"\u003eRunning Rstudio Server locally\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-rstudio-server-with-singularity\" class=\"anchor\" href=\"#running-rstudio-server-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server with Singularity\u003c/h2\u003e\n\u003cp\u003eWith this approach Rstudio Server runs in a Singularity container (based on \u003ca href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\"\u003erocker/rstudio\u003c/a\u003e).\u003cbr\u003e\nThe conda environment gets mounted into the container - like that there\u0027s no need to rebuild the container to add a package and\n\u003ccode\u003einstall.packages\u003c/code\u003e can be used without issues. The container-based approach has the following benefits:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAuthentication works (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003e#3\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSeveral separate instances of Rstudio server can run in parallel, even without the \u003cem\u003ePro\u003c/em\u003e version.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/quick_start.html\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:grst/rstudio-server-conda.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e rstudio-server-conda/singularity\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eActivate the target conda env or set the environment variable \u003ccode\u003eCONDA_PREFIX\u003c/code\u003e\nto point to the location of the conda env.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the \u003ccode\u003erun_singularity.sh\u003c/code\u003e script. In particular, you may need to add additional bind mounts\n(e.g. a global data directory).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExecute the \u003ccode\u003erun_singularity.sh\u003c/code\u003e script. It will automatically build the container if it is not available.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ePORT=8787 PASSWORD=notsafe ./run_singularity.sh\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLog into Rstudio\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen rstudio server at \u003ccode\u003ehttp://localhost:8787\u003c/code\u003e (or whatever port you specified)\u003c/li\u003e\n\u003cli\u003elogin with your default username and the password you specified via the \u003ccode\u003ePASSWORD\u003c/code\u003e environment variable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-rstudio-server-with-podmandocker\" class=\"anchor\" href=\"#running-rstudio-server-with-podmandocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server with Podman/Docker\u003c/h2\u003e\n\u003cp\u003eThis approach is similar to \u003ca href=\"#running-rstudio-server-with-singularity\"\u003eSingularity\u003c/a\u003e, but uses\nDocker or Podman and a \u003ccode\u003edocker-compose.yml\u003c/code\u003e file instead.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNo access to shared group directories (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/14\"\u003e#14\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e or \u003ca href=\"https://podman.io/\" rel=\"nofollow\"\u003ePodman\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/docker/compose\"\u003edocker-compose\u003c/a\u003e or \u003ca href=\"https://github.com/containers/podman-compose\"\u003epodman-compose\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-1\" class=\"anchor\" href=\"#usage-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:grst/rstudio-server-conda.git\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the rstudio container (fetches the latest version of \u003ca href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\"\u003erocker/rstudio\u003c/a\u003e and adds some custom scripts)\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e rstudio-server-conda/docker\ndocker-compose build     \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or podman-compose\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy the docker-compose.yml file into your project directory and adjust the paths.\u003c/p\u003e\n\u003cp\u003eYou may want to add additional volumes with your data.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s\"\u003e[...]\u003c/span\u003e\n   \u003cspan class=\"pl-ent\"\u003eports\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e port on the host : port in the container (the latter is always 8787)\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e8889:8787\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003evolumes\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e mount conda env into exactely the same path as on the host system - some paths are hardcoded in the env.\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/anaconda3/envs/R400:/home/sturm/anaconda3/envs/R400\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Share settings between rstudio instances\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/.local/share/rstudio/monitored/user-settings:/root/.local/share/rstudio/monitored/user-settings\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e mount the working directory containing your R project.\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/projects:/projects\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003eenvironment\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e password used for authentication\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003ePASSWORD=notsafe\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e repeat the path of the conda environment (must be identical to the path in \"volumes\")\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003eCONDAENV=/home/sturm/anaconda3/envs/R400\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun your project-specific instance of Rstudio-server\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker-compose up \u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLog into Rstudio\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eOpen your server at \u003ccode\u003ehttp://localhost:8889\u003c/code\u003e (or whatever port you specified)\u003c/li\u003e\n\u003cli\u003eLogin with the user \u003ccode\u003erstudio\u003c/code\u003e (when using Docker) or \u003ccode\u003eroot\u003c/code\u003e (when using Podman) and the password you specified\nin the \u003ccode\u003edocker-compose.yml\u003c/code\u003e. If you are using Podman and login with \u003ccode\u003erstudio\u003c/code\u003e you won\u0027t have permissions to\naccess the mounted volumes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-locally\" class=\"anchor\" href=\"#running-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Locally\u003c/h2\u003e\n\u003cp\u003eWith this approach a locally installed Rstudio server is ran such that it uses the conda env.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-known-limitations-1\" class=\"anchor\" href=\"#known-limitations-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eno authentication (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003e#3\u003c/a\u003e). Use this approach only in a secure network!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites-2\" class=\"anchor\" href=\"#prerequisites-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.rstudio.com/products/rstudio/download-server/\" rel=\"nofollow\"\u003erstudio server\u003c/a\u003e installed locally\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-2\" class=\"anchor\" href=\"#usage-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repo\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/grst/rstudio-server-conda.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun rstudio server in the conda env\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd rstudio-server-conda/local\nconda activate my_project\n./start_rstudio_server.sh 8787  # use any free port number here. \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConnect to Rstudio\u003c/p\u003e\n\u003cp\u003eYou should now be able to connect to rstudio server on the port you specify.\n\u003cstrong\u003eIf an R Session has previously been running, you\u0027ll need to rstart the Rsession now\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eObviously, if your env does not have a version of \u003ccode\u003eR\u003c/code\u003e installed, this will either not\nwork at all, or fall back to the system-wide R installation.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-it-works\" class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow it works\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRstudio server, can be started in non-daemonized mode by each user individually on a custom port (similar to a jupyter notebook). This instance can then run in a conda environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; conda activate my_project\n\u0026gt; /usr/lib/rstudio-server/bin/rserver \\\n   --server-daemonize=0 \\\n   --www-port 8787 \\\n   --rsession-which-r=$(which R) \\\n   --rsession-ld-library-path=$CONDA_PREFIX/lib\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo avoid additional problems with library paths, also \u003ccode\u003ersession\u003c/code\u003e needs to run within the conda environment. This is achieved by wrapping \u003ccode\u003ersession\u003c/code\u003e into the \u003ca href=\"https://github.com/grst/rstudio-server-conda/blob/master/local/rsession.sh\"\u003ersession.sh\u003c/a\u003e script. The path to the wrapped \u003ccode\u003ersession\u003c/code\u003e executable can be passed to \u003ccode\u003erserver\u003c/code\u003e as command line argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erserver # ...\n    --rsession-path=rsession.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen using multiple users a unique \u003ccode\u003esecret-cookie-key\u003c/code\u003e has to be generated for each user. The path to the secret cookie key can be passed to \u003ccode\u003erserver\u003c/code\u003e as a command line parameter.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euuid \u0026gt; /tmp/rstudio-server/${USER}_secure-cookie-key\nrserver # ...\n  --secure-cookie-key-file /tmp/rstudio-server/${USER}_secure-cookie-key\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 0,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623386962.0
  },
  {
    "data_format": 2,
    "description": "To build hpc benchmark and mpi with cuda support sif",
    "filenames": [
      "hpcc_intel.def",
      "hpl_intel_cuda.def",
      "hpc_mpi_cuda.def",
      "bert.def"
    ],
    "full_name": "perambluate/singularity-definition-files-for-HPC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc_mpi_cuda_singu_def_file\" class=\"anchor\" href=\"#hpc_mpi_cuda_singu_def_file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehpc_mpi_cuda_singu_def_file\u003c/h1\u003e\n\u003cp\u003eA collect of definition files to build images for singularity containers, which includes hpc benchmarks and mpis with cuda support.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4181\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1588998487.0
  },
  {
    "data_format": 2,
    "description": "Deep phenotyping dashboard",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "dptools/dpdash",
    "latest_release": null,
    "readme": "\u003cp\u003eRead the documentation \u003ca href=\"http://docs.neuroinfo.org/dpdash/en/latest/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e!\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1622821974.0
  },
  {
    "data_format": 2,
    "description": "Container files for sc-benchmark in Docker and Singularity with Nix",
    "filenames": [
      "Singularity"
    ],
    "full_name": "XSEDE/nix-container-sc-benchmark",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nix-container-sc-benchmark\" class=\"anchor\" href=\"#nix-container-sc-benchmark\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enix-container-sc-benchmark\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/5358\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity container with Nix to be used in XSEDE compute environment (currently in development)\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 11,
    "topics": [],
    "updated_at": 1623176988.0
  },
  {
    "data_format": 2,
    "description": "A container for running the perfzero benchmark in an XSEDE environment",
    "filenames": [
      "Singularity"
    ],
    "full_name": "XSEDE/nix-container-perfzero",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nix-container-perfzero\" class=\"anchor\" href=\"#nix-container-perfzero\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enix-container-perfzero\u003c/h1\u003e\n\n\u003cp\u003eSingularity container with Nix to be used in XSEDE compute environment (\u003cstrong\u003ecurrently in development\u003c/strong\u003e)\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 12,
    "topics": [],
    "updated_at": 1623176998.0
  },
  {
    "data_format": 2,
    "description": "Simulating, Reconstructing and Analysing Data for FEL IDI Experiments",
    "filenames": [
      "Singularity",
      "Singularity.simple",
      "Singularity.py38"
    ],
    "full_name": "fzimmermann89/idi",
    "latest_release": "210609",
    "readme": "\u003cp\u003eCAVE: Hic sunt dracones\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eThe code is a mess, undocumented and only certain code paths are tested.\u003c/em\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-idi---incoherent-diffraction-imaging\" class=\"anchor\" href=\"#idi---incoherent-diffraction-imaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIDI - INCOHERENT DIFFRACTION IMAGING\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4824\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/fzimmermann89/idi/actions/workflows/test.yml/badge.svg\" alt=\"tests\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1aa13bc475e383774716a28c54db51e680a438815882cb99e8443eb94a873db/68747470733a2f2f7777772e7472617669732d63692e636f6d2f667a696d6d65726d616e6e38392f6964692e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://www.travis-ci.com/fzimmermann89/idi.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity Image now at \u003ca href=\"https://cloud.sylabs.io/library/_container/607b669a4ad4aa1fdea0c43c\" rel=\"nofollow\"\u003elibrary://fzimmermann89/idi/idi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eConda Pacakges at \u003ca href=\"https://anaconda.org/zimmf/idi\" rel=\"nofollow\"\u003ezimmf/idi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePIP Source at \u003ca href=\"https://pypi.org/project/idi/\" rel=\"nofollow\"\u003eidi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWheels at \u003ca href=\"https://github.com/fzimmermann89/idi/releases/latest\"\u003eReleases\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-content-of-the-repo\" class=\"anchor\" href=\"#content-of-the-repo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtent of the repo\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eipynb: example notebooks\u003c/li\u003e\n\u003cli\u003esimulation: simulation of incoherent images\u003c/li\u003e\n\u003cli\u003ereconstruction: direct and ft based reconstruction\u003c/li\u003e\n\u003cli\u003eutil: some small utilities for data analysis, geometry and random distributions, etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-preparation-for-slac-sdf\" class=\"anchor\" href=\"#preparation-for-slac-sdf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epreparation for slac sdf:\u003c/h2\u003e\n\u003cp\u003eUse Singulariy, if using OOD launcher, use the following to start a jupyterhub\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    function jupyter() { singularity run --app jupyter --nv -B /sdf,/gpfs,/scratch,/lscratch library://fzimmermann89/idi/idi $@; }\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-preparation-for-sacla\" class=\"anchor\" href=\"#preparation-for-sacla\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epreparation for sacla:\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDownload and install miniconda, setup ssh tunnel for web access.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda create -n local3 python=3.7 numpy mkl mkl-dev ipython ipykernel cython jinja2 numba numexpr matplotlib six scipy jupyterlab\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003econda activate local3\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epip install https://github.com/fzimmermann89/idi/\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003epython -m ipykernel install --user --name local-simulation-env3 --display-name \"local simulation(py37)\"\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(C) Felix Zimmermann\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [
      "idi",
      "reconstruction",
      "simulation",
      "xray",
      "incoherent-images",
      "fel"
    ],
    "updated_at": 1626970680.0
  },
  {
    "data_format": 2,
    "description": "The source code of POPCORN planner (also a MILP Compilation collaborated with Chiara Piacentini)",
    "filenames": [
      "Singularity"
    ],
    "full_name": "Emresav/popcorn",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-popcorn\" class=\"anchor\" href=\"#popcorn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epopcorn\u003c/h1\u003e\n\u003cp\u003ePOPCORN is a temporal-numeric planning software that I have developed during my PhD studies at King\u0027s College London under supervision of Prof Maria Fox and Prof Derek Long. POPCORN can reason about action-specific numeric parameters that can take their values from relatively large domains. The planner has a freedom of choosing their values during planning process. Practically speaking, this means that you can now model actions that can have multiple flexible numeric parameters, such as the withdrawal amount from the cashpoint (e.g. \u003ccode\u003e10\u0026lt;= ?cash \u0026lt;= 100\u003c/code\u003e), or the refuel amount, as in PDDL community these parameters can only be defined with fixed values. The language we use when developing our domains, the extended version of PDDL, is quite straightforward; so I highly recommend users to inspect them to decide whether this is something that you want in your planning scenarios. This work was published on ECAI 2016 under the title of \u003ca href=\"https://kclpure.kcl.ac.uk/portal/files/56331945/FAIA285_1185.pdf\" rel=\"nofollow\"\u003ePlanning Using Actions with Control Parameters\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAlso note that this code base includes a MILP Compilation of optimal numeric planning problems with control parameters. I collaborated with \u003cem\u003eChiara Piacentini\u003c/em\u003e on this work.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h1\u003e\n\u003cp\u003ePOPCORN makes use of various other tools that are available online, these are:\u003c/p\u003e\n\u003cp\u003eFor parsing the PDDL domain: Flex, Bison\nOptimisation tools: CLP, CoinUtils, CBC, CBLAS, CGL, CPLEX\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eapt-get update\napt-get -y install g++ make flex bison cmake doxygen coinor-clp coinor-libcbc-dev coinor-libclp-dev coinor-libcoinutils-dev coinor-libosi-dev coinor-libcgl-dev libbz2-dev libgsl-dev libz-dev\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e: You will need to install CPLEX from their website!\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-popcorn\" class=\"anchor\" href=\"#running-popcorn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning POPCORN\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository to your local machine:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://github.com/Emresav/popcorn.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eExport the directories of CPLEX libraries:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eexport CPLEX=\"/my/location/to/libcplex.a\"\nexport ILOCPLEX=\"/my/location/to/libilocplex.a\"\nexport CONCERT=\"/my/location/to/libconcert.a\"\nexport CPLEX_INCLUDES=\"/my/location/to/cplex/include\"\nexport CONCERT_INCLUDES=\"/my/location/to/concert/include\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eCompiling the source code:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003ecd /planner/planner\nls -la\ncmake .\nmake clean\nmake popf3-clp\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003e\n\u003cp\u003eFeatures of POPCORN\nIn order to see the available features of POPCORN, simply run the executable: \u003ccode\u003e/planner/planner/popf/popf3-clp\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRunning POPCORN:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e/planner/planner/popf/popf3-clp $DOMAINFILE $PROBLEMFILE \u0026gt; $PLANFILE\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat\u0027s it! I have included various domains and problem instances that are used during my experiments. Almost all of them are new and quite rich in terms of temporal and numeric features. Have fun, and please do not hesitate to contact me if you have any issues during compiling and running the planner.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1595445458.0
  },
  {
    "data_format": 2,
    "description": "Neurotransmitter prediction from EM.",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "funkelab/synister",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-package-for-training-validating-and-testing-synister-networks\" class=\"anchor\" href=\"#package-for-training-validating-and-testing-synister-networks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePackage for training, validating and testing Synister networks.\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eRelated Paper\u003c/strong\u003e \u003ca href=\"https://www.biorxiv.org/content/10.1101/2020.06.12.148775v2\" rel=\"nofollow\"\u003eNeurotransmitter Classification from Electron Microscopy Images at Synaptic Sites in Drosophila.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRelated Repositories\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/funkelab/synistereq\"\u003ePackage for on demand predictions with a production network.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/funkelab/synisterbrain\"\u003ePackage for high performance full brain predictions.\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/nilsec/synisterest\"\u003eWebservice.\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSingularity\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd singularity\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003emake\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eConda\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003econda create -n synister python=3.6 numpy scipy cython pylp -c funkey\nconda activate synister\npip install -r requirements.txt\npip install .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-0-creating-a-mongo-db-database-with-the-provided-data\" class=\"anchor\" href=\"#0-creating-a-mongo-db-database-with-the-provided-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e0. Creating a mongo DB database with the provided data.\u003c/h3\u003e\n\u003cp\u003eAn export of the three collections constituting the synister FAFB database used for all described experiments can be found at \u003ccode\u003edata/fafb_v3\u003c/code\u003e. The three files contain:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLocation, id, skid, brain region and split for each synapse (synapses(_v3).json).\u003c/li\u003e\n\u003cli\u003eSkid, neurotransmitter, hemilineage id for each skeleton (skeletons(_v3).json).\u003c/li\u003e\n\u003cli\u003eHemilineage name, hemilineage id for each hemilineage (hemilineages(_v3).json).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo reproduce the experiment each json file should be imported as a collection with names \"synapses\", \"skeletons\", \"hemilineages\" in one mongo database (for additional instructions on how to import json files in a mongo db click \u003ca href=\"https://docs.mongodb.com/database-tools/mongoimport/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e). Dictionary keys are field names. Provided splits can be reproduced using \u003ccode\u003esynister/split.py\u003c/code\u003e, which searches for the optimally balanced split in terms of neurotransmitter distribution for any given superset, such as hemilineage id, skeleton id or brain region.\u003c/p\u003e\n\u003cp\u003eFor training on other data, recreate the database scheme shown here (required are a \"synapses\" and a \"skeletons\" collection) and adapt config files to match the new database name.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-training-a-network\" class=\"anchor\" href=\"#1-training-a-network\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Training a network.\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-prepare-training\" class=\"anchor\" href=\"#prepare-training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare training\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003epython prepare_training.py -d \u0026lt;base_dir\u0026gt; -e \u0026lt;experiment_name\u0026gt; -t \u0026lt;train_id\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis creates a new directory at the specified path and initialises default config files for the run.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-run-training\" class=\"anchor\" href=\"#run-training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Training\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd \u0026lt;base_dir\u0026gt;/\u0026lt;experiment_name\u0026gt;/02_train/setup_t\u0026lt;train_id\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eEdit config files to match architecture, database and compute resources to train with.\u003c/p\u003e\n\u003cp\u003eFor example configs, training a VGG on the skeleton split, inside a singularity container, on a gpu queue see:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexample_configs/train_config.ini\n\n[Training]\nsynapse_types = gaba, acetylcholine, glutamate, serotonin, octopamine, dopamine\ninput_shape = 16, 160, 160\nfmaps = 12\nbatch_size = 8\ndb_credentials = synister_data/credentials/db_credentials.ini\ndb_name_data = synister_v3\nsplit_name = skeleton\nvoxel_size = 40, 4, 4\nraw_container = /nrs/saalfeld/FAFB00/v14_align_tps_20170818_dmg.n5\nraw_dataset = volumes/raw/s0\ndownsample_factors = (1,2,2), (1,2,2), (1,2,2), (2,2,2)\nnetwork = VGG\nfmap_inc = 2, 2, 2, 2\nn_convolutions = 2, 2, 2, 2\nnetwork_appendix = None\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eexample_configs/worker_config.ini\n\n[Worker]\nsingularity_container = synister/singularity/synister.img\nnum_cpus = 5\nnum_block_workers = 1\nnum_cache_workers = 5\nqueue = gpu_any\nmount_dirs = /nrs, /scratch, /groups, /misc\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFinally, to submit the train job with the desired number of iterations run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003epython train.py \u0026lt;num_iterations\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe recommend training for at least 500,000 iterations for FAVB_v3 splits.\u003c/p\u003e\n\u003cp\u003eFor visualizing training progress run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003etensorboard --logdir \u0026lt;base_dir\u0026gt;/\u0026lt;experiment_name\u0026gt;/02_train/setup_t\u0026lt;train_id\u0026gt;/log\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSnapshots are written to:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003e\u0026lt;base_dir\u0026gt;/\u0026lt;experiment_name\u0026gt;/02_train/setup_t\u0026lt;train_id\u0026gt;/snapshots\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-validating-a-trained-network\" class=\"anchor\" href=\"#2-validating-a-trained-network\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Validating a trained network.\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-prepare-validation-runs\" class=\"anchor\" href=\"#prepare-validation-runs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare validation runs\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003epython prepare_prediction.py -v -d \u0026lt;base_dir\u0026gt; -e \u0026lt;experiment_name\u0026gt; -t \u0026lt;train_id\u0026gt; -i \u0026lt;iter_0\u0026gt; \u0026lt;iter_1\u0026gt; \u0026lt;iter_2\u0026gt; ... \u0026lt;iter_N\u0026gt; \u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will create N prediction directories with appropriately initialized config files, one for each given train iteration \u0026lt;iter_k\u0026gt;. The -v flag sets the split part of the chosen split type to validation, only pulling those synapses from the DB that are tagged as validation synapses.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexample_configs/worker_config.ini\n\n[Predict]\ntrain_checkpoint = \u0026lt;experiment_name\u0026gt;/02_train/setup_t\u0026lt;train_id\u0026gt;/model_checkpoint_\u0026lt;iter_k\u0026gt;\nexperiment = \u0026lt;experiment_name\u0026gt;\ntrain_number = 0\npredict_number = 0\nsynapse_types = gaba, acetylcholine, glutamate, serotonin, octopamine, dopamine\ninput_shape = 16, 160, 160\nfmaps = 12\nbatch_size = 8\ndb_credentials = synister_data/credentials/db_credentials.ini\ndb_name_data = synister_v3\nsplit_name = skeleton\nvoxel_size = 40, 4, 4\nraw_container = /nrs/saalfeld/FAFB00/v14_align_tps_20170818_dmg.n5\nraw_dataset = volumes/raw/s0\ndownsample_factors = (1, 2, 2), (1, 2, 2), (1, 2, 2), (2, 2, 2)\nsplit_part = validation\noverwrite = False\nnetwork = VGG\nfmap_inc = 2, 2, 2, 2\nn_convolutions = 2, 2, 2, 2\nnetwork_appendix = None\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor most use cases the automatically initialized predict config does not require any edits. If run as is, predictions will be written into the database under:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;db_name\u0026gt;_predictions.\u0026lt;split_name\u0026gt;_\u0026lt;experiment_name\u0026gt;_t\u0026lt;train_id\u0026gt;_p\u0026lt;predict_id\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo start the prediction, run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd \u0026lt;base_dir\u0026gt;/\u0026lt;experiment_name\u0026gt;/03_predict/setup_t\u0026lt;train_id\u0026gt;_p\u0026lt;predict_id\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003epython predict.py\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf the collection already exists the script will abort. A collection can be overwritten by setting overwrite=True in the predict config. Parallel prediction with multiple GPUs can be done by setting num_block_workers=num_gpus in the worker_config file. Prediction speed and expected time to finish will be shown in the console.\u003c/p\u003e\n\u003cp\u003eFor submitting multiple predictions to the cluster at once run the provided convenience script:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003epython start_predictions -d \u0026lt;base_dir\u0026gt; -e \u0026lt;experiment_name\u0026gt; -t \u0026lt;train_id\u0026gt; -p \u0026lt;predict_id_0\u0026gt; \u0026lt;predict_id_1\u0026gt; ... \u0026lt;predict_id_N\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-testing-a-trained-network\" class=\"anchor\" href=\"#3-testing-a-trained-network\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Testing a trained network.\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-prepare-test-runs\" class=\"anchor\" href=\"#prepare-test-runs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare test runs\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003epython prepare_prediction.py -d \u0026lt;base_dir\u0026gt; -e \u0026lt;experiment_name\u0026gt; -t \u0026lt;train_id\u0026gt; -i \u0026lt;iter_0\u0026gt; \u0026lt;iter_1\u0026gt; \u0026lt;iter_2\u0026gt; ... \u0026lt;iter_N\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSimilar to validation this prepares the relevant dictionaries and config files but sets the split part to \"test\".\nStarting the prediction follow the same pattern as before:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd \u0026lt;base_dir\u0026gt;/\u0026lt;experiment_name\u0026gt;/03_predict/setup_t\u0026lt;train_id\u0026gt;_p\u0026lt;predict_id\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003epython predict.py\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1624487646.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v1.0.0"
    ],
    "full_name": "baxpr/naleg-roi",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-roi-analysis-for-sodium-leg-mri-scans\" class=\"anchor\" href=\"#roi-analysis-for-sodium-leg-mri-scans\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eROI analysis for sodium leg MRI scans\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003esrc   Matlab code for extracting calibrating ROI values from sodium scans\nbin   Compiled matlab code (needed to create simgularity container)\n\ncompile_matlab.sh              Shell script to compile matlab code\nspm_make_standalone_local.m    Prepare SPM distribution for compilation\n\nSimgularity.v1.0.0    Singularity recipe to build container with compiled matlab\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1594830481.0
  },
  {
    "data_format": 2,
    "description": "A quick reference repository for using the robots in the COR lab",
    "filenames": [
      "containers/singularity/Singularity.panda_ros_kinetic",
      "containers/singularity/Singularity.panda_ros_noetic",
      "containers/singularity/Singularity.panda_ros_melodic",
      "containers/singularity/Singularity.husky_ros_melodic",
      "containers/singularity/Singularity.ros_melodic"
    ],
    "full_name": "rickstaa/COR-robotics-lab-reference",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cor-robotics-lab-reference\" class=\"anchor\" href=\"#cor-robotics-lab-reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCOR-robotics-lab-reference\u003c/h1\u003e\n\u003cp\u003eA quick reference repository for using the robots in the COR lab. This repository contains several code examples, a \u003ca href=\"https://github.com/rickstaa/COR-robotics-lab-reference/discussions\"\u003ediscussion forum\u003c/a\u003e with FAQ that you might have while working with the robot and a \u003ca href=\"https://github.com/rickstaa/COR-robotics-lab-reference/wiki\"\u003ewiki\u003c/a\u003e with several helpfull documents\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-reserve-the-robots\" class=\"anchor\" href=\"#how-to-reserve-the-robots\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to reserve the robots\u003c/h2\u003e\n\u003cp\u003ePlease check the \u003cg-emoji class=\"g-emoji\" alias=\"spiral_calendar\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f5d3.png\"\u003e\ud83d\uddd3\ufe0f\u003c/g-emoji\u003e \u003ca href=\"https://github.com/rickstaa/COR-robotics-lab-reference/wiki/%F0%9F%97%93%EF%B8%8F-Robot-reservation-forum\"\u003erobot reservation form\u003c/a\u003e wiki page for more information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-work-with-the-robots\" class=\"anchor\" href=\"#how-to-work-with-the-robots\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to work with the robots\u003c/h2\u003e\n\u003cp\u003ePlease check \u003cg-emoji class=\"g-emoji\" alias=\"safety_vest\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f9ba.png\"\u003e\ud83e\uddba\u003c/g-emoji\u003e \u003ca href=\"https://github.com/rickstaa/COR-robotics-lab-reference/wiki/Panda-safety-guidelines\"\u003esafety-guidelines\u003c/a\u003e in the wiki before working with the robot.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624697768.0
  },
  {
    "data_format": 2,
    "description": "A Singularity image definition file built on top of the Ubuntu 20.04 docker image with R (4.1), RStudio Server (1.4.1717), and additional linux dependencies for common R packages installed.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "j-andrews7/singularity-rstudio",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rstudio-server\" class=\"anchor\" href=\"#singularity-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity RStudio Server\u003c/h1\u003e\n\u003cp\u003eThis repo contains a Singularity file that contains R 4.1 and RStudio 1.4.1717. It has several additional linux dependencies installed that are required for common bioinformatics packages (openssl, libproj, libbz2, etc). If you have others you\u0027d like added, feel free to open a PR (or make your own fork and add whatever you need).\u003c/p\u003e\n\u003cp\u003eThe Singularity image for this can be pulled via \u003ccode\u003esingularity pull library://j-andrews7/default/rstudio:4.1.0\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThis was mostly configured to run on HPCs in interactive jobs where users likely don\u0027t have the appropriate permissions for RStudio server to work properly. This requires a number of bindings to be made to the image and a secure cookie file to be provided. The cookie file can be produced with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Only needs to be run once.\nmkdir -p \"$HOME/rstudio-tmp/tmp/rstudio-server\"\nuuidgen \u0026gt; \"$HOME/rstudio-tmp/tmp/rstudio-server/secure-cookie-key\"\nchmod 0600 \"$HOME/rstudio-tmp/tmp/rstudio-server/secure-cookie-key\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn general, you can launch a script similar to the following from within an interactive job on your respective HPC to get it running, and it will print the IP address and port the server is running on that you can pop into your browser:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#!/bin/sh\n\nworkdir=${HOME}/rstudio-tmp\n\nmkdir -p -m 700 ${workdir}/run ${workdir}/tmp ${workdir}/var/lib/rstudio-server \ncat \u0026gt; ${workdir}/database.conf \u0026lt;\u0026lt;END\nprovider=sqlite\ndirectory=/var/lib/rstudio-server\nEND\n\n# Set R_LIBS_USER to a path specific to rocker/rstudio to avoid conflicts with\n# personal libraries from any R installation in the host environment\ncat \u0026gt; ${workdir}/rsession.sh \u0026lt;\u0026lt;END\n#!/bin/sh\nexport R_LIBS_USER=${HOME}/R/rstudio/4.1\nexec rsession \"\\${@}\"\nEND\n\nchmod +x ${workdir}/rsession.sh\n\nexport SINGULARITY_BIND=\"${workdir}/run:/run,${workdir}/tmp:/tmp,${workdir}/database.conf:/etc/rstudio/database.conf,${workdir}/rsession.sh:/etc/rstudio/rsession.sh,${workdir}/var/lib/rstudio-server:/var/lib/rstudio-server\"\n\n# Do not suspend idle sessions.\n# Alternative to setting session-timeout-minutes=0 in /etc/rstudio/rsession.conf\n# https://github.com/rstudio/rstudio/blob/v1.4.1106/src/cpp/server/ServerSessionManager.cpp#L126\nexport SINGULARITYENV_RSTUDIO_SESSION_TIMEOUT=0\n\n# Get unused socket per https://unix.stackexchange.com/a/132524\n# Tiny race condition between the python \u0026amp; singularity commands\nreadonly PORT=$(python -c \u0027import socket; s=socket.socket(); s.bind((\"\", 0)); print(s.getsockname()[1]); s.close()\u0027)\n# Get node IP address.\nreadonly ADD=$(nslookup `hostname` | grep -i address | awk -F\" \" \u0027{print $2}\u0027 | awk -F# \u0027{print $1}\u0027 | tail -n 1)\n\ncat 1\u0026gt;\u0026amp;2 \u0026lt;\u0026lt;END\n\"Running RStudio at $ADD:$PORT\"\nEND\n\nsingularity exec --cleanenv rstudio_4.1.0.sif \\\n    rserver --www-port ${PORT} \\\n            --rsession-path=/etc/rstudio/rsession.sh\n            --secure-cookie-key-file ${workdir}/tmp/rstudio-server/secure-cookie-key\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis repo is distributed under the GNU-GPL3 license. See the LICENSE file for more details.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624916222.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.beta",
      "Singularity"
    ],
    "full_name": "huynhngoc/head-neck-analysis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-head-and-neck-cancer-analysis\" class=\"anchor\" href=\"#head-and-neck-cancer-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHead and Neck cancer analysis\u003c/h1\u003e\n\u003cp\u003eStart by running \u003ccode\u003esetup.sh\u003c/code\u003e to download the singularity container\nThen, submit slurm jobs like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the setup from the \u003ccode\u003econfig/2d_unet.json\u003c/code\u003e file, train for 200 epochs\nand store the results in the folder \u003ccode\u003e$HOME/logs/hn_perf/2d_unet/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo customize model and prediction checkpoints\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm.sh config/3d_vnet_32_normalize.json 3d_vnet_32_normalize 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo continue an experiment\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm_cont.sh config/3d_vnet_32_normalize/model/model.030.h5 3d_vnet_32_normalize 100 --model_checkpoint_period 5 --prediction_checkpoint_period 5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo plot performance\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm_vis.sh 3d_vnet_32_normalize\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run test\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch slurm_test.sh 3d_vnet_32/model/model.030.h5 3d_vnet_32\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, if your cluster does not have slurm installed, simply omit the \u003ccode\u003esbatch\u003c/code\u003e\npart of the call above, thus running\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./slurm.sh config/2d_unet.json 2d_unet 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eManually build\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity build --fakeroot Singularity deoxys.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRemember to login to a gpu session to use the gpu\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eqlogin --partition=gpu --gres=gpu:1\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625647816.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "AdamWilsonLab/singularity-geospatial-r",
    "latest_release": "0.0.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-status\" class=\"anchor\" href=\"#singularity-status\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Status\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4930\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThis repository includes a definition file for a singularity container \u003cem\u003eand\u003c/em\u003e instructions for starting up an instance on CENTOS in a HPC environment.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setting-up-the-singularity-instance\" class=\"anchor\" href=\"#setting-up-the-singularity-instance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetting up the Singularity Instance\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSSH to the server\u003c/li\u003e\n\u003cli\u003eRun the following lines to create a new directory in the scratch drive and pull the desired container from the Singularity Hub (or other source):\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /panasas/scratch/grp-adamw/singularity/$USER\ncd /panasas/scratch/grp-adamw/singularity/$USER;\nsingularity pull -F shub://AdamWilsonLab/singularity-geospatial-r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you are downloading from github, use something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /panasas/scratch/grp-adamw/singularity/$USER;\nwget -O singularity-geospatial-r_latest.sif https://github.com/AdamWilsonLab/singularity-geospatial-r/releases/download/0.0.1/AdamWilsonLab-singularity-geospatial-r.latest.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eCreate symlinks to singularity folder in project storage to prevent disk space problems in the home directory.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /projects/academic/adamw/singularity/$USER/.singularity\nln -s /projects/academic/adamw/singularity/$USER/.singularity .singularity\n\n# Symlinks for RStudio\nmkdir -p /projects/academic/adamw/rstudio/$USER/rstudio\nmv .local/share/rstudio /projects/academic/adamw/rstudio/$USER/\n\nmkdir -p ~/.local/share\nln -s /projects/academic/adamw/rstudio/$USER/rstudio ~/.local/share/rstudio\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eRun the \u003ca href=\"https://github.com/AdamWilsonLab/singularity-geospatial-r/blob/main/singularity_start.sh\"\u003esingularity_start.sh\u003c/a\u003e script to start up a singularity instance. You can just copy paste the code into the terminal.  This includes a few system specific settings for the Buffalo CCR.  This should only need to be done once (as long as the instance keeps running, server is not restarted, etc.).  If the instance stops for any reason, you\u0027ll need to rerun this script.  You can confirm it\u0027s running with \u003ccode\u003esingularity instance list\u003c/code\u003e or by checking \u003ccode\u003ehtop\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-connecting-to-rstudio\" class=\"anchor\" href=\"#connecting-to-rstudio\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConnecting to RStudio\u003c/h2\u003e\n\u003cp\u003eAfter running the steps above, you should be able to do just the following to begin working.  If the server restarts you will need to re-run step 4 above.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eConnect to the instance via SSH with port Forwarding.  You will need to be on campus or connected via VPN.  See notes below for *nix and windows.\u003c/li\u003e\n\u003cli\u003eOpen RStudio at localhost:8787 in your local browser and login with user/password from #4 above.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container-geospatial-r\" class=\"anchor\" href=\"#singularity-container-geospatial-r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Container: Geospatial R\u003c/h2\u003e\n\u003cp\u003eThis container builds upon the \u003ca href=\"https://hub.docker.com/r/rocker/geospatial\" rel=\"nofollow\"\u003erocker geospatial container\u003c/a\u003e, which I ported to \u003ca href=\"https://singularity-hub.org/collections/4908\" rel=\"nofollow\"\u003eSingularity here\u003c/a\u003e.  This repository/collection then \u003ca href=\"https://github.com/AdamWilsonLab/singularity-geospatial-r/blob/main/Singularity.latest\"\u003eadds additional packages in this file\u003c/a\u003e.  That\u0027s the file to modify if you want to add more linux packages, etc.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-connecting-via-ssh\" class=\"anchor\" href=\"#connecting-via-ssh\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConnecting via SSH\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nix-systems-mac-and-linux\" class=\"anchor\" href=\"#nix-systems-mac-and-linux\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e*NIX systems (Mac and Linux)\u003c/h2\u003e\n\u003cp\u003eUse terminal to ssh to the server as explained in \u003ca href=\"https://github.com/AdamWilsonLab/singularity-geospatial-r/blob/main/singularity_start.sh\"\u003esingularity_start.sh\u003c/a\u003e.\nAdd something like the following to your .ssh/config file to simplify connecting with port forwarding via ssh.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHost rserver\nHostName HOST\nLocalForward 8787 HOST:PORT_NUMBER\nUser adamw\nForwardX11 yes\nForwardAgent yes\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-windows\" class=\"anchor\" href=\"#windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWindows\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-putty-instructions\" class=\"anchor\" href=\"#putty-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePuTTY Instructions\u003c/h3\u003e\n\u003cp\u003eOn Windows you will need to use PuTTY or a similar terminal program.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIn PuTTY, enter the server address (host name) and \"22\" (port) on the \"Session\" tab.\u003c/li\u003e\n\u003cli\u003eOn the \"SSH/Tunnels\" tab, enter the port number of the rsession  under \u201cSource port\u201d and type in HOST:PORT (replace with the actual server IP address + the port number) as the destination address. Then, click \"Add\".\u003c/li\u003e\n\u003cli\u003eConnect and login as usual in the terminal.\u003c/li\u003e\n\u003cli\u003ePoint the web browser to \u003ccode\u003ehttp://localhost:PORT\u003c/code\u003e (where PORT is the port number)\" and log in with the user name and the previously generated password.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODOs\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eSeparate container from startup and monitor script\u003c/li\u003e\n\u003cli\u003eSwitch to a docker image\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-development-notes\" class=\"anchor\" href=\"#development-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment Notes\u003c/h1\u003e\n\u003cp\u003eI started with \u003ca href=\"https://github.com/nickjer/singularity-rstudio/blob/master/.travis.yml\"\u003enickjer\u0027s very helpful example\u003c/a\u003e and updated it to pull from the geospatial version of the versioned rocker stack instead of the repository based R.  This should make it easier to keep up to date.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-errors\" class=\"anchor\" href=\"#errors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eErrors\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-unable-to-connect-to-service\" class=\"anchor\" href=\"#unable-to-connect-to-service\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUnable to connect to service\u003c/h3\u003e\n\u003cp\u003eThis error can appear in the web browser when connecting via localhost.  This can be caused by RStudio not being able to write session files in the right place.  Confirm that:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe directory \u003ccode\u003e/projects/academic/adamw/rstudio/$USER/rstudio\u003c/code\u003e exists\u003c/li\u003e\n\u003cli\u003eand is linked to \u003ccode\u003e~/.local/share/rstudio\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-could-not-acquire-revocation-list-file-lock\" class=\"anchor\" href=\"#could-not-acquire-revocation-list-file-lock\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCould not acquire revocation list file lock\u003c/h3\u003e\n\u003cp\u003eThe error \"Could not acquire revocation list file lock\" resolved with help from \u003ca href=\"https://www.gitmemory.com/issue/rocker-org/rocker-versioned/213/726807289\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-database-error-7\" class=\"anchor\" href=\"#database-error-7\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edatabase error 7\u003c/h3\u003e\n\u003cp\u003eStarting in early 2021, something changed that resulted in the following error when starting a new instance:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eERROR database error 7 (sqlite3_statement_backend::loadOne: attempt to write a readonly database) [description: Could not delete expired revoked cookies from the database, description: Could not read revoked cookies from the database]; OCCURRED AT virtual rstudio::core::Error rstudio::core::database::Connection::execute(rstudio::core::database::Query\u0026amp;, bool*) src/cpp/core/Database.cpp:480; LOGGED FROM: int main(int, char* const*) src/cpp/server/ServerMain.cpp:729\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI solved this by binding an address outside the container to \u003ccode\u003e/var/lib/rstudio-server\u003c/code\u003e when starting the instance as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--bind $RSTUDIO_DB:/var/lib/rstudio-server\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003ccode\u003e$RSTUDIO_DB\u003c/code\u003e is just a path outside the container.  I got this idea from \u003ca href=\"https://community.rstudio.com/t/permissions-related-to-upgrade-to-rstudio-server-open-source-1-4/94256/3\" rel=\"nofollow\"\u003ethis post\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local-rocker-updates\" class=\"anchor\" href=\"#local-rocker-updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal rocker updates\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003edocker run -d -p 8787:8787 -e PASSWORD=really_clever_password -v ~/Documents:~/Documents rocker/rstudio\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-useful-links\" class=\"anchor\" href=\"#useful-links\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful Links\u003c/h1\u003e\n\u003cp\u003eA few links I found useful while developing this container\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\" rel=\"nofollow\"\u003ehttps://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://hub.docker.com/r/rocker/geospatial\" rel=\"nofollow\"\u003ehttps://hub.docker.com/r/rocker/geospatial\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://singularity-hub.org/collections/4930\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/4930\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pawseysc.github.io/singularity-containers/23-web-rstudio/index.html\" rel=\"nofollow\"\u003ehttps://pawseysc.github.io/singularity-containers/23-web-rstudio/index.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.rocker-project.org/use/singularity/\" rel=\"nofollow\"\u003ehttps://www.rocker-project.org/use/singularity/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003ehttps://github.com/grst/rstudio-server-conda/issues/3\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1625164854.0
  },
  {
    "data_format": 2,
    "description": "Singularity container for DIVAnd",
    "filenames": [
      "Singularity"
    ],
    "full_name": "gher-ulg/DIVAnd-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/gher-ulg/DIVAnd-singularity/actions?query=workflow%3A%22Singularity+Build%22\"\u003e\u003cimg src=\"https://github.com/gher-ulg/DIVAnd-singularity/workflows/Singularity%20Build/badge.svg\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-divand-singularity\" class=\"anchor\" href=\"#divand-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDIVAnd-singularity\u003c/h1\u003e\n\u003cp\u003eSingularity container for DIVAnd\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eInstall singularity: \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003ehttps://sylabs.io/docs/\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding\u003c/h1\u003e\n\u003cp\u003eAfter checking out the source, the singularity container can be build using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erm -f DIVAnd.sif; sudo singularity build DIVAnd.sif Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe first command is only needed if you already had the \u003ccode\u003e.sif\u003c/code\u003e file in your system.\u003cbr\u003e\nThe \u003cem\u003ebuild\u003c/em\u003e operation lasts severall minutes due to the download and installation of languages and libraries.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h1\u003e\n\u003cp\u003eContainer images are build using GitHub actions.\nGo to \u003ca href=\"https://github.com/gher-ulg/DIVAnd-singularity/actions\"\u003ehttps://github.com/gher-ulg/DIVAnd-singularity/actions\u003c/a\u003e choose the lastest commit and go to artefact.\nDownload and unzip the image file and run with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run DIVAnd.sif\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1627222877.0
  },
  {
    "data_format": 2,
    "description": "A LaTeX Beamer template for presentations using the Metropolis theme.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "mmore500/presentation-template",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-presentation-template\" class=\"anchor\" href=\"#presentation-template\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePresentation Template\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1774\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://circleci.com/gh/mmore500/presentation-template\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9bb351ac98d51572070514cd96a68d96cdfdde431b5980e081069173ce4be9c7/68747470733a2f2f636972636c6563692e636f6d2f67682f6d6d6f72653530302f70726573656e746174696f6e2d74656d706c6174652e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/mmore500/presentation-template.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA LaTeX Beamer template for presentations using the Metropolis theme.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker\" class=\"anchor\" href=\"#docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker\u003c/h3\u003e\n\u003cp\u003eIf you want to build the container, after cloning this repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker build -t presentation-template \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo generate your pdf, you should bind the directory with main.tex to \u003ccode\u003e/data\u003c/code\u003e\nin the container, and provide a prefix for your output. That looks like this, and\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run -it -v \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e:/data presentation-template mypdf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfter this, the files will be in your present working directory.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ls my\u003cspan class=\"pl-k\"\u003e*\u003c/span\u003e\nmypdf.aux  mypdf.bbl  mypdf.blg  mypdf.fdb_latexmk  mypdf.fls  mypdf.log  mypdf.nav  mypdf.out  mypdf.pdf  mypdf.snm  mypdf.toc\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you don\u0027t want to build the container (it takes quite some time) this development\ncontainer is provided at \u003ca href=\"https://hub.docker.com/r/mmore500/presentation-template/\" rel=\"nofollow\"\u003emmore500/presentation-template\u003c/a\u003e. You can run it as follows:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://www.github.com/mmore500/presentation-template\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e presentation-template\ndocker run -it -v \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e:/data mmore500/presentation-template mypdf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn \u003ca href=\"example\"\u003eexample\u003c/a\u003e output is provided. Have fun!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h3\u003e\n\u003cp\u003eFirst, build the container.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build presentation-template.simg Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNext, run it and bind the present working directory to data.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --bind \u003cspan class=\"pl-smi\"\u003e$PWD\u003c/span\u003e:/data presentation-template.simg mypdf\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003cp\u003eYou should build the container with the version provided as a \u003ccode\u003e--build-arg\u003c/code\u003e\nas follows. For example, to build the version \u003ccode\u003e1.0.1-rc\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ docker build -t mmore500/presentation-template:1.0.1-rc --build-arg Version=1.0.1-rc \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\n$ docker push mmore500/presentation-template:1.0.1-rc\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to--what-you-get\" class=\"anchor\" href=\"#how-to--what-you-get\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow To \u0026amp; What You Get\u003c/h3\u003e\n\u003cp\u003eThe original post for the package is \u003ca href=\"https://twitter.com/MorenoMatthewA/status/1048676082952626177\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-authorship\" class=\"anchor\" href=\"#authorship\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthorship\u003c/h3\u003e\n\u003cp\u003eMatthew Andres Moreno\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ematthew.andres.moreno@gmail.com\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.github.com/vsoch\"\u003e@vsoch\u003c/a\u003e contributed Dockerfile and build / run instructions, continuous integration\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [
      "latex-beamer",
      "latex-beamer-template",
      "sans-forgetica",
      "singularity-container",
      "docker-container"
    ],
    "updated_at": 1596213882.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.guppy3.6.0cpu-conda-api",
      "Singularity.myR_4-0-2_rstudio_1.3",
      "Singularity.guppy4.0.14gpu-conda-api",
      "Singularity.deepbinner-api",
      "Singularity.guppy3.6.0gpu-conda-api",
      "Singularity.myR_3-6-3",
      "Singularity.guppy-cpu-conda",
      "Singularity.guppy4.5.4gpu-conda-api",
      "Singularity.guppy3.4gpu-conda-api",
      "Singularity.cpu-guppy3.4-conda-api",
      "Singularity.guppy4.2.2gpu-conda-api",
      "Singularity.guppy5.0.7gpu-conda-api"
    ],
    "full_name": "vibaotram/singularity-container",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4054\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-container\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-images-supporting-basedmux-workflow\" class=\"anchor\" href=\"#singularity-images-supporting-basedmux-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity images supporting \u003ca href=\"https://github.com/vibaotram/baseDmux.git\"\u003ebaseDmux workflow\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.guppy-cpu-conda\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 CPU, Miniconda3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:guppy-cpu-conda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.cpu-guppy3.4-conda-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 CPU, Miniconda3, ONT_FAST5_API\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:cpu-guppy3.4-conda-api\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.guppy3.4gpu-conda-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining GUPPY version 3.4 GPU, Miniconda3, ONT_FAST5_API\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:guppy3.4gpu-conda-api\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity.deepbinner-api\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003econtaining deepbinner 2.0.0, ONT_FAST5_API, python3\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eshub://vibaotram/singularity-container:deepbinner-api\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 3,
    "topics": [
      "singularity"
    ],
    "updated_at": 1626013391.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.latest"
    ],
    "full_name": "cschu/dada2_container",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-create-study-specific-roi-image-in-mni-space\" class=\"anchor\" href=\"#create-study-specific-roi-image-in-mni-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate study-specific ROI image in MNI space\u003c/h1\u003e\n\u003cp\u003ePMAT resting state connectivity study.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inputs\" class=\"anchor\" href=\"#inputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInputs:\u003c/h2\u003e\n\u003cp\u003eAll should be matched to the same T1 image.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eT1 image in atlas space (typically BIAS_NORM resource of cat12 assessor)\u003c/li\u003e\n\u003cli\u003eDeformation from T1 subject space to atlas space (typically DEF_FWD resource of cat12 assessor)\u003c/li\u003e\n\u003cli\u003eSUBJECT directory of Freesurfer output (typically SUBJECT resource of freesurfer_dev assessor)\u003c/li\u003e\n\u003cli\u003eTemporal lobe segmentation (typically SEG resource of Temporal_Lobe assessor)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003erois_PMAT.nii.gz            Region of interest image\nrois_PMAT-labels.csv        Region labels and volumes\nmakerois-PMAT.pdf           Visual report of final ROI image\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-regions-of-interest\" class=\"anchor\" href=\"#regions-of-interest\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRegions of interest\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spheres-atlas-space\" class=\"anchor\" href=\"#spheres-atlas-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSpheres (atlas space)\u003c/h3\u003e\n\u003cp\u003eSource: \u003cem\u003eLibby LA, Ekstrom AD, Ragland JD, Ranganath C. Differential connectivity of perirhinal and parahippocampal cortices within human hippocampal subregions revealed by high-resolution functional imaging. J Neurosci. 2012;32(19):6550-6560. doi:10.1523/JNEUROSCI.3711-11.2012\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eMethod: \u003cem\u003eSchr\u00f6der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-entorhinal-cortex-atlas-space\" class=\"anchor\" href=\"#entorhinal-cortex-atlas-space\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEntorhinal cortex (atlas space)\u003c/h3\u003e\n\u003cp\u003eAnterior lateral and posterior medial sections. Source and method: \u003cem\u003eSchr\u00f6der TN, Haak K V., Jimenez NIZ, et al. Functional topography of the human entorhinal cortex. Elife. 2015;4(October 2016):1-17. doi:10.7554/eLife.06738\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-temporal-lobe-subject-space-warped\" class=\"anchor\" href=\"#temporal-lobe-subject-space-warped\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTemporal lobe (Subject space, warped)\u003c/h3\u003e\n\u003cp\u003eHead for anterior hippocampus; body and tail combined for posterior hippocampus. Method: \u003cem\u003ePlassard AJ, McHugo M, Heckers S, Landman BA. Multi-Scale Hippocampal Parcellation Improves Atlas-Based Segmentation Accuracy. Proc SPIE Int Soc Opt Eng. 2017 Feb 11;10133:101332D. doi: 10.1117/12.2254425. Epub 2017 Feb 24. PMID: 28781411; PMCID: PMC5544133.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-parahippocampal-perirhinal-subject-space-warped\" class=\"anchor\" href=\"#parahippocampal-perirhinal-subject-space-warped\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParahippocampal, perirhinal (Subject space, warped)\u003c/h3\u003e\n\u003cp\u003eGenerated by Freesurfer 6. Parahippocampal (1016,2016) and perirhinal (surface patch resampled to volume, overlap with parahippocampus was assigned to perirhinal). Method: \u003cem\u003eBruce Fischl, Andre van der Kouwe, Christophe Destrieux, Eric Halgren, Florent Segonne, David H. Salat, Evelina Busa, Larry J. Seidman, Jill Goldstein, David Kennedy, Verne Caviness, Nikos Makris, Bruce Rosen, and Anders M. Dale. Automatically Parcellating the Human Cerebral Cortex. Cerebral Cortex January 2004; 14:11-22.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618484418.0
  },
  {
    "data_format": 2,
    "description": "Singularity base images that will be build on singularity hub and can be used to build other images",
    "filenames": [
      "Singularity.TxnDoubletDetection",
      "Singularity.DemultiplexingSoftwares",
      "Singularity.AllSoftwares",
      "Singularity.R4_python368",
      "Singularity.Anne_demultiplexing_test",
      "Singularity.R363_python368",
      "Singularity.DoubletDetection"
    ],
    "full_name": "powellgenomicslab/SingularityBaseImages",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularitybaseimages\" class=\"anchor\" href=\"#singularitybaseimages\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularityBaseImages\u003c/h1\u003e\n\u003cp\u003eA repo for singularity images. This is linked to singularity hub and all results can be pulled from there.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-hub-images\" class=\"anchor\" href=\"#singularity-hub-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Hub Images\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.R363_python368\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTo pull: \u003ccode\u003esingularity pull shub://drneavin/SingularityBaseImages:r363_python368\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eContains:\n\u003cul\u003e\n\u003cli\u003eR 3.6.3\u003c/li\u003e\n\u003cli\u003epython 3.6.8\u003c/li\u003e\n\u003cli\u003econda\u003c/li\u003e\n\u003cli\u003eSome basic R packages (see the definition file to see all installed)\u003c/li\u003e\n\u003cli\u003eSome basic python package (see the definition file to see all installed)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.R4_python368\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTo pull: \u003ccode\u003esingularity pull shub://drneavin/SingularityBaseImages:r4_python368\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eContains:\n\u003cul\u003e\n\u003cli\u003eR 4.0.3\u003c/li\u003e\n\u003cli\u003epython 3.6.8\u003c/li\u003e\n\u003cli\u003econda\u003c/li\u003e\n\u003cli\u003eSome basic R packages (see the definition file to see all installed)\u003c/li\u003e\n\u003cli\u003eSome basic python package (see the definition file to see all installed)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSingularity.TxnDoubletDetection\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eTo pull: \u003ccode\u003esingularity pull shub://drneavin/SingularityBaseImages:txndoubletdetection\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eBuilt on top of \u003ccode\u003eSingularity.R4_python368\u003c/code\u003e image\u003c/li\u003e\n\u003cli\u003eAlso contains:\n\u003cul\u003e\n\u003cli\u003eDoubletDetection\u003c/li\u003e\n\u003cli\u003eDoubletDecon\u003c/li\u003e\n\u003cli\u003eDoubletFinder\u003c/li\u003e\n\u003cli\u003escds\u003c/li\u003e\n\u003cli\u003escrublet\u003c/li\u003e\n\u003cli\u003escDoubletFinder\u003c/li\u003e\n\u003cli\u003esolo\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1611115640.0
  },
  {
    "data_format": 2,
    "description": "PhysiCell Invasion Model",
    "filenames": [
      "src/addons/PhysiBoSSa/MaBoSS-env-2.0/containers/singularity/Singularity"
    ],
    "full_name": "vincent-noel/pc4ecm",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-folding-at-home-gpu\" class=\"anchor\" href=\"#folding-at-home-gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efolding-at-home-gpu\u003c/h1\u003e\n\u003cp\u003egpu image for folding at home\u003c/p\u003e\n\u003cp\u003esimple merge of nvidia cl image with folding at home v7.5.1 to enable gpu processing.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1588946949.0
  },
  {
    "data_format": 2,
    "description": "Singularity Image for RNA-Seq analysis",
    "filenames": [
      "Singularity",
      "Singularity.test_jupyter"
    ],
    "full_name": "duke-chsi-informatics/singularity-rnaseq",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rnaseq\" class=\"anchor\" href=\"#singularity-rnaseq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularity-rnaseq\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-jupyter\" class=\"anchor\" href=\"#running-jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Jupyter\u003c/h2\u003e\n\u003cp\u003eRun this to start Jupyter:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run --app jupyter library://granek/duke-chsi-informatics/singularity-rstudio:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen follow the instructions that Jupyter printed to the terminal when you started it up to access Jupyter in your web browser\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-accessing-jupyter-on-a-remote-server\" class=\"anchor\" href=\"#accessing-jupyter-on-a-remote-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAccessing Jupyter on a remote server\u003c/h3\u003e\n\u003cp\u003eIf you are running the container on a remote server, you will need to set up port forwarding with ssh to be able to access Jupyter.  Run this command to forward the default Jupyter port (8888)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -L 8888:localhost:8888 bug\n\u003c/code\u003e\u003c/pre\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote if the default Jupyter port is not available, Jupyter will choose a different port.  In this case you will need to substitute the port that Jupyter outputs for 8888 in the ssh port forwarding command above.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-a-slurm-cluster\" class=\"anchor\" href=\"#running-on-a-slurm-cluster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on a SLURM Cluster\u003c/h2\u003e\n\u003cp\u003eYou can use this image interactively on a SLURM-managed cluster by running launching RStudio or Jupyter. The following instructions work on the Duke Compute Cluster (DCC).  Doing this on other cluster will require some modification and may not work, depending on how the cluster is configured.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rstudio\" class=\"anchor\" href=\"#rstudio\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRStudio\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003essh to DCC login node: \u003ccode\u003essh NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003erun tmux on login node: \u003ccode\u003etmux new -s container_demo\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun this on login node: \u003ccode\u003esrun -A chsi -p chsi --mem=100G -c 30 --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003ehostname -A\u003c/code\u003e on compute node and record results\u003c/li\u003e\n\u003cli\u003eRun on the following on a compute node and note the port, username, and password that the command prints:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eRun on local machine: \u003ccode\u003essh -L PORT:COMPUTE_HOSTNAME:PORT NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eWhere PORT is the port returned but the \"singularity run\" commmand\u003c/li\u003e\n\u003cli\u003eWhere COMPUTE_HOSTNAME is the hostname returned by running \"hostname -A\" on the compute node\u003c/li\u003e\n\u003cli\u003eWhere NETID is your NetID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eGo to \"localhost:PORT\" in a webrowser and enter the username and password printed by the \"singularity run\" commmand\u003c/li\u003e\n\u003cli\u003eHave fun!!\u003c/li\u003e\n\u003cli\u003eAt the end of an analysis you will probably want to copy results to your directory in \u003ccode\u003e/work\u003c/code\u003e or \u003ccode\u003e/hpc/group\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter\" class=\"anchor\" href=\"#jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003essh to dcc-login-01.rc.duke.edu\u003c/li\u003e\n\u003cli\u003erun tmux on login node: \u003ccode\u003etmux new -s container_demo\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun this on login node: \u003ccode\u003esrun -A chsi -p chsi --mem=100G -c 30 --pty bash -i\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun on compute node:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /scratch/josh/rnaseq_demo/rawdata /scratch/josh/rnaseq_demo/workspace\n\nsingularity run \\\n\t--app jupyter \\\n\t--bind /scratch/josh/rnaseq_demo/rawdata:/data \\\n\t--bind /scratch/josh/rnaseq_demo/workspace:/workspace \\\n\tlibrary://granek/duke-chsi-informatics/singularity-rnaseq\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eRun on local machine: \u003ccode\u003essh -L PORT:COMPUTE_HOSTNAME:PORT NETID@dcc-login-01.rc.duke.edu\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eWhere PORT is the number after \u003ccode\u003ehttp://127.0.0.1:\u003c/code\u003e in the URL given by Jupyter (defaults to 8888, but Jupyter will use a different one if the default is in use, or if a different port is supplied as an argument using \u003ccode\u003e--port\u003c/code\u003e when running the singularity container\u003c/li\u003e\n\u003cli\u003eWhere COMPUTE_HOSTNAME is the hostname returned by running \"hostname -A\" on the compute node\u003c/li\u003e\n\u003cli\u003eWhere NETID is your NetID\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCopy the URL supplied by jupyter that starts \u003ccode\u003ehttp://127.0.0.1:\u003c/code\u003e and paste it in a webbrowser\u003c/li\u003e\n\u003cli\u003eHave fun!!\u003c/li\u003e\n\u003cli\u003eAt the end of an analysis you will probably want to copy results to your directory in \u003ccode\u003e/work\u003c/code\u003e or \u003ccode\u003e/hpc/group\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-jupyter-on-gpu-node\" class=\"anchor\" href=\"#jupyter-on-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter on GPU node\u003c/h3\u003e\n\u003cp\u003eSame as above, but the srun command should use the \u003ccode\u003echsi-gpu\u003c/code\u003e partition and request a gpu, but less CPUs and Memory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esrun -A chsi -p chsi-gpu --gres=gpu:1 --mem=15866 -c 2 --pty bash -i\u003c/code\u003e\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1619726636.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for metabat2 (https://bitbucket.org/berkeleylab/metabat/src/master/)",
    "filenames": [
      "Singularity",
      "Singularity.2.15",
      "Singularity.2.15-3-g367a7ef"
    ],
    "full_name": "powerPlant/metabat2-srf",
    "latest_release": null,
    "readme": "\u003cp\u003eSingularity recipe files for MetaBAT: A robust statistical framework for reconstructing genomes from metagenomic data\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618472328.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.gpu",
      "Singularity.cpu"
    ],
    "full_name": "sleeepyjack/variant_calling",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-variant-calling\" class=\"anchor\" href=\"#variant-calling\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVariant Calling\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies:\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e- make\n- Singularity (\u0026gt;= v3.2)\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1589243145.0
  },
  {
    "data_format": 2,
    "description": "Local Ancestry Inference",
    "filenames": [
      "singularity/Singularity_defs.def"
    ],
    "full_name": "pmonnahan/AncInf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ancestry-inference\" class=\"anchor\" href=\"#ancestry-inference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAncestry Inference\u003c/h1\u003e\n\u003cp\u003eHuman local ancestry inference using \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738819/\" rel=\"nofollow\"\u003eRFmix\u003c/a\u003e.  The basic workflow is to parse the input PLINK file by chromosome, perform reference-based haplotype phasing on the data using \u003ca href=\"https://odelaneau.github.io/shapeit4/\" rel=\"nofollow\"\u003eShapeIt4\u003c/a\u003e, and, finally, perform local ancestry inference with RFMix.  More information is provided in the \u003cem\u003ePipeline Overview\u003c/em\u003e below.  With the RFMix output, admixture mapping (i.e. associating local ancestry with phenotype) can be accomplished via a separate pipeline found \u003ca href=\"https://github.com/pmonnahan/admixMap\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\n\n\u003cp\u003e\u003cstrong\u003eTable of Contents\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#requirements\"\u003eRequirements\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#snakemake\"\u003eSnakemake\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#singularity\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-the-workflow\"\u003eRunning the workflow\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#other-notes\"\u003eOther Notes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#debugging-and-error-reports\"\u003eDebugging and error reports\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#pipeline-overview\"\u003ePipeline Overview\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#input-data\"\u003eInput Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#output\"\u003eOutput\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#reference-population\"\u003eReference population\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#phasing\"\u003ePhasing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#local-ancestry-inference\"\u003eLocal Ancestry Inference\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003ca href=\"https://github.com/pmonnahan/AncInf/blob/master/Pipeline_DAG.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/pmonnahan/AncInf/raw/master/Pipeline_DAG.png\" alt=\"Pipeline DAG\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-snakemake\" class=\"anchor\" href=\"#snakemake\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSnakemake\u003c/h3\u003e\n\u003cp\u003eThe pipeline is coordinated and run on an HPC (or locally) using \u003cem\u003eSnakemake\u003c/em\u003e.  To install snakemake, first create a virtual environment via:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load python3/3.6.3_anaconda5.0.1\nconda install -c conda-forge mamba\nmamba create -c conda-forge -c bioconda -n \u0026lt;your_environment_name\u0026gt; snakemake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will create a new virtual environment and install \u003ccode\u003esnakemake\u003c/code\u003e.  Then, activate this environment and perform following installations:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate \u0026lt;your_environment_name\u0026gt;\nconda install numpy yaml pandas\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnytime you need to run the pipeline, activate this environment beforehand via:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate \u0026lt;environment_name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you choose not to create an environment, you must ensure that these packages are installed and available for your python installation.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h3\u003e\n\u003cp\u003eThe installation of the individual programs used throughout this pipeline can be completely avoid by utilizing a Singularity image.  This image is too large to be hosted on Github, although you can find the definitions file used to create the image \u003ca href=\"https://github.com/pmonnahan/AncInf/blob/master/singularity/Singularity_defs.def\"\u003ehere\u003c/a\u003e.  Building of images is still not currently supported at MSI, so I used a Vagrant virtual machine, which comes with Singularity pre-configured/installed (\u003ca href=\"https://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\" rel=\"nofollow\"\u003ehttps://app.vagrantup.com/singularityware/boxes/singularity-2.4/versions/2.4\u003c/a\u003e).  I can also share the img file directly upon request.\u003c/p\u003e\n\u003cp\u003eHowever, in order to utilize the singularity image, \u003cem\u003esingularity\u003c/em\u003e must be installed on the HPC.  Currently, the pipeline assumes that \u003cem\u003esingularity\u003c/em\u003e will be available as a module and can be loaded into the environment via the command specified in the config.yml file, in the \u003ccode\u003emodule\u003c/code\u003e entry under the  \u003ccode\u003esingularity\u003c/code\u003e section.  The default setting will work for MSI at UMN.\u003c/p\u003e\n\u003cp\u003eSingularity settings in config.yml\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity:\n  use_singularity: \u0027true\u0027\n  image: \u0027/home/pmonnaha/pmonnaha/singularity/AncestryInference.sif\n  module: \u0027module load singularity\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-workflow\" class=\"anchor\" href=\"#running-the-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the workflow\u003c/h2\u003e\n\u003cp\u003eClone this repository to the location where you want to store the output of the pipeline.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/pmonnahan/AncInf.git rfmix_test\ncd rfmix_test\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe critical files responsible for executing the pipeline are contained in the \u003cem\u003e./workflow\u003c/em\u003e subdirectory contained within the cloned repo.  They are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSnakefile\u003c/li\u003e\n\u003cli\u003econfig.yml\u003c/li\u003e\n\u003cli\u003ecluster.yaml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003cem\u003eSnakefile\u003c/em\u003e is the primary workhouse of snakemake, which specifies the dependencies of various parts of the pipeline and coordinates execution.  No modifications to the \u003cem\u003eSnakefile\u003c/em\u003e are necessary.\u003c/p\u003e\n\u003cp\u003eIn order for the \u003cem\u003eSnakefile\u003c/em\u003e to locate all of the necessary input and correctly submit jobs to the cluster, \u003cstrong\u003eboth\u003c/strong\u003e the \u003cem\u003econfig.yaml\u003c/em\u003e and \u003cem\u003ecluster.yaml\u003c/em\u003e need to be modified. Open these files and change the required entries that are indicated with \u0027MODIFY\u0027.  Other fields do not require modification, although this may be desired given the particulars of the run you wish to implement.  Details on each entry in the config file (e.g. what the program expects in each entry as well as the purpose of the entry) are provided in the \u003cem\u003ePipeline Overview\u003c/em\u003e at the bottom.\u003c/p\u003e\n\u003cp\u003eThe entire pipeline can be executed on a local machine (not recommended) or on an HPC, and the \u003cem\u003ecluster.yaml\u003c/em\u003e file is required only for the latter.  For a local run, change the \u003ccode\u003elocal_run\u003c/code\u003e entry to \u003ccode\u003etrue\u003c/code\u003e under the \u003ccode\u003erun_settings\u003c/code\u003e section of the config file, and launch snakemake from within the parent directory by the simple command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHowever, multiple steps in the pipeline have high resource demands, and so are unlikely to be able to be run locally.  This option exists primarily for testing and troubleshooting, so the remainder of the  documentation assumes that the pipeline will be executed on an HPC.  In order to coordinate the use of the HPC, the following modifications to the snakemake command are required:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --cluster \"sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type} -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml -j 32\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere -j specifies the number of jobs that can be submitted at once.\u003c/p\u003e\n\u003cp\u003eOne additional setting in the \u003cem\u003econfig.yml\u003c/em\u003e is needed in order to correctly submit jobs to the HPC.  The relevant entries are under the \u003ccode\u003erun_settings\u003c/code\u003e section of the config file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erun_settings:\n  local_run: \u0027false\u0027\n  cluster_config: \u0027workflow/cluster_slurm.yaml\u0027\n  scheduler: \u0027slurm\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere, it is necessary that the \u003ccode\u003ecluster_config\u003c/code\u003e entry is set to the path of the cluster_slurm.yaml file that will be used in the snakemake command.  Also, the scheduler must correspond to the syntax used in the snakemake command and cluster.yaml file.  I should point out that these additional changes are needed for responsibly using PLINK within a snakemake framework, and are not directly needed for snakemake.  PLINK will attempt to auto-detect available resources upon running regardless of the resources that were requested when the job was submitted.  Therefore, we have to read and parse the requested resources in the cluster config file in order for them to be communicated to PLINK from within the Snakefile.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-other-notes\" class=\"anchor\" href=\"#other-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOther notes\u003c/h3\u003e\n\u003cp\u003eIt is recommended that \u003cem\u003esnakemake\u003c/em\u003e is run as an interactive session on an HPC.  \u003cem\u003eSnakemake\u003c/em\u003e will launch the specified number (via the -j flag) of jobs, and then will hang and wait for them to finish.  As jobs finish (and assuming no errors), \u003cem\u003esnakemake\u003c/em\u003e will launch additional jobs keeping the total running jobs at whatever -j is set for.  Although \u003cem\u003esnakemake\u003c/em\u003e should not use a lot of memory, it could have long run times, which is generally not advisable on login nodes.\u003c/p\u003e\n\u003cp\u003eOne attractive feature of \u003cem\u003esnakemake\u003c/em\u003e is its ability to keep track of the progress and dependencies of the different stages of the pipeline.  Specifically, if an error is encountered or the pipeline otherwise stops before the final step, \u003cem\u003esnakemake\u003c/em\u003e can resume the pipeline where it left off, avoiding redundant computation for previously completed tasks.  To do so, simply resubmit the original \u003cem\u003esnakemake\u003c/em\u003e command.\u003c/p\u003e\n\u003cp\u003eTo run a specific part of the pipeline, do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake -R \u0026lt;rule_name\u0026gt; --cluster \"sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type} -o {cluster.o} -e {cluster.e} -A {cluster.A}\" --cluster-config workflow/cluster_yale.yaml -j 20 --rerun-incomplete\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003cem\u003erule_name\u003c/em\u003e indicates the \u0027rule\u0027 (i.e. job) in the Snakefile that you wish to run.  Or, you can request a specific file by providing the filename at the end of the command.  You may need to include the -F (i.e. force) if the output file already exists and you want to overwrite it.\u003c/p\u003e\n\u003cp\u003eAlso, it is often very helpful to do a \u0027dry-run\u0027 of the pipeline in which the different steps and dependencies are printed to screen, but no actual jobs are executed.  This can be helpful to ensure that config entries are correct, etc.  To perform a dry-run, do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake -nrp\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: It is convenient to make an alias in your ~/.bashrc file to run snakemake on the cluster without having to type the --cluster... part of the command every time.  For me, it looked like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ealias snakeslurm=\"snakemake -k --cluster \u0027sbatch --no-requeue --partition={cluster.p} --time={cluster.time} --mem={cluster.mem} --ntasks={threads} --job-name={cluster.job-name} --nodes={cluster.nodes} --mail-user={cluster.mail-user} --mail-type={cluster.mail-type} -o {cluster.o} -e {cluster.e} -A {cluster.A}\u0027 --cluster-config workflow/cluster_slurm.yaml\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis way, I can just do:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakeslurm -j 25\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo launch snakemake on the cluster.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-unlocking-the-working-directory\" class=\"anchor\" href=\"#unlocking-the-working-directory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUnlocking the working directory\u003c/h4\u003e\n\u003cp\u003eWhen \u003cem\u003esnakemake\u003c/em\u003e is launched it will place a lock on the working directory, such that other \u003cem\u003esnakemake\u003c/em\u003e runs are prohibited from starting.  When \u003cem\u003esnakemake\u003c/em\u003e finishes or errors out, it will remove this lock.  However, sometimes this lock is not correctly removed.  This can occur, for example, if the VPN drops connection while \u003cem\u003esnakemake\u003c/em\u003e is running.  If you receive a \"Directory cannot be locked...\" error message from \u003cem\u003esnakemake\u003c/em\u003e and you are sure that no other \u003cem\u003esnakemake\u003c/em\u003e processes are currently running, you can unlock the directory by:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esnakemake --unlock\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, you can run the usual \u003cem\u003esnakemake\u003c/em\u003e command to restart the pipeline.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-debugging-and-error-reports\" class=\"anchor\" href=\"#debugging-and-error-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDebugging and error reports\u003c/h4\u003e\n\u003cp\u003eShould an error be encountered in a job, snakemake will halt the pipeline and indicate in the terminal that an error has occurred.  The offending job will also be printed in red in the terminal window.  More information on why the job failed can be found in the \u0027stdout\u0027 and \u0027stderr\u0027 files that are output to the \u003cem\u003e\u0027OandE\u0027\u003c/em\u003e directory and will be labelled with the jobname.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-overview\" class=\"anchor\" href=\"#pipeline-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Overview\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-input-data\" class=\"anchor\" href=\"#input-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput Data\u003c/h3\u003e\n\u003cp\u003eThe pipeline expects as input a single set of PLINK files (.bed, .fam, .bim) that has gone through basic QC steps (missingness, hwe, maf, etc).  I have written QC pipelines for non-imputed and imputed data, which are available \u003ca href=\"https://github.com/pmonnahan/DataPrep\"\u003ehere\u003c/a\u003e and \u003ca href=\"https://github.com/pmonnahan/DataPrep/tree/master/postImpute\"\u003ehere\u003c/a\u003e, respectively.  It is technically possible to use imputed data in ancestry inference, although this is not widely seen throughout the literature.\u003c/p\u003e\n\u003cp\u003eThe input PLINK files are specified in the \u003ccode\u003equery\u003c/code\u003e entry within the config file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003equery: \"PATH_TO_PLINK_PREFIX\" \nsamples: \"all\"  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe user can also provide a path to a file in the \u003ccode\u003esamples\u003c/code\u003e entry, in which case the program will subset the \u003ccode\u003equery\u003c/code\u003e dataset to include only the samples in the file (one sample per line).\u003c/p\u003e\n\u003cp\u003eIt is assumed that the query coordinates and chromosome names are consistent with those used in the reference VCF (see below).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h3\u003e\n\u003cp\u003eAll output is labelled using the prefix specified in the \u003ccode\u003eoutname\u003c/code\u003e entry in the config file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eoutname: \"AncInf\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe RFMix results will be output to the \u003cem\u003erfmix\u003c/em\u003e directory that is automatically created. RFMix outputs a number of files, but the most relevant files are those ending in \u003cem\u003e.Q\u003c/em\u003e (which contain the global ancestry percentage estimates for each individual) and the files ending in \u003cem\u003e.msp.tsv\u003c/em\u003e (which contain the maximum-likelihood ancestry state in each window analyzed; i.e. local ancestry).  The \u003cem\u003e.Q\u003c/em\u003e files can be easily filtered to isolate individuals of a given ethnicity, based on user-provided thresholds.\u003c/p\u003e\n\u003cp\u003eA set of phased BCF files (separated by chromosome) are generated as an intermediate step and are saved to the \u003cem\u003ephased\u003c/em\u003e directory.  This directory will also contain the phased BCF of the individuals from the reference population.\u003c/p\u003e\n\u003cp\u003eA good initial check that the results make sense is to simply look at the average local ancestry along a chromosome.  A full collection of these images (one for each chromosome) will be created and output into the \u003cem\u003echrom_plots\u003c/em\u003e folder within the master run directory.  These averages should remain fairly stable across the chromosome.  Any large, sudden changes in the dominant ancestral component are indicative of issues in phasing or ancestry inference.  Furthermore, these chromosome plots should be inspected to identify areas of suspect inference.  For example, drastic changes in average ancestry is often observed near centromeres or telomeres.  These can also likely be flagged by low SNP counts in the inferred windows (which is reported in the \u003cem\u003e.msp.tsv\u003c/em\u003e files).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-reference-population\" class=\"anchor\" href=\"#reference-population\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference population\u003c/h3\u003e\n\u003cp\u003eThe reference VCF to be used for phasing as well as for ancestry inference is provided under the \u003ccode\u003ereference\u003c/code\u003e section of the config file.  The pipeline is currently set up to use the 1000Genomes VCF (available \u003ca href=\"https://www.internationalgenome.org/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e or by request) for the reference population.  However, any VCF should work in theory as long as the necessary accessory files are provided.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ereference:\n  vcf: \"PATH_TO_REFERENCE_VCF\"\n  subpops: \"accessory/1000G_PopLabels.txt\"\n  genmap: \"PATH_TO_DATA_SUBDIRECTORY/genetic_map_hg19.txt\"\n  phased_bcf: \u0027none`  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere are two required files that need to accompany the reference VCF, and these are provided at the \u003ccode\u003esubpops\u003c/code\u003e and \u003ccode\u003egenmap\u003c/code\u003e entries.  The \u003ccode\u003esubpops\u003c/code\u003e file should be a text file with two columns: sample ID as it appears in the VCF in the first column and the subpopulation label for that sample in the second column.  If using the 1000Genomes VCF, then the \u003ccode\u003esubpop\u003c/code\u003e file was automatically downloaded to the \u003cem\u003eaccessory\u003c/em\u003e subdirectory.  The \u003ccode\u003egenmap\u003c/code\u003e file specifies the genetic map for the reference genome and is too large to be hosted on GitHub.  However, the hg19 genetic map is available \u003ca href=\"https://mathgen.stats.ox.ac.uk/impute/1000GP_Phase3.html\" rel=\"nofollow\"\u003ehere\u003c/a\u003e or by request.  The file contains 3 space-delimited columns: chromosome, base position, genetic position.\u003c/p\u003e\n\u003cp\u003eIt is assumed that the reference VCF file has been filtered, phased, and indexed. The VCF does NOT need to be subsetted to include only the individuals from the desired reference subpopulations.  This is accomplished by the initial steps of the pipeline, using the \u003ccode\u003esubpops\u003c/code\u003e file described above along with the comma-separated lists (no spaces!) in the \u003ccode\u003eref_pops\u003c/code\u003e and \u003ccode\u003epop_names\u003c/code\u003e entries under the \u003ccode\u003erfmix\u003c/code\u003e section of the config file.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erfmix:\n  ref_pops: \"YRI,GWD,ESN,CEU,IBS,TSI\" # No spaces!!\n  pop_names: \"AFR,AFR,AFR,EUR,EUR,EUR\" \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBased on the information contained in the \u003ccode\u003esubpops\u003c/code\u003e file described above, individuals corresponding to the subpopulation names provided in \u003ccode\u003eref_pops\u003c/code\u003e entry are extracted from the reference VCF.  In addition, a new file is created at:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e accessory/Population_Map_File.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e, which re-labels the subsetted individuals with the corresponding value in the \u003ccode\u003epop_names\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThere is expected to be a 1:1 ordered correspondence between the subpopulation labels \u003ccode\u003eref_pops\u003c/code\u003e and the superpopulation names in \u003ccode\u003epop_names\u003c/code\u003e.  In this example where we are interesting in inferring 2-way admixture between AFR and EUR populations, all YRI, GWD, and ESN individuals would be extracted and re-labelled as AFR individuals, while the CEU, IBS, and TSI individuals would be labelled as EUR individuals.  This scheme was developed to allow for flexibility in the inclusion/exclusion of particular subpopulations.\u003c/p\u003e\n\u003cp\u003eRFMix will sample randomly from within these superpopulations to generate the training/test sets needed for the machine learning algorithm.  It is best if the reference individuals from a superpopulation are evenly distributed across subpopulations, so that a single subpopulation does not dominate during the resampling.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-phasing\" class=\"anchor\" href=\"#phasing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePhasing\u003c/h3\u003e\n\u003cp\u003eThe config file has the following options for modifying the behavior of haplotype phasing in ShapeIt4:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ephase:\n  threads: \"12\"\n  pbwt_depth: \"4\"\n  sequence_data: \u0027true\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIncreasing the \u003ccode\u003epbwt_depth\u003c/code\u003e may increase the phasing accuracy, but comes at a substantial computational cost.  The \u003ccode\u003esequence_data\u003c/code\u003e entry should be set to false if the data comes from an array.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-local-ancestry-inference\" class=\"anchor\" href=\"#local-ancestry-inference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal Ancestry Inference\u003c/h3\u003e\n\u003cp\u003eIn addition to the \u003ccode\u003eref_pops\u003c/code\u003e and \u003ccode\u003epop_names\u003c/code\u003e, the \u003ccode\u003erfmix\u003c/code\u003e section of the config file provides a number of options for modifying the behavior of RFMix.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erfmix:\n  ref_pops: \"YRI,GWD,ESN,CEU,IBS,TSI\" # No spaces!!\n  pop_names: \"AFR,AFR,AFR,EUR,EUR,EUR\" \n  generations: \"8\"\n  reanalyze_reference: \"true\" \n  window_size: \"0.02\" \n  threads: \"12\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003egenerations\u003c/code\u003e entry specifies the number of generations in the past when admixture between the superpopulations is assumed to have begun.  Values used in the literature are typically approximations based off of historical events or genomic dating methods.  \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4289685/#:~:text=Patterns%20of%20Genetic%20Ancestry%20of%20Self%2DReported%20Latinos\u0026amp;text=On%20average%2C%20we%20estimate%20that,%2C%20and%206.2%25%20African%20ancestry\" rel=\"nofollow\"\u003eBryc et al 2015\u003c/a\u003e provide a good reference for African American and Latinx ancestry inference.  For both scenarios, they modelled admixture between Europeans and Native Americans at 11-12 generations ago and subsequent admixture with Africans 6-8 generations ago.  Unfortunately, RFMix only allows the user to specify a single value, so I have used \u00278\u0027 for African Americans (modelling 2-way admixture between AFR and EUR) and \u002712\u0027 for Latinx individuals (modelling 3-way admixture between AFR, EUR, and AMR)\u003c/p\u003e\n\u003cp\u003eIn the case that a set of reference haplotypes may not be of \"pure\" ancestry and may themselves be somewhat admixed, the option --reanalyze-reference will cause the program to iteratively analyze the reference haplotypes as if they were query haplotypes, in addition to analyzing the query input (see the \u003ca href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738819/\" rel=\"nofollow\"\u003eRFmix\u003c/a\u003e paper for a more thorough explanation of this procedure).  This is often advised for inferring local ancestry in Latinx populations, where a 3-way AFR, EUR, and AMR admixture is modelled.  However, it is likely not necessary for inferring ancestry in African American populations, where the ancestral populations likely do not contain any admixed individuals.\u003c/p\u003e\n\u003cp\u003eThe last relevant option is the window size in which ancestry is to be inferred.  This value is specified in centiMorgans (cM).  Default is 0.2 cM, which corresponds to ~100 - 150 kb windows.  For a given window, there is a minimum requirement on the number of SNPs, and windows will be expanded to meet this requirement regardless of the specified window size.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1616969120.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "imaging/nipy/Singularity"
    ],
    "full_name": "andyrevell/docker_GitHub",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cntdocker\" class=\"anchor\" href=\"#cntdocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCNTdocker\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h2\u003e\n\u003cp\u003eDockerfiles to create Docker images used by the CNT at the university of Pennsylvania\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directory-contents-explanation\" class=\"anchor\" href=\"#directory-contents-explanation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirectory contents explanation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-eeg\" class=\"anchor\" href=\"#eeg\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eEEG\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common EEG analysis tools. Usually python 3\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eechobase\u003c/strong\u003e: Dockerfiles used to create images that can calculate functional connectivity of EEG\nAlso has ieegpy python package used to interface with iEEG.org\nEchobase code is from \u003ca href=\"https://github.com/andyrevell/paper001\"\u003ehttps://github.com/andyrevell/paper001\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUbuntu 18.04\nPython 2.7 and Python 3.6\nNumpy 1.18.4\npandas 1.0.3\nscipy 1.4.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-imaging\" class=\"anchor\" href=\"#imaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eImaging\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common MRI analysis tools.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  Python 2.7, Python 3.6, Python 3.7\n  dcm2niix\n  dsistudio\n  ANTS\n  Freesurfer\n  FSL 6.0.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ml\" class=\"anchor\" href=\"#ml\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eml\u003c/strong\u003e:\u003c/h3\u003e\n\u003cp\u003eDockerfiles used to create images with common machine learning tools.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ewavenet\u003c/strong\u003e: Dockerfile to create compatible dependencies to use with Goodgle Deepmind wavenet paper\n\u003ca href=\"https://deepmind.com/blog/article/wavenet-generative-model-raw-audio\" rel=\"nofollow\"\u003eWavenet blog\u003c/a\u003e\n\u003ca href=\"https://arxiv.org/pdf/1609.03499.pdf\" rel=\"nofollow\"\u003eWavenet paper\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  tensorflow 1.0.0\n  pandas 0.19.2\n  librosa 0.5.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eTensorflow_2.1\u003c/strong\u003e: Dockerfile to create compatible dependencies to with tensorflow 2.1\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Ubuntu 18.04\n  tensorflow 2.1\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1600370006.0
  },
  {
    "data_format": 2,
    "description": "variant annotation workflow with VEP",
    "filenames": [
      "container/Singularity.vep-96.0"
    ],
    "full_name": "stevekm/vep-annotation-nf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-vep-annotation-nf\" class=\"anchor\" href=\"#vep-annotation-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003evep-annotation-nf\u003c/h1\u003e\n\u003cp\u003eDemo pipeline for annotating variants in .vcf files using \u003ca href=\"https://useast.ensembl.org/info/docs/tools/vep/index.html\" rel=\"nofollow\"\u003eVariant Effect Predictor\u003c/a\u003e (VEP).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h1\u003e\n\u003cp\u003eClone this repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/stevekm/vep-annotation-nf.git\ncd vep-annotation-nf\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nextflow\" class=\"anchor\" href=\"#nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNextflow\u003c/h2\u003e\n\u003cp\u003eInstall \u003ccode\u003enextflow\u003c/code\u003e in the current directory with the command in the Makefile.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake install\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-vep-docker\" class=\"anchor\" href=\"#vep-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVEP: Docker\u003c/h2\u003e\n\u003cp\u003eTo install VEP using Docker, run the Makefile command in the \u003ccode\u003econtainer\u003c/code\u003e directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd container\nmake docker-build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-vep-conda\" class=\"anchor\" href=\"#vep-conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVEP: conda\u003c/h2\u003e\n\u003cp\u003eTo install VEP using \u003ccode\u003econda\u003c/code\u003e (for NYULMC Big Purple HPC), instead run the \u003ccode\u003econda-install\u003c/code\u003e recipe from the Makefile in the parent repo directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake conda-install\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference-files\" class=\"anchor\" href=\"#reference-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference Files\u003c/h2\u003e\n\u003cp\u003eVEP reference files will be downloaded automatically by the pipeline. However the hg19 genome fasta, fasta.fai, and fasta.dict files must also be obtained (not included; try \u003ca href=\"https://support.illumina.com/sequencing/sequencing_software/igenome.html\" rel=\"nofollow\"\u003ethese\u003c/a\u003e). On NYULMC Big Purple, all required files are already cached and no download should be needed. On other systems, the command line arguments specifying the genome fasta files should be provided separately when running, or place the files \u003ccode\u003egenome.fa\u003c/code\u003e, \u003ccode\u003egenome.fa.fai\u003c/code\u003e, and \u003ccode\u003egenome.dict\u003c/code\u003e inside the included \u003ccode\u003eref\u003c/code\u003e directory.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h1\u003e\n\u003cp\u003eThe Makefile includes shortcuts to help run the pipeline easier on NYULMC Big Purple HPC.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe command can also be used to run on other systems, it will simply invoke the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./nextflow run main.nf -resume\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNextflow \u003ccode\u003eparams\u003c/code\u003e values can be passed on the command line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./nextflow run main.nf -resume --ref_fa /path/to/genome.fa --ref_fai /path/to/genome.fa.fai --ref_dict /path/to/genome.dict\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h1\u003e\n\u003cp\u003eOutput files will be collected in the \u003ccode\u003eoutput\u003c/code\u003e directory.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-software\" class=\"anchor\" href=\"#software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware\u003c/h1\u003e\n\u003cp\u003eTested on RHEL 7, macOS 10.12\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow (Java 8+)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ebash\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGNU \u003ccode\u003emake\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePython 2.7+\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [
      "vcf",
      "annotation",
      "vep",
      "nextflow"
    ],
    "updated_at": 1617871975.0
  },
  {
    "data_format": 2,
    "description": "Generic viral Illumina sequence analysis pipeline",
    "filenames": [
      "Singularity",
      "singularity/Singularity.2.0.0"
    ],
    "full_name": "peterk87/nf-villumina",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-peterk87nf-villumina\" class=\"anchor\" href=\"#peterk87nf-villumina\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epeterk87/nf-villumina\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eGeneric viral Illumina sequence analysis pipeline\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/peterk87/nf-villumina\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c95912a5b97ffebe518b92d2612faba172f193f3aec7d85e0b8ee6a88db89b94/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f6e662d76696c6c756d696e612e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/nf-villumina.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/peterk87/nf-villumina\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/14da47af1d6b7d4d6e7909986afbce060794df560644ce6b565495a43df45b94/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f70657465726b38372f6e662d76696c6c756d696e612e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/peterk87/nf-villumina.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/2925\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with a \u003ca href=\"https://sylabs.io/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003enf-villumina\u003c/code\u003e will\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eremove low quality reads (\u003ca href=\"https://github.com/OpenGene/fastp\"\u003efastp\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003efilter for reads from a taxonomic group of interest (by default superkingdom \u003ca href=\"https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?mode=Info\u0026amp;id=10239\u0026amp;lvl=3\u0026amp;lin=f\u0026amp;keep=1\u0026amp;srchmode=1\u0026amp;unlock\" rel=\"nofollow\"\u003eViruses\u003c/a\u003e (taxid=10239)) using \u003ca href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\"\u003eKraken2\u003c/a\u003e and \u003ca href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\u003eCentrifuge\u003c/a\u003e classification results\u003c/li\u003e\n\u003cli\u003eperform \u003cem\u003ede novo\u003c/em\u003e assembly with [Unicycler] and [Shovill] on the taxonomic classification filtered reads\u003c/li\u003e\n\u003cli\u003esearch all contig sequences using NCBI nucleotide \u003ca href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\"\u003eBLAST\u003c/a\u003e against a database of your choice (we recommend the version 5 NCBI nt DB)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e You will need to create/download databases for \u003ca href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\"\u003eKraken2\u003c/a\u003e, \u003ca href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\u003eCentrifuge\u003c/a\u003e and \u003ca href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\"\u003eBLAST\u003c/a\u003e in order to get the most out of this workflow!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pre-requisites\" class=\"anchor\" href=\"#pre-requisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePre-requisites\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-taxonomic-classification-for-kraken2-and-centrifuge\" class=\"anchor\" href=\"#taxonomic-classification-for-kraken2-and-centrifuge\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTaxonomic Classification for \u003ca href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\"\u003eKraken2\u003c/a\u003e and \u003ca href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\u003eCentrifuge\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eFor taxonomic classification with \u003ca href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\"\u003eKraken2\u003c/a\u003e and \u003ca href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\u003eCentrifuge\u003c/a\u003e, you will need to download (or build) databases for these programs so that you may use them within the \u003ccode\u003enf-villumina\u003c/code\u003e workflow.\u003c/p\u003e\n\u003cp\u003eYou can point to the Kraken2 and Centrifuge database with \u003ccode\u003eexport KRAKEN2_DB=/path/to/kraken2/database\u003c/code\u003e and \u003ccode\u003eexport CENTRIFUGE_DB=/path/to/centrifuge/database/prefix\u003c/code\u003e in your \u003ccode\u003e~/.bashrc\u003c/code\u003e so you don\u0027t need to specify it each time you run the workflow with \u003ccode\u003e--kraken2_db /path/to/kraken2/standard2 --centrifuge_db /path/to/centrifuge/nt-2018-03-03/nt\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-kraken2-dbs\" class=\"anchor\" href=\"#kraken2-dbs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKraken2 DBs\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eMiniKraken2_v2_8GB: (5.5GB) 8GB Kraken 2 Database built from the Refseq bacteria, archaea, and viral libraries and the GRCh38 human genome\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://monash.figshare.com/articles/GTDB_r89_54k/8956970\" rel=\"nofollow\"\u003eGTDB_r89_54k Kraken2 DBs\u003c/a\u003e: There are multiple Kraken2 DBs of various sizes available for download. For more info, see \u003ca href=\"https://github.com/rrwick/Metagenomics-Index-Correction\"\u003ehttps://github.com/rrwick/Metagenomics-Index-Correction\u003c/a\u003e and the manuscript: M\u00e9ric, Wick et al. (2019) Correcting index databases improves metagenomic studies. doi: \u003ca href=\"https://doi.org/10.1101/712166\" rel=\"nofollow\"\u003ehttps://doi.org/10.1101/712166\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-centrifuge-dbs\" class=\"anchor\" href=\"#centrifuge-dbs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCentrifuge DBs\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eNCBI nucleotide non-redundant sequences (2018-03-03) (64 GB)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://monash.figshare.com/ndownloader/files/16378439\" rel=\"nofollow\"\u003eGTDB_r89_54k Centrifuge DB (108 GB tar file)\u003c/a\u003e: For more info, see \u003ca href=\"https://github.com/rrwick/Metagenomics-Index-Correction\"\u003ehttps://github.com/rrwick/Metagenomics-Index-Correction\u003c/a\u003e and the manuscript: M\u00e9ric, Wick et al. (2019) Correcting index databases improves metagenomic studies. doi: \u003ca href=\"https://doi.org/10.1101/712166\" rel=\"nofollow\"\u003ehttps://doi.org/10.1101/712166\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-blast-dbs\" class=\"anchor\" href=\"#blast-dbs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\"\u003eBLAST\u003c/a\u003e DBs\u003c/h3\u003e\n\u003cp\u003eFor nf-villumina, you must have a version 5 BLAST DB with embedded taxonomic information installed, e.g. version 5 \u003ccode\u003ent\u003c/code\u003e DB (see \u003ca href=\"https://ftp.ncbi.nlm.nih.gov/blast/db/v5/\" rel=\"nofollow\"\u003ehttps://ftp.ncbi.nlm.nih.gov/blast/db/v5/\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003eYou can download pre-built \u003ca href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi\" rel=\"nofollow\"\u003eBLAST\u003c/a\u003e DBs like \u003ccode\u003ent\u003c/code\u003e and \u003ccode\u003enr\u003c/code\u003e from \u003ca href=\"https://ftp.ncbi.nlm.nih.gov/blast/db/\" rel=\"nofollow\"\u003ethe NCBI FTP site\u003c/a\u003e using the \u003ccode\u003eupdate_blastdb.pl\u003c/code\u003e script included with your install of BLAST+ to download and/or update your local BLAST databases.\u003c/p\u003e\n\u003cp\u003eShow all available databases:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ update_blastdb.pl --showall\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eDownload the BLASTDB version 5 \"nt\" database to your current directory decompressing files and deleting original compressed archives:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eupdate_blastdb.pl --blastdb_version 5 nt --decompress\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e For ease of use, all databases should be downloaded to the same directory (e.g. \u003ccode\u003e/opt/DB/blast\u003c/code\u003e set in \u003ccode\u003e$BLASTDB\u003c/code\u003e environment variable in your \u003ccode\u003e~/.bashrc\u003c/code\u003e)\u003c/p\u003e\n\u003cp\u003eCheck that your database has been downloaded properly and has taxids associated with the sequences contained within it:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ blastdbcheck -db nt -must_have_taxids -verbosity 3\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe peterk87/nf-villumina pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003epeterk87/nf-villumina was originally written by Peter Kruczkiewicz.\u003c/p\u003e\n\u003cp\u003eBootstrapped with \u003ca href=\"https://github.com/nf-core/tools\"\u003enf-core/tools\u003c/a\u003e \u003ccode\u003enf-core create\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThank you to the \u003ca href=\"https://github.com/nf-core/tools\"\u003enf-core/tools\u003c/a\u003e team for a great tool for bootstrapping creation of a production ready Nextflow workflows.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603317815.0
  },
  {
    "data_format": 2,
    "description": "automated benchmarking of recombination detection methods",
    "filenames": [
      "Singularity",
      "simg/Singularity.3seq"
    ],
    "full_name": "fredjaya/rec-bench",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-rec-bench\" class=\"anchor\" href=\"#rec-bench\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erec-bench\u003c/h1\u003e\n\u003cp\u003eAutomated benchmarking of recombination detection methods\u003c/p\u003e\n\u003cp\u003eEternally a WIP - many things are hardcoded\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eNextflow\nconda\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/fredjaya/rec-bench.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNextflow doesn\u0027t appear to create the conda environment properly. Create manually.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f environment.yml\nconda activate fredjaya-rec-bench-0.1.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote: conda processes currently hardcoded in \u003ccode\u003emain.nf\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003erec-bench\u003c/code\u003e has five modes that must be specified with \u003ccode\u003e--mode\u003c/code\u003e as follows:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e--mode sim\u003c/code\u003e\tGenerate simulation datasets\n\u003ccode\u003e--mode sim_v\u003c/code\u003e\tVisualise/summarise simulation outputs\n\u003ccode\u003e--mode div\u003c/code\u003e\tBenchmark recombination detection methods using simulated data\n\u003ccode\u003e--mode emp\u003c/code\u003e\tDetect recombination in empirical sequence alignments\n\u003ccode\u003e--mode class\u003c/code\u003e\tCalculate classification metrics\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[ ] Update readme\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617772984.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipes for bioinformatics software",
    "filenames": [
      "lyveset/1.1.4f/Singularity.lyveset.1.1.4f",
      "spades/3.13.0/Singularity.spades.3.13.0",
      "seqsero2/1.0.0/Singularity.seqsero2.1.0.0",
      "seqsero2/0.1/Singularity.seqsero2-0.1",
      "quast/5.0.0/Singularity.quast.5.0.0"
    ],
    "full_name": "kapsakcj/singularities",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularities\" class=\"anchor\" href=\"#singularities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003esingularities\u003c/h1\u003e\n\u003cp\u003eSingularity recipes for bioinformatics software. Build singularity images with these recipes (sudo required) or download/pull the images from \u003ca href=\"https://singularity-hub.org/collections/2778\" rel=\"nofollow\"\u003esingularity-hub.org\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repo is \u003cstrong\u003eWORK IN PROGRESS\u003c/strong\u003e. Feel free to try the recipes/Singularity builds, but they are \u003cstrong\u003enot tested deeply and are in no way guaranteed to work. Proceed at your own risk\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eIt is somewhat modeled after \u003ca href=\"https://github.com/StaPH-B/docker-builds\"\u003ehttps://github.com/StaPH-B/docker-builds\u003c/a\u003e , but with Singularity recipes instead.\u003c/p\u003e\n\u003cp\u003eSysadmins for High Performance Cluster computers almost always favor Sinularity over Docker :) so I\u0027m starting to learn the ways of Singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-available-singularity-images\" class=\"anchor\" href=\"#available-singularity-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailable Singularity images\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eSoftware\u003c/th\u003e\n\u003cth align=\"center\"\u003eVersion\u003c/th\u003e\n\u003cth align=\"center\"\u003eLink\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eSPAdes\u003c/td\u003e\n\u003ctd align=\"center\"\u003e3.13.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\u003ehttp://cab.spbu.ru/software/spades/\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eQUAST\u003c/td\u003e\n\u003ctd align=\"center\"\u003e5.0.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/ablab/quast\"\u003ehttps://github.com/ablab/quast\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eLyve-SET\u003c/td\u003e\n\u003ctd align=\"center\"\u003e1.1.4f\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/lskatz/lyve-SET\"\u003ehttps://github.com/lskatz/lyve-SET\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003eSeqSero2\u003c/td\u003e\n\u003ctd align=\"center\"\u003e0.1, 1.0.0\u003c/td\u003e\n\u003ctd align=\"center\"\u003e\u003ca href=\"https://github.com/denglab/SeqSero2\"\u003ehttps://github.com/denglab/SeqSero2\u003c/a\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThese Singularity images can be built if you have Singularity installed and \u003cstrong\u003ehave sudo/admin priveleges\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# build an image using a recipe (called Singularity in this example)\nsudo singularity build my-new-singularity-image.simg /path/to/Singularity\n\n# download the repo\ngit clone https://github.com/kapsakcj/singularities.git\n# another example using the SPAdes recipe\nsudo singularity build my-new-spades-3.13.0-image.simg /path/to/Singularity.spades.3.13.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese Singularity images are also available to download from singularity-hub.org if you \u003cstrong\u003edon\u0027t have sudo priveleges\u003c/strong\u003e (no build necessary!). The badge below is a link to the singularity-hub.org collection.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2778\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe name of the Singularity hub collection is \u003ccode\u003ekapsakcj/singularities\u003c/code\u003e and the tag is specified by the extenion of the Singularity recipe file. For example the recipe, \u003ccode\u003e/spades/3.13.0/Singularity.spades.3.13.0\u003c/code\u003e, can be downloaded like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# download an image like so, and name it whatever you want with the --name flag\nsingularity pull --name my-new-spades-3.13.0-image shub://kapsakcj/singularities:spades.3.13.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-useful-links-and-resources\" class=\"anchor\" href=\"#useful-links-and-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful links and resources\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity v2.6 User guide \u003ca href=\"https://www.sylabs.io/guides/2.6/user-guide/index.html\" rel=\"nofollow\"\u003ehttps://www.sylabs.io/guides/2.6/user-guide/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularityHub \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eExcellent tutorial on Singularity (using v2.5) from Sylabs, many other links within \u003ca href=\"https://github.com/Singularity-tutorial/Singularity-tutorial.github.io\"\u003ehttps://github.com/Singularity-tutorial/Singularity-tutorial.github.io\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eHow to build a container using Singularity Hub, linked to a github repo with Singularity recipes \u003ca href=\"https://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container\"\u003ehttps://github.com/singularityhub/singularityhub.github.io/wiki/Build-A-Container\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tipstricksthings-to-remember-about-singularity--many-from-jake-garfin--\" class=\"anchor\" href=\"#tipstricksthings-to-remember-about-singularity--many-from-jake-garfin--\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTips/Tricks/Things-to-remember about Singularity [ many from Jake Garfin :) ]\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity automatically brings your user \u0026amp; group into the container with you (ie. no \u003ccode\u003e-u $(id -u):$(id -g)\u003c/code\u003e needed like in Docker)\u003c/li\u003e\n\u003cli\u003eSingularity (by default) wants to mount your entire home directory inside the container as well. Use \u003ccode\u003e--cleanenv\u003c/code\u003e and \u003ccode\u003e--containall\u003c/code\u003e to keep things separate and bring in specific directories you want with \u003ccode\u003e-B /local-dir:/dir-in-container\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eDocker images converted to Singularity that want to write to system directories owned by root aren\u0027t going to work out of the box.\u003c/li\u003e\n\u003cli\u003eIf you are making a container with something that uses perl, add this to the recipe in the \u003ccode\u003e%environment\u003c/code\u003e section to prevent locale settings errors (see lyveset recipe)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e%environment\n    export LC_ALL=C\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTO-DO\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to: Singularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinks to docs for installing\u003c/li\u003e\n\u003cli\u003eHow to download an image from singularity hub\u003c/li\u003e\n\u003cli\u003eHow to download an image fromm dockerhub\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esingularity pull docker://staphb/skesa\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eHow to take a recipe and build locally (sudo required)\u003c/li\u003e\n\u003cli\u003eHow to run a Singularity container\n\u003cul\u003e\n\u003cli\u003eDifferent ways to run - \u003ccode\u003esingularity exec [...]\u003c/code\u003e or ./name-of-singularity.simg [...]\u003c/li\u003e\n\u003cli\u003eMounting DIRs - default way (mount entire \u003ccode\u003e$HOME\u003c/code\u003e DIR), or way to mount a specific DIR and not entire \u003ccode\u003e$HOME\u003c/code\u003e DIR\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCreate SingularityHub account\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elink to this repo\u003c/li\u003e\n\u003cli\u003ecreate autobuilds for each recipe\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1583250575.0
  },
  {
    "data_format": 2,
    "description": "Some benchmark singularity images for pycbc / pycbc inference",
    "filenames": [
      "Singularity"
    ],
    "full_name": "gwastro/pycbc_bench",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pycbc_bench\" class=\"anchor\" href=\"#pycbc_bench\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epycbc_bench\u003c/h1\u003e\n\u003cp\u003eSome benchmark singularity images for pycbc / pycbc inference\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-build-singularity-image\" class=\"anchor\" href=\"#build-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ebuild singularity image\u003c/h1\u003e\n\u003cp\u003esudo singularity build pycbcb.img Singularity\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-run-pycbc-inspiral\" class=\"anchor\" href=\"#run-pycbc-inspiral\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003erun pycbc inspiral\u003c/h1\u003e\n\u003cp\u003esingularity run --cleanenv --app inspiral pycbcb.img\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 8,
    "topics": [],
    "updated_at": 1559569839.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for Stacks (http://catchenlab.life.illinois.edu/stacks/)",
    "filenames": [
      "Singularity",
      "Singularity.2.0",
      "Singularity.2.1",
      "Singularity.2.2"
    ],
    "full_name": "powerPlant/stacks-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2270\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the Stacks software pipeline for building loci from short-read sequences\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1611661899.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for GROMACS (http://www.gromacs.org/)",
    "filenames": [
      "Singularity.2019.2",
      "Singularity.2018.3",
      "Singularity.2020.2",
      "Singularity.2019.4",
      "Singularity.2020.1",
      "Singularity.2018",
      "Singularity",
      "Singularity.2018.4",
      "Singularity.2019.6",
      "Singularity.2019.5",
      "Singularity.2019.1",
      "Singularity.2019.3",
      "Singularity.2020",
      "Singularity.2018.2",
      "Singularity.2019",
      "Singularity.2018.1",
      "Singularity.2018.5"
    ],
    "full_name": "powerPlant/gromacs-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2264\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for the GROMACS molecular dynamics package\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626174291.0
  },
  {
    "data_format": 2,
    "description": "Target driven visual navigation using deep reinforcement learning implemented in Pytorch",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jkulhanek/a2cat-vn-pytorch",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-target-driven-visual-navigation\" class=\"anchor\" href=\"#target-driven-visual-navigation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etarget-driven-visual-navigation\u003c/h1\u003e\n\u003cp\u003eTarget driven visual navigation using deep reinforcement learning implemented in Pytorch\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1607415822.0
  },
  {
    "data_format": 2,
    "description": "Synaptic Partner Detection in 3D Microscopy Volumes",
    "filenames": [
      "singularity/Singularity_py2.7.recipe",
      "singularity/Singularity_py3.recipe"
    ],
    "full_name": "funkelab/synful",
    "latest_release": "v1.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://zenodo.org/badge/latestdoi/166422086\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e31cfa1af0774be894dee535edc05a6536309dc42e048f576dc489a330b1f8ec/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3136363432323038362e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/166422086.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-synful\" class=\"anchor\" href=\"#synful\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSynful\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eSynful: A project for the automated detection of synaptic partners in Electron Microscopy brain data using U-Nets (type of Convolutional Neural Network).\u003c/p\u003e\n\u003cp\u003eThis repository provides train and predict scripts for synaptic partner detection. For more details, see our \u003ca href=\"https://www.biorxiv.org/content/10.1101/2019.12.12.874172v1\" rel=\"nofollow\"\u003ebioRxiv preprint\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe used the method to predict 244 Million synaptic partners in the full adult fly brain (FAFB) dataset.\nPlease see \u003ca href=\"https://github.com/funkelab/synful_fafb\"\u003ehttps://github.com/funkelab/synful_fafb\u003c/a\u003e for data dissemination and benchmark datasets.\u003c/p\u003e\n\u003cp\u003ePlease don\u0027t hesitate to open\nan issue or write us an email (\u003ca href=\"mailto:buhmannj@janelia.hhmi.org\"\u003eJulia\nBuhmann\u003c/a\u003e or \u003ca href=\"mailto:funkej@janelia.hhmi.org\"\u003eJan\nFunke\u003c/a\u003e) if you have any questions!\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[x] Add train scripts\u003c/li\u003e\n\u003cli\u003e[x] Add inference scripts\u003c/li\u003e\n\u003cli\u003e[x] Add download links for pretrained models\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-method\" class=\"anchor\" href=\"#method\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMethod\u003c/h2\u003e\n\u003cp\u003eThe pipeline processes 3D raw data in two steps into synaptic partners:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003einference of a) \u003ccode\u003esyn_indicator_mask\u003c/code\u003e (postsynaptic locations) and b) \u003ccode\u003edirection_vector\u003c/code\u003e (vector pointing from postsynaptic location to its presynaptic partner)\u003c/li\u003e\n\u003cli\u003esynapse extraction: a) locations extractions based on \u003ccode\u003esyn_indicator_mask\u003c/code\u003e and b) finding presynaptic partner based on \u003ccode\u003edirection_vector\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"docs/_static/method_overview.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/_static/method_overview.png\" alt=\"method_figure\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-system-requirements\" class=\"anchor\" href=\"#system-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSystem Requirements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eHardware requirements\n\u003cul\u003e\n\u003cli\u003etraining and prediction requires at least one GPU with sufficient memory (12 GB)\u003c/li\u003e\n\u003cli\u003eFor instance, we mostly used \u003ccode\u003eGeForce GTX TITAN X 12 GB\u003c/code\u003e for our project\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSoftware requirements\n\u003cul\u003e\n\u003cli\u003eSoftware has been tested on Linux (Ubuntu 16.04)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-guide\" class=\"anchor\" href=\"#installation-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation Guide\u003c/h2\u003e\n\u003cp\u003efrom source (creating a conda env is optional, but recommended).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eClone this repository.\u003c/li\u003e\n\u003cli\u003eIn a terminal:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda create -n \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003econda_env_name\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e python=3.6\n\u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e activate \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003econda_env_name\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e synful\npip install -r requirements.txt\npython setup.py install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you are interested in using the package for training and prediction, additionally add tensorflow and funlib.learn.tensorflow to your conda env:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda install tensorflow-gpu=1.14 cudatoolkit=10.0\npip install git+git://github.com/funkelab/funlib.learn.tensorflow@0712fee6b6c083c6bfc86e76f475b2e40b3c64f2\n\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-install-time\" class=\"anchor\" href=\"#install-time\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall time\u003c/h4\u003e\n\u003cp\u003eInstallation should take around 5 mins (including 3 mins for the tensorflow installation).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-training\" class=\"anchor\" href=\"#training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining\u003c/h2\u003e\n\u003cp\u003eTraining scripts are found in\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etrain/\u0026lt;setup\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003ccode\u003e\u0026lt;setup\u0026gt;\u003c/code\u003e is the name of a particular network configuration.\nIn such a  directory, you will find two files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003egenerate_network.py\u003c/code\u003e (generates a tensorflow network based on the parameter.json file in the same directoy)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etrain.py\u003c/code\u003e (starts training)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo get started, have a look at the train script in \u003ca href=\"train/setup01\"\u003etrain/setup01/train.py\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo start training:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython generate_network.py parameter.json\npython train.py parameter.json\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003esetup01: parameter.json is set to train a network on post-synaptic sites (single-task network)\u003c/li\u003e\n\u003cli\u003esetup02: parameter.json is set to train on direction vectors (single-task network)\u003c/li\u003e\n\u003cli\u003esetup03: parameter.json is set to train on both post-synaptic sites and direction vectors (multi-task network)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-details-on-hyperparameters\" class=\"anchor\" href=\"#details-on-hyperparameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetails on hyperparameters\u003c/h4\u003e\n\u003cp\u003eWhen training a network, you can set following hyperparameters in \u003ccode\u003escripts/train/\u0026lt;setup01/02/03\u0026gt;/parameter.json\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eParameters to set the architecture of the network (also see \u003ca href=\"https://github.com/funkelab/funlib.learn.tensorflow/blob/master/funlib/learn/tensorflow/models/unet.py#L506\"\u003edoc\u003c/a\u003e where we create the U-Net)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003einput_size\u003c/code\u003e: the dimensions of the cube that is used as input (called a mini-batch)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003edownsample_factor\u003c/code\u003e = [[1, 3, 3], [1, 3, 3], [3, 3, 3]] creates a U-Net with four resolution levels\n\u003cul\u003e\n\u003cli\u003ethe first one being the original resolution, the second one with downsampled feature maps with factos [1, 3, 3] etc.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003efmap_num\u003c/code\u003e: Number of feature maps in the first layer (we used 4 in the paper)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003efmap_inc_factor\u003c/code\u003e: In each layer, we use \u003ccode\u003efmap_inc_factor\u003c/code\u003e to increase our number of feature maps (we used 5 and 12 in the paper)\n\u003cul\u003e\n\u003cli\u003eEg. if we have \u003ccode\u003efmap_num = 4\u003c/code\u003e and \u003ccode\u003efmap_inc_factor = 5\u003c/code\u003e , we have 20 in our first layer, 100 in our second layer ...\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eunet_model\u003c/code\u003e: vanilla, or dh_unet; vanille=single-task network, dh_unet=multitask network with two different upsampling paths\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTraining parameters\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003elearning_rate\u003c/code\u003e: we used the AdamOptimizer across all experiments, with beta1=0.95,beta2=0.999,epsilon=1e-8\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eST / MT parameters\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eloss_comb_type\u003c/code\u003e: in a multi-task setting, how to combine the two different losses\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003em_loss_scale\u003c/code\u003e : loss weight for post-synaptic mask\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ed_loss_scale\u003c/code\u003e : loss weight for direction vector field\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBalancing parameters needed to account for sparsity of synaptic sites\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ereject_probability\u003c/code\u003e : 0.95 - p_rej in paper --\u0026gt; reject empty mini-batches with probability \u003ccode\u003ereject_probability\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eclip_range\u003c/code\u003e : the loss is scaled with the inverse class frequency ratio of foreground-and background voxels, clipping at \u003ccode\u003eclip_range\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-training-runtime\" class=\"anchor\" href=\"#training-runtime\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining runtime\u003c/h4\u003e\n\u003cp\u003eTraining takes between 3 and 10 days (depending on the size of the network), but you should see reasonable results within a day (after 90k iterations).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-monitoring-training\" class=\"anchor\" href=\"#monitoring-training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMonitoring Training\u003c/h3\u003e\n\u003cp\u003eTo visualize snapshots that are produced during training use this \u003ca href=\"scripts/visualization/visualize_snapshot.py\"\u003escript\u003c/a\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -i visualize_snapshot.py 300001 setup01\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ein order to load iteration \u003ccode\u003e300001\u003c/code\u003e of training setup \u003ccode\u003esetup01\u003c/code\u003e (use -1 to indicate most recent snapshot)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-inference\" class=\"anchor\" href=\"#inference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInference\u003c/h2\u003e\n\u003cp\u003eOnce you trained a network, you can use this script to run inference:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd scripts/predict/\npython predict_blockwise.py predict_template.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAdapt following parameters in the configfile \u0026lt;scripts/predict/predict_template.json\u0026gt;:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003edb_host\u003c/code\u003e --\u0026gt; Put here the name of your running mongodb server (this is used to track which chunks are processed)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eraw_file\u003c/code\u003e --\u0026gt; Put here the filepath of your raw data (as an example you can use the CREMI data that you can download from \u003ca href=\"http://www.cremi.org\" rel=\"nofollow\"\u003ewww.cremi.org\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor a full list of parameters and explanation, see: \u0026lt;scripts/predict/predict_blockwise.py\u0026gt;.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-inference-runtime\" class=\"anchor\" href=\"#inference-runtime\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInference runtime\u003c/h4\u003e\n\u003cp\u003eProcessing a CREMI cube (5 microns X 5 microns x 5 microns) takes ~4 minutes on a single GPU.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pretrained-models\" class=\"anchor\" href=\"#pretrained-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePretrained Models\u003c/h2\u003e\n\u003cp\u003eWe provide pretrained models, that we discuss in detail in our \u003ca href=\"https://www.biorxiv.org/content/10.1101/2019.12.12.874172v2\" rel=\"nofollow\"\u003ebioRxiv preprint\u003c/a\u003e. You will find the results of our gridsearch and the parameters that we used in Figure 3 \u003ccode\u003eValidation results on CREMI dataset\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eWe provide four models that you can download from \u003ca href=\"https://www.dropbox.com/s/301382766164ism/pretrained.zip?dl=0\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePlease extract the zip file into \u0026lt;scripts/train/\u0026gt; of this repository, this will add for each model a setup directory with the necassary config files, tensorflow checkpoint and predict script.\u003c/p\u003e\n\u003cp\u003eFor instance for \u003ccode\u003ep_setup52\u003c/code\u003e (marked orange in Figure 3, one of the best performing models), you will get all relevant files in \u0026lt;scripts/train/p_setup52\u0026gt;.\nTo run inference, you have to change the setup parameter in the predict config file to \u003ccode\u003ep_setup52\u003c/code\u003e and proceed according to \u003ca href=\"#Inference\"\u003einference section\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-details-about-the-provided-models\" class=\"anchor\" href=\"#details-about-the-provided-models\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetails about the provided models\u003c/h4\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003esetup\u003c/th\u003e\n\u003cth\u003especs\u003c/th\u003e\n\u003cth\u003ef-score with seg\u003c/th\u003e\n\u003cth\u003ef-score without\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ep_setup52 (+p_setup10)\u003c/td\u003e\n\u003ctd\u003ebig, curriculum, CE, ST\u003c/td\u003e\n\u003ctd\u003e0.76\u003c/td\u003e\n\u003ctd\u003e0.74\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ep_setup51\u003c/td\u003e\n\u003ctd\u003ebig, curriculum, CE, MT_2\u003c/td\u003e\n\u003ctd\u003e0.76\u003c/td\u003e\n\u003ctd\u003e0.73\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ep_setup54 (+p_setup05)\u003c/td\u003e\n\u003ctd\u003esmall, curriculum, MSE, ST\u003c/td\u003e\n\u003ctd\u003e0.76\u003c/td\u003e\n\u003ctd\u003e0.7\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ep_setup45 (+p_setup05)\u003c/td\u003e\n\u003ctd\u003esmall, standard, MSE, MT2\u003c/td\u003e\n\u003ctd\u003e0.73\u003c/td\u003e\n\u003ctd\u003e0.68\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eNote, that for the models that have an underlying ST architecture we also indicate the setup for the corresponding direction-vector-models (p_setup05+p_setup10).\nIf you want to use the model with highest accuracy, pick \u003ccode\u003ep_setup52\u003c/code\u003e; If you want to use a model that gives reasonnable results, but also has fast inference runtime, pick \u003ccode\u003ep_setup54\u003c/code\u003e.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1616581090.0
  },
  {
    "data_format": 2,
    "description": "uresnet based deep neutral network for the segmentation of high resolution cryo-EM tomographs",
    "filenames": [
      "Singularity"
    ],
    "full_name": "yee379/uresnet-tomo-seg",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-uresnet-tomo-seg\" class=\"anchor\" href=\"#uresnet-tomo-seg\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003euresnet-tomo-seg\u003c/h1\u003e\n\u003cp\u003euresnet based deep neutral network for the segmentation of high resolution cryo-EM tomographs\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1577150888.0
  },
  {
    "data_format": 2,
    "description": "Sentinel 2 ARD processor",
    "filenames": [
      "mpi-base/Singularity",
      "base/Singularity"
    ],
    "full_name": "jncc/s2-ard-processor",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-s2-ard-processor\" class=\"anchor\" href=\"#s2-ard-processor\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eS2 ARD Processor\u003c/h1\u003e\n\u003cp\u003eDocker based sentinel 2 Analysis ready production system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-base\" class=\"anchor\" href=\"#base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBase\u003c/h2\u003e\n\u003cp\u003eA base docker image packaging Dr Pete Buntings Python Atmospheric and Radiometric Correction of Satellite Imagery (ARCSI) software (\u003ca href=\"https://www.arcsi.remotesensing.info/\" rel=\"nofollow\"\u003ehttps://www.arcsi.remotesensing.info/\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eBased on the official ContinuumIO Miniconda3 release with python 3.5, base package contains a minimal installaition of ARCSI and its dependencies using the conda package manger, correct as of version 3.1.6 (conda reporting 3.6.1).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build-or-pull-arcsi-base\" class=\"anchor\" href=\"#build-or-pull-arcsi-base\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild or Pull arcsi-base\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-build-image\" class=\"anchor\" href=\"#build-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild image\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003edocker build -t jncc/arcsi-base ./base/\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOR\u003c/strong\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-pull-image-direction-from-docker-hub\" class=\"anchor\" href=\"#pull-image-direction-from-docker-hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePull Image direction from docker hub\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003edocker pull jncc/arcsi-base\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-run-image-interactively\" class=\"anchor\" href=\"#run-image-interactively\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun image interactively\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003edocker run -i -v \u0026lt;local mount point\u0026gt;:/data -t jncc/arcsi-base /bin/bash\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTo run a container and get help on ARCSI commandline options do:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003edocker run -t jncc/arcsi-base arcsi.py -h\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSee below under \"Docker example\" for a more detailed Sentinel-2 example.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker-example\" class=\"anchor\" href=\"#docker-example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker example\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker run -i -t -v \u003cspan class=\"pl-smi\"\u003e${local_data}\u003c/span\u003e:/data jncc/arcsi-base \\\n    arcsi.py -s sen2 --stats -f KEA --fullimgouts -p RAD SHARP SATURATE CLOUDS TOPOSHADOW STDSREF DOSAOTSGL METADATA FOOTPRINT \\\n    --interp near --outwkt /data/\u003cspan class=\"pl-smi\"\u003e${PATH_TO_OUTPUT_PROJ_WKT}\u003c/span\u003e --projabbv \u003cspan class=\"pl-smi\"\u003e${PROJ_ABBREVIATION}\u003c/span\u003e -t /data/tmp/ -o /data/output/ \\\n    --dem /data/\u003cspan class=\"pl-smi\"\u003e${PATH_TO_DEM}\u003c/span\u003e -i /data/inputs/\u003cspan class=\"pl-smi\"\u003e${SINGLE_INPUT_FILE}\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-see-also\" class=\"anchor\" href=\"#see-also\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSee also\u003c/h3\u003e\n\u003cp\u003eThanks to Markus Neteler (\u003ca href=\"https://github.com/mundialis/docker-arcsi\"\u003ehttps://github.com/mundialis/docker-arcsi\u003c/a\u003e), Edward P. Morris and Angelos Tzotsos for their work on the orignal ARCSI Dockerfile.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1625843131.0
  },
  {
    "data_format": 2,
    "description": "Recipes for singularity and docker containers used in CBRAIN",
    "filenames": [
      "ANTs/Singularity.ants_v2.1.0-gGIT-N",
      "QEEG/Singularity.qeeg.v1.0-gGit-S",
      "FreeSurfer/Singularity.FreeSurfer_v5.3",
      "dcm2nii/Singularity.dcm2nii_v4AUGUST2014",
      "FSL/Singularity.fsl_v6.0.1",
      "FSL/Singularity.fsl_v5.0.9"
    ],
    "full_name": "aces/cbrain-containers-recipes",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cbrain-containers-recipes\" class=\"anchor\" href=\"#cbrain-containers-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecbrain-containers-recipes\u003c/h1\u003e\n\u003cp\u003eRecipes for singularity and docker containers used in CBRAIN\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 9,
    "topics": [
      "singularity",
      "docker",
      "cbrain"
    ],
    "updated_at": 1568207307.0
  },
  {
    "data_format": 2,
    "description": "Lab pipelines using Snakemake + Singularity + SCIF",
    "filenames": [
      "chip-seq.scif/Singularity",
      "rna-seq-multisamples/Singularity"
    ],
    "full_name": "BennerLab/pipelines",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://sci-f.github.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a7912c9863e897576e5d434d91e359d254976266bee2f9b1405197941f940bdf/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66696c6573797374656d2d736369656e74696669632d626c75652e737667\" alt=\"scif\" data-canonical-src=\"https://img.shields.io/badge/filesystem-scientific-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://snakemake.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/93b6e9faa8e75932017af0ff2ca7db9493cc08e51c462e71143809db606cb04d/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d253345253344253230342e362e302d626c75652e737667\" alt=\"snakemake\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%3E%3D%204.6.0-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8ed7d71f6eadf7149f22c334c2b29e0479f493cfaced652052e122f59b5920be/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d253345253344253230322e342e322d626c75652e737667\" alt=\"singularity\" data-canonical-src=\"https://img.shields.io/badge/singularity-%3E%3D%202.4.2-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-svenner-lab-documentation\" class=\"anchor\" href=\"#svenner-lab-documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSvenner Lab Documentation\u003c/h1\u003e\n\u003cp\u003eThis repository contains the Svenner lab pipelines for various types of sequencing data. All pipelines are implemented in Snakemake and use the Singularity + Scientific Filesystem to create reproducible research environments.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipelines\" class=\"anchor\" href=\"#pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipelines\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/BennerLab/pipelines/tree/master/chip-seq.scif\"\u003eChIP-seq Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/BennerLab/pipelines/tree/master/rna-seq-multisamples\"\u003eRNA-seq Multisample Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1625275230.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.nipype-plus-jupyter-plus-seaborn"
    ],
    "full_name": "sajjadtorabian/singularity_recipes",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipes\" class=\"anchor\" href=\"#singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipes\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nipype-plus-jupyter\" class=\"anchor\" href=\"#nipype-plus-jupyter\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNipype Plus Jupyter\u003c/h2\u003e\n\u003cp\u003eWe first need to download the Docker layers. Normally this happens automatically with \u003ccode\u003esingularity build\u003c/code\u003e or \u003ccode\u003esingularity pull\u003c/code\u003e, but if you aren\u0027t using the development version 3.0 branch of Singularity that has a fixed bug with respect to whiteout files, you will have issue when you do these commands with nipype (note that it has whiteout files). What we did (because didn\u0027t feel like installing another version of Singularity) was to do a pull of nipype with the \u003ca href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003eSingularity Global Client\u003c/a\u003e that will download the fixed layers and then put them in the same cache that Singularity uses. Then we will have what we need :)  Here is how you can install and use the client:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ pip install sregistry\n$ sregistry pull nipype/nipype:latest\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow we want to debug the build and find the missing path! To do this, you can build a completely empty image to look around in. The recipe looks like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Singularity.nipype-plus-jupyter-empty\nBootstrap: docker\nFrom: nipype/nipype:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe above you can save to whatever file you want, we\u0027re calling ours \u003ccode\u003eSingularity.nipype-plus-jupyter-empty\u003c/code\u003e we can then build like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build npjup.simg Singularity.nipype-plus-jupyter-empty\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen we can shell inside and find locations of things.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell npjup.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe are able to see what will be exported in the environment at runtime, and then source it to add these locations to the path (so we can find executables there!)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecat /environment\n$ cat /.singularity.d/env/10-docker.sh \nexport PATH=\"/opt/conda/bin:/usr/lib/ants:/opt/afni:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\nexport LANG=\"en_US.UTF-8\"\nexport LC_ALL=\"C.UTF-8\"\nexport ND_ENTRYPOINT=\"/neurodocker/startup.sh\"\nexport MATLABCMD=\"/opt/mcr/v92/toolbox/matlab\"\nexport FORCE_SPMMCR=\"1\"\nexport LD_LIBRARY_PATH=\"/usr/lib/x86_64-linux-gnu:/opt/mcr/v92/runtime/glnxa64:/opt/mcr/v92/bin/glnxa64:/opt/mcr/v92/sys/os/glnxa64:\"\nexport FREESURFER_HOME=\"/opt/freesurfer\"\nexport ANTSPATH=\"/usr/lib/ants\"\nexport MKL_NUM_THREADS=\"1\"\nexport OMP_NUM_THREADS=\"1\"\nexport CONDA_DIR=\"/opt/conda\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThere we see the location of conda! Strange that it wasn\u0027t where we expected. Let\u0027s add to the path and then we have pip\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport PATH=/opt/conda/bin:$PATH\nwhich pip\n/opt/conda/bin/pip\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we can update the recipe \u003ca href=\"Singularity.nipype-plus-jupyter\"\u003eSingularity.nipype-plus-jupyter\u003c/a\u003e with our found pip.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBootstrap: docker\nFrom: nipype/nipype:latest\n\n%labels\n  Maintainer Sajjad\n  Version v1.0\n\n%post\n    export PATH=/opt/conda/bin:$PATH\n    pip install --upgrade pip\n    pip install jupyter\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd build the image\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build npjup.simg Singularity.nipype-plus-jupyter\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e:)\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1534761419.0
  },
  {
    "data_format": 2,
    "description": "recipes of Singularity",
    "filenames": [
      "Singularity.snpeff",
      "Singularity.blast-latest",
      "Singularity.cufflinks",
      "Singularity.blast-legacy",
      "Singularity.vcftools",
      "Singularity.bwa",
      "Singularity.gatk",
      "Singularity.samtools",
      "Singularity.trimmomatic",
      "Singularity.rooting_nj"
    ],
    "full_name": "CompBio-TDU-Japan/containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainers\u003c/h1\u003e\n\u003cp\u003erecipes of Singularity\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1548055469.0
  },
  {
    "data_format": 2,
    "description": "Nextflow pipeline for Illumina NGS demultiplexing",
    "filenames": [
      "containers/dos2unix-7.4.0/Singularity.dos2unix-7.4.0",
      "containers/multiqc-1.5/Singularity.multiqc-1.5",
      "containers/fastqc-0.11.7/Singularity.fastqc-0.11.7",
      "containers/report-r-3.4.3/Singularity.report-r-3.4.3",
      "containers/python-2.7/Singularity.python-2.7",
      "containers/bcl2fastq-2.17.1/Singularity.bcl2fastq-2.17.1"
    ],
    "full_name": "NYU-Molecular-Pathology/demux-nf",
    "latest_release": "19.04.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-demux-nf\" class=\"anchor\" href=\"#demux-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edemux-nf\u003c/h1\u003e\n\u003cp\u003eNextflow pipeline for demultiplexing Illumina Next-Gen sequencing data.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003eClone this repository:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/NYU-Molecular-Pathology/demux-nf.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deployment\" class=\"anchor\" href=\"#deployment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeployment\u003c/h2\u003e\n\u003cp\u003eThe included \u003ccode\u003edeploy\u003c/code\u003e recipe should be used to create a new directory for demultiplexing based on a currently existing sequencing run directory. Include arguments that describe the configuration for your sequencing run.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd demux-nf\nmake deploy RUNID=170809_NB501073_0019_AH5FFYBGX3 SAMPLESHEET=SampleSheet.csv SEQTYPE=Archer\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003earguments:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eRUNID\u003c/code\u003e: the identifier given to the run by the sequencer\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSAMPLESHEET\u003c/code\u003e: the samplesheet required for demultiplexing with \u003ccode\u003ebcl2fastq\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSEQTYPE\u003c/code\u003e: the type of sequencing; currently only \u003ccode\u003eArcher\u003c/code\u003e or \u003ccode\u003eNGS580\u003c/code\u003e are used\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSEQDIR\u003c/code\u003e: parent directory where the sequencer outputs its data (pre-configured for NYU server locations)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ePRODDIR\u003c/code\u003e: parent directory where demultiplexing output should be stored (pre-configured for NYU server locations)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis will first check that the specified run exists on the server before cloning into a new directory at the given production output location and configuring it for demultiplexing using the subsequent commands described here.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-workflow\" class=\"anchor\" href=\"#run-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun Workflow\u003c/h2\u003e\n\u003cp\u003eAssuming you used \u003ccode\u003emake deploy\u003c/code\u003e or \u003ccode\u003emake config\u003c/code\u003e to prepare your demultiplexing directory, the following command can be used to automatically run the workflow based on the pre-defined settings and settings from your current system.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExtra parameters to be passed to Nextflow can be supplied with the \u003ccode\u003eEP\u003c/code\u003e argument:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run EP=\u0027--samplesheet SampleSheet.csv --runDir /path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo submit the parent Nextflow pipeline as a job on the HPC cluster:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake submit\n\n# with a different submission queue:\nmake submit SUBQ=fn_long\n\n# with a different submission time:\nmake submit SUBQ=cpu_long SUBTIME=\u0027--time=6-00:00:00\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor alternative \u003ccode\u003erun\u003c/code\u003e methods, consult the \u003ccode\u003eMakefile\u003c/code\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h1\u003e\n\u003cp\u003eDemultiplexing metadata for the workflow can be provided through several methods, evaluated in the following order:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eparameters can be supplied directly to Nextflow via CLI\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run main.nf --runID 12345\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eif the file \u003ccode\u003econfig.json\u003c/code\u003e is present, non-\u003ccode\u003enull\u003c/code\u003e parameters will be retrieved\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e{\n    \"runDir\": \"/path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3\",\n    \"samplesheet\": \"SampleSheet.csv\",\n    \"runID\": \"170809_NB501073_0019_AH5FFYBGX3\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ethis file is generated automatically during the \u003ccode\u003edeploy\u003c/code\u003e step, using the included \u003ccode\u003econfig.py\u003c/code\u003e script\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ethe following items in the current directory will be used if present:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSampleSheet.csv\u003c/code\u003e: default samplesheet file\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003erunDir\u003c/code\u003e : default sequencing run source directory (can be a symlink)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003erunID.txt\u003c/code\u003e: a text file, the first line of which will be used as the run ID\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-extras\" class=\"anchor\" href=\"#extras\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExtras\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e(re)initialize configurations (overwrites old \u003ccode\u003econfig.json\u003c/code\u003e):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake config RUNDIR=/path/to/sequencer/data/170809_NB501073_0019_AH5FFYBGX3 SAMPLESHEET=SampleSheet.csv RUNID=170809_NB501073_0019_AH5FFYBGX3\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eupdate an existing directory to the latest version of this repo:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake update\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eclean up workflow intermediary files to save space (workflow cannot be resumed after this):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake finalize\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eclean up output from all old workflows (saves current workflow output):\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake clean\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003edelete the output from all workflows:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake clean-all\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003emark that the demultiplexing suceeded and the results passed QC for downstream analysis:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake passed\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003edeploy a new NGS580 analysis using the current results:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake deploy-NGS580\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003emake a \u0027deliverables\u0027 directory with just the results for samples for a specific client\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003emake deliverable CLIENT=somelab SHEET=list_of_clients_samples.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-software\" class=\"anchor\" href=\"#software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware\u003c/h1\u003e\n\u003cp\u003eRequired:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eJava 8 (Nextflow)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePython 2.7+\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGNU \u003ccode\u003emake\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOptional; must be installed to system or available with Singularity containers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ebcl2fastq\u003c/code\u003e version 2.17.1\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFastQC version 0.11.7\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eR (3.3.0+, with \u003ccode\u003eknitr\u003c/code\u003e and \u003ccode\u003ermarkdown\u003c/code\u003e libraries)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePandoc 1.13.1+\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 2,
    "topics": [
      "nextflow",
      "pipeline",
      "demultiplexing",
      "bcl2fastq"
    ],
    "updated_at": 1599538142.0
  },
  {
    "data_format": 2,
    "description": "Singularity containers with software installed via Spack",
    "filenames": [
      "Singularity.busco",
      "Singularity.gcc",
      "Singularity.spack",
      "Singularity.trinity",
      "Singularity.openmpi"
    ],
    "full_name": "ResearchIT/spack-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-centos7-reprozipfslbuild-centos5\" class=\"anchor\" href=\"#centos7-reprozipfslbuild-centos5\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecentos7-reprozip.fslbuild-centos5\u003c/h1\u003e\n\u003cp\u003ePreFreeSurfer-Converting Docker to Singularity (centos7-reprozip.fslbuild-centos5)\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 7,
    "topics": [
      "spack",
      "openmpi",
      "singularity"
    ],
    "updated_at": 1557449681.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "agladstein/SimPrily_update",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-simprily_update\" class=\"anchor\" href=\"#simprily_update\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimPrily_update\u003c/h1\u003e\n\u003cp\u003eCreated by Ariella Gladstein, based on \u003ca href=\"https://agladstein.github.io/SimPrily/index.html\" rel=\"nofollow\"\u003eSimPrily\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h2\u003e\n\u003cp\u003eSimPrily runs genome simulations with user defined parameters or parameters randomly generated by priors and computes genomic statistics on the simulation output.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eRun genome simulation with model defined by prior distributions of parameters and demographic model structure.\u003c/li\u003e\n\u003cli\u003eTake into account SNP array ascertainment bias by creating pseudo array based on priors of number of samples of discovery populations and allele frequency cut-off.\u003c/li\u003e\n\u003cli\u003eCalculate genomic summary statistics on simulated genomes and pseudo arrays.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThis is ideal for use with Approximate Bayesian Computation on whole genome or SNP array data.\u003c/p\u003e\n\u003cp\u003eUses c++ programs macs and GERMLINE. For more information on these programs, see:\u003cbr\u003e\n\u003ca href=\"https://github.com/gchen98/macs\"\u003ehttps://github.com/gchen98/macs\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://github.com/sgusev/GERMLINE\"\u003ehttps://github.com/sgusev/GERMLINE\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003ecd to the directory you want to work in,\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/agladstein/SimPrily.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-environment-set-up\" class=\"anchor\" href=\"#environment-set-up\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnvironment Set up\u003c/h4\u003e\n\u003cp\u003eIf using Vagrant (this is recommended if running on non-Linux OS):\u003c/p\u003e\n\u003cp\u003eStart Vagrant, ssh into Vagrant, cd to SimPrily directory:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003evagrant up\nvagrant ssh\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e /vagrant\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eInstall the virtual environment and install the requirements.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./setup/setup_env_vbox_2.7.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf not using Vagrant, just install the virtual environment and install the requirements:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./setup/setup_env_2.7.sh\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003ee.g. One Test simulation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython simprily.py -p examples/eg1/param_file_eg1_asc.txt -m examples/eg1/model_file_eg1_asc.csv -g genetic_map_b37/genetic_map_GRCh37_chr1.txt.macshs -a array_template/ill_650_test.bed -i 1 -o output_dir -v\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor quick help:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython simprily.py --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003esimprily.py\u003c/code\u003e takes 4 required arguments and 2 optional arguments, and help, verbose, and profile options.\u003c/p\u003e\n\u003cp\u003eRun as\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython simprily.py [-h] -p PARAM -m MODEL -i ID -o OUT [-g MAP] [-a ARRAY] [-v] [--profile]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-required\" class=\"anchor\" href=\"#required\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequired\u003c/h5\u003e\n\u003cp\u003e\u003ccode\u003e-p PARAM\u003c/code\u003e or \u003ccode\u003e--param PARAM\u003c/code\u003e = The location of the parameter file\u003cbr\u003e\n\u003ccode\u003e-m MODEL\u003c/code\u003e or \u003ccode\u003e--model MODEL\u003c/code\u003e = The location of the model file\u003cbr\u003e\n\u003ccode\u003e-i ID\u003c/code\u003e or \u003ccode\u003e--id ID\u003c/code\u003e = The unique identifier of the job\u003cbr\u003e\n\u003ccode\u003e-o OUT\u003c/code\u003e or \u003ccode\u003e--out OUT\u003c/code\u003e = The location of the output directory\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-optional\" class=\"anchor\" href=\"#optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptional\u003c/h5\u003e\n\u003cp\u003e\u003ccode\u003e-h\u003c/code\u003e or \u003ccode\u003e--help\u003c/code\u003e = shows a help message and exists\u003cbr\u003e\n\u003ccode\u003e-v\u003c/code\u003e = increase output verbosity. This includes 3 levels, \u003ccode\u003e-v\u003c/code\u003e, \u003ccode\u003e-vv\u003c/code\u003e, and \u003ccode\u003e-vvv\u003c/code\u003e\u003cbr\u003e\n\u003ccode\u003e--profile\u003c/code\u003e = Print a log file containing the time in seconds and memory use in Mb for main functions\u003cbr\u003e\n\u003ccode\u003e-g MAP\u003c/code\u003e or \u003ccode\u003e--map MAP\u003c/code\u003e = The location of the genetic map file\u003cbr\u003e\n\u003ccode\u003e-a ARRAY\u003c/code\u003e or \u003ccode\u003e--array ARRAY\u003c/code\u003e = The location of the array template file, in bed form\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput\u003c/h4\u003e\n\u003cp\u003eThree subdirectories are created in the directory specified in the \u003ccode\u003eoutput_dir\u003c/code\u003e argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eoutput_dir/results\noutput_dir/sim_data\noutput_dir/germline_out\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-intermediate-files\" class=\"anchor\" href=\"#intermediate-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntermediate files\u003c/h5\u003e\n\u003cp\u003eIntermediate files go to \u003ccode\u003eoutput_dir/sim_data\u003c/code\u003e and \u003ccode\u003eoutput_dir/germline_out\u003c/code\u003e.\u003cbr\u003e\n\u003ccode\u003eoutput_dir/sim_data\u003c/code\u003e contains PLINK formated .ped and .map files created from the pseudo array, which are necessary to run GERMLINE.\u003cbr\u003e\n\u003ccode\u003eoutput_dir/germline_out\u003c/code\u003e contains the GERMLINE .match output and .log. The .match contains all of the identified IBD segments.\u003cbr\u003e\nThese files are NOT automatically removed in python script, but are unnecessary once the job is complete.\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-results-files\" class=\"anchor\" href=\"#results-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults files\u003c/h5\u003e\n\u003cp\u003eOutput files go to \u003ccode\u003eoutput_dir/results\u003c/code\u003e.\u003cbr\u003e\n\u003ccode\u003eoutput_dir/results\u003c/code\u003e contains the parameter values used in the simulation and the summary statistics calculated from the simulation.\u003cbr\u003e\nThe first line is a header with the parameter names and summary statistics names.\nThe second line is the parameter values and summary statistics values.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-abc_update_wfpy\" class=\"anchor\" href=\"#abc_update_wfpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eABC_update_wf.py\u003c/h2\u003e\n\u003cp\u003eThis script creates all the necessary files for running ABC on simulations, and runs ABC.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCombines the simulated results into one file in \u003ccode\u003eobs{}/chr{}/ABC/results_combined.txt\u003c/code\u003e (unless the file already exists).\u003c/li\u003e\n\u003cli\u003eFor chr1 randomly picks one of the simulations to use as observed data,\nand for all other chromosomes uses the parameter values of the observed data from chr1 to simulate observed data,\nand create file in \u003ccode\u003eobs{}/chr{}/ABC/results_observed.txt\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eRun R to get PLS components.\u003c/li\u003e\n\u003cli\u003eUse ABCtoolbox to transform summary stats to PLS components for simulated and observed data.\u003c/li\u003e\n\u003cli\u003eUse ABCtoolbox to get posteriors of parameters.\u003c/li\u003e\n\u003cli\u003eCreate parameter file with posterior file.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-1\" class=\"anchor\" href=\"#usage-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eABC_update_wf.py path_sim param_file_name chrom obs\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhere,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003epath_sim\u003c/code\u003e is the path to simulation output (before \u003ccode\u003eobs{}\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eparam_file_name\u003c/code\u003e is the parameter file used to perform the simulations\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003echrom\u003c/code\u003e is the chromosome number\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eobs\u003c/code\u003e is the iteration with observed data.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-hpc-workflow\" class=\"anchor\" href=\"#hpc-workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPC Workflow\u003c/h2\u003e\n\u003cp\u003eFor chromosome 1 use \u003ccode\u003echeckque.sh\u003c/code\u003e to submit jobs to Ocelote.\u003c/p\u003e\n\u003cp\u003eArguments are \u003ccode\u003egoal_number\u003c/code\u003e, \u003ccode\u003emax_que\u003c/code\u003e, \u003ccode\u003echr\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e/home/u15/agladstein/SimPrily_update/update_test/checkque.sh 10000 500 1\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen, run ABC with \u003ccode\u003eABC_update_wf.py\u003c/code\u003e with the appropriate chromosome:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ersync -za SimPrily_update/ /xdisk/agladstein/SimPrily_update\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e /xdisk/agladstein/SimPrily_update\nqsub update_test/PBS/run_ABC_chr1.pbs\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen, run the simulations with the appropriate chromosome:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ersync -za SimPrily_update/ /xdisk/agladstein/SimPrily_update\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e /xdisk/agladstein/SimPrily_update\nqsub update_test/PBS/run_sims_update_chr2.pbs\u003c/pre\u003e\u003c/div\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-known-issues\" class=\"anchor\" href=\"#known-issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown Issues\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eIf exponential growth is large, macs simulation will not finish. (This is a macs bug).\u003c/li\u003e\n\u003cli\u003eIf the same id is used with the same output dir as a previous run, the .map file will be appended to.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1615073384.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for NWChem",
    "filenames": [
      "Singularity.6.6-openmpi"
    ],
    "full_name": "ResearchIT/nwchem",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-nwchem\" class=\"anchor\" href=\"#singularity-recipe-for-nwchem\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipe for NWChem\u003c/h1\u003e\n\u003cp\u003eThis repo contains recipes to run \u003ca href=\"http://www.nwchem-sw.org/index.php/Main_Page\" rel=\"nofollow\"\u003eNWChem\u003c/a\u003e\nwithin a \u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container, which can be built\nusing \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eVersions:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e6.6 - NWChem with OpenMPI installed via EPEL\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to Use:\u003c/h2\u003e\n\u003cp\u003eYou need to have openmpi v1 installed on your local machine (via yum or as a module).\nTesting was performed with openmpi 1.10.6.\u003c/p\u003e\n\u003cp\u003eRun example:\u003c/p\u003e\n\u003cp\u003empirun -np 2 singularity run shub://ResearchIT/nwchem:6.6-openmpi test.nw\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-alternative-method\" class=\"anchor\" href=\"#alternative-method\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAlternative method:\u003c/h2\u003e\n\u003cp\u003euse the provided bash wrapper and module file to use the nwchem singularity container like a standard module\n(this assumes you have a singularity/2.4 and openmpi/1 modules)\u003c/p\u003e\n\u003cp\u003ee.g.\u003c/p\u003e\n\u003cp\u003emodule load nwchem/6.6\u003c/p\u003e\n\u003cp\u003empirun -np 2 nwchem test.nw\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 6,
    "topics": [
      "nwchem",
      "singularity"
    ],
    "updated_at": 1551769652.0
  },
  {
    "data_format": 2,
    "description": "https://gitlab.kitware.com/paraview/paraview-superbuild.git",
    "filenames": [
      "Scripts/singularity/Singularity.egl",
      "Scripts/singularity/Singularity.osmesa"
    ],
    "full_name": "zenotech/paraview-superbuild",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"Documentation/img/paraview100.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"Documentation/img/paraview100.png\" alt=\"ParaView-Superbuild\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eParaView-Superbuild, henceforth referred to as \"superbuild\", is a project to\nbuild ParaView and its dependencies. ParaView itself can be easily built using\nCMake as long as the required external dependencies are available on the build\nmachine. However, ParaView\u0027s several external dependencies, e.g. Qt, CGNS,\nFFMPEG, etc. can be very tedious to build. Also, if you want to generate\nredistributable binaries, you need to take extra care when building and\npackaging these dependencies. To make our lives easier in supporting both these\nuse-cases, the superbuild project was born.\u003c/p\u003e\n\u003cp\u003eAlthough primarily designed to build the official ParaView binaries, the\nsuperbuild has since been regularly used to build and install ParaView\non various supercomputing systems.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-obtaining-the-source\" class=\"anchor\" href=\"#obtaining-the-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eObtaining the source\u003c/h1\u003e\n\u003cp\u003eTo obtain the superbuild source locally, clone this repository using\n\u003ca href=\"https://git-scm.org\" rel=\"nofollow\"\u003eGit\u003c/a\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ git clone --recursive https://gitlab.kitware.com/paraview/paraview-superbuild.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding\u003c/h1\u003e\n\u003cp\u003eThe superbuild can be built with a Makefiles or Ninja CMake generator. The IDE\ngenerators (Xcode and Visual Studio) are not supported.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cp\u003eThe superbuild tries to provide all of its own dependencies, but some tooling\nis assumed to be available on the host machine.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCompiler toolchain\n\u003cul\u003e\n\u003cli\u003eGCC 4.9 or newer\u003c/li\u003e\n\u003cli\u003eXcode 10 or newer (older is probably supported, but untested)\u003c/li\u003e\n\u003cli\u003eMSVC 2017 or newer\u003c/li\u003e\n\u003cli\u003eICC (minimum version unknown)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTools\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003epkg-config\u003c/code\u003e is used on non-Windows platforms to find dependencies in\nsome projects\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eninja\u003c/code\u003e (or \u003ccode\u003emake\u003c/code\u003e) for building\u003c/li\u003e\n\u003cli\u003ePython (if not built by the superbuild) for building packages\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-a-specific-version\" class=\"anchor\" href=\"#building-a-specific-version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a specific version\u003c/h2\u003e\n\u003cp\u003eThe superbuild project uses the same versioning scheme as ParaView,\nand gets tagged for every release of ParaView.  For example, to build\nParaView version 5.7.1, checkout the \u003ccode\u003ev5.7.0\u003c/code\u003e tag of ParaView and\nsuperbuild.\u003c/p\u003e\n\u003cp\u003eCurrently available tags are shown\n\u003ca href=\"https://gitlab.kitware.com/paraview/paraview-superbuild/-/tags\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo checkout a specific tag from the superbuild git repository:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ cd paraview-superbuild\n$ git fetch origin # ensure you have the latest state from the main repo\n$ git checkout v5.7.0 # replace `v5.7.0` with tag name of your choice\n$ git submodule update\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point, your superbuild has all of the \u003cem\u003erules\u003c/em\u003e that were used\nwhen building the selected version of ParaView. Also, note that it\u0027s\npossible to build a version of ParaView using a different superbuild\nversion.  For example, you could use superbuild \u003ccode\u003ev5.7.0\u003c/code\u003e, to build the\nlatest master (i.e., development) version of ParaView, or a custom\nbranch.  This is done by first checking out the superbuild for the\nappropriate version and then setting the CMake variables that affect\nwhich ParaView source is to be used.  There are several ways to\ncontrol how superbuild finds its source packages:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf you want to use git to checkout ParaView source (default), then set\n\u003ccode\u003eparaview_SOURCE_SELECTION\u003c/code\u003e to \u003ccode\u003egit\u003c/code\u003e, ensure \u003ccode\u003eparaview_GIT_REPOSITORY\u003c/code\u003e is\npointing to the ParaView git repository you want to clone (by default it is\nset to the offical ParaView repository) and then set the \u003ccode\u003eparaview_GIT_TAG\u003c/code\u003e\nto be a specific tagname or branch available for the selected git\nrepository. Use \u003ccode\u003emaster\u003c/code\u003e for latest development code, \u003ccode\u003ev5.7.0\u003c/code\u003e for the\n5.7.0 release, \u003ccode\u003erelease\u003c/code\u003e for latest stable release, or a specific ParaView\ncommit SHA. In this setup, when building the superbuild, it will clone and\ncheckout the appropriate revision from the ParaView git repository automatically.\u003c/li\u003e\n\u003cli\u003eInstead of letting superbuild do the cloning and updating of the ParaView\nsource, you can also manually check it out and keep it updated as needed.\nTo use this configuration, set \u003ccode\u003eparaview_SOURCE_SELECTION\u003c/code\u003e to \u003ccode\u003esource\u003c/code\u003e, and\nset \u003ccode\u003eparaview_SOURCE_DIR\u003c/code\u003e to point to a custom ParaView source tree. See \u0027offline\nbuilds\u0027 below for instructions to download needed dependency packages.\u003c/li\u003e\n\u003cli\u003eAnother option is to use a source tarball of a ParaView release. For that,\nset \u003ccode\u003eparaview_SOURCE_SELECTION\u003c/code\u003e to the version to build such as \u003ccode\u003e5.7.0\u003c/code\u003e.\nThe superbuild offers the lastest stable release as well as release\ncandidate in preparation for the release. This is the best way to build a\nreleased version of ParaView.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e If you switch to a superbuild version older than 5.2, the instructions\ndescribed on this page are not relevant since the superbuild was refactored and\nchanged considerably for 5.2. For older versions, refer to instructions on the\n\u003ca href=\"http://www.paraview.org/Wiki/index.php?title=ParaView/Superbuild\u0026amp;oldid=59804\" rel=\"nofollow\"\u003eWiki\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eALSO NOTE:\u003c/strong\u003e Since this README is expected to be updated for each version,\nonce you checkout a specfic version, you may want to refer to the README for\nthat specific version.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-incremental-builds\" class=\"anchor\" href=\"#incremental-builds\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIncremental builds\u003c/h2\u003e\n\u003cp\u003eThe superbuild is kind of na\u00efve for changes to project sources within the\nsuperbuild. This is due to the superbuild not tracking all source files for\neach project and instead only \"stamp files\" to indicate the steps performed.\u003c/p\u003e\n\u003cp\u003eWhen changing the source of a subproject, the best solution is to delete the\n\"stamp file\" for the build step of that project:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ rm superbuild/$project/stamp/$project-build\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand to rerun the superbuild\u0027s build step.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-projects-and-features\" class=\"anchor\" href=\"#projects-and-features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eProjects and Features\u003c/h2\u003e\n\u003cp\u003eThe superbuild contains multiple projects which may be used to enable\ndifferent features within the resulting ParaView build. Most projects involve\ndownloading and adding the feature to the resulting package, but there are a\nfew which are used just to enable features within ParaView itself.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eparaview\u003c/code\u003e project must be enabled to build ParaView.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eparaviewsdk\u003c/code\u003e project enables the building of a package which includes\nheaders and libraries suitable for developing against ParaView. It is only available\non Linux (at the moment).\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eparaviewweb\u003c/code\u003e project adds web services into the resulting package.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003eparaviewgettingstartedguide\u003c/code\u003e, and \u003ccode\u003eparaviewtutorialdata\u003c/code\u003e packages add\nstartup documentation and example data to the package.\u003c/p\u003e\n\u003cp\u003eParaView supports multiple rendering engines including \u003ccode\u003eegl\u003c/code\u003e, \u003ccode\u003emesa\u003c/code\u003e,\n\u003ccode\u003eosmesa\u003c/code\u003e, and \u003ccode\u003eqt5\u003c/code\u003e. All of these are incompatible with each other. If none of\nthese are chosen, a UI-less ParaView will be built (basically just\n\u003ccode\u003epvpython\u003c/code\u003e). On Windows and macOS, only the \u003ccode\u003eqt5\u003c/code\u003e rendering engine is\navailable.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003epython\u003c/code\u003e package is available to enable Python support in the package. In\naddition, the \u003ccode\u003ematplotlib\u003c/code\u003e and \u003ccode\u003enumpy\u003c/code\u003e packages are available.\u003c/p\u003e\n\u003cp\u003eThe following packages enable other features within ParaView:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eadios\u003c/code\u003e: Enable readers and writers for visualization data in the ADIOS\nfile format.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003elas\u003c/code\u003e: Enable reading the LAS file format\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecosmotools\u003c/code\u003e: Enables Cosmo file format readers and related filters and\nalgorithms.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003effmpeg\u003c/code\u003e: Video encoding library for macOS and Linux.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eospray\u003c/code\u003e: A ray tracing rendering backend from Intel.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esilo\u003c/code\u003e: Support reading the silo file format.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etbb\u003c/code\u003e: Improved parallel processing support within various VTK and\nParaView filters and algorithms.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evisitbridge\u003c/code\u003e: Enables readers for file formats provided from the VisIt\nproject.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evortexfinder2\u003c/code\u003e: A collection of tools to visualize and analyze vortices.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evrpn\u003c/code\u003e: Virtual reality support through the VRPN interface.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evtkm\u003c/code\u003e: VTK-m Accelerator Filters\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003exdmf3\u003c/code\u003e: A meta file format built on top of HDF5.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-offline-builds\" class=\"anchor\" href=\"#offline-builds\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOffline builds\u003c/h2\u003e\n\u003cp\u003eThe superbuild has a \u003ccode\u003edownload-all\u003c/code\u003e target that will download all of\nthe files from the network that are necessary for the currently\nconfigured build. By default, they are placed into the \u003ccode\u003edownloads\u003c/code\u003e\ndirectory of the build tree.  This superbuild-plus-downloads tree may\nthen be copied to a non-networked machine and pointed at using the\n\u003ccode\u003esuperbuild_download_location\u003c/code\u003e variable (or placed in the default\nlocation).\u003c/p\u003e\n\u003cp\u003eNote that the \u003ccode\u003envidiaoptix\u003c/code\u003e and \u003ccode\u003envidiamdl\u003c/code\u003e project sources are not available\nat their URLs in the superbuild outside of Kitware due to their sources being\nbehind click-wrapping. They may be manually downloaded from these web pages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003envidiaoptix\u003c/code\u003e: \u003ca href=\"https://developer.nvidia.com/designworks/optix/download\" rel=\"nofollow\"\u003ehttps://developer.nvidia.com/designworks/optix/download\u003c/a\u003e\nThough older versions are available here:\n\u003ca href=\"https://developer.nvidia.com/designworks/optix/downloads/legacy\" rel=\"nofollow\"\u003ehttps://developer.nvidia.com/designworks/optix/downloads/legacy\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003envidiamdl\u003c/code\u003e: \u003ca href=\"https://developer.nvidia.com/mdl-sdk\" rel=\"nofollow\"\u003ehttps://developer.nvidia.com/mdl-sdk\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-overriding-downloaded-archives\" class=\"anchor\" href=\"#overriding-downloaded-archives\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverriding downloaded archives\u003c/h3\u003e\n\u003cp\u003eOn rare occasions, you may want to replace a downloaded archive with a different\nversion. You may replace the archive with a newer version preserving its\nname, however, on doing so, the hash verification will most likely fail during\nthe build step. To skip the hash verification for archives that have been\nmanually changed, set the \u003ccode\u003exxx_SKIP_VERIFICATION\u003c/code\u003e option, where \u003ccode\u003exxx\u003c/code\u003e\nis the name of the project. \u003ccode\u003exxx_SKIP_VERIFICATION\u003c/code\u003e must be passed on command line\nwhen invoking CMake using \u003ccode\u003e-Dxxx_SKIP_VERIFICATION:BOOL=TRUE\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAlternatively, you can edit the \u003ccode\u003eversions.cmake\u003c/code\u003e files in the source repository\nand modify the \u003ccode\u003eURL_MDF5\u003c/code\u003e or \u003ccode\u003eURL_HASH\u003c/code\u003e values for the specific project with\nupdated hashes.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing\" class=\"anchor\" href=\"#installing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling\u003c/h2\u003e\n\u003cp\u003eThe superbuild supports the \u003ccode\u003einstall\u003c/code\u003e target by selecting a template package\nusing the \u003ccode\u003eSUPERBUILD_DEFAULT_INSTALL\u003c/code\u003e variable. The default and availability\ndepends on the platform and selected projects, but valid values for this\ninclude:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eparaview/ZIP\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparaview/DragNDrop\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparaview/TGZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparaview/TXZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparaviewsdk/TGZ\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eparaviewsdk/TXZ\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe CMake cache editors (\u003ccode\u003eccmake\u003c/code\u003e and \u003ccode\u003ecmake-gui\u003c/code\u003e) have dropdown options for\nthe supported options.\u003c/p\u003e\n\u003cp\u003eThe selected package logic will be used to install ParaView and its\ndependencies into \u003ccode\u003eCMAKE_INSTALL_PREFIX\u003c/code\u003e rather than being placed into a\npackage. For example, the \u003ccode\u003eDragNDrop\u003c/code\u003e generator creates \u003ccode\u003e.app\u003c/code\u003e bundles which\nwill be created whereas the \u003ccode\u003eTGZ\u003c/code\u003e, \u003ccode\u003eTXZ\u003c/code\u003e, and \u003ccode\u003eZIP\u003c/code\u003e generators use the standard\n\u003ccode\u003ebin/\u003c/code\u003e, \u003ccode\u003elib/\u003c/code\u003e, etc. directories.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-caveats\" class=\"anchor\" href=\"#caveats\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCaveats\u003c/h3\u003e\n\u003cp\u003eIf using the \u003ccode\u003egit\u003c/code\u003e source selection for ParaView, the build will rerun when\nusing the \u003ccode\u003einstall\u003c/code\u003e target due to limitations in the external project\nmechanisms and the way CPack works. There are two ways to avoid this:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe \u003ccode\u003eSUPERBUILD_OFFLINE_BUILD\u003c/code\u003e option may be set to \u003ccode\u003eON\u003c/code\u003e to unlink the git\nupdate step from the configure/build steps; or\u003c/li\u003e\n\u003cli\u003ethe initial build can just be run using the \u003ccode\u003einstall\u003c/code\u003e target instead of\nthe usual \u003ccode\u003emake \u0026amp;\u0026amp; make install\u003c/code\u003e pattern.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-external-plugins\" class=\"anchor\" href=\"#external-plugins\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExternal plugins\u003c/h2\u003e\n\u003cp\u003eThe superbuild supports building more plugins into ParaView using the\n\u003ccode\u003eparaviewexternalplugins\u003c/code\u003e project. As an example, to build two external\nplugins \u003ccode\u003ea\u003c/code\u003e and \u003ccode\u003eb\u003c/code\u003e, the following settings should be used:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eENABLE_paraviewexternalplugins:BOOL=ON\u003c/code\u003e: Enables building using external\nplugins.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eparaview_PLUGINS_EXTERNAL:STRING=a;b\u003c/code\u003e: The list of plugins to build.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eparaview_PLUGIN_a_PATH:PATH=/path/to/plugin/a\u003c/code\u003e: The path to plugin \u003ccode\u003ea\u003c/code\u003e\u0027s\nsource directory. It must contain a \u003ccode\u003eplugins.cmake\u003c/code\u003e to be picked up by\nParaView.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eparaview_PLUGIN_b_PATH:PATH=/path/to/plugin/b\u003c/code\u003e: Same as above, but for\nplugin \u003ccode\u003eb\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cmake-variables\" class=\"anchor\" href=\"#cmake-variables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCMake Variables\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-style-guide\" class=\"anchor\" href=\"#style-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStyle Guide\u003c/h3\u003e\n\u003cp\u003eNote that currently not all project and configuration variables follow this\nstyle guide but any new projects should use this convention while any\nexisting projects and configuration variables will transition to this over\ntime.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAll references to a given project name will be lowercase.\u003c/li\u003e\n\u003cli\u003eUnderscores will be used as word seperators in variable names.\u003c/li\u003e\n\u003cli\u003eAll project specific configuration variables will be lower-case project\nname followed by upper-case setting name.\nExamples include:\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003emesa_USE_SWR\u003c/code\u003e : Enable the OpenSWR driver for (OS)Mesa.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eospray_BUILD_ISA\u003c/code\u003e : Select the SIMD architecture used to build OSPray.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInternal variables used within a given project\u0027s projectname.cmake file\nwill be all lower-case.\u003c/li\u003e\n\u003cli\u003eMultiple versions:\n\u003cul\u003e\n\u003cli\u003eUse the \u003ccode\u003esuperbuild_set_selectable_source\u003c/code\u003e macro to allow multiple\nversions of a given project.\u003c/li\u003e\n\u003cli\u003eSpecify source selection versions as numeric, i.e. without any \"v\" or\n\"V\" prefix.\u003c/li\u003e\n\u003cli\u003eIf the project is going through a release candidate cycle, add the\navailable RCs as additional sources as they become availabe.  Once\na final release is made, replace all the RCs with the updated release.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build-variables\" class=\"anchor\" href=\"#build-variables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild Variables\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003esuperbuild_download_location\u003c/code\u003e (default \u003ccode\u003e${CMAKE_BINARY_DIR}/downloads\u003c/code\u003e):\nThe location to store downloaded source artifacts. Usually, it is changed\nso that it is preserved across a wipe of the build directory.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSUPERBUILD_PROJECT_PARALLELISM\u003c/code\u003e (default based on the number of available\nprocessors): When using a Makefiles generator, subproject builds use \u003ccode\u003e-j\u003c/code\u003e\nexplicitly with this number.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eENABLE_xxx\u003c/code\u003e (generally, default \u003ccode\u003eOFF\u003c/code\u003e): If selected, the \u003ccode\u003exxx\u003c/code\u003e project\nwill be built within the superbuild. See above for descriptions of the\nvarious projects. \u003ccode\u003eENABLE_\u003c/code\u003e flags are not shown for projects which must be\nenabled due to a project depending on it (e.g., \u003ccode\u003evisitbridge\u003c/code\u003e requires\n\u003ccode\u003eboost\u003c/code\u003e, so enabling \u003ccode\u003evisitbridge\u003c/code\u003e will hide the \u003ccode\u003eENABLE_boost\u003c/code\u003e option).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eUSE_SYSTEM_xxx\u003c/code\u003e (default \u003ccode\u003eOFF\u003c/code\u003e): If selected, the \u003ccode\u003exxx\u003c/code\u003e project from the\nbuild environment is used instead of building it within the superbuild.\nNot all projects support system copies (the flag is not available if so).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSUPERBUILD_DEBUG_CONFIGURE_STEPS\u003c/code\u003e (default \u003ccode\u003eOFF\u003c/code\u003e): If set, the superbuild\nwill log configure steps for each \u003ccode\u003exxx\u003c/code\u003e project into\n\u003ccode\u003esuperbuild/xxx/stamp/xxx-configure-*.log\u003c/code\u003e files.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCMAKE_BUILD_TYPE\u003c/code\u003e (default \u003ccode\u003eRelease\u003c/code\u003e): The build type to use for the\nbuild. Can be \u003ccode\u003eRelease\u003c/code\u003e, \u003ccode\u003eRelWithDebInfo\u003c/code\u003e, or (on not-Windows) \u003ccode\u003eDebug\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe following flags affect ParaView directly:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eparaview_SOURCE_SELECTION\u003c/code\u003e (default \u003ccode\u003e5.9.0\u003c/code\u003e): The source to use for\nParaView itself. The version numbers use the source tarballs from the\nwebsite for the release. The \u003ccode\u003esource\u003c/code\u003e selection uses the\n\u003ccode\u003eparaview_SOURCE_DIR\u003c/code\u003e variable to look at a checked out ParaView source\ndirectory. The \u003ccode\u003egit\u003c/code\u003e selection has the superbuild clone and builds a\ncheckout of ParaView from git repository controlled by the\n\u003ccode\u003eparaview_GIT_REPOSITORY\u003c/code\u003e and \u003ccode\u003eparaview_GIT_TAG\u003c/code\u003e variables. By default, the\n\u003ccode\u003emaster\u003c/code\u003e branch of the main repository is used.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: When using the \u003ccode\u003esource\u003c/code\u003e selection, incremental builds to the\nsuperbuild may not rebuild ParaView even if the source tree has changed.\nThis is because the superbuild is \"blind\" to the source tree other than\nits existence.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eCMAKE_BUILD_TYPE_paraview\u003c/code\u003e (default is the same as the superbuild):\nParaView may be built with a different build type (e.g., \u003ccode\u003eRelease\u003c/code\u003e vs.\n\u003ccode\u003eRelWithDebInfo\u003c/code\u003e) as the rest of the superbuild using this variable. In\naddition to \u003ccode\u003e\u0026lt;SAME\u0026gt;\u003c/code\u003e which uses \u003ccode\u003eCMAKE_BUILD_TYPE\u003c/code\u003e, any valid value for\n\u003ccode\u003eCMAKE_BUILD_TYPE\u003c/code\u003e is also valid.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eBUILD_SHARED_LIBS_paraview\u003c/code\u003e (default is the same as the superbuild):\nParaView may be built with a different selection for BUILD_SHARED_LIBS flag\nthan the rest of the superbuild using this variable. For example,\nto build ParaView static while building other projects in the superbuild\n(e.g. MPI, Python, etc.) as shared, set \u003ccode\u003eBUILD_SHARED_LIBS\u003c/code\u003e to \u003ccode\u003eON\u003c/code\u003e\nand \u003ccode\u003eBUILD_SHARED_LIBS_paraview\u003c/code\u003e to \u003ccode\u003eOFF\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ePARAVIEW_BUILD_WEB_DOCUMENTATION\u003c/code\u003e (default \u003ccode\u003eOFF\u003c/code\u003e): Have ParaView build\nits HTML documentation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003emesa_USE_SWR\u003c/code\u003e (default \u003ccode\u003eON\u003c/code\u003e): If \u003ccode\u003emesa\u003c/code\u003e is enabled, this enables\nIntel\u0027s software rasterization backend (x86 only).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ePARAVIEW_INITIALIZE_MPI_ON_CLIENT\u003c/code\u003e (default \u003ccode\u003eON\u003c/code\u003e): If \u003ccode\u003empi\u003c/code\u003e is enabled, this\nenables MPI to be initialized automatically when running the GUI or pvpython.\nSome readers use MPI IO and thus must have MPI initialized in order to be\nused so this is the default for general ease of use. For some MPI implementations,\na code that initializes MPI must be run with the appropriate mpi launcher\n(e.g. mpirun) which in this case it may be desirable to disable this option.\nNote that the \u003ccode\u003e--mpi\u003c/code\u003e or \u003ccode\u003e--no-mpi\u003c/code\u003e command line options to paraview and\npvpython can be used to override this option.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ePARAVIEW_EXTRA_CMAKE_ARGUMENTS\u003c/code\u003e (default \u003ccode\u003e\"\"\u003c/code\u003e: Extra CMake arguments to\npass to ParaView\u0027s configure step. This can be used to set CMake variables\nfor the build that are otherwise not exposed in the superbuild itself.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ePARAVIEW_ENABLE_VRPLUGIN\u003c/code\u003e (default \u003ccode\u003eON\u003c/code\u003e): Enables the VRPlugin. If\n\u003ccode\u003evrpn\u003c/code\u003e is enabled, the VRPlugin will support input devices through a VRPN\nconnection. VRUI support is enabled unconditionally on Linux.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-paraview-editions\" class=\"anchor\" href=\"#paraview-editions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParaView editions\u003c/h4\u003e\n\u003cp\u003eA typical ParaView build includes several modules and dependencies. While these\nare necessary for a fully functional application, there are cases (e.g. in situ\nuse-cases) where a build with limited set of features is adequate. ParaView build supports\nthis using the \u003ccode\u003ePARAVIEW_BUILD_EDITION\u003c/code\u003e setting. Supported values for this setting are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eCORE\u003c/code\u003e: Build modules necessary for core ParaView functionality.\nThis does not include rendering.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eRENDERING\u003c/code\u003e: Build modules necessary for supporting rendering including views\nand representations. This includes everything in \u003ccode\u003eCORE\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCATALYST\u003c/code\u003e: Build all modules necessary for in situ use cases without\nrendering and optional components like NetCDF- and HDF5-based readers and\nwriters.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCATALYST_RENDERING\u003c/code\u003e: Same as \u003ccode\u003eCATALYST\u003c/code\u003e but with rendering supported added.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCANONICAL\u003c/code\u003e (default): Build modules necessary for standard ParaView build.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-packaging-variables\" class=\"anchor\" href=\"#packaging-variables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePackaging Variables\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ePARAVIEW_PACKAGE_SUFFIX\u003c/code\u003e (default based on selected options): The suffix\nfor the name generated by the package.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eparaview_PLUGINS_AUTOLOAD\u003c/code\u003e: List of plugins to autoload in the packaged\nParaView.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-packaging\" class=\"anchor\" href=\"#packaging\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePackaging\u003c/h1\u003e\n\u003cp\u003eThe packages may be built using the \u003ccode\u003ecpack-paraview\u003c/code\u003e tests via \u003ccode\u003ectest\u003c/code\u003e. The\neasiest way to build all available packages is to run \u003ccode\u003ectest -R cpack\u003c/code\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-learning-resources\" class=\"anchor\" href=\"#learning-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLearning Resources\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eGeneral information is available at the \u003ca href=\"http://www.paraview.org\" rel=\"nofollow\"\u003eParaView Homepage\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCommunity discussion takes place on the \u003ca href=\"http://www.paraview.org/mailing-lists/\" rel=\"nofollow\"\u003eParaView Mailing Lists\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCommercial \u003ca href=\"http://www.kitware.com/products/support.html\" rel=\"nofollow\"\u003esupport\u003c/a\u003e and \u003ca href=\"http://www.kitware.com/products/protraining.php\" rel=\"nofollow\"\u003etraining\u003c/a\u003e\nare available from \u003ca href=\"http://www.kitware.com/\" rel=\"nofollow\"\u003eKitware\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-reporting-bugs\" class=\"anchor\" href=\"#reporting-bugs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReporting Bugs\u003c/h1\u003e\n\u003cp\u003eIf you have found a bug:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eIf you have a patch, please read the \u003ca href=\"CONTRIBUTING.md\"\u003eCONTRIBUTING.md\u003c/a\u003e document.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOtherwise, please join one of the \u003ca href=\"http://www.paraview.org/mailing-lists/\" rel=\"nofollow\"\u003eParaView Mailing Lists\u003c/a\u003e and ask\nabout the expected and observed behaviors to determine if it is\nreally a bug.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFinally, if the issue is not resolved by the above steps, open\nan entry in the \u003ca href=\"http://www.paraview.org/Bug\" rel=\"nofollow\"\u003eParaView Issue Tracker\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h1\u003e\n\u003cp\u003eLike ParaView, ParaView-Superbuild is distributed under the OSI-approved BSD\n3-clause License. See \u003ca href=\"Copyright.txt\"\u003eCopyright.txt\u003c/a\u003e for details. For additional licenses,\nrefer to \u003ca href=\"http://www.paraview.org/paraview-license/\" rel=\"nofollow\"\u003eParaView Licenses\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h1\u003e\n\u003cp\u003eSee \u003ca href=\"CONTRIBUTING.md\"\u003eCONTRIBUTING.md\u003c/a\u003e for instructions to contribute.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 5,
    "topics": [
      "paraview-superbuild",
      "cmake",
      "superbuild"
    ],
    "updated_at": 1612294938.0
  },
  {
    "data_format": 2,
    "description": "modified version of nicMSlesions",
    "filenames": [
      "Singularity"
    ],
    "full_name": "kbronik2017/nicpython36",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ms_cnn\" class=\"anchor\" href=\"#ms_cnn\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMS_CNN\u003c/h1\u003e\n\u003cp\u003e[This is a modified version of nicMSlesions (\u003ca href=\"https://github.com/NIC-VICOROB/nicMSlesions\"\u003ehttps://github.com/NIC-VICOROB/nicMSlesions\u003c/a\u003e)]\n\u003cbr\u003e\n\u003ca href=\"CNN.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"300\" src=\"CNN.jpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-this--version-support-additionally-the-following-functionalities\" class=\"anchor\" href=\"#this--version-support-additionally-the-following-functionalities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThis  version support additionally the following functionalities:\u003c/h1\u003e\n\u003cdl\u003e\n  \u003cdt\u003e(1) Runnable on a Mac system/computer\u003c/dt\u003e\n  \u003cdt\u003e(2) Cold start and warm start support:\u003c/dt\u003e\n  \u003cdd\u003e- Allowing to re-create the architecture of the model\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to use the saved weights of the model\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to use  the training configuration and avoiding to run preprocessing again\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to resume training exactly where it left off(interrupting the training is     \n    allowed throughout the training process)\u003c/dd\u003e\n  \u003cdd\u003e- Allowing to use pretrained model\u003c/dd\u003e\n  \u003cdt\u003e(3) Supporting Python 3\u003c/dt\u003e\n  \u003cdt\u003e(4) Integrated Tensorborad [to provide the measurements and visualisations of TensorFlow execution (to understand, debug, and optimisation of  the TensorFlow programs)]\u003c/dt\u003e\n  \u003cdt\u003e(5) Checking whether a file or directory is relevant for Training and Testing\u003c/dt\u003e \n  \u003cdt\u003e(6) Easy HPC (High Performance Computing) support\u003c/dt\u003e \n  \u003cdt\u003e(7) Bias correction of masks using FSL\u003c/dt\u003e\n  \u003cdt\u003e(8) Registration, moving all images to the Flair, T1 or Standard space\u003c/dt\u003e\n\u003c/dl\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"BR.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"500\" src=\"BR.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"note.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"100\" src=\"note.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n# Running the Program!\n\u003cp\u003eThis modified version can be run with or without a GUI (similar to original version)\u003c/p\u003e\n\u003cp\u003eAfter lunching the graphical user interface, user will need to provide necessary information to start training/testing as follows:\u003c/p\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"GUI_NM.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"500\" src=\"GUI_NM.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003ch1\u003e\u003c/h1\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\" class=\"anchor\" href=\"#running-the-program-on-the-hpc-cluster-using-nvidia-gpuswithout-any-additional-librarydependency-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the Program on the HPC cluster using NVIDIA GPUs(without any additional library/dependency installation):\u003c/h1\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"hpc.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"200\" src=\"hpc.jpeg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003cp\u003eFirst, user will need to be sure that \"singularity\"\n\u003ca href=\"https://singularity.lbl.gov/\" rel=\"nofollow\"\u003ehttps://singularity.lbl.gov/\u003c/a\u003e\nis available on local or remote machine.\u003c/p\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity pull docker://kbronik/ms_cnn_ucl:latest  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAfter running the above, a singularity image using docker hub (docker://kbronik/ms_cnn_ucl:latest) will be generated:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - path to singularity//..///ms_cnn_ucl_latest.sif  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFinally:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity run --nv   (path to singularity)//..///ms_cnn_ucl_latest.sif  python  (path to nicpython36)/nic_train_network_batch.py (or other nic-python code)\u003c/pre\u003e\u003c/div\u003e\n\u003cbr\u003e\n \u003cp\u003e\u003ca href=\"note_HPC.jpg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg height=\"120\" src=\"note_HPC.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \n\u003ch1\u003e\n\u003ca id=\"user-content-for-an-interactive-session\" class=\"anchor\" href=\"#for-an-interactive-session\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor an interactive session:\u003c/h1\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity shell   (path to singularity)//..///ms_cnn_ucl_latest.sif \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - \u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e activate idp\n  - python (path to nicpython36)/app.py\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-for-an-interactive-session-tensorflow-on-cpu-only\" class=\"anchor\" href=\"#for-an-interactive-session-tensorflow-on-cpu-only\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor an interactive session (TensorFlow on CPU only):\u003c/h1\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e  - singularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e  docker://kbronik/ms-ucl-cnn-cpu:CPU_Latest  python  (path to nicpython36)/app.py \u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [
      "convolutional-neural-networks",
      "nicmslesions",
      "nicmslesions-python3"
    ],
    "updated_at": 1587563855.0
  },
  {
    "data_format": 2,
    "description": "Run CRISPResso on genome editing experiments",
    "filenames": [
      "Singularity"
    ],
    "full_name": "czbiohub/nf-core-crisprvar",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-corecrisprvar\" class=\"anchor\" href=\"#nf-corecrisprvar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-core/crisprvar\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eRun CRISPResso on genome editing experiments\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nf-core/crisprvar\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8bfb8f49f69e465c023fb291cefc2ccbea97c9a7cc04377260fa6052d9370b28/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f6372697370727661722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nf-core/crisprvar.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/crisprvar\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5e5f9f1b9479b9d19c758902e0a06e81ab060fa6a9207a5e935aee26edc728ac/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f6372697370727661722e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/crisprvar.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe nf-core/crisprvar pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1579658690.0
  },
  {
    "data_format": 2,
    "description": "Singularity image to serve as base for all project images.  Defaults to starting up RStudio with an auto-selected port and password ",
    "filenames": [
      "Singularity.3.6.0",
      "Singularity.4.0.2",
      "Singularity.3.6.1",
      "Singularity.4.0.3",
      "Singularity.mro.4.0.3"
    ],
    "full_name": "granek/singularity-rstudio-base",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3197\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis Singularity image is intended to serve as base for all project images.\u003c/p\u003e\n\u003cp\u003eBy default it starts up RStudio with an auto-selected port and password\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-running-singularity-image\" class=\"anchor\" href=\"#running-singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Singularity Image\u003c/h1\u003e\n\u003cp\u003eRun a singularity-rstudio-base container with \u003ccode\u003esingularity run shub://granek/singularity-rstudio-base\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tmp-issues\" class=\"anchor\" href=\"#tmp-issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e/tmp issues\u003c/h2\u003e\n\u003cp\u003eIt is recommended to do one of the following when running this image. There is no need to do both:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSet \"mount tmp = no\" in \u003ccode\u003e/etc/singularity/singularity.conf\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eIf #1 is not an option, the following command can be used to bind mount \u003ccode\u003e/tmp\u003c/code\u003e in the container to a \"private\" tmp directory:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eSINGTMP=\"/tmp/${USER}_$$_tmp\"; mkdir -p $SINGTMP; singularity run --bind $SINGTMP:/tmp shub://granek/singularity-rstudio-base\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tmp-issues-tldr\" class=\"anchor\" href=\"#tmp-issues-tldr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e/tmp issues TLDR\u003c/h3\u003e\n\u003cp\u003eIf a second user tries on the same server tries to run an RStudio container they will have permission issues with \u003ccode\u003e/tmp/rstudio-server\u003c/code\u003e, which will be owned by the user who first ran an RStudio container.\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1624649294.0
  },
  {
    "data_format": 2,
    "description": "Demultiplex sequencing experiments with Nextflow",
    "filenames": [
      "Singularity"
    ],
    "full_name": "czbiohub/demux",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nf-coredemux\" class=\"anchor\" href=\"#nf-coredemux\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enf-core/demux\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eDemultiplex sequencing experiments\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nf-core/demux\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/21a4d7a0262a3b096d2521e08bc4e18bf7340ea1b97ea36ca247a7d06ccd04b1/68747470733a2f2f7472617669732d63692e6f72672f6e662d636f72652f64656d75782e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nf-core/demux.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/67e0b26cefcc362513a5e2e7613f4638251b0ab5f029eba762be3d49a716c325/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d254532253839254135302e33322e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.32.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/nfcore/demux\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8b87efd0f3651289c43d7f37a2a1092964f35f8501ccbcbe7ef15bfc1b38ae67/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f6e66636f72652f64656d75782e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/nfcore/demux.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a3255d37d1c67555563853bd8243caf50984d85343da02fb7b297b21a1076c45/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d617661696c61626c652d3745344337342e737667\" alt=\"Singularity Container available\" data-canonical-src=\"https://img.shields.io/badge/singularity-available-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h3\u003e\n\u003cp\u003eThe nf-core/demux pipeline comes with documentation about the pipeline, found in the \u003ccode\u003edocs/\u003c/code\u003e directory:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"docs/installation.md\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003ePipeline configuration\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/local.md\"\u003eLocal installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/configuration/adding_your_own.md\"\u003eAdding your own system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/usage.md\"\u003eRunning the pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/output.md\"\u003eOutput and how to interpret the results\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"docs/troubleshooting.md\"\u003eTroubleshooting\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1562208393.0
  },
  {
    "data_format": 2,
    "description": "Experimental Singularity container for MLCommons DeepCAM Climate Segmentation Benchmark",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ianjamesx/DeepCAM_Singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deepcam-singularity\" class=\"anchor\" href=\"#deepcam-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepCAM Singularity\u003c/h1\u003e\n\u003cp\u003eSingularity container for MLCommons DeepCAM Climate Segmentation Benchmark, reference implementation developed by Lawrence Berkeley National Laboratory\u003c/p\u003e\n\u003cp\u003eCurrently in progress\u003c/p\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1624406680.0
  },
  {
    "data_format": 2,
    "description": "An exact weighted model-counting framework based on algebraic decision diagrams and tensors.",
    "filenames": [
      "lg/Singularity",
      "dmc/Singularity"
    ],
    "full_name": "vardigroup/DPMC",
    "latest_release": "mc-2021",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-dpmc-dynamic-programming-for-model-counting\" class=\"anchor\" href=\"#dpmc-dynamic-programming-for-model-counting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDPMC (Dynamic Programming for Model Counting)\u003c/h1\u003e\n\u003cp\u003eDPMC computes weighted model counts of formulas in conjunctive normal form (CNF)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe DPMC framework runs in two phases:\n\u003cul\u003e\n\u003cli\u003ePlanning phase: \u003ca href=\"./lg\"\u003eLG\u003c/a\u003e or \u003ca href=\"./htb\"\u003eHTB\u003c/a\u003e constructs a join tree of a CNF formula\u003c/li\u003e\n\u003cli\u003eExecution phase: \u003ca href=\"./dmc\"\u003eDMC\u003c/a\u003e computes the model count of the formula using the join tree\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDevelopers:\n\u003cul\u003e\n\u003cli\u003eJeffrey Dudek\u003c/li\u003e\n\u003cli\u003eVu Phan\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-releases\" class=\"anchor\" href=\"#releases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/vardigroup/DPMC/releases\"\u003eReleases\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e2021/05/25: \u003ca href=\"https://github.com/vardigroup/DPMC/releases/tag/mc-2021\"\u003emc-2021\u003c/a\u003e \u003ca href=\"https://zenodo.org/badge/latestdoi/280443175\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a66989e99eb192ab9857e39b3f1e218d0f4b7bcd8b478436fdace72cf61b408c/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3238303434333137352e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/280443175.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"./mcc\"\u003eModel Counting Competition MC-2021\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2021/05/23: \u003ca href=\"https://github.com/vardigroup/DPMC/releases/tag/v2.0.0\"\u003ev2.0.0\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eSAT-2021 paper: \u003cstrong\u003eProCount: Weighted Projected Model Counting with Graded Project-Join Trees\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAuthors: Jeffrey M. Dudek, Vu H. N. Phan, Moshe Y. Vardi\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e2020/07/20: \u003ca href=\"https://github.com/vardigroup/DPMC/releases/tag/v1.0.0\"\u003ev1.0.0\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003eCP-2020 paper: \u003cstrong\u003e\u003ca href=\"https://arxiv.org/abs/2008.08748\" rel=\"nofollow\"\u003eDPMC: Weighted Model Counting by Dynamic Programming on Project-Join Trees\u003c/a\u003e\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eAuthors: Jeffrey M. Dudek, Vu H. N. Phan, Moshe Y. Vardi\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-example-files\" class=\"anchor\" href=\"#example-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"./examples\"\u003eExample files\u003c/a\u003e\n\u003c/h2\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgment\" class=\"anchor\" href=\"#acknowledgment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"./ACKNOWLEDGMENT.md\"\u003eAcknowledgment\u003c/a\u003e\n\u003c/h2\u003e\n",
    "stargazers_count": 1,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1623698161.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity.clockwork",
      "singularity/Singularity.preprocessing"
    ],
    "full_name": "Pathogen-Genomics-Cymru/tb-pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tb-pipeline\" class=\"anchor\" href=\"#tb-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTB Pipeline\u003c/h1\u003e\n\u003cp\u003eThis pipeline takes as input reads presumed to be from one of 10 mycobacterial genomes: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis. Input should be in the form of one directory containing pairs of fastq(.gz) or bam files.\u003c/p\u003e\n\u003cp\u003ePipeline cleans and QCs reads with fastp and FastQC, classifies with Kraken2 \u0026amp; Mykrobe, removes non-bacterial content, and - by alignment to any minority genomes - disambiguates mixtures of bacterial reads. Cleaned reads are aligned to either of the 10 supported genomes and variants called. Produces as output one directory per sample, containing cleaned fastqs, sorted, indexed BAM, VCF, and summary reports.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eRequires \u003ccode\u003eNXF_VER\u0026gt;=20.11.0-edge\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe workflow is designed to run with either docker \u003ccode\u003e-profile docker\u003c/code\u003e or singularity \u003ccode\u003e-profile singularity\u003c/code\u003e. Before running the workflow, the images will need to be built by running either \u003ccode\u003edocker/docker_build.sh\u003c/code\u003e or  \u003ccode\u003esingularity/singularity_build.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eE.g. to run the workflow:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eNXF_VER=20.11.0-edge nextflow run main.nf -profile singularity --filetype fastq --input_dir fq_dir --pattern \"*_R{1,2}.fastq.gz\" --unmix_myco yes \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\nNXF_VER=20.11.0-edge nextflow run main.nf -profile docker --filetype bam --input_dir bam_dir --unmix_myco no \\\n--output_dir . --kraken_db /path/to/database --bowtie2_index /path/to/index --bowtie_index_name hg19_1kgmaj\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-params\" class=\"anchor\" href=\"#params\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParams\u003c/h2\u003e\n\u003cp\u003eThe following parameters should be set in \u003ccode\u003enextflow.config\u003c/code\u003e or specified on the command line:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003einput_dir\u003c/strong\u003e\u003cbr\u003e\nDirectory containing fastq OR bam files\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efiletype\u003c/strong\u003e\u003cbr\u003e\nFile type in input_dir. Either \"fastq\" or \"bam\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003epattern\u003c/strong\u003e\u003cbr\u003e\nRegex to match fastq files in input_dir, e.g. \"*_R{1,2}.fq.gz\". Only mandatory if --filetype is \"fastq\"\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eoutput_dir\u003c/strong\u003e\u003cbr\u003e\nOutput directory for results\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eunmix_myco\u003c/strong\u003e\u003cbr\u003e\nDo you want to disambiguate mixed-mycobacterial samples by read alignment? Either \"yes\" or \"no\":\n\u003cul\u003e\n\u003cli\u003eIf \"yes\" workflow will remove reads mapping to any minority mycobacterial genomes but in doing so WILL ALMOST CERTAINLY ALSO reduce coverage of the principal species\u003c/li\u003e\n\u003cli\u003eIf \"no\" then mixed-mycobacterial samples will be left alone. Mixtures of mycobacteria + non-mycobacteria will still be disambiguated\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003especies\u003c/strong\u003e\u003cbr\u003e\nPrincipal species in each sample, assuming genus Mycobacterium. Default \u0027null\u0027. If parameter used, takes 1 of 10 values: abscessus, africanum, avium, bovis, chelonae, chimaera, fortuitum, intracellulare, kansasii, tuberculosis. Using this parameter will apply an additional sanity test to your sample\n\u003cul\u003e\n\u003cli\u003eIf you DO NOT use this parameter (default option), pipeline will determine principal species from the reads and consider any other species a contaminant\u003c/li\u003e\n\u003cli\u003eIf you DO use this parameter, pipeline will expect this to be the principal species. It will fail the sample if reads from this species are not actually the majority\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ekraken_db\u003c/strong\u003e\u003cbr\u003e\nDirectory containing \u003ccode\u003e*.k2d\u003c/code\u003e Kraken2 database files (k2_pluspf_16gb_20200919 recommended, obtain from \u003ca href=\"https://benlangmead.github.io/aws-indexes/k2\" rel=\"nofollow\"\u003ehttps://benlangmead.github.io/aws-indexes/k2\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie2_index\u003c/strong\u003e\u003cbr\u003e\nDirectory containing Bowtie2 index (obtain from \u003ca href=\"ftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\" rel=\"nofollow\"\u003eftp://ftp.ccb.jhu.edu/pub/data/bowtie2_indexes/hg19_1kgmaj_bt2.zip\u003c/a\u003e). The specified path should NOT include the index name\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebowtie_index_name\u003c/strong\u003e\u003cbr\u003e\nName of the bowtie index, e.g. hg19_1kgmaj\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cbr\u003e\n\u003cp\u003eFor more information on the parameters run \u003ccode\u003enextflow run main.nf --help\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe path to the singularity images can also be changed in the singularity profile in \u003ccode\u003enextflow.config\u003c/code\u003e. Default value is \u003ccode\u003e${baseDir}/singularity\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-stub-run\" class=\"anchor\" href=\"#stub-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStub-run\u003c/h2\u003e\n\u003cp\u003eTo test the stub run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eNXF_VER=20.11.0-edge nextflow run main.nf -stub -config testing.config\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checkpoints\" class=\"anchor\" href=\"#checkpoints\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheckpoints\u003c/h2\u003e\n\u003cp\u003eCheckpoints used throughout this workflow to fail a sample/issue warnings:\u003c/p\u003e\n\u003cp\u003eprocesses preprocessing:checkFqValidity or preprocessing:checkBamValidity\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e(Fail) If sample does not pass fqtools \u0027validate\u0027 or samtools \u0027quickcheck\u0027, as appropriate.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eprocess preprocessing:countReads\u003cbr\u003e\n2. (Fail) If sample contains \u0026lt; 100k pairs of raw reads.\u003c/p\u003e\n\u003cp\u003eprocess preprocessing:fastp\u003cbr\u003e\n3. (Fail) If sample contains \u0026lt; 100k pairs of cleaned reads, required to all be \u0026gt; 50bp (cleaning using fastp with --length_required 50 --average_qual 10 --low_complexity_filter --correction --cut_right --cut_tail --cut_tail_window_size 1 --cut_tail_mean_quality 20).\u003c/p\u003e\n\u003cp\u003eprocess preprocessing:kraken2\u003cbr\u003e\n4. (Fail) If the top family hit is not Mycobacteriaceae\u003cbr\u003e\n5. (Fail) If there are fewer than 100k reads classified as Mycobacteriaceae \u003cbr\u003e\n6. (Warn) If the top family classification is mycobacterial, but this is not consistent with top genus and species classifications\u003cbr\u003e\n7. (Warn) If the top family is Mycobacteriaceae but no G1 (species complex) classifications meet minimum thresholds of \u0026gt; 5000 reads or \u0026gt; 0.5% of the total reads (this is not necessarily a concern as not all mycobacteria have a taxonomic classification at this rank)\u003cbr\u003e\n8. (Warn) If sample is mixed or contaminated - defined as containing reads \u0026gt; the 5000/0.5% thresholds from multiple non-human species\u003cbr\u003e\n9. (Warn) If sample contains multiple classifications to mycobacterial species complexes, each meeting the \u0026gt; 5000/0.5% thresholds\u003cbr\u003e\n10. (Warn) If no species classification meets the 5000/0.5% thresholds\u003cbr\u003e\n11. (Warn) If no genus classification meets the 5000/0.5% thresholds\u003c/p\u003e\n\u003cp\u003eprocess preprocessing:identifyBacterialContaminants\u003cbr\u003e\n12. (Fail) If regardless of what Kraken reports, Mykrobe does not make a species-level mycobacterial classification (note that we do not use Kraken mycobacterial classifications other than to determine whether 100k reads are family Mycobacteriaceae; for higher-resolution classification, we defer to Mykrobe)\u003cbr\u003e\n13. (Fail) If the sample is not contaminated and the top species hit is not one of the 10 supported Mycobacteria: abscessus|africanum|avium|bovis|chelonae|chimaera|fortuitum|intracellulare|kansasii|tuberculosis\u003cbr\u003e\n14. (Fail) If the sample is not contaminated and the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003cbr\u003e\n15. (Warn) If the top Mykrobe species hit, on the basis of highest % coverage, does not also have the highest median depth\u003cbr\u003e\n16. (Warn) If we are unable to associate an NCBI taxon ID to any given contaminant species, which means we will not be able to locate its genome, and thereby remove it as a contaminant\u003cbr\u003e\n17. (Warn) If we are unable to determine a URL for the latest RefSeq genome associated with a contaminant species\u0027 taxon ID\u003cbr\u003e\n18. (Warn) If no complete genome could be found for a contaminant species. The workflow will proceed with alignment-based contaminant removal, but you\u0027re warned that there\u0027s reduced confidence in detecting reads from this species\u003c/p\u003e\n\u003cp\u003eprocess preprocessing:downloadContamGenomes\u003cbr\u003e\n19. (Fail) If a contaminant is detected but we are unable to download a representative genome, and thereby remove it\u003c/p\u003e\n\u003cp\u003eprocess preprocessing:summarise\u003cbr\u003e\n20. (Fail) If after having taken an alignment-based approach to decontamination, Kraken still detects a contaminant species\u003cbr\u003e\n21. (Fail) If after having taken an alignment-based approach to decontamination, the top species hit is not one of the 10 supported Mycobacteria\u003cbr\u003e\n22. (Fail) If, after successfully removing contaminants, the top species hit is contrary to the species expected (e.g. \"avium\" rather than \"tuberculosis\" - only tested if you provide that expectation)\u003c/p\u003e\n\u003cp\u003eprocess clockwork:alignToRef\u003cbr\u003e\n23. (Fail) If \u0026lt; 100k reads could be aligned to the reference genome\u003cbr\u003e\n24. (Fail) If, after aligning to the reference genome, the average read mapping quality \u0026lt; 10\u003cbr\u003e\n25. (Fail) If \u0026lt; 50% of the reference genome was covered at 10-fold depth\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1627295221.0
  },
  {
    "data_format": 2,
    "description": "Bisulfite-seq data Workflow Automation Software and Protocols",
    "filenames": [
      "Singularity",
      "Singularity.v1.0",
      "Singularity.v0.9",
      "Singularity.v1.1"
    ],
    "full_name": "BrendelGroup/BWASP",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols\" class=\"anchor\" href=\"#bwasp--bisulfite-seq-data-workflow-automation-software-and-protocols\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBWASP : Bisulfite-seq data Workflow Automation Software and Protocols\u003c/h1\u003e\n\u003cp\u003eThe BWASP repository encompasses code and scripts developed in the\n\u003ca href=\"http://brendelgroup.org/\" rel=\"nofollow\"\u003eBrendel Group\u003c/a\u003e for analyses of bisulfite sequencing\ndata.\nThe entire workflow relies on various other open source software as well as\n\u003ca href=\"https://www.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e scripts from the companion\n\u003ca href=\"https://github.com/BrendelGroup/BWASPR\"\u003eBWASPR\u003c/a\u003e repository.\nThe code conforms to our \u003ca href=\"https://brendelgroup.github.io/\" rel=\"nofollow\"\u003eRAMOSE\u003c/a\u003e\nphilosophy: it generates \u003cstrong\u003ereproducible\u003c/strong\u003e, \u003cstrong\u003eaccurate\u003c/strong\u003e, and \u003cstrong\u003emeaningful\u003c/strong\u003e\nresults; it is \u003cstrong\u003eopen\u003c/strong\u003e (source) and designed to be \u003cstrong\u003escalable\u003c/strong\u003e and\n\u003cstrong\u003eeasy\u003c/strong\u003e to use.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start-\" class=\"anchor\" href=\"#quick-start-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start \u003ca href=\"https://singularity-hub.org/collections/1203\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h2\u003e\n\u003cp\u003eInput to the BWASP workflow consists of accession numbers or fastq files of\nbisulfite-sequencing reads as well as the appropriate genome assembly (and, if\navailable, genome annotation).\nOutput (after read quality control and mapping) are \u003cem\u003e*.mcalls\u003c/em\u003e files that list\nthe sufficiently covered genomic Cs and their methylation percentage in the\ngiven sample.\nThe scripts in the \u003cem\u003ebin\u003c/em\u003e directory take care of minor tasks in the overall\nworkflow, but configuration and execution is via\n\u003ca href=\"https://www.gnu.org/software/make/\" rel=\"nofollow\"\u003eGNU make\u003c/a\u003e using edited copies of the\nmakefiles provided in the \u003cem\u003emakefiles\u003c/em\u003e directory.\nAll the BWASP dependencies are encapsulated in a\n\u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container available from our\n\u003ca href=\"http://BrendelGroup.org/SingularityHub/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e.\nThus, once you know what you are doing, execution could be as simple as\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull http://BrendelGroup.org/SingularityHub/bwasp.sif\nsingularity exec bwasp.sif make\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(assuming you have prepared a suitable makefile in your working directory).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-realistic-start\" class=\"anchor\" href=\"#realistic-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRealistic Start\u003c/h2\u003e\n\u003cp\u003ePlease find detailed installation instructions and options in the\n\u003ca href=\"./INSTALL.md\"\u003eINSTALL\u003c/a\u003e document.\nOnce all preparatory steps are taken care of, see the \u003ca href=\"./HOWTO.md\"\u003eHOWTO\u003c/a\u003e\ndocument for a complete example of how to implement and run a workflow.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference\" class=\"anchor\" href=\"#reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference\u003c/h2\u003e\n\u003cp\u003eAmy L. Toth, Murat Ozturk, Saranya Sankaranarayanan, and Volker P. Brendel\n(2018) \u003cem\u003eEstimating the size and dynamics of the CpG methylome of social\ninsects.\u003c/em\u003e To be submitted.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003ePlease direct all comments and suggestions to\n\u003ca href=\"mailto:vbrendel@indiana.edu\"\u003eVolker Brendel\u003c/a\u003e\nat \u003ca href=\"http://brendelgroup.org/\" rel=\"nofollow\"\u003eIndiana University\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1625951112.0
  },
  {
    "data_format": 2,
    "description": "Part of the sc-eQTLgen consortium pipeline. Step 1, where the QC is done.",
    "filenames": [
      "Singularity.Imputation",
      "Singularity.WGpipeline"
    ],
    "full_name": "sc-eQTLgen-consortium/WG1-pipeline-QC",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wg1-pipeline-qc\" class=\"anchor\" href=\"#wg1-pipeline-qc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWG1-pipeline-QC\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/44268007/89252548-35b96f80-d659-11ea-97e9-4b4176df5f08.png\" width=\"300\" height=\"140\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePart of the sceQTL-Gen consortium pipeline. Step 1, where the QC is done.\u003c/p\u003e\n\u003cp\u003ePlease see the \u003ca href=\"https://github.com/sc-eQTLgen-consortium/WG1-pipeline-QC/wiki\"\u003eWiki\u003c/a\u003e for information on running the QC pipeline.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623743752.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "tjhendrickson/BIDS_scripts",
    "latest_release": "v1.0",
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-bids-conversion-scripts\" class=\"anchor\" href=\"#bids-conversion-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBIDS Conversion Scripts\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h3\u003e\n\u003cp\u003eThis repository contains scripts that will assist in the conversion of raw DICOM data sets to \u003ca href=\"http://bids.neuroimaging.io/format\" rel=\"nofollow\"\u003eBIDS\u003c/a\u003e. The \"heuristics\" folder contains example scripts that have been used to convert data from DICOM to BIDS. \u003cstrong\u003eThis folder may be helpful to build your own heuristics script.\u003c/strong\u003e For additional information on how to create a heuristic script see the \u003ca href=\"https://github.com/nipy/heudiconv\"\u003eheudiconv\u003c/a\u003e github page.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-container-hosting\" class=\"anchor\" href=\"#container-hosting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer Hosting\u003c/h3\u003e\n\u003cp\u003ePull the most recent container below, (\u003cstrong\u003eNOTE: you only have to do this once!\u003c/strong\u003e):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://tjhendrickson/BIDS_scripts:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-usage\" class=\"anchor\" href=\"#singularity-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Usage\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run /path/to/singularity/images/directory/imagename.img --help\nusage: [-h] [--output_dir OUTPUT_DIR] [--dicom_dir DICOM_DIR]\n       [--study_name STUDY_NAME] [--ses_id SES_ID] [--subj_id SUBJ_ID]\n       [--heuristic HEURISTIC] [--dry_run]\n\nScript that controls BIDS conversion for individual studies\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --output_dir OUTPUT_DIR\n                        The directory that the BIDS data will be outputted to\n  --dicom_dir DICOM_DIR\n                        The directory that houses unzipped dicom\n                        directories/files.\n  --study_name STUDY_NAME\n                        What is the shorthand name for this study?\n  --ses_id SES_ID       scanning session id\n  --subj_id SUBJ_ID     subject id\n  --heuristic HEURISTIC\n                        Path to heuristic file, if the file is already within\n                        the container (i.e. within heuristics folder) you do\n                        not have to specify a path.\n  --dry_run             Dry run. A dicominfo_*.tsv file will generate within\n                        .heudiconv/\u0027subj_id\u0027/info directory which can be used\n                        to create heuristic script\n\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eThis application must be run with either the \"--heuristic\" or \"--dry_run\" argument, it will fail otherwise.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eUse the \"--dry_run\" argument to take a closer look at the acquistion parameters for a scanning session.\u003c/p\u003e\n\u003cp\u003eTo run a single participant with dry_run argument:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -B /home/timothy/sandbox_DO_NOT_DELETE/BIDS/142_CIFASD_4:/output_dir \\\n-B /path/to/dicom/data/dir:/dicom_dir /path/to/singularity/images/directory/imagename.img \\\n--output_dir /output_dir --dicom_dir /dicom_dir --ses_id 10000 --subj_id 1000 --dry_run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will output a hidden folder (named .heudiconv) along with sub-folders based on arguments provided to \"--subj_id\" and \"--ses_id\" respectively.\nWithin the sub-folders will be a tsv file that begins with \"dicominfo\". \u003cstrong\u003eBased on the example above the path to the file will be \".heudiconv/1000/ses-10000/info/dicominfo_ses-10000.tsv\"\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eUse this tsv file to design a heuristics script to organize your eventual nifti data. \u003cstrong\u003eSee this tutorial for a how to on heuristic script creation (\u003ca href=\"http://reproducibility.stanford.edu/bids-tutorial-series-part-2a/#heuman3\" rel=\"nofollow\"\u003eheuristics tutorial\u003c/a\u003e)\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eTo run a single participant with heuristic argument:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run -B /home/timothy/sandbox_DO_NOT_DELETE/BIDS/142_CIFASD_4:/output_dir \\\n-B /path/to/dicom/data/dir:/dicom_dir -B /path/to/heuristics/script:/heuristic.py \\\n /path/to/singularity/images/directory/imagename.img \\\n--output_dir /output_dir --dicom_dir /dicom_dir --ses_id 10000 --subj_id 1000 --heuristic /heuristic.py\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-important-notes\" class=\"anchor\" href=\"#important-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImportant Notes\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-gotchas\" class=\"anchor\" href=\"#gotchas\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGotchas\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e1) Bind Mounting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIn order to run this container you will have to use \"bind mounting\", meaning you will have to link local folders/files to existing folders/files within the container with the -B flag. In the example above the local folder \"/home/timothy/sandbos_DO_NOT_DELETE/BIDS/142_CIFASD_4\" becomes \"/output_dir\" within the container as they are separated by a colon (:). \u003cstrong\u003eNotice that in both cases above the output and dicom folder and heuristic file are bound to /output_dir, /dicom_dir and /heuristic.py respectively, this is very important.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2) DICOM Data Formatting\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDue to the restrictive nature of BIDS the dicom data must be in a particular format in order for the conversion to work properly. This application will copy dicom data directories by searching for either the --subj_id or --ses_id argument present within a dicom directory name, place them in a separate directory, and rearrange them. So for example if the dicom directory is named \"XYXY4776XYXY\" --subj_id 4776 the application will find the \"4776\" pattern.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3) Subject ID and Session ID names\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou must use alphanumerics (i.e. letters or numbers) only (\u003cstrong\u003eno special characters\u003c/strong\u003e) with your subject IDs (subj_id) and session IDs (ses_id). \u003cstrong\u003eNote the \"--ses_id\" argument is optional\u003c/strong\u003e!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-best-practices\" class=\"anchor\" href=\"#best-practices\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBest Practices\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e1) Initial Conversion\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhile testing the initial BIDS conversion it is best to start with one or two datasets and specify the \u0027--dry_run\u0027 argument (see above for an example of usage).\nThis will create a dicom_info tsv file which can be used for heuristic script creation.\nSee Step 3 of \u0027Run HeuDiConv on ses-001 scans to get the dicominfo file\u0027 within \u003ca href=\"http://reproducibility.stanford.edu/bids-tutorial-series-part-2a/#heuman2\" rel=\"nofollow\"\u003eStanford BIDS Tutorial\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e2) BIDS Validator\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce satisfied with an initial BIDS conversion, prior to running the conversion on an entire study first ensure that the BIDS converted dataset meets the BIDS specification by using the \u003ca href=\"http://incf.github.io/bids-validator/\" rel=\"nofollow\"\u003eBIDS validator web version\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e3) Longitudinal Format\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eWhen converting data to BIDS you can certainly have a cross sectonal directory format such as below:\nBIDS_output\nsub-01\nsub-02\nsub-03\nsub-04\netc\nHowever, I suggest placing data within a longitudinal directory format even if you have cross-sectional data:\nBIDS_output\nsub-01\nses-01\nses-02\nsub-02\nses-01\netc\u003c/p\u003e\n\u003cp\u003eYou can control the BIDS directory format by providing both the arguments --subj_id --ses_id for a conversion, if you only specify one of the two arguments the data will be outputted in a cross-sectional format.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1624658162.0
  },
  {
    "data_format": 2,
    "description": "Contains tools for building and distributing the JEDI/JCSDA Containers",
    "filenames": [
      "Singularity.tutorial",
      "Singularity.tutorial-test",
      "Singularity.gnu-openmpi-dev",
      "Singularity.clang-mpich-dev"
    ],
    "full_name": "JCSDA-internal/containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h1\u003e\n\u003cp\u003eThis repository contains tools for building Singularity and Charliecloud containers.\u003c/p\u003e\n\u003cp\u003eFurthermore, since Intel Docker containers cannot be distributed through Docker Hub, they are also handled here.\u003c/p\u003e\n\u003cp\u003eThe instructions below are intended for the JEDI core team, who are responsible for maintaining JEDI containers and distributing them publicly or privately.\u003c/p\u003e\n\u003cp\u003eHowever, since the JEDI core team cannot legally distribute intel containers for licensing reasons, JEDI users and developers are encouraged to build their own intel development container.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"myIntel/Intel.md\"\u003eSee here for instructions on how to build your own JEDI Intel development container: Docker, Singularity, or Charliecloud\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-organization-of-repository\" class=\"anchor\" href=\"#organization-of-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOrganization of Repository\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003etop-level directory: tools for building Singularity, Docker, and Charliecloud containers\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003evagrant\u003c/code\u003e: tools for building Vagrant virtual machines that are provisioned to run JEDI containers\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emodulefiles\u003c/code\u003e, \u003ccode\u003erunscripts\u003c/code\u003e: These directories contain sample modulefiles and batch scripts for running JEDI \"Supercontainers\" across nodes on HPC systems\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emyIntel\u003c/code\u003e is intended to help users from the general JEDI community build their own JEDI intel development containers.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eintel19\u003c/code\u003e contains deprecated build tools for intel Parallel Studio.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eexamples\u003c/code\u003e is a sandbox, containing instructive examples of how to implement features that may not be used now but might be used in the future.  An example is how to build writable singularity containers.   These scripts are not maintained; there is no guarantee that they will run as is.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eIn order to build Docker, Singularity, or Charliecloud containers, you will of course need to have the appropriate software installed, namely \u003ccode\u003edocker\u003c/code\u003e, \u003ccode\u003esingularity\u003c/code\u003e, or \u003ccode\u003echarliecloud\u003c/code\u003e.  Members of the JEDI core team can launch an AWS node with all of these pre-installed.  Or, you can install them yourself as described in the JEDI documentation.\u003c/p\u003e\n\u003cp\u003eThe scripts in this directory also assume that you have root privileges.\u003c/p\u003e\n\u003cp\u003eAlso, core developers often find it necessary to access feature or bugfix branches of the jedi stack for testing purposes.  So, the \u003ccode\u003ebuild_container.sh\u003c/code\u003e script uses the JCSDA-internal (private) jedi-stack repo.  For this reason, you need to provide an ssh key for access.  This script uses a generic academy ssh key to ensure that it has read-only access to selected JCSDA repositories.  If you do not have access to this key, you can replace it with another by changing the \u003ccode\u003eKEY\u003c/code\u003e variable in \u003ccode\u003ebuild_containers.sh\u003c/code\u003e.  But it is recommended to retain the read-only access.  You can build the \u003ccode\u003emyIntel\u003c/code\u003e container without an ssh key.\u003c/p\u003e\n\u003cp\u003eNote: to build the tutorial container you have to copy the ssh key into the directory \u003ccode\u003essh-key\u003c/code\u003e and modify the singularity recipe file accordingly if it has a different name.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-a-dev-container\" class=\"anchor\" href=\"#build-a-dev-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild a dev container\u003c/h2\u003e\n\u003cp\u003eTo build a Singularity, Charliecloud, and/or a Docker container, enter this and respond to the prompts to build the containers of your choice.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./build_containers.sh \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhere \u003ccode\u003e\u0026lt;name\u0026gt;\u003c/code\u003e matches one of the available Dockerfile extensions, e.g. \u003ccode\u003egnu-openmpi-dev\u003c/code\u003e.  It also accepts an optional second argument to specify a tag.  The default tag is \u003ccode\u003ebeta\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor the the gnu and clang containers, the Singularity containers are built directly from the images on Docker Hub.  A Docker container will only be created if you choose to build a Charliecloud container, which is then built from the Docker container.\u003c/p\u003e\n\u003cp\u003eFor the \u003ccode\u003eintel-impi-dev\u003c/code\u003e container, a Docker file is always created and then the Singularity and Charliecloud containers are created from that.\u003c/p\u003e\n\u003cp\u003eThe intel Docker container is the one used for CI so it is kept relatively compact.  If you wish to add additional components such as Vtune, it is recommended you use the companion scripts in the \u003ccode\u003emyIntel\u003c/code\u003e directory.  These scripts are simplified in the root directory but they are intended for use by the general JEDI user and developer community.  The main simplification is that there is no need to supply an ssh key because those scripts only access the public jedi-stack repo.\u003c/p\u003e\n\u003cp\u003eTo build the tutorial container, just specify \u003ccode\u003etutorial\u003c/code\u003e for the \u003ccode\u003e\u0026lt;name\u0026gt;\u003c/code\u003e.  The tutorial container is exclusively a Singularity container and uses GNU-OpenMPI: There are no clang or intel options and there are no Docker or Charliecloud containers created.\u003c/p\u003e\n\u003cp\u003eThe Singularity and Charliecloud container files will be placed in a subdirectory called \u003ccode\u003econtainers\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eNote: building the Mellanox-enabled HPC container isn\u0027t yet automated.  For this or other non-standard cases, you can edit the Dockerfiles, Singularity files, and scripts manually as needed.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-test-the-container\" class=\"anchor\" href=\"#test-the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest the container\u003c/h2\u003e\n\u003cp\u003eBefore distributing a container, it\u0027s always important to test it.  A good test is usually to enter the container and then build and test fv3-bundle.\u003c/p\u003e\n\u003cp\u003eTo enter the Singularity container, enter:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity shell -e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.sif\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd, for CharlieCloud, you can do this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir -p \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/ch-jedi\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/ch-jedi\nch-tar2dir \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003epath-to-tarfile\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e/ch-jedi-gnu-openmpi-dev.tar.gz \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\nch-run ch-jedi-gnu-openmpi-dev -- bash\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-distribute-the-latest-container\" class=\"anchor\" href=\"#distribute-the-latest-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDistribute the latest container\u003c/h2\u003e\n\u003cp\u003eThe latest Singularity containers are made available on Sylabs Cloud, the latest Charliecloud containers are made available on a public AWS S3 bucket, and the latest intel containers are made available on a private AWS S3 bucket.  The purpose of the \u003ccode\u003epush_containers.sh\u003c/code\u003e script is to push the new container to these distribution sites.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ebeta\u003c/code\u003e tag is a special case.  If the tag is \u003ccode\u003ebeta\u003c/code\u003e, it is assumed that, after it passes tests, this container is ready to be deployed as \u003ccode\u003elatest\u003c/code\u003e.  In this case, a copy of the current \u003ccode\u003elatest\u003c/code\u003e container is saved with the tag \u003ccode\u003erevert\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSo, the typical workflow would be to enter\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./push_containers.sh \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAs with \u003ccode\u003ebuild_containers.sh\u003c/code\u003e, \u003ccode\u003epush_containers.sh\u003c/code\u003e accepts an optional second argument which is a tag.  This is sometimes useful for experimental cases but is not part of the normal workflow.\u003c/p\u003e\n\u003cp\u003eFor instructions on how to download these containers, see \u003ca href=\"https://jointcenterforsatellitedataassimilation-jedi-docs.readthedocs-hosted.com/en/latest/using/jedi_environment/containers.html#available-containers\" rel=\"nofollow\"\u003ethe JEDI Documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tagged-releases\" class=\"anchor\" href=\"#tagged-releases\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTagged Releases\u003c/h2\u003e\n\u003cp\u003eMost developers use the latest development containers but it\u0027s also useful to have tagged containers that accompany JEDI releases.  This is particularly relevant for scientific users (as opposed to developers) who may wish to use tagged releases and containers for reproducibility in research.  Tagged containers can also be used to provide stability for operational or Near-Real-Time (NRT) workflows.\u003c/p\u003e\n\u003cp\u003eSylabs cloud has a storage quota (currently 11 GB) that would be quickly overwhelmed if we were to store many release containers there.  So, this is reserved for \"latest\" and \"revert\".\u003c/p\u003e\n\u003cp\u003eTagged singularity containers are distributed on the \u003ca href=\"http://data.jcsda.org/pages/containers.html\" rel=\"nofollow\"\u003eJCSDA Public Container Repository\u003c/a\u003e along with the latest and tagged Charliecloud containers.  Tagged docker release containers can be obtained from Docker Hub.  For example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker pull jcsda/docker-gnu-openmpi-dev:v1.0.0\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1627309513.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v3.8-torch1.5.0-dj0.12.4",
      "Singularity.v3.8-torch1.9.0-dj0.12.7",
      "Singularity.v3.8-torch1.7.0-dj0.12.7"
    ],
    "full_name": "sinzlab/pytorch-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pytorch-singularity\" class=\"anchor\" href=\"#pytorch-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epytorch-singularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4939\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository contains Singularity definition files used for PyTorch development in the Sinzlab.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1625646389.0
  },
  {
    "data_format": 2,
    "description": "A collection of scripts to run variant aggregation tests from whole-genome sequencing data.",
    "filenames": [
      "Singularity",
      "Singularity_via_docker"
    ],
    "full_name": "hmgu-itg/burden_testing",
    "latest_release": "v1.5.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mummy-the-wrapped-monster\" class=\"anchor\" href=\"#mummy-the-wrapped-monster\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMummy (the wrapped MONSTER)\u003c/h1\u003e\n\u003cp\u003eThis is a pipeline to run genome-wide burdent tests using sequencing data. Head over to \u003ca href=\"https://github.com/hmgu-itg/burden_testing/wiki\"\u003ethe wiki\u003c/a\u003e for detailed instructions on how to run it.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626144695.0
  },
  {
    "data_format": 2,
    "description": "A tool to find and annotate signals in next-generation association studies",
    "filenames": [
      "Singularity"
    ],
    "full_name": "hmgu-itg/peakplotter",
    "latest_release": "v0.3.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-peakplotter--automatically-annotate-hits-from-genome-wide-association-results\" class=\"anchor\" href=\"#peakplotter--automatically-annotate-hits-from-genome-wide-association-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePeakPlotter : automatically annotate hits from genome-wide association results\u003c/h1\u003e\n\u003cp\u003ePeakPlotter takes away the annoying task of running regional association plots and annotating variants for your association studies results. It is compatible with sequencing as well as GWAS data. It is compatible with any format (GEMMA, SNPTEST, Bolt-LMM...) that produces the relevant columns: chromosome, position, unique ID, P-value, reference and non-reference alleles.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eAfter installing the prerequisites (see below), clone the repository and install using \u003ccode\u003epip\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/hmgu-itg/peakplotter.git\n\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e peakplotter\n\npython3 -m pip install \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\n\npeakplotter-data-setup \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e This only needs to be run once\u003c/span\u003e\n\npeakplotter --help\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or \u003c/span\u003e\npython3 -m peakplotter --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eA \u003ccode\u003eSingularity\u003c/code\u003e definition file is also available in the repository if you wish to build a container to use \u003ccode\u003epeakplotter\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003ePeakPlotter has has non-python dependencies.\u003cbr\u003e\nIn order to run PeakPlotter you need to install the following tools and add the executables to your \u003ccode\u003ePATH\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePlink 1.9 or newer (\u003ca href=\"https://www.cog-genomics.org/plink2/index\" rel=\"nofollow\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eLocusZoom Standalone 1.4 or newer (\u003ca href=\"http://genome.sph.umich.edu/wiki/LocusZoom_Standalone\" rel=\"nofollow\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eTabix (\u003ca href=\"https://github.com/samtools/htslib\"\u003eavailable here\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eMoreutils (for \u003ccode\u003esponge\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePeakPlotter will throw a \u003ccode\u003eMissingExecutableError\u003c/code\u003e if you have any of the above tools missing in your \u003ccode\u003ePATH\u003c/code\u003e environment variable.\u003cbr\u003e\nAdd the necessary tools to your \u003ccode\u003ePATH\u003c/code\u003e like below:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=/path/to/locuszoom:/path/to/plink:\u003cspan class=\"pl-smi\"\u003e$PATH\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you want to make these changes permanent, do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003eexport PATH=/path/to/locuszoom:/path/to/plink:$PATH\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.bashrc\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ peakplotter --help\nUsage: peakplotter [OPTIONS]\n\n  PeakPlotter\n\nOptions:\n  -a, --assoc-file FILE    Path to the association file. It can be gzipped,\n                           provided that it bears the .gz extension. Its first\n                           line must be a header, coherent with the name\n                           arguments below. It must be tab-separated, bgzipped\n                           and tabixed (tabix is available as part of\n                           bcftools)  [required]\n  -f, --bfiles TEXT        Binary PLINK (.bed/.bim/.fam) file base name. This\n                           should contain the genotypes \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e at least all the\n                           variants \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e the assoc_file, but it can contain\n                           more. Please note that this is the base name,\n                           without the .bed/.bim/.fam extension.  [required]\n  -o, --out DIRECTORY      Output directory to store all output files.\n                           [required]\n  -chr, --chr-col TEXT     Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e chromosome names.\n                           [required]\n  -ps, --pos-col TEXT      Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e chromosomal position.\n                           [required]\n  -rs, --rs-col TEXT       Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e unique SNP ids (RS-id or\n                           chr:pos).  [required]\n  -p, --pval-col TEXT      Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e p-values.  [required]\n  -a1, --a1-col TEXT       Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e reference or major allele\n                           (used \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e predicting consequence).  [required]\n  -a2, --a2-col TEXT       Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e alternate or minor allele.\n                           [required]\n  -maf, --maf-col TEXT     Name of the column \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e non-reference or minor\n                           allele frequency.  [required]\n  -b, --build INTEGER      Assembly build (37 or 38)  [default: 38]\n  -s, --signif FLOAT       The significance level above which to \u003cspan class=\"pl-k\"\u003edeclare\u003c/span\u003e a\n                           variant significant. Scientific notation (such as\n                           5e-8) is fine.\n  -bp, --flank-bp INTEGER  Flanking size \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e base pairs \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e drawing plots\n                           (defaults to 500kb, i.e. 1Mbp plots) around lead\n                           SNPs.\n  --overwrite              Overwrite output directory \u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e it already exists.\n  --help                   Show this message and exit.\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing\" class=\"anchor\" href=\"#testing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTesting\u003c/h2\u003e\n\u003cp\u003eRun \u003ccode\u003epytest\u003c/code\u003e at the root of the repository to run the testsuite.\u003c/p\u003e\n\u003cp\u003eThere aren\u0027t a lot of tests right now, and this is a work in progress. If you encounter any bugs, please raise an issue at the \u003ca href=\"https://github.com/hmgu-itg/peakplotter/issues\"\u003eissue page\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626939397.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.lofar_sksp_ddf_public",
      "Singularity.lofar_sksp_base",
      "Singularity.lofar_sksp"
    ],
    "full_name": "tikk3r/lofar-grid-hpccloud",
    "latest_release": "v3.2",
    "readme": "\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5f618187158129a12605b61c2558a97b7014bf61a63dcbb58ecc23d53ade59a4/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f762f72656c656173652f74696b6b33722f6c6f6661722d677269642d687063636c6f75643f736f72743d73656d766572\" data-canonical-src=\"https://img.shields.io/github/v/release/tikk3r/lofar-grid-hpccloud?sort=semver\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b498f0b23c001d15b8b32b01a58375128a6fd5886fbefc3906a2164b36556ef5/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f74696b6b33722f6c6f6661722d677269642d687063636c6f75642e7376673f6c6f676f3d676974687562\" data-canonical-src=\"https://img.shields.io/github/license/tikk3r/lofar-grid-hpccloud.svg?logo=github\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ca href=\"https://zenodo.org/badge/latestdoi/136925861\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ad3ce50d6d0bdd702c67f43f248e79b036a12ebf23efdccde0d13eb15d31bf9e/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3133363932353836312e737667\" data-canonical-src=\"https://zenodo.org/badge/136925861.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-lofar-grid-hpccloud\" class=\"anchor\" href=\"#lofar-grid-hpccloud\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elofar-grid-hpccloud\u003c/h1\u003e\n\u003cp\u003eThis repository hold resources for deploying the LOFAR software (genericpipeline) and related tools through Singularity containers. These containers are general, but at the same time somewhat tailored for SKSP use.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003emaster\u003c/code\u003e branch is empty. Currently the images are based on the Fedora 31 Linux distribution, which is available from \u003ca href=\"https://hub.docker.com/_/fedora\" rel=\"nofollow\"\u003eDockerHub\u003c/a\u003e. Recipes to build this container can be found on the \u003ccode\u003efedora\u003c/code\u003e branch.\u003c/p\u003e\n\u003cp\u003eTo build a full LOFAR Singularity image, do the following:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Singularity.lofarbase\u003c/p\u003e\n\u003cp\u003esudo singularity build lofar_sksp_base.sif Singularity.lofar_sksp_base\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild Singularity.lofar (use the \u003ccode\u003eFrom: localimage\u003c/code\u003e part instead of the Singularity Hub part)\u003c/p\u003e\n\u003cp\u003esudo singularity build lofar_sksp.sif Singularity.lofar_sksp\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePre-built containers are public hosted at \u003ca href=\"https://lofar-webdav.grid.sara.nl/software/shub_mirror/tikk3r/lofar-grid-hpccloud/\" rel=\"nofollow\"\u003eSURFSara\u003c/a\u003e. Sort by date to find the latest container there.\u003c/p\u003e\n\u003cp\u003eVisit the  \u003ca href=\"https://github.com/tikk3r/lofar-grid-hpccloud/wiki\"\u003ewiki\u003c/a\u003e for more detailed information and build instructions.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626955952.0
  },
  {
    "data_format": 2,
    "description": "exSeek: extracellular RNA analysis tool for noninvasive biomarker",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "james20141606/exSeek",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-exseek\" class=\"anchor\" href=\"#exseek\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eexSeek\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.com/lulab/exSeek-dev\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/17b4878574e7d41e83f8bedc7e94f3500d4ebeefba3c587ded3efcbc5730c3b3/68747470733a2f2f7472617669732d63692e636f6d2f6c756c61622f65785365656b2d6465762e7376673f746f6b656e3d4379526755577371574363744b7641784d58746f266272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/lulab/exSeek-dev.svg?token=CyRgUWsqWCctKvAxMXto\u0026amp;branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-workflow\" class=\"anchor\" href=\"#workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"assets/whole_pipe.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"assets/whole_pipe.png\" alt=\"workflow\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eInstall required software packages according to \u003ca href=\"docs/requirements.md\"\u003erequirements\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDownload the scripts:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/lulab/exSeek-dev.git\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prepare-genome-and-annotations\" class=\"anchor\" href=\"#prepare-genome-and-annotations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare genome and annotations\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eProcessed files\u003c/strong\u003e: \u003ccode\u003e/BioII/lulab_b/shared/genomes/hg38\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-download-and-process-genome-sequences\" class=\"anchor\" href=\"#download-and-process-genome-sequences\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload and process genome sequences\u003c/h3\u003e\n\u003cp\u003eRefer to the \u003ca href=\"docs/genome_and_annotations.md\"\u003edocumentation\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-extract-gtfs-and-generate-mapping-indexes\" class=\"anchor\" href=\"#extract-gtfs-and-generate-mapping-indexes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExtract GTFs and generate mapping indexes\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --snakefile snakemake/prepare_genome.snakemake \\\n    --configfile snakemake/config.yaml \\\n    --rerun-incomplete -k\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input-files\" class=\"anchor\" href=\"#input-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput files\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFile name\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${input_dir}/fastq/${sample_id}.fastq\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRead files (single-end sequencing)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\n\u003ccode\u003e${input_dir}/fastq/${sample_id}_1.fastq\u003c/code\u003e, \u003ccode\u003e${input_dir}/fastq/${sample_id}_2.fastq\u003c/code\u003e\n\u003c/td\u003e\n\u003ctd\u003eRead files (paired-end sequencing)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${input_dir}/sample_ids.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eA text file with one sample ID per line.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${input_dir}/sample_classes.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eA tab-deliminated file (with header) with two columns: sample_id, label\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${input_dir}/batch_info.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eA comma-deliminated file (with header) with at least two columns: sample_id, batch1, batch2, ...\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${input_dir}/reference_genes.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eA text file with reference gene IDs.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${input_dir}/compare_groups.yaml\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eA YAML file defining positive and negative classes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003ecompare_groups.yaml\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eEvery key-value pairs defines a compare group and a negative-positive class pair:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003eNormal-CRC\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e[\"Healthy Control\", \"Colorectal Cancer\"]\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003eAll parameters are specified in a configuration file in \u003ca href=\"https://en.wikipedia.org/wiki/YAML\" rel=\"nofollow\"\u003eYAML\u003c/a\u003e format.\u003c/p\u003e\n\u003cp\u003eAn example configuration file is (snakemake/config.yaml).\u003c/p\u003e\n\u003cp\u003eThe parameter values in the configuration file can also be overrided through the \u003ccode\u003e--config\u003c/code\u003e option in \u003ca href=\"https://snakemake.readthedocs.io/en/stable/executable.html\" rel=\"nofollow\"\u003esnakemake\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe following parameters should be changed:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eParameter\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003cth\u003eExample\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003egenome_dir\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eDirectory for genome and annotation files\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003egenome/hg38\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003edata_dir\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eDirectory for input files\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003edata/scirep\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003etemp_dir\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eTemporary directory\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003etmp\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003esample_id_file\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eA text file containing sample IDs\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003edata/scirep/sample_ids.txt\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eoutput_dir\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eDirectory for all output files\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eoutput/scirep\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003etools_dir\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eDirectory for third-party tools\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ealigner\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eMapping software\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003ebowtie2\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eadaptor\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e3\u0027 adaptor sequence for single-end RNA-seq\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eAGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003epython2\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003ePath to Python 2\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/apps/anaconda2/bin/python\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003epython3\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003ePath to Python 3\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/apps/anaconda2/bin/python\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-line-arguments-for-snakemake\" class=\"anchor\" href=\"#command-line-arguments-for-snakemake\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand line arguments for snakemake\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eOption\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e--config\u003c/td\u003e\n\u003ctd\u003eAdditional configuration parameters\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-j\u003c/td\u003e\n\u003ctd\u003eNumber of parallel jobs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e--dryrun\u003c/td\u003e\n\u003ctd\u003eDo not execute\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e-k\u003c/td\u003e\n\u003ctd\u003eDo not stop when an independent job fails\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-submit-jobs-to-a-computer-cluster-using-snakemake\" class=\"anchor\" href=\"#submit-jobs-to-a-computer-cluster-using-snakemake\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubmit jobs to a computer cluster using snakemake\u003c/h2\u003e\n\u003cp\u003ePlease refer the \u003ca href=\"https://snakemake.readthedocs.io/en/stable/snakefiles/configuration.html#cluster-configuration\" rel=\"nofollow\"\u003elink\u003c/a\u003e for descriptions of cluster configuration file.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ibm-lsf\" class=\"anchor\" href=\"#ibm-lsf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIBM LSF\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eConfiguration file\u003c/strong\u003e: \u003ccode\u003esnakemake/cluster.yaml\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example configuration:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003e__default__\u003c/span\u003e:\n  \u003cspan class=\"pl-ent\"\u003equeue\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eZ-LU\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e{rule}.{wildcards}\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003estderr\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003elogs/cluster/{rule}/{wildcards}.stderr\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003estdout\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003elogs/cluster/{rule}/{wildcards}.stdout\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003ethreads\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e{threads}\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003eresources\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003espan[hosts=1]\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eCommonly used parameters\u003c/strong\u003e\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eParameter\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e__default__\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRule name (\u003ccode\u003e__default__\u003c/code\u003e) for default configuration)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003equeue\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eQueue name\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ename\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eJob name\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003estderr\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eLog file for standard error\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003estdout\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eLog file for standard output\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ethreads\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eNumber of parallel threads for a job\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eresources\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eResource requirements. \u003ccode\u003espan[hosts=1]\u003c/code\u003e prevents parallel jobs from being submitted to different nodes\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eRun snakemake\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --snakefile snakemake/\u003cspan class=\"pl-smi\"\u003e${snakefile}\u003c/span\u003e \\\n    --configfile snakemake/config.yaml \\\n    --cluster \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ebsub -q {cluster.queue} -J {cluster.name} -e {cluster.stderr} \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e-o {cluster.stdout} -R {cluster.resources} -n {cluster.threads}\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e \\\n    --cluster-config snakemake/cluster.yaml \\\n    --rerun-incomplete -k -j40\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: replace \u003ccode\u003e${snakefile}\u003c/code\u003e with a Snakefile.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quality-control\" class=\"anchor\" href=\"#quality-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuality control\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --snakefile snakemake/quality_control.snakemake \\\n    --configfile snakemake/config.yaml \\\n    --rerun-incomplete -k\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mapping-small-rna-seq\" class=\"anchor\" href=\"#mapping-small-rna-seq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMapping (small RNA-seq)\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-generate-snakemake-rules-for-sequential-mapping\" class=\"anchor\" href=\"#generate-snakemake-rules-for-sequential-mapping\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate snakemake rules for sequential mapping\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebin/generate_snakemake.py sequential_mapping --rna-types rRNA,miRNA,piRNA,Y_RNA,srpRNA,tRNA,snRNA,snoRNA,lncRNA,mRNA,tucpRNA \\\n    -o snakemake/mapping_small/sequential_mapping.snakemake\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --snakefile snakemake/mapping_small.snakemake \\\n    --configfile snakemake/config.yaml \\\n    --rerun-incomplete -k\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output-files\" class=\"anchor\" href=\"#output-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput files\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFile name\u003c/th\u003e\n\u003cth\u003eDescrpition\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003esnakemake/sequential_mapping.snakemake\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eSnakefile for sequential mapping. Required by snakemake/mapping_small.snakemake\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/cutadapt/${sample_id}.fastq\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eReads with adaptor trimmed\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/tbam/${sample_id}/${rna_type}.bam\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eBAM files in transcript coordinates\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/gbam/${sample_id}/${rna_type}.bam\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eBAM files in genome coordinates\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/unmapped/${sample_id}/${rna_type}.fa.gz\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eUnmapped reads in each step\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/fastqc/${sample_id}_fastqc.html\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eFastQC report file\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/summary/fastqc.html\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eSummary report for FastQC (HTML)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/summary/fastqc.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eSummary table for FastQC\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/summary/fastqc.ipynb\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eSummary report for FastQC (Jupyter notebook)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/summary/read_counts.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eSummary table for read counts\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/stats/mapped_read_length_by_sample/${sample_id}\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eLength distribution of mapped reads\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-generate-expression-matrix\" class=\"anchor\" href=\"#generate-expression-matrix\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate expression matrix\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --snakefile snakemake/expression_matrix.snakemake \\\n    --configfile snakemake/config.yaml \\\n    --rerun-incomplete -k\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output-files-1\" class=\"anchor\" href=\"#output-files-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput files\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFile name\u003c/th\u003e\n\u003cth\u003eDescrpition\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/count_matrix/transcript.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eCount matrix of transcripts\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/count_matrix/htseq.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eCount matrix of genes generated using HTSeq-count\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/count_matrix/featurecounts.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eCount matrix of genes generated using featureCounts\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/counts_by_biotype/${count_method}/${sample_id}/${rna_type}\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eGene/transcript counts generated using a feature counting tool\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eCount matrix\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFile path: \u003ccode\u003e${output_dir}/count_matrix/transcript.txt\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eFirst row: sample IDs\u003c/li\u003e\n\u003cli\u003eFirst column: feature names\u003c/li\u003e\n\u003cli\u003eFeature name: \u003ccode\u003egene_id|gene_type|gene_name\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-call-domains-for-long-rna\" class=\"anchor\" href=\"#call-domains-for-long-rna\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCall domains for long RNA\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esnakemake --snakefile snakemake/call_domains_long.snakemake \\\n    --configfile snakemake/config.yaml \\\n    --rerun-incomplete -k\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output-files-2\" class=\"anchor\" href=\"#output-files-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput files\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFile name\u003c/th\u003e\n\u003cth\u003eDescrpition\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/domain_counts/${bin_size}/${pvalue}/${sample_id}.bed\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRead counts in long RNA domains (BED format with read counts in Column 5\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/count_matrix/domain_${pvalue}.txt\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRead count matrix of long RNA domains\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/domains/${bin_size}/${pvalue}.bed\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eLong RNA domain locations\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e${output_dir}/domains_recurrence/${bin_size}/${pvalue}.bed\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eRecurrence of long RNA domains among samples (Column 5)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003cstrong\u003eRead count matrix\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFile path: \u003ccode\u003e${output_dir}/count_matrix/domain_long.txt\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eFirst row: sample IDs\u003c/li\u003e\n\u003cli\u003eFirst column: feature names\u003c/li\u003e\n\u003cli\u003eFeature name: \u003ccode\u003egene_id|gene_type|gene_name|domain_id|transcript_id|start|end\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-normalization\" class=\"anchor\" href=\"#normalization\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNormalization\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output-files-3\" class=\"anchor\" href=\"#output-files-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput files\u003c/h3\u003e\n\u003cp\u003e| File name | Description |\n| \u003ccode\u003e${output_dir}/normalized_matrix/${normalization_method}.${imputation_method}.${batch_removal_method}.txt\u003c/code\u003e |\n| \u003ccode\u003e${output_dir}/matrix_processing/normalization/${normalization_method}.txt\u003c/code\u003e |\n| \u003ccode\u003e${output_dir}/matrix_processing/imputation/${normalization_method}.${imputation_method}.txt\u003c/code\u003e |\n| \u003ccode\u003e${output_dir}/matrix_processing/batch_removal/${batch_removal_method}.${batch_index}.txt\u003c/code\u003e |\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [
      "bioinformatics",
      "machie-learning"
    ],
    "updated_at": 1610690721.0
  },
  {
    "data_format": 2,
    "description": "Cell Interaction by Multiplet Sequencing (CIM-Seq) uses computational deconvolution of RNA-seq data from partially dissociated tissue to create cell interaction maps.",
    "filenames": [
      "inst/containers/Singularity"
    ],
    "full_name": "EngeLab/CIMseq",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cim-seq\" class=\"anchor\" href=\"#cim-seq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCIM-seq\u003c/h1\u003e\n\u003cp\u003eRelease build: \u003ca href=\"https://travis-ci.com/jasonserviss/CIMseq\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a40d6a99b66a0483bfc629d3c366e4bda0ddfb63f31cab19bf5a091e5fe5da20/68747470733a2f2f7472617669732d63692e636f6d2f6a61736f6e736572766973732f43494d7365712e7376673f6272616e63683d6d6173746572\" data-canonical-src=\"https://travis-ci.com/jasonserviss/CIMseq.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDevel build: \u003ca href=\"https://travis-ci.com/jasonserviss/CIMseq\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1cf4a1545fdea91e82705800328f938f63eeb43f67d6c106a6a7cffeb42c7664/68747470733a2f2f7472617669732d63692e636f6d2f6a61736f6e736572766973732f43494d7365712e7376673f6272616e63683d646576656c\" data-canonical-src=\"https://travis-ci.com/jasonserviss/CIMseq.svg?branch=devel\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTest coverage: \u003ca href=\"https://codecov.io/github/jasonserviss/CIMseq?branch=master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e5f3e48f6917619c7701365cadda7bdb2f3dfd193622e089e8d9a8338307eb69/68747470733a2f2f636f6465636f762e696f2f67682f6a61736f6e736572766973732f43494d7365712f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"Coverage status\" data-canonical-src=\"https://codecov.io/gh/jasonserviss/CIMseq/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAdvances in single-cell biology has enabled us to investigate isolated single cells at an unprecedented scale and resolution. However, cells in multicellular organisms are largely defined by their spatial organization within organ structures, which means that general methods for studying direct cell interaction on a large scale are needed. We propose a novel method, Cell Interaction by Multiplet Sequencing (CIM-Seq) that uses computational deconvolution of RNA-seq data from partially dissociated tissue to create cell interaction maps. We applied CIM-seq to human fetal pancreas, demonstrating that it recapitulates known cell interactions such as acinar-ductal cell contacts. Furthermore, we discover a strong link between a mesenchymal cell subtype and endocrine progenitor cells, and identify the set of genes that distinguishes this mesenchymal subtype. Thus, CIM-Seq is a general method for cell interaction studies that can be used on cell types defined to an arbitrary resolution allowing identification of interacting sub-cell types or cell states.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626527683.0
  },
  {
    "data_format": 2,
    "description": "Open source simulation engine for coarse-grained Brownian dynamics",
    "filenames": [
      "Singularity"
    ],
    "full_name": "Betterton-Lab/C-GLASS",
    "latest_release": "v0.2.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-c-glass\" class=\"anchor\" href=\"#c-glass\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eC-GLASS\u003c/h1\u003e\n\u003cp\u003eA \u003cstrong\u003eC\u003c/strong\u003eoarse-\u003cstrong\u003eG\u003c/strong\u003erained \u003cstrong\u003eL\u003c/strong\u003eiving \u003cstrong\u003eA\u003c/strong\u003ective \u003cstrong\u003eS\u003c/strong\u003eystem \u003cstrong\u003eS\u003c/strong\u003eimulator\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.com/Betterton-Lab/C-GLASS\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/061b295758d2c64d80b7d3e97ceebc53e21cc632602b7b32c77f046c105d84ac/68747470733a2f2f7472617669732d63692e636f6d2f426574746572746f6e2d4c61622f432d474c4153532e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/Betterton-Lab/C-GLASS.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.5281/zenodo.3841613\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/96212f14675a05209e745328444de906fc58f72d1794af88b9db02b4fe6f24e5/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333834313631332e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3841613.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"figs/cglass_snapshot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"figs/cglass_snapshot.png\" alt=\"A simulation using C-GLASS\" title=\"A simulation using C-GLASS\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eFirst clone the repo, including submodule dependencies.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/Betterton-Lab/C-GLASS\ncd C-GLASS\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eC-GLASS can either be run in a container using Docker or Singularity, or be built from source using CMake.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-with-docker\" class=\"anchor\" href=\"#running-with-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning with Docker\u003c/h3\u003e\n\u003cp\u003eA pre-built image of C-GLASS is available as a \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e image. To download the image, run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker pull jeffmm/cglass\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo use the image, run the provided script to launch a Docker container named \u003ccode\u003ecglass_latest\u003c/code\u003e in the background\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./launch_docker.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou may also build the Docker image yourself by providing the launch script with the \u003ccode\u003e-b\u003c/code\u003e flag.\u003c/p\u003e\n\u003cp\u003eTo launch C-GLASS, run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e cglass_latest cglass.exe [optional-flags] [parameter-file]\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-with-singularity\" class=\"anchor\" href=\"#running-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning with Singularity\u003c/h3\u003e\n\u003cp\u003eIf you are using Singularity, C-GLASS is also available as a Singularity image. The command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull docker://jeffmm/cglass\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewill create a local file named \u003ccode\u003ecglass_latest.sif\u003c/code\u003e. You may then run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e cglass_latest.sif cglass.exe [optional-flags] [parameter-file]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe Singularity image may also be built locally using the provided recipe in the file \u003ccode\u003eSingularity\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-from-source\" class=\"anchor\" href=\"#building-from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding from source\u003c/h3\u003e\n\u003cp\u003eC-GLASS is ready to be built from source using CMake, provided several dependencies are installed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCMake (version 3.13+)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jbeder/yaml-cpp\"\u003elibyaml-cpp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003elibgsl-dev\u003c/li\u003e\n\u003cli\u003elibopenmpi-dev\u003c/li\u003e\n\u003cli\u003elibfftw3-dev\u003c/li\u003e\n\u003cli\u003elibboost-math1.67-dev\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIncluded is a script for building C-GLASS with CMake. To build C-GLASS (without graphics or parallelization) run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./install.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThere are additional flags for building with OpenMP, building with graphics, installing C-GLASS in \u003ccode\u003e/usr/local\u003c/code\u003e, etc. To see a menu of options, run\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./install.sh -h\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-with-graphics\" class=\"anchor\" href=\"#building-with-graphics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding with graphics\u003c/h3\u003e\n\u003cp\u003eC-GLASS is available with graphics for Mac OSX. To install on Mac OSX, you will need the glew and glfw3 libraries, both of which can be installed using \u003ca href=\"https://brew.sh/\" rel=\"nofollow\"\u003eHomebrew\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebrew install glew\nbrew install glfw\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou may also need to help CMake find your OpenGL Framework libraries.\u003c/p\u003e\n\u003cp\u003eSeveral other libraries are required for running C-GLASS with graphics on Linux or in WSL. See the \u003ccode\u003esrc/CMakeLists.txt\u003c/code\u003e file for a comprehensive list of libraries passed to the compiler when building C-GLASS with graphics on WSL.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-c-glass\" class=\"anchor\" href=\"#running-c-glass\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning C-GLASS\u003c/h2\u003e\n\u003cp\u003eThe C-GLASS executable is run as\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecglass.exe [optional-flags] [parameter-file] \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe following flags are available:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--help, -h\n    Show the help menu which gives short descriptions about each of the flags\n    as well as binary usage\n \n --run-name rname, -r rname \n    Overwrites the parameter \"run_name\" with rname which serves as a prefix for\n    all outputs \n\n--n-runs num, -n num\n    Overwrites the parameter \"n_runs\" with num, which tells the simulation how\n    many times to run the given parameter set with different random number\n    generator seeds.\n\n--movie, -m\n    Uses the parameters file params_file to load any output files that were\n    generated from previous runs of the simulation to replay the graphics and\n    record the frames as bitmaps into the directory specified with the\n    \"movie_directory\" parameter\n\n--analysis, -a\n    Loads posit/spec files into the simulation for analysis in the same manner\n    as the movie flag\n\n-reduce reduce_factor, -R reduce_factor\n    Reads in output files and writes new output files that are smaller by a\n    factor of reduce_factor, effectively reducing time resolution of output\n    data.\n\n--load, -l\n    Specifies to load any checkpoint files corresponding to the given parameter\n    file, which can be used to continue a simulation that ended prematurely.\n    New simulation will be given the name old_simulation_name_reload00n where n\n    is the number of reloads performed on that simulation.\n\n--with-reloads, -w\n    If running analyses or making movies, C-GLASS will look for parameter files\n    that have the same run name but with the reload00n addendum and attempt to\n    open the corresponding output files whenever it reached EOF while reading\n    an output file.\n\n--blank, -b\n    Generates all relevant parameter files using the SimulationManager without\n    running the simulations. Useful for generating many parameter files from\n    parameter sets (discussed below) and deploying simulations on different\n    processors and/or machines.\n\n--auto-graph, -G\n    By default, C-GLASS will wait for the user to press the ESC key in the\n    OpenGL graphics window before starting to run the simulation. Providing\n    this flag will cause the simulation to begin immediately without user\n    input. Goes great with the -m flag for creating multiple movies without\n    input from the user.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameter-files\" class=\"anchor\" href=\"#parameter-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameter files\u003c/h2\u003e\n\u003cp\u003eAll parameters used in the simulation, along with their default values and data types, are specified in the \u003ccode\u003edefault_config.yaml\u003c/code\u003e file in the \u003ccode\u003econfig\u003c/code\u003e folder.\u003c/p\u003e\n\u003cp\u003eThe parameter file is a YAML file and looks like:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003eglobal_param_1\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003egp1_value\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003eglobal_param_2\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003egp2_value\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003especies\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003eglobal_species_param_1\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003egsp1_value\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003eglobal_species_param_2\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003egsp2_value\u003c/span\u003e\n\u003cspan class=\"pl-ent\"\u003especific_species_name\u003c/span\u003e:\n    \u003cspan class=\"pl-ent\"\u003especies_param_1\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003esp1_value\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003especies_param_2\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003esp2_value\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee the \u003ccode\u003eexamples\u003c/code\u003e folder for examples of parameter files.\u003c/p\u003e\n\u003cp\u003eNotice that there are three parameter types: global parameters, global species parameters, and species parameters. Global parameters are parameters that are common to the entire system, such system size, integration time step, etc. Species parameters are unique to the specified species, such as \u003ccode\u003efilament\u003c/code\u003e. There is also an optional global species parameter type that affects every species, such as the frequency to write to output files.\u003c/p\u003e\n\u003cp\u003eWhat do I mean by species? C-GLASS assumes that any given simulation will likely have many copies of one kind of thing, which I call a species, perhaps interacting with other species of other kinds. In a system of interacting spheres, the species is \u0027sphere.\u0027 In a system of interacting semiflexible filaments, the species is \u0027filament.\u0027 Simulations can have many types of species all interacting with each other with different species-species interaction potentials.\u003c/p\u003e\n\u003cp\u003eIf any parameter is not specified in the parameter file, any instance of that parameter in the simulation will assume its default value specified in the \u003ccode\u003econfig/default_config.yaml\u003c/code\u003e file.\u003c/p\u003e\n\u003cp\u003eSome important global parameters are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eseed\n    simulation seed to use with random number generator \nrun_name\n    prefix for all output files\nn_runs\n    number of individual runs of each parameter type\nn_random\n    number of samples from a random parameter space (see more below)\nn_dim\n    number of dimensions of simulation\nn_periodic\n    number of periodic dimensions of simulation\ndelta   \n    simulation time step\nn_steps\n    total number of steps in each simulation\nsystem_radius\n    \"box radius\" of system\ngraph_flag\n    run with graphics enabled\nn_graph\n    how many simulation steps to take between updating graphics\nmovie_flag\n    whether to record the graphics frames into bitmaps\nmovie_directory\n    local directory used to save the recorded bitmaps\nthermo_flag\n    whether to output thermodynamics outputs (stress tensors, etc)\nn_thermo\n    how often to output the thermodynamics outputs\npotential_type\n    can be \u0027wca\u0027 or \u0027soft\u0027 for now\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSome important global species parameters are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enum\n    how many to insert into system\ninsertion_type\n    how to insert object into system (e.g. random)\noverlap\n    whether species can overlap at initiation\ndraw_type\n    (orientation, fixed, or bw) how to color the object\ncolor\n    a double that specifies the RGB value of the object\nposit_flag\n    whether to output position files\nn_posit\n    how often to output position files\nspec_flag\n    whether to output species files\nn_spec\n    how often to output species files\ncheckpoint_flag\n    whether to output checkpoint files\nn_checkpoint\n    how often to output checkpoint files\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-anchor-parameters\" class=\"anchor\" href=\"#anchor-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnchor parameters\u003c/h3\u003e\n\u003cp\u003eC-GLASS has the capability to independently control crosslinker and motor protein anchor parameters. Anchor parameters are controlled within the Crosslink map in the input Yaml file:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-ent\"\u003eCrosslink\u003c/span\u003e:\n  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e other crosslink params here\u003c/span\u003e\n  \u003cspan class=\"pl-ent\"\u003eAnchors\u003c/span\u003e:\n     - \u003cspan class=\"pl-ent\"\u003evelocity_s\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e50\u003c/span\u003e\n       \u003cspan class=\"pl-ent\"\u003ecolor\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e3.5\u003c/span\u003e\n     - \u003cspan class=\"pl-ent\"\u003ecolor\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4.5\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOnly two anchors are permitted per crosslinker or motor protein. The anchor parameters obey the following rules when parameters are left blank:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf no anchors are listed, the anchor parameters will both be set to default.\u003c/li\u003e\n\u003cli\u003eIf one anchor is listed, the other anchor will copy its parameters. Any unlisted parameters will be set to default.\u003c/li\u003e\n\u003cli\u003eIf two anchors are listed, and one anchor has a parameter that the other doesn\u0027t, the one that doesn\u0027t have the parameter will copy the parameter from the other.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn the above example, Anchor 1 will have velocity_s=50, velocity_d=0 (default), color=3.5, and Anchor 2 will have velocity_s=50 (copied), velocity_d=0 (default), color=4.5.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-advanced-usage\" class=\"anchor\" href=\"#advanced-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvanced usage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-running-unit-tests\" class=\"anchor\" href=\"#running-unit-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning unit tests\u003c/h3\u003e\n\u003cp\u003eOne may run C-GLASS\u0027s unit tests by passing \u003ccode\u003e-DTESTS=TRUE\u003c/code\u003e to CMake\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir build\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e build\ncmake -DTESTS=TRUE ..\nmake\nmake \u003cspan class=\"pl-c1\"\u003etest\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-adding-new-parameters\" class=\"anchor\" href=\"#adding-new-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdding new parameters\u003c/h3\u003e\n\u003cp\u003eC-GLASS comes with it\u0027s own parameter initialization tool, \u003ccode\u003econfigure_C-GLASS.exe\u003c/code\u003e, which is installed automatically along with the C-GLASS binary using CMake. The configurator makes it easy to add new parameters to the simulation without mucking around in the source code. Just add your new parameter to \u003ccode\u003econfig/default_config.yaml\u003c/code\u003e file using the following format:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enew_parameter_name: [default_parameter_value, parameter_type] \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen run the configurator using\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./configure_cglass.exe config/default_config.yaml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning configure_cglass.exe will look at all the parameters in the default config file and add them seamlessly to the proper C-GLASS headers, and you can begin using them after recompiling C-GLASS using CMake.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-parameter-sets\" class=\"anchor\" href=\"#parameter-sets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameter sets\u003c/h3\u003e\n\u003cp\u003eUsing parameter sets, it becomes easier to run many simulations over a given parameter space. There are two types of parameter sets possible with C-GLASS: defined and random. Each parameter set type works the same with both global parameters and species parameters.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-defined-parameter-sets\" class=\"anchor\" href=\"#defined-parameter-sets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDefined parameter sets\u003c/h4\u003e\n\u003cp\u003eDefined parameter sets are specified by the \u003ccode\u003eV\u003c/code\u003e prefix in the parameter file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eseed: 4916819461895\nrun_name: defined_set\nn_runs: N\nparameter_name1: param_value1\nparameter_name2: [V, param_value2, param_value3]\nparameter_name3: [V, param_value4, param_value5]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eParameters specified in this way (as lists of parameters) will be iterated over until every possible combination of parameters has been run. In this example, C-GLASS will run N simulations each of the following 4 parameter sets:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eseed: random_seed_1\nrun_name: defined_set_v000\nn_runs: N\nparameter_name1: param_value1\nparameter_name2: param_value2\nparameter_name3: param_value4\n\nseed: random_seed_2\nrun_name: defined_set_v001\nn_runs: N\nparameter_name1: param_value1\nparameter_name2: param_value2\nparameter_name3: param_value5\n\nseed: random_seed_3\nrun_name: defined_set_v002\nn_runs: N\nparameter_name1: param_value1\nparameter_name2: param_value3\nparameter_name3: param_value4\n\nseed: random_seed_4\nrun_name: defined_set_v003\nn_runs: N\nparameter_name1: param_value1\nparameter_name2: param_value3\nparameter_name3: param_value5\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-random-parameter-sets\" class=\"anchor\" href=\"#random-parameter-sets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRandom parameter sets\u003c/h4\u003e\n\u003cp\u003eRandom parameter sets are designed specifically to be used with polynomial-chaos theory for n-dimensional parameter spaces for large n. Random sets are used in the following way:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eseed: 2546954828254\nn_runs: N\nn_random: M\nparameter_name1: param_value1\nparameter_name2: [R, A, B] # sets to random real in range (A,B)\nparameter_name3: [RINT, C, D] # sets to random int in range [C,D]\nparameter_name4: [RLOG, F, G] # sets to 10^K for rand real K in range (F,G)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGiven this parameter file, C-GLASS will run N simulations each of M random parameter sets. The random parameter sets are generated in ranges specified in the lists that are prefixed by the R, RINT, RLOG options.\u003c/p\u003e\n\u003cp\u003eIn this example, the sampled parameter space has dimensionality of n=3, since there are only three parameters we are sampling over. Each parameter set will have a random real number for parameter_name2 in the the range (A,B), a random integer in the range [C,D] for parameter_name3, and will set parameter_name4 to 10^K for random real number K in the range (F,G).  C-GLASS will then run each parameter set N times each with a unique seed, and repeat this random process M times. It will therefore take N samples of M random points in the n-dimensional parameter space.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-interactions\" class=\"anchor\" href=\"#interactions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInteractions\u003c/h3\u003e\n\u003cp\u003eThe Interaction Manager in C-GLASS was written with short-range interactions in mind. For this reason, interactions are treated by considering pair-wise interactions between neighboring interactor-elements that make up a composite object (e.g. small, rigid segments that compose a flexible filament). For this reason, interactions use cell lists to improve performance. Furthermore, simulating large objects in C-GLASS requires representing the object as a composite of smaller, simple objects. An example of how a large object should be decomposed into simple objects is done in the Filament class.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-potentials\" class=\"anchor\" href=\"#potentials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePotentials\u003c/h3\u003e\n\u003cp\u003eC-GLASS is designed to be able to use interchangable potentials for various objects. However, potentials need to be added manually as a subclass of PotentialBase, included in PotentialManager, and a corresponding potential_type added to definitions.h for lookup purposes (see the InitPotentials method in PotentialManager.h for examples).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h3\u003e\n\u003cp\u003eC-GLASS has four output types. Three are species specific (posit, spec, checkpoint), and the fourth is the statistical information file (thermo). All files are written in binary.\u003c/p\u003e\n\u003cp\u003eThe posit file has the following header format:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eint n_steps, int n_posit, double delta \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFollowed by n_steps/n_posit lines of data with the format:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edouble position[3]\ndouble scaled_position[3]\ndouble orientation[3]\ndouble diameter\ndouble length\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere the scaled position is position mapped into the periodic coordinate space. The position itself gives the particle trajectory over time independent of periodicity.\u003c/p\u003e\n\u003cp\u003eThe spec file is a custom output file for each species, and can have the same information as the posit file or additional information if needed.\u003c/p\u003e\n\u003cp\u003eThe checkpoint file is almost a copy of the spec file, except it also contains the random number generator information and is overwritten every n_checkpoint steps in the simulation. It can therefore be used to resume a simulation that ended prematurely.\u003c/p\u003e\n\u003cp\u003eThe thermo file contains the following header information:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eint n_steps, int n_thermo, double delta, int n_dim\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003efollowed by n_steps/n_thermo lines of data in the following format:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edouble unit_cell[9]\ndouble pressure_tensor[9]\ndouble pressure\ndouble volume\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere the pressure is the isometric pressure, and the pressure tensor is calculated from the time-averaged stress tensor.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data-analysis\" class=\"anchor\" href=\"#data-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData analysis\u003c/h3\u003e\n\u003cp\u003eIf analysis operations of output files are already defined for your species, as is the case for the Filament species, analyzing outputs is a simple matter. First, make sure the desired analysis flag is set in the species parameters for that species.\u003c/p\u003e\n\u003cp\u003eFor example, in the Filament species there is a persistence length analysis that produces .mse2e files that tracks the mean-square end-to-end distance of semiflexible filaments. This is triggered by a parameter lp_analysis=1, which can be set in the parameter file.\u003c/p\u003e\n\u003cp\u003eAnaylses are run by running C-GLASS in the following way:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecglass.exe -a parameter_file.yaml.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNOTE: It is important to keep in mind that the parameter_file should be identical to the parameter file used to generate the outputs. There are a few exceptions that only affect post-processing, such as analysis flags, but this is true in general.\u003c/p\u003e\n\u003cp\u003eThe way inputs and outputs are meant to work in C-GLASS is such that during a simulation, output data are generated in the posit, spec, and checkpoint formats, and during analysis, the same output data are read back into the data structures in C-GLASS for processing. The .posit files just contain bare-bones information that allow many types of simple analyses, but .spec files should in general contain all the necessary information to recreate the trajectory for a member of a species.\u003c/p\u003e\n\u003cp\u003eFor a new species analysis method, the analysis routines should be defined in the species container class, rather than the species member class, and called by the inherited RunAnalysis method of the SpeciesBase class (and likewise for analysis initialization and finalization, see examples for details).\u003c/p\u003e\n\u003cp\u003eFor example, the RunSpiralAnalysis routine is called by the RunAnalysis method in FilamentSpecies, which uses the Filament .spec file as an input to do the necessary analysis, whose results are placed into a new file ending in filament.spiral. See Filament and FilamentSpecies for examples of how analyses can be initialized, processed, etc.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directory-structure\" class=\"anchor\" href=\"#directory-structure\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirectory structure\u003c/h2\u003e\n\u003cp\u003eThe directory structure is as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eC-GLASS\n\u251c\u2500\u2500 include\n\u2502   \u2514\u2500\u2500 cglass\n\u2502       \u2514\u2500\u2500 (header files)\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 CMakeLists.txt\n\u2502   \u251c\u2500\u2500 executable\n\u2502   \u2502   \u251c\u2500\u2500 CMakeLists.txt\n\u2502   \u2502   \u2514\u2500\u2500 cglass_main.cpp\n\u2502   \u251c\u2500\u2500 configurator\n\u2502   \u2502   \u251c\u2500\u2500 CMakeLists.txt\n\u2502   \u2502   \u2514\u2500\u2500 configurator.cpp\n\u2502   \u2514\u2500\u2500 (source files)\n\u251c\u2500\u2500 config\n\u2502   \u2514\u2500\u2500 default_config.yaml\n\u251c\u2500\u2500 analysis\n\u2502   \u2514\u2500\u2500 (Python analysis files)\n\u251c\u2500\u2500 scripts\n\u2502   \u2514\u2500\u2500 (utility files)\n\u251c\u2500\u2500 examples\n\u2502   \u2514\u2500\u2500 (parameter file examples)\n\u251c\u2500\u2500 docker\n\u2502   \u2514\u2500\u2500 Dockerfile\n\u251c\u2500\u2500 extern\n\u2502   \u2514\u2500\u2500 KMC\n\u251c\u2500\u2500 tests\n\u2502   \u251c\u2500\u2500 CMakeLists.txt\n\u2502   \u251c\u2500\u2500 catch2\n\u2502   \u2502   \u2514\u2500\u2500 catch.hpp\n\u2502   \u2514\u2500\u2500 (C-GLASS unit tests)\n\u251c\u2500\u2500 docs\n\u2502   \u251c\u2500\u2500 CMakeLists.txt\n\u2502   \u2514\u2500\u2500 main.md\n\u251c\u2500\u2500 figs\n\u2502   \u2514\u2500\u2500 (example simulation figures)\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 install.sh\n\u251c\u2500\u2500 launch_docker.sh\n\u251c\u2500\u2500 .travis.yml\n\u2514\u2500\u2500 .gitignore\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-c-glass\" class=\"anchor\" href=\"#about-c-glass\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout C-GLASS\u003c/h2\u003e\n\u003cp\u003eC-GLASS is written in C++ and designed for general coarse-grained physics simulations of active living matter, produced with modularity and scalability in mind. All objects in the simulation are representable as a composite of what I call \"simple\" objects (points, spheres, rigid cylinders, and 2d polygon surfaces would all qualify). For short-range interactions, C-GLASS uses cell and neighbor lists for improved performance and OpenMP for parallelization.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThis software is licensed under the terms of the BSD-3 Clause license. See the \u003ccode\u003eLICENSE\u003c/code\u003e for more details.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1625759937.0
  },
  {
    "data_format": 2,
    "description": "A re-write of the gemBS pipeline framework in Rust",
    "filenames": [
      "Singularity",
      "texlive/Singularity.tex"
    ],
    "full_name": "heathsc/gemBS-rs",
    "latest_release": "v4.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-gembs-rs\" class=\"anchor\" href=\"#gembs-rs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egemBS-rs\u003c/h1\u003e\n\u003cp\u003eA rewrite of the \u003ca href=\"https://github.com/heathsc/gemBS\"\u003egemBS\u003c/a\u003e pipeline\nframework from Python/C into Rust.\u003c/p\u003e\n\u003cp\u003egemBS is a high performance bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3, a high performance read aligner and\nbs_call, a high performance variant and methyation caller, into a streamlined and efficient pipeline for\nbisulfite sequence analysis.\u003c/p\u003e\n\u003cp\u003eThe manuscript describing the original gemBS pipeline is available\n\u003ca href=\"https://doi.org/10.1093/bioinformatics/bty690\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe rewrite of the pipeline into Rust has two aims: (1) to have a more\nrobust pipeline and (2) to provide a more flexible platform for future\ndevelopments.  All of the tools developed for the pipeline except for the GEM3 mapper (being an external project that is also very stable!) have now been re-written in Rust. These include bs_call, the methylation and SNV-variant caller, and the methylation and SNP extractions tools mextr and snpxtr.  In all cases the running times are comparable to the original C versions.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eThe pipeline uses samtools for generating sorted BAM/CRAM files from GEM3 and bcftools for merging and indexing BCF files produced by bs_call.  In addition, many of the tools link to htslb to enable reading of BAM/CRAM and reading/writing of BCF files.  Samtools and htslib are automatically installed during the installation of the gemBS pipeline.   There is also an optional dependency on TeXLive which is used to produce pdf versions of the QC reports.  If requested by the user this is also installed with the pipeline.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicensing\u003c/h2\u003e\n\u003cp\u003egemBS is licensed under the GPL.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h2\u003e\n\u003cp\u003eUse \u003ccode\u003egit clone --recursive\u003c/code\u003e to retrieve the complete source code including the code from external projects such as \u003ccode\u003egem3-mapper\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/heathsc/gemBS-rs.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configure--install\" class=\"anchor\" href=\"#configure--install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfigure \u0026amp; Install\u003c/h2\u003e\n\u003cp\u003eBefore starting the installation of gemBS, you will need to install\nor check the installation of several packages.\u003c/p\u003e\n\u003cp\u003ea) gcc with development libraries\u003c/p\u003e\n\u003cp\u003eb) rust (for installation instructions see \u003ca href=\"https://www.rust-lang.org/learn/get-started\" rel=\"nofollow\"\u003ehere\u003c/a\u003e).  Note that if you have rust already installed you should update it using \u003ccode\u003erustup update\u003c/code\u003e before trying to compile gemBS.\u003c/p\u003e\n\u003cp\u003ec) zlib, libz2, lzma, openssl, libcurl, libncurses, wget, expat, ncurses, openssl, freetype, fontconfig\u003c/p\u003e\n\u003cp\u003eIf you are working on a clean (fairly recent) Ubuntu installation, you\ncan install everything required with the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eapt-get install -y build-essential git autoconf wget lbzip2 pkg-config cmake\napt-get install -y zlib1g-dev libbz2-dev libexpat1-dev\napt-get install -y libncurses5-dev liblzma-dev libssl-dev libcurl4-openssl-dev curl\napt-get install -y libfreetype6-dev libfontconfig1-dev\ncurl https://sh.rustup.rs -sSf \u0026gt; rust.sh \u0026amp;\u0026amp; sh rust.sh -y\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eDownload the gemBS distribution if you haven\u0027t already done so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/heathsc/gemBS-rs.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom the main gemBS-rs directory type the following to make the default config file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake gemBS_config.mk\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen look at the file gemBS_config.mk and make any changes that are required.  When the file is OK the pipeline and components can be built and installed by typing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf the make and install process is successful, a shell script called gemBS will be created in the main gemBS-rs directory.  This file should be copied to a directory that is in your PATH so that gemBS can be invoked from anywhere.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-check-your-installation\" class=\"anchor\" href=\"#check-your-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheck your installation\u003c/h2\u003e\n\u003cp\u003eFor checking your installation follow this\n\u003ca href=\"http://statgen.cnag.cat/gemBS/UserGuide/_build/html/example.html\" rel=\"nofollow\"\u003eworked example\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eDocumentation can be found at\n\u003ca href=\"http://statgen.cnag.cat/gemBS/\" rel=\"nofollow\"\u003egemBS\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-changelog\" class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChangelog:\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e4.0 First release of gemBS-rs (4th release of gemBS)\n4.0.1 Correct bug preventing that caused non-stranded mapping to fail\n4.0.2 Move to versions 1.12 of htslib/samtools/bcftools\n4.0.2 Change way we iterate over SAM/BAM/CRAM files to the same way used in samtools \n      view (the old way did not always work with cram files)\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1621670204.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "snakemake/workflow/envs/Singularity"
    ],
    "full_name": "radio1988/OneStopRNAseq",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-csci5980\" class=\"anchor\" href=\"#csci5980\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecsci5980\u003c/h1\u003e\n\u003cp\u003eFinal project for CSci 5980: deep learning for automatic music translation.\u003c/p\u003e\n\u003cp\u003eFollow theses steps to install all package dependencies for running the model:\u003c/p\u003e\n\u003cp\u003eWe first install software dependencies for manipulating raw audio (\u003ccode\u003effmpeg\u003c/code\u003e):\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eCreate a local software directory\n\u003ccode\u003emkdir ~/software\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the NASM assembler (dependency of ffmpeg):\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software\nwget https://www.nasm.us/pub/nasm/releasebuilds/2.14.02/nasm-2.14.02.tar.bz2\ntar -xvf nasm-2.14.02.tar.bz2\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e nasm-2.14.02\n./configure --prefix=\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/nasm/\nmake install\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"pl-smi\"\u003e$PATH\u003c/span\u003e:\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/nasm/bin/\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMake sure that NASM assembler installed correctly:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enasm -v\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe output should look something like:\n\u003ccode\u003eNASM version 2.14.02 compiled on Mar 11 2020\u003c/code\u003e\u003c/p\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eInstall ffmpeg:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software\nwget https://ffmpeg.org/releases/ffmpeg-4.2.2.tar.bz2\ntar -xvf ffmpeg-4.2.2.tar.bz2\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e ffmpeg-4.2.2\n./configure --prefix=\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/ffmpeg/\nmake install\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"pl-smi\"\u003e$PATH\u003c/span\u003e:\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/software/ffmpeg/bin/\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"5\"\u003e\n\u003cli\u003eMake sure that ffmpeg installed correctly:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003effmpeg -version\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe output should look something like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003effmpeg version 4.2.2 Copyright (c) 2000-2019 the FFmpeg developers\nbuilt with gcc 4.4.7 (GCC) 20120313 (Red Hat 4.4.7-23)\nconfiguration: --prefix=/home/csci5980/piehl008/software/ffmpeg/\nlibavutil      56. 31.100 / 56. 31.100\nlibavcodec     58. 54.100 / 58. 54.100\nlibavformat    58. 29.100 / 58. 29.100\nlibavdevice    58.  8.100 / 58.  8.100\nlibavfilter     7. 57.100 /  7. 57.100\nlibswscale      5.  5.100 /  5.  5.100\nlibswresample   3.  5.100 /  3.  5.100\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003eNow, we can make the virtual environment and install python packages.  First, create the virtual environment by running:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ccode\u003econda create --name audio-proj python=3.7\u003c/code\u003e\u003c/p\u003e\n\u003col start=\"7\"\u003e\n\u003cli\u003eNext, install packages by running\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/csci5980\nconda install --name audio-proj --file requirements.txt --channel defaults --channel conda-forge\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e(Note: this can take a while - and you need to say yes to installing everything after it solves the environment)\u003c/p\u003e\n\u003col start=\"8\"\u003e\n\u003cli\u003eTo activate the virtual environment, you can now run \u003ccode\u003esource activate audio-proj\u003c/code\u003e. Note: you should do this to test that you can activate the virtual evironment, but you probably shouldn\u0027t run a lot unless you are submitting jobs to the queue.  If you want to use this virtual environment through the MSI notebooks, check out the tutorial at \u003ca href=\"https://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\" rel=\"nofollow\"\u003ehttps://sunju.org/teach/DL-Spring-2020/TensorFlowPyTorch.html\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-adding-the-virtual-environment-to-jupyter-notebooks\" class=\"anchor\" href=\"#adding-the-virtual-environment-to-jupyter-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdding the Virtual Environment to Jupyter Notebooks\u003c/h3\u003e\n\u003cp\u003eNow that we have created the virtual environment, we can add it to the Jupyter notebook kernels so that we can use the virtual environment through MSI\u0027s notebook server. To do this, we have to add the kernel specifications to the known Jupyter kernels for our user:\u003c/p\u003e\n\u003col start=\"9\"\u003e\n\u003cli\u003eIf you haven\u0027t already, activate your virtual environment by running \u003ccode\u003esource activate audio-proj\u003c/code\u003e. Then enter\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ewhich python\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYour output should tell you where the python executable for this virtual environment lives - the output for me displays \u003ccode\u003e~/.conda/envs/audio-proj/bin/python\u003c/code\u003e.  If you see something that looks like \u003ccode\u003e/panfs/roc/msisoft/anaconda/anaconda3-2018.12/bin/python\u003c/code\u003e, go back and make sure that you have the virtual environment active and try again. After you have an ouput that clearly has the name of the virtual environment in the directory path (i.e. contains audio-proj in it), continue to the next step.\u003c/p\u003e\n\u003col start=\"10\"\u003e\n\u003cli\u003eNow, we need to create the kernel configuration. To do this run\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.local/share/jupyter/kernels/audio-proj\nnano \u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.local/share/jupyter/kernels/audio-proj/kernel.json\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe nano command will open a very basic text editor that you can navigate with the arrow keys. Enter the following:\u003c/p\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e{\n \"argv\": [\n  \"~/.conda/envs/audio-proj/bin/python\", #replace this with your path from step 9 above! (and delete this comment)\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"Audio Project Kernel\",\n \"language\": \"python\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere you replace the first line of the argv array with whatever executable path was output from step 9 above (it likely will be identical to this). To exit the nano text editor, type \u003ccode\u003eCtrl-x \u0026lt;RETURN\u0026gt;\u003c/code\u003e and then type \u003ccode\u003eY \u0026lt;RETURN\u0026gt;\u003c/code\u003e to save the file.\u003c/p\u003e\n\u003col start=\"11\"\u003e\n\u003cli\u003eNow that you have saved the kernel file, you should be able to go to \u003ccode\u003ehttps://notebooks.msi.umn.edu/\u003c/code\u003e and when you click on the \u003ccode\u003eNew\u003c/code\u003e tab to create a new file, you should be able to select \u003ccode\u003eAudio Project Kernel\u003c/code\u003e as an available kernel to run your newly created file in.\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623730067.0
  },
  {
    "data_format": 2,
    "description": "Recipe for VEP + Cache",
    "filenames": [
      "Singularityfiles/Singularity.99-GRCh38-merged",
      "Singularityfiles/Singularity.99-GRCh37-merged"
    ],
    "full_name": "matmu/vep",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containerized-variant-effect-predictor-vep--cache\" class=\"anchor\" href=\"#containerized-variant-effect-predictor-vep--cache\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainerized Variant Effect Predictor (VEP) + Cache\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/intent/tweet?hashtags=Ensembl,VEP,Singularity,Docker\u0026amp;url=https://github.com/matmu/vep\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/90bc908826728c0e4261acfff5619fd732c7be2b2a00624fce6363c9a3623c90/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f75726c2f687474702f736869656c64732e696f2e7376673f7374796c653d736f6369616c\" alt=\"Twitter\" data-canonical-src=\"https://img.shields.io/twitter/url/http/shields.io.svg?style=social\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u00a0+ \u003ca href=\"#Introduction\"\u003eIntroduction\u003c/a\u003e \u003cbr\u003e\n\u00a0+ \u003ca href=\"#Building-image-with-Singularity\"\u003eBuilding image with Singularity\u003c/a\u003e \u003cbr\u003e\n\u00a0+ \u003ca href=\"#Run-VEP\"\u003eRun VEP\u003c/a\u003e \u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0|-- \u003ca href=\"#More-options\"\u003eMore options\u003c/a\u003e \u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0|-- \u003ca href=\"#Examples\"\u003eExamples\u003c/a\u003e \u003cbr\u003e\n\u00a0+ \u003ca href=\"#Post-processing\"\u003ePost-processing\u003c/a\u003e \u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0|-- \u003ca href=\"#Split-VEP\"\u003eSplit VEP\u003c/a\u003e \u003cbr\u003e\n\u00a0\u00a0\u00a0\u00a0|-- \u003ca href=\"#Filtering-by-VEP-annotations\"\u003eFiltering by VEP annotations\u003c/a\u003e \u003cbr\u003e\n\u00a0+ \u003ca href=\"#VEP-plugins\"\u003eVEP plugins\u003c/a\u003e \u003cbr\u003e\n\u00a0+ \u003ca href=\"#Build-and-run-VEP-with-Docker\"\u003eBuild \u0026amp; run VEP with Docker\u003c/a\u003e \u003cbr\u003e\n\u00a0+ \u003ca href=\"#Acknowledgments\"\u003eAcknowledgements\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis documentation describes the usage of the Docker image at \u003ca href=\"https://hub.docker.com/r/matmu/vep\" rel=\"nofollow\"\u003ehttps://hub.docker.com/r/matmu/vep\u003c/a\u003e which contains the bioinformatics tool \u003cstrong\u003eEnsembl Variant effect predictor (VEP)\u003c/strong\u003e for annotating genetic variants. The image comes with\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMerged cache including RefSeq and Ensembl transcripts (VEP parameter --merged required)\u003c/li\u003e\n\u003cli\u003eReference genome and index\u003c/li\u003e\n\u003cli\u003ePlugins (annotation data is not included)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-available-versions\" class=\"anchor\" href=\"#available-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailable versions\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eHuman:\u003c/strong\u003e \u003ca href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38-merged.yml\"\u003e\u003cimg src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38-merged.yml/badge.svg\" alt=\"103-GRCh38-merged\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38.yml\"\u003e\u003cimg src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCh38.yml/badge.svg\" alt=\"103-GRCh38\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/101-GRCh38/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/101-GRCh38/badge.svg\" alt=\"101-GRCh38\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/100-GRCh38/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/100-GRCh38/badge.svg\" alt=\"100-GRCh38\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/100-GRCh38-merged/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/100-GRCh38-merged/badge.svg\" alt=\"100-GRCh38-merged\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/100-GRCh37/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/100-GRCh37/badge.svg\" alt=\"100-GRCh37\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/100-GRCh37-merged/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/100-GRCh37-merged/badge.svg\" alt=\"100-GRCh37-merged\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/99-GRCh38-merged/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/99-GRCh38-merged/badge.svg\" alt=\"99-GRCh38-merged\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/99-GRCh37-merged/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/99-GRCh37-merged/badge.svg\" alt=\"99-GRCh37-merged\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003cbr\u003e\n\u003cstrong\u003eMouse:\u003c/strong\u003e \u003ca href=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCm39.yml\"\u003e\u003cimg src=\"https://github.com/matmu/vep/actions/workflows/docker.103-GRCm39.yml/badge.svg\" alt=\"103-GRCm39\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/101-GRCm38/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/101-GRCm38/badge.svg\" alt=\"101-GRCm38\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/100-GRCm38/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/100-GRCm38/badge.svg\" alt=\"100-GRCm38\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/matmu/vep/workflows/100-GRCm38-merged/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/matmu/vep/workflows/100-GRCm38-merged/badge.svg\" alt=\"100-GRCm38-merged\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe term \u003ccode\u003emerged\u003c/code\u003e refers to the merged Ensembl/RefSeq cache. To be consistent with the Ensembl website, chose Ensembl cache only (i.e. without the term \u003ccode\u003emerged\u003c/code\u003e). Examples for available versions are \u003cstrong\u003e99-GRCh38\u003c/strong\u003e (VEP 99 with Ensembl cache for reference GRCh38) or \u003cstrong\u003e99-GRh37-merged\u003c/strong\u003e (VEP 99 with Ensembl/Refseq cache for reference GRCh37).\u003c/p\u003e\n\u003cp\u003eYou can also visit \u003ca href=\"https://hub.docker.com/r/matmu/vep/tags\" rel=\"nofollow\"\u003ehttps://hub.docker.com/r/matmu/vep/tags\u003c/a\u003e to get a list of available versions.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e If you require a container for a species not mentioned above, feel free to contact us or even better, create an issue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-image-with-singularity\" class=\"anchor\" href=\"#build-image-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild image with Singularity\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity build vep.\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eversion\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg docker://matmu/vep:\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eversion\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ccode\u003e\u0026lt;version\u0026gt;\u003c/code\u003e is a tag representing the Ensembl version and the species + version of the reference genome.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-vep\" class=\"anchor\" href=\"#run-vep\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun VEP\u003c/h2\u003e\n\u003cp\u003eTo run VEP execute\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eversion\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg vep [options]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewhereby \u003ccode\u003e\u0026lt;version\u0026gt;\u003c/code\u003e is replaced by a respective version (see above), e.g. \u003ccode\u003e99-CRCh38\u003c/code\u003e. It is essential to add the VEP option \u003ccode\u003e--merged\u003c/code\u003e when using an image with merged Ensembl/Refseq cache. For species except homo sapiens, also the parameter \u003ccode\u003e--species\u003c/code\u003e (e.g. \u003ccode\u003e--species mus_musculus\u003c/code\u003e), has to be set as well.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-more-options\" class=\"anchor\" href=\"#more-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMore options\u003c/h3\u003e\n\u003cp\u003eThe options for base cache/plugin directories, species and assembly are set to the right values by default and do not need to be set by the user.\u003c/p\u003e\n\u003cp\u003eVisit \u003ca href=\"http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html\" rel=\"nofollow\"\u003ehttp://www.ensembl.org/info/docs/tools/vep/script/vep_options.html\u003c/a\u003e for detailed information about all VEP options. Detailed information about \u003cstrong\u003einput/output formats\u003c/strong\u003e can be found at \u003ca href=\"https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#defaultout\" rel=\"nofollow\"\u003ehttps://www.ensembl.org/info/docs/tools/vep/vep_formats.html#defaultout\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-minimum-output-format-compressed-tab-delimited\" class=\"anchor\" href=\"#minimum-output-format-compressed-tab-delimited\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMinimum (output format: compressed tab delimited)\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.100-GRCh38-merged.simg vep --dir /opt/vep/.vep --merged --offline --cache --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf[.gz] --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.txt.gz --tab --compress_output bgzip\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf[.gz] --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.txt.gz --tab --compress_output bgzip\u003c/pre\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.100-GRCm38.simg vep --dir /opt/vep/.vep --offline --cache --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf[.gz] --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.txt.gz --tab --compress_output bgzip -species mus_musculus\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-minimum-output-format-compressed-vcf\" class=\"anchor\" href=\"#minimum-output-format-compressed-vcf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMinimum (output format: compressed vcf)\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf[.gz] --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf.gz --vcf --compress_output bgzip\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-full-annotation\" class=\"anchor\" href=\"#full-annotation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFull annotation\u003c/h4\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.100-GRCh38.simg vep --dir /opt/vep/.vep --offline --cache --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf[.gz] --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf.gz --vcf --compress_output bgzip --everything --nearest symbol        \u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-post-processing\" class=\"anchor\" href=\"#post-processing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePost-processing\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-split-vep\" class=\"anchor\" href=\"#split-vep\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSplit VEP\u003c/h3\u003e\n\u003cp\u003eThere is a plugin for \u003ccode\u003ebcftools\u003c/code\u003e that allows to split VEP annotations as well as sample information in a VCF file and convert it to a text file: \u003ca href=\"http://samtools.github.io/bcftools/howtos/plugin.split-vep.html\" rel=\"nofollow\"\u003ehttp://samtools.github.io/bcftools/howtos/plugin.split-vep.html\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-filtering-by-vep-annotations\" class=\"anchor\" href=\"#filtering-by-vep-annotations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFiltering by VEP annotations\u003c/h3\u003e\n\u003cp\u003eIf you chose to output the VEP annotations as text file, any command line tool (e.g. \u003ccode\u003eawk\u003c/code\u003e) or even \u003ccode\u003eExcel\u003c/code\u003e can be used for filtering the results. For VCF files, the image includes a VEP filtering script which can be executed by\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eversion\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg filter_vep [options]\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptions\u003c/h4\u003e\n\u003cp\u003eVisit \u003ca href=\"https://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html\" rel=\"nofollow\"\u003ehttps://www.ensembl.org/info/docs/tools/vep/script/vep_filter.html\u003c/a\u003e for detailed info about available options.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-filtering-examples\" class=\"anchor\" href=\"#filtering-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFiltering examples\u003c/h4\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-filter-for-rare-variants\" class=\"anchor\" href=\"#filter-for-rare-variants\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFilter for rare variants\u003c/h5\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eversion\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.simg filter_vep --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.filtered.vcf --only_matched --filter \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e(IMPACT is HIGH or IMPACT is MODERATE or IMPACT is LOW) and (BIOTYPE is protein_coding) and ((PolyPhen \u0026gt; 0.446) or (SIFT \u0026lt; 0.05)) and (EUR_AF \u0026lt; 0.001 or gnomAD_NFE_AF \u0026lt; 0.001 or (not EUR_AF and not gnomAD_NFE_AF))\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-vep-plugins\" class=\"anchor\" href=\"#vep-plugins\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVEP plugins\u003c/h2\u003e\n\u003cp\u003eVEP allows several other annotations sources (aka Plugins). Their respective Perl modules are included in the image, the annotation files have to be added seperately, however. The list of plugins as well as instructions on how to download and pre-process the annotation files can be found at: \u003ca href=\"http://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html\" rel=\"nofollow\"\u003ehttp://www.ensembl.org/info/docs/tools/vep/script/vep_plugins.html\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e vep.100-GRCh38-merged.simg vep --dir /opt/vep/.vep --merged --offline --cache --input_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.vcf[.gz] --output_file \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003efilename\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e.txt.gz --tab --compress_output bgzip --plugin CADD,/path/to/ALL.TOPMed_freeze5_hg38_dbSNP.tsv.gz\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-and-run-vep-with-docker\" class=\"anchor\" href=\"#build-and-run-vep-with-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild and run VEP with Docker\u003c/h2\u003e\n\u003cp\u003eTo pull the image and run the container with Docker use\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run matmu/vep:\u0026lt;version\u0026gt; vep [options]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUnlike Singularity, the directories of \u003cstrong\u003ePlugin\u003c/strong\u003e annotation files (e.g. \u003ccode\u003e/path/to/dir\u003c/code\u003e) have to be explicitely bound to a target directory (e.g. \u003ccode\u003e/opt/data\u003c/code\u003e) within the container with option \u003ccode\u003e-v\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -v /path/to/dir:/opt/data matmu/vep:\u0026lt;version\u0026gt; vep [options]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h2\u003e\n\u003cp\u003eThis document has been created by \u003cstrong\u003eJulia Remes\u003c/strong\u003e \u0026amp; \u003cstrong\u003eMatthias Munz\u003c/strong\u003e, \u003cstrong\u003eUniversity of L\u00fcbeck\u003c/strong\u003e, \u003cstrong\u003eGermany\u003c/strong\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1614861605.0
  },
  {
    "data_format": 2,
    "description": "Oxford Nanopore reference mapping, taxonomic classification, de novo assembly workflow primarily for viral sequence data",
    "filenames": [
      "singularity/Singularity.1.1.0",
      "singularity/Singularity.1.0.0"
    ],
    "full_name": "peterk87/nf-virontus",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-peterk87nf-virontus\" class=\"anchor\" href=\"#peterk87nf-virontus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epeterk87/nf-virontus\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eVirontus viral Oxford Nanopore sequence analysis pipeline\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/peterk87/nf-virontus\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ca2382eaedc143481936b2847287dfadcc9737054d5f078007bb7dcc0f5474bb/68747470733a2f2f7472617669732d63692e6f72672f70657465726b38372f6e662d7669726f6e7475732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/peterk87/nf-virontus.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1a7b876aea11f8490a824ae9376e2b0108e8b19b424effa1b67d0a7afcfe096e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d25453225383925413531392e31302e302d627269676874677265656e2e737667\" alt=\"Nextflow\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://bioconda.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/aabd08395c6e4571b75e7bf1bbd8ac169431a98dd75f3611f89e992dd0fcb477/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f696e7374616c6c253230776974682d62696f636f6e64612d627269676874677265656e2e737667\" alt=\"install with bioconda\" data-canonical-src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/peterk87/nf-virontus\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/701f9cda36830e80f60b8cb5e6108a4b9fdfcb6f09698b97e11a87e15dd71a93/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f6175746f6d617465642f70657465726b38372f6e662d7669726f6e7475732e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/docker/automated/peterk87/nf-virontus.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4297\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTable of Contents\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#peterk87nf-virontus\"\u003epeterk87/nf-virontus\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#introduction\"\u003eIntroduction\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#1-install-nextflow\"\u003e1) Install \u003c/a\u003e\u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#2-install-singularity\"\u003e2) Install \u003c/a\u003e\u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#3-install-virontus\"\u003e3) Install Virontus\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#usage\"\u003eUsage\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#preparing-your-data\"\u003ePreparing your data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#recommended-steps\"\u003eRecommended Steps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#example\"\u003eExample\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#credits\"\u003eCredits\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTOC created by \u003ca href=\"https://github.com/ekalinin/github-markdown-toc\"\u003egh-md-toc\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h3\u003e\n\u003cp\u003eThe Virontus pipeline is for the analysis of viral shotgun and amplicon  Oxford Nanopore sequence data. Given basecalled (and demultiplexed) Nanopore reads, Virontus produces one or more consensus sequences from read mapping with \u003ca href=\"https://github.com/lh3/minimap2\"\u003eMinimap2\u003c/a\u003e and variant calling with \u003ca href=\"https://github.com/nanoporetech/medaka\"\u003eMedaka\u003c/a\u003e and \u003ca href=\"https://www.nature.com/articles/s41467-019-12493-y\" rel=\"nofollow\"\u003eLongshot\u003c/a\u003e results with respect to one or more reference sequences. For amplicon sequencing, the user should provide a BED file containing primer coordinates with respect to a reference sequence so that the primer sequences can be trimmed using \u003ca href=\"https://github.com/andersen-lab/ivar\"\u003eiVar\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOptionally, Virontus will perform taxonomic classification with \u003ca href=\"https://ccb.jhu.edu/software/kraken2/\" rel=\"nofollow\"\u003eKraken2\u003c/a\u003e and \u003ca href=\"https://ccb.jhu.edu/software/centrifuge/manual.shtml\" rel=\"nofollow\"\u003eCentrifuge\u003c/a\u003e if index paths are provided. Reads can be filtered by taxonomic classification. By default viral and unclassified reads are filtered.\u003c/p\u003e\n\u003cp\u003eDe novo assembly with \u003ca href=\"https://github.com/rrwick/Unicycler\"\u003eUnicycler\u003c/a\u003e can be optionally performed if desired (specify \u003ccode\u003e--do_unicycler_assembly\u003c/code\u003e when running Virontus). If taxonomic classification is performed then taxonomically filtered reads will be assembled, otherwise all reads will be used for assembly.\u003c/p\u003e\n\u003cp\u003eThe Virontus pipeline is built using \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e and \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e containers making installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h3\u003e\n\u003cp\u003eYou will need to install \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e in order to run the Virontus pipeline.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e is recommended for portable and reproducible execution of the pipeline with the \u003ccode\u003e-profile singularity\u003c/code\u003e command-line argument.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-1-install-nextflow\" class=\"anchor\" href=\"#1-install-nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1) Install \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eIf you have \u003ca href=\"https://conda.io/\" rel=\"nofollow\"\u003eConda\u003c/a\u003e installed, you can install \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e with the following command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda install -c bioconda -c conda-forge nextflow\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-2-install-singularity\" class=\"anchor\" href=\"#2-install-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2) Install \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eInstalling \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e is optional but recommended for portability and reproducibility of results.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-3-install-virontus\" class=\"anchor\" href=\"#3-install-virontus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3) Install Virontus\u003c/h4\u003e\n\u003cp\u003eNextflow will automatically download the latest version of Virontus. You can show the Virontus help message with usage information with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run peterk87/nf-virontus --help\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cp\u003eShow usage information with\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003enextflow run peterk87/nf-virontus --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou should see the following\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eN E X T F L O W  ~  version 20.01.0\nLaunching `main.nf` [awesome_pauling] - revision: 9aeb19496b\nWARN: DSL 2 IS AN EXPERIMENTAL FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE\n==================================================================\npeterk87/nf-virontus   ~  version 1.1.0\n==================================================================\n\n  Git info: null - null [null]\n\nUsage:\nGiven some barcoded and demultiplexed reads, the typical command for running the pipeline is as follows:\n\n  nextflow run peterk87/nf-virontus \\\n    --reads \"reads/*.fastq\" \\\n    --outdir results \\\n    --ref_fasta refs.fa \\\n    -profile singularity # recommended to run with Singularity\n\nThe above assumes that you have a Centrifuge DB and Kraken2 DB located at\n/opt/DB/centrifuge/nt-2018-03-03/nt and /opt/DB/kraken2/standard2,\nrespectively, OR that you have set $CENTRIFUGE_DB and $KRAKEN2_DB env\nvariables. It also assumes that you have Singularity installed on your\nlocal machine and will automatically pull and use the Singularity image for\nthis workflow from Singularity-Hub.org.\n\nNOTE: For best results, please ensure you have Singularity installed prior to running this workflow.(https://sylabs.io/guides/3.3/user-guide/quick_start.html#quick-installation-steps)\n\nNote:\nThe argument supplied to \"--reads\" must be quoted if using \"*\" and other\ncharacters and symbols that could be shell expanded!\n\nMandatory Options:\n  --reads   Input reads directory and pattern (default: \"reads/*.fastq\")\n  --ref_fasta      Reference genomes multiFASTA file (one or more references\n                   in a single file) (default: \"./refs.fasta\")\nAmplicon Sequencing Options:\n  --bedfile        BED format file with amplicon sequencing primers info (optional).\n                   Produced as output from PrimalScheme.\nConsensus Generation Options:\n  --low_coverage   Low coverage threshold (default=3).\n                   Replace consensus sequence positions below this depth\n                   threshold with a low coverage character\n                   (see --low_cov_char)\n  --no_coverage    No coverage threshold (default=0).\n                   Replace consensus sequence positions with less than or\n                   equal this depth with a no coverage character\n                   (see --no_cov_char)\n  --low_cov_char   Low coverage character (default=\"N\")\n  --no_cov_char    No coverage character (default=\"-\")\n\nCluster Options:\n  --slurm_queue     Name of SLURM queue to run workflow on; use with -profile slurm\n\n\nTaxonomic Classification Options:\n  --centrifuge_db   Path to Centrifuge DB and prefix. If not specified, will\n                    try to get from $CENTRIFUGE_DB env variable or see if\n                    \"/opt/DB/centrifuge/nt-2018-03-03/nt\" exists.\n                    (default: null)\n  --kraken2_db      Path to Kraken2 DB directory. . If not specified, will\n                    try to get from $KRAKEN2_DB env variable or see if\n                    \"/opt/DB/kraken2/standard2\" exists.\n                    (default: null)\n  --taxids          Taxonomic IDs to filter reads by. Multiple taxids should\n                    be delimited by commas (`--taxids 1,2,3`). To disable\n                    filtering of reads based on taxids, do not provide a\n                    value for the `--taxids` argument:\n                    `nextflow run ... --taxids --reads ...`\n                    (default: 10239 (Viruses))\n  --exclude_unclassified_reads  Exclude unclassified reads from taxonomic\n                                classification filtered reads (default: false)\n\nDe Novo Assembly Options:\n  --do_unicycler_assembly       Assemble filtered reads using Unicycler? (default: false)\n\nOther Options:\n  --outdir          The output directory where the results will be saved\n                    (default: results)\n  -w/--work-dir     The temporary directory where intermediate data will be\n                    saved (default: ./work)\n  -profile          Configuration profile to use. [standard, singularity,\n                    conda, slurm] (default \u0027standard\u0027)\n  --tracedir        Pipeline run info output directory (default:\n                    results/pipeline_info)\n\nNote:\nIt is recommended that this workflow be executed with Singularity using the\nSingularity profile (`-profile singularity`) for maximum reproducibility and\nease of execution on different platforms.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-preparing-your-data\" class=\"anchor\" href=\"#preparing-your-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreparing your data\u003c/h4\u003e\n\u003cp\u003eIt is assumed that your data has been basecalled using the latest version of ONT Guppy (\u003ccode\u003eguppy_basecaller\u003c/code\u003e/\u003ccode\u003eguppy_basecall_server\u003c/code\u003e) and barcode demultiplexed using \u003ccode\u003eguppy_barcoder\u003c/code\u003e with the appropriate settings for the kits used.\u003c/p\u003e\n\u003cp\u003eAfter basecalling and demultiplexing, it is recommended that all reads belonging to a particular barcode be concatenated together and optionally renamed to represent the sample to which the reads belong. Virontus will extract the sample name for each input reads FASTQ file from the base filename of the FASTQ file (e.g. sample name will be \u003ccode\u003esample\u003c/code\u003e from filename \u003ccode\u003esample1.fastq\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eBelow is an example \u003ccode\u003eguppy_barcoder\u003c/code\u003e command for more lenient barcode demultiplexing:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eguppy_barcoder \\\n  -q 0 \\\n  --min_score 30 \\\n  --detect_mid_strand_barcodes \\\n  --allow_inferior_barcodes \\\n  --trim_barcodes \\\n  -i basecalled-reads/ \\\n  -s demuxed-reads \\\n  --arrangements_files barcode_arrs_nb12.cfg\u003c/pre\u003e\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-q 0\u003c/code\u003e to output less files per barcode\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--min_score 30\u003c/code\u003e for a lower barcode score threshold (default: 60)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--detect_mid_strand_barcodes\u003c/code\u003e to detect mid strand barcodes\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--trim_barcodes\u003c/code\u003e to trim barcodes from read sequences\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--arrangements_files\u003c/code\u003e to specify the barcodes used\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e It\u0027s recommended to use the default setting if possible to avoid misassigning reads into the incorrect barcodes.\u003c/p\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-recommended-steps\" class=\"anchor\" href=\"#recommended-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecommended Steps\u003c/h5\u003e\n\u003col\u003e\n\u003cli\u003eBasecall reads using Guppy\u003c/li\u003e\n\u003cli\u003eDemultiplex reads using \u003ccode\u003eguppy_barcoder\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eConcatenate reads belonging to the same barcode into a single file (\u003ccode\u003ecat barcode01/*.fastq \u0026gt; concat-reads/barcode01.fastq\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e[Optionally] rename concatenated barcoded reads with appropriate sample name (\u003ccode\u003emv concat-reads/barcode01.fastq concat-reads/sample1.fastq\u003c/code\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h4\u003e\n\u003cp\u003eExample command\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ nextflow run peterk87/nf-virontus \\\n    -resume \\\n    -profile singularity \\\n    --reads \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ereads/*.fq\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \\\n    --ref_fasta MN908947.3.fa \\\n    --low_coverage 3 \\\n    --bedfile nCoV-2019.bed\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhat you will see in the terminal:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eN E X T F L O W  ~  version 20.01.0\nLaunching `../main.nf` [ecstatic_davinci] - revision: 9aeb19496b\nWARN: DSL 2 IS AN EXPERIMENTAL FEATURE UNDER DEVELOPMENT -- SYNTAX MAY CHANGE IN FUTURE RELEASE\n=======================================================\npeterk87/nf-virontus v1.1.0\n=======================================================\nPipeline Name         : peterk87/nf-virontus\nPipeline Version      : 1.1.0\nRun Name              : ecstatic_davinci\nReads                 : reads/*.fq\nRef Sequences FASTA   : MN908947.3.fa\nPrimer Scheme         : nCoV-2019.bed\nConsensus No Coverage : \u0026lt;=0X positions replaced with \u0027-\u0027\nConsensus Low Coverage: \u0026lt;3X positions replaced with \u0027N\u0027\nCentrifuge DB         : null\nKraken2 DB            : null\nTaxids                : Filtering for taxids belonging to 10239\nUnicycler Assembly?   : No\nMax Memory            : 256 GB\nMax CPUs              : 48\nMax Time              : 10d\nOutput dir            : results\nWorking dir           : ./work\nContainer Engine      : singularity\nContainer             : virontus.simg\nCurrent home          : /home/pkruczkiewicz\nCurrent user          : pkruczkiewicz\nCurrent path          : ./\nScript dir            : ./nf-virontus\nConfig Profile        : standard\nCommand-Line          : nextflow run peterk87/nf-virontus -profile singularity -resume --reads \u0027reads/*.fq\u0027 --ref_fasta MN908947.3.fa --low_coverage 3 --bedfile nCoV-2019.bed\nNextflow version      : 20.01.0\n=========================================\nexecutor \u0026gt;  local (18)\n[0a/142458] process \u0026gt; REC2FASTA  [100%] 1 of 1 \u2714\n[a3/3168c5] process \u0026gt; MAP        [100%] 3 of 3 \u2714\n[0d/8a698f] process \u0026gt; IVAR_TRIM  [100%] 3 of 3 \u2714\n[76/f82320] process \u0026gt; MAP_STATS  [100%] 3 of 3 \u2714\n[cc/de6b36] process \u0026gt; MEDAKA     [100%] 3 of 3 \u2714\n[74/058b57] process \u0026gt; LONGSHOT   [100%] 3 of 3 \u2714\n[b4/5ed366] process \u0026gt; BCF_FILTER [100%] 3 of 3 \u2714\n[a3/ae8e3a] process \u0026gt; CONSENSUS  [ 100%] 3 of 3 \u2714\n[e3/f75ddb] process \u0026gt; COVERAGE_PLOT [100%] 3 of 3 \u2714\n\nPipeline execution summary\nCompleted at: 30-Apr-2020 14:00:11\nDuration    : 1m 40s\nCPU hours   : 0.1 (58.9% cached)\nSucceeded   : 18\nCached      : 4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExample output file tree structure:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eresults/\n\u251c\u2500\u2500 consensus\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NB02-MN908947.3.consensus.fasta\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NB04-MN908947.3.consensus.fasta\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 unclassified-MN908947.3.consensus.fasta\n\u251c\u2500\u2500 mapping\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NB02\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bamfiles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 NB02-MN908947.3.bam \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 NB02-MN908947.3.trim.bam \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 NB02-MN908947.3-depths.tsv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 NB02-MN908947.3.flagstat\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 NB02-MN908947.3.idxstats\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 NB04\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 bamfiles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 NB04-MN908947.3.bam \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 NB04-MN908947.3.trim.bam \n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 NB04-MN908947.3-depths.tsv\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 NB04-MN908947.3.flagstat\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 NB04-MN908947.3.idxstats\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 unclassified\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 bamfiles\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 unclassified-MN908947.3.bam \n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 unclassified-MN908947.3.trim.bam \n\u2502\u00a0\u00a0     \u251c\u2500\u2500 unclassified-MN908947.3-depths.tsv\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 unclassified-MN908947.3.flagstat\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 unclassified-MN908947.3.idxstats\n\u251c\u2500\u2500 pipeline_info\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 execution_dag.dot\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 execution_report.html\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 execution_timeline.html\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 execution_trace.txt\n\u251c\u2500\u2500 plots\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 coverage_plot-NB02-VS-MN908947.3-log_scale.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 coverage_plot-NB02-VS-MN908947.3.pdf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 coverage_plot-NB04-VS-MN908947.3-log_scale.pdf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 coverage_plot-NB04-VS-MN908947.3.pdf\n\u251c\u2500\u2500 refs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 MN908947.3.fa\n\u2514\u2500\u2500 vcf\n    \u251c\u2500\u2500 NB02-MN908947.3.longshot.filt.vcf\n    \u251c\u2500\u2500 NB02-MN908947.3.longshot.vcf\n    \u251c\u2500\u2500 NB02-MN908947.3.medaka.vcf\n    \u251c\u2500\u2500 NB04-MN908947.3.longshot.filt.vcf\n    \u251c\u2500\u2500 NB04-MN908947.3.longshot.vcf\n    \u251c\u2500\u2500 NB04-MN908947.3.medaka.vcf\n    \u251c\u2500\u2500 unclassified-MN908947.3.longshot.filt.vcf\n    \u251c\u2500\u2500 unclassified-MN908947.3.longshot.vcf\n    \u2514\u2500\u2500 unclassified-MN908947.3.medaka.vcf\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003epeterk87/nf-virontus was originally written by Peter Kruczkiewicz.\u003c/p\u003e\n\u003cp\u003eBootstrapped with \u003ca href=\"https://github.com/nf-core/tools\"\u003enf-core/tools\u003c/a\u003e \u003ccode\u003enf-core create\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThank you to the \u003ca href=\"https://github.com/nf-core/tools\"\u003enf-core/tools\u003c/a\u003e team for a great tool for bootstrapping creation of a production ready Nextflow workflows.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1603317652.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity.subsample",
      "containers/Singularity.make_prg_dependencies"
    ],
    "full_name": "mbhall88/pandora_analysis_pipeline",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\" alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\" alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\" class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHEMnet - Haematoxylin \u0026amp; Eosin and Molecular neural network\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA deep learning automated cancer diagnosis software using molecular labelling to improve pathological annotation of\nHaematoxylin and Eosin (H\u0026amp;E) stained tissue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDocker\u003c/p\u003e\n\u003cp\u003eYou can download and run the docker image using the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConda\u003c/p\u003e\n\u003cp\u003eInstall Openslide (this is necessary to open whole slide images) - download it \u003ca href=\"https://openslide.org/download/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCreate a conda environment from the \u003ccode\u003eenvironment.yml\u003c/code\u003e file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f environment.yml\nconda activate HEMnet\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-slide-preparation\" class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSlide Preparation\u003c/h3\u003e\n\u003cp\u003eName slides in the format: \u003ccode\u003eslide_id_TP53\u003c/code\u003e for TP53 slides and \u003ccode\u003eslide_id_HandE\u003c/code\u003e for H\u0026amp;E slides\nThe \u003ccode\u003eTP53\u003c/code\u003e and \u003ccode\u003eHandE\u003c/code\u003e suffix is used by HEMnet to identify the stain used.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-generate-training-and-testing-datasets\" class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Generate training and testing datasets\u003c/h3\u003e\n\u003cp\u003ea. Generate train dataset\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eb. Generate test dataset\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_test_dataset.py -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh -n non_cancer_thresh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-t\u003c/code\u003e is the relative path to the template slide from which all other slides will be normalised against. The template\nslide should be the same for each step.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is the tile magnification. e.g. if  the input is \u003ccode\u003e10\u003c/code\u003e then the tiles will be output at 10x\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-a\u003c/code\u003e is the align magnification. Paired TP53 and H\u0026amp;E slides will be registered at this magnification.\nTo reduce computation time we recommend this be less than the tile magnification - a five times downscale generally works well.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-c\u003c/code\u003e cancer threshold to apply to the DAB channel. DAB intensities less than this threshold indicate cancer.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-n\u003c/code\u003e non-cancer threshold to apply to the DAB channel. DAB intensities greater than this threshold indicate no cancer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\" href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Train and evaluate model\u003c/h3\u003e\n\u003cp\u003ea. Training model\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython train.py -b /path/to/base/directory -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size -s -w -f -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is CNN base model. eg. \u003ccode\u003eresnet50\u003c/code\u003e, \u003ccode\u003evgg16\u003c/code\u003e, \u003ccode\u003evgg19\u003c/code\u003e, \u003ccode\u003einception_v3\u003c/code\u003e and \u003ccode\u003exception\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-g\u003c/code\u003e is number of GPUs for training.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e\u003c/code\u003e is training epochs. Default is \u003ccode\u003e100\u003c/code\u003e epochs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-a\u003c/code\u003e is batch size. Default is \u003ccode\u003e32\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-s\u003c/code\u003e is option to save the trained model weights.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-w\u003c/code\u003e is option to used transfer learning. Model will used pre-trained weights from ImageNet at the initial stage.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-f\u003c/code\u003e is fine-tuning option. Model will re-train CNN base.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eb. Test model prediction\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython test.py  -b /path/to/base/directory -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory -w model_weights -m cnn_base -g num_gpus -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-w\u003c/code\u003e is path to trained model. eg. \u003ccode\u003etrained_model.h5\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is CNN base model (same to training step).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-g\u003c/code\u003e is number of GPUs for prediction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ec. Evaluate model performance and visualise model prediction\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython visualisation.py -b /path/to/base/directory -t /relative/path/to/training_output_directory -p /relative/path/to/test_output_directory  -o /relative/path/to/output/directory -i sample\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-t\u003c/code\u003e is path to training outputs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-p\u003c/code\u003e is path to test outputs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-i\u003c/code\u003e is name of Whole Slide Image for visualisation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\" href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Apply model to diagnose new images\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_inference.py -s \u0027/path/to/new/HE/Slides/\u0027 -o \u0027/path/to/output/directory/\u0027 -t \u0027/path/to/template/slide/\u0027 -nn \u0027/path/to/trained/model/\u0027 -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePredict on TCGA images with our pretrained model for colorectal cancer using \u003ca href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\" rel=\"nofollow\"\u003egoogle colab\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting HEMnet\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-the-team\" class=\"anchor\" href=\"#the-team\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe Team\u003c/h2\u003e\n\u003cp\u003ePlease contact Dr Quan Nguyen (\u003ca href=\"mailto:quan.nguyen@uq.edu.au\"\u003equan.nguyen@uq.edu.au\u003c/a\u003e), Andrew Su (\u003ca href=\"mailto:a.su@uqconnect.edu.au\"\u003ea.su@uqconnect.edu.au\u003c/a\u003e),\nand Xiao Tan (\u003ca href=\"mailto:xiao.tan@uqconnect.edu.au\"\u003exiao.tan@uqconnect.edu.au\u003c/a\u003e) for issues, suggestions,\nand we are very welcome to collaboration opportunities.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1604591459.0
  },
  {
    "data_format": 2,
    "description": "A neural network software for using Molecular labelling to improve pathological annotation of H and E tissues ",
    "filenames": [
      "Environments/Singularity.cpu"
    ],
    "full_name": "BiomedicalMachineLearning/HEMnet",
    "latest_release": "v1.0.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://mybinder.org/v2/gh/BiomedicalMachineLearning/HEMnet/master?filepath=Development\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\" alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://imjoy.io/#/app?plugin=https://github.com/BiomedicalMachineLearning/HEMnet/blob/master/Demo/HEMnet_Tile_Predictor.imjoy.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3f085c08b146a17bddb97fdd1f4258df4bc1c24a0435364e4856b7fbe8471e61/68747470733a2f2f696d6a6f792e696f2f7374617469632f62616467652f6c61756e63682d696d6a6f792d62616467652e737667\" alt=\"launch ImJoy\" data-canonical-src=\"https://imjoy.io/static/badge/launch-imjoy-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-hemnet---haematoxylin--eosin-and-molecular-neural-network\" class=\"anchor\" href=\"#hemnet---haematoxylin--eosin-and-molecular-neural-network\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHEMnet - Haematoxylin \u0026amp; Eosin and Molecular neural network\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription\u003c/h2\u003e\n\u003cp\u003eA deep learning automated cancer diagnosis software using molecular labelling to improve pathological annotation of\nHaematoxylin and Eosin (H\u0026amp;E) stained tissue.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eDocker\u003c/p\u003e\n\u003cp\u003eYou can download and run the docker image using the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull andrewsu1/hemnet    \ndocker run -it andrewsu1/hemnet\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConda\u003c/p\u003e\n\u003cp\u003eInstall Openslide (this is necessary to open whole slide images) - download it \u003ca href=\"https://openslide.org/download/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCreate a conda environment from the \u003ccode\u003eenvironment.yml\u003c/code\u003e file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f environment.yml\nconda activate HEMnet\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-slide-preparation\" class=\"anchor\" href=\"#slide-preparation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSlide Preparation\u003c/h3\u003e\n\u003cp\u003eName slides in the format: \u003ccode\u003eslide_id_TP53\u003c/code\u003e for TP53 slides and \u003ccode\u003eslide_id_HandE\u003c/code\u003e for H\u0026amp;E slides\nThe \u003ccode\u003eTP53\u003c/code\u003e and \u003ccode\u003eHandE\u003c/code\u003e suffix is used by HEMnet to identify the stain used.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-generate-training-and-testing-datasets\" class=\"anchor\" href=\"#1-generate-training-and-testing-datasets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Generate training and testing datasets\u003c/h3\u003e\n\u003cp\u003ea. Generate train dataset\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_train_dataset.py -b /path/to/base/directory -s relative/path/to/slides -o relative/path/to/output/directory  -t relative/path/to/template_slide.svs -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eb. Generate test dataset\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_test_dataset.py -b /path/to/base/directory -s /relative/path/to/slides -o /relative/path/to/output/directory -t relative/path/to/template_slide -m tile_mag -a align_mag -c cancer_thresh -n non_cancer_thresh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-t\u003c/code\u003e is the relative path to the template slide from which all other slides will be normalised against. The template\nslide should be the same for each step.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is the tile magnification. e.g. if  the input is \u003ccode\u003e10\u003c/code\u003e then the tiles will be output at 10x\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-a\u003c/code\u003e is the align magnification. Paired TP53 and H\u0026amp;E slides will be registered at this magnification.\nTo reduce computation time we recommend this be less than the tile magnification - a five times downscale generally works well.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-c\u003c/code\u003e cancer threshold to apply to the DAB channel. DAB intensities less than this threshold indicate cancer.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-n\u003c/code\u003e non-cancer threshold to apply to the DAB channel. DAB intensities greater than this threshold indicate no cancer.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-train-and-evaluate-model\" class=\"anchor\" href=\"#2-train-and-evaluate-model\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Train and evaluate model\u003c/h3\u003e\n\u003cp\u003ea. Training model\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython train.py -b /path/to/base/directory -t relative/path/to/training_tile_directory -l relative/path/to/validation_tile_directory -o /relative/path/to/output/directory -m cnn_base -g num_gpus -e epochs -a batch_size -s -w -f -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is CNN base model. eg. \u003ccode\u003eresnet50\u003c/code\u003e, \u003ccode\u003evgg16\u003c/code\u003e, \u003ccode\u003evgg19\u003c/code\u003e, \u003ccode\u003einception_v3\u003c/code\u003e and \u003ccode\u003exception\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-g\u003c/code\u003e is number of GPUs for training.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-e\u003c/code\u003e is training epochs. Default is \u003ccode\u003e100\u003c/code\u003e epochs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-a\u003c/code\u003e is batch size. Default is \u003ccode\u003e32\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-s\u003c/code\u003e is option to save the trained model weights.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-w\u003c/code\u003e is option to used transfer learning. Model will used pre-trained weights from ImageNet at the initial stage.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-f\u003c/code\u003e is fine-tuning option. Model will re-train CNN base.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eb. Test model prediction\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython test.py  -b /path/to/base/directory -t relative/path/to/test_tile_directory -o /relative/path/to/output/directory -w model_weights -m cnn_base -g num_gpus -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-w\u003c/code\u003e is path to trained model. eg. \u003ccode\u003etrained_model.h5\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-m\u003c/code\u003e is CNN base model (same to training step).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-g\u003c/code\u003e is number of GPUs for prediction.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ec. Evaluate model performance and visualise model prediction\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epython visualisation.py -b /path/to/base/directory -t /relative/path/to/training_output_directory -p /relative/path/to/test_output_directory  -o /relative/path/to/output/directory -i sample\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOther parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e-t\u003c/code\u003e is path to training outputs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-p\u003c/code\u003e is path to test outputs.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e-i\u003c/code\u003e is name of Whole Slide Image for visualisation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-apply-model-to-diagnose-new-images\" class=\"anchor\" href=\"#3-apply-model-to-diagnose-new-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Apply model to diagnose new images\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003epython HEMnet_inference.py -s \u0027/path/to/new/HE/Slides/\u0027 -o \u0027/path/to/output/directory/\u0027 -t \u0027/path/to/template/slide/\u0027 -nn \u0027/path/to/trained/model/\u0027 -v\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePredict on TCGA images with our pretrained model for colorectal cancer using \u003ca href=\"https://colab.research.google.com/github/BiomedicalMachineLearning/HEMnet/blob/master/Demo/TCGA_Inference.ipynb\" rel=\"nofollow\"\u003egoogle colab\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-results\" class=\"anchor\" href=\"#results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResults\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-hemnet\" class=\"anchor\" href=\"#citing-hemnet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting HEMnet\u003c/h2\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-the-team\" class=\"anchor\" href=\"#the-team\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe Team\u003c/h2\u003e\n\u003cp\u003ePlease contact Dr Quan Nguyen (\u003ca href=\"mailto:quan.nguyen@uq.edu.au\"\u003equan.nguyen@uq.edu.au\u003c/a\u003e), Andrew Su (\u003ca href=\"mailto:a.su@uqconnect.edu.au\"\u003ea.su@uqconnect.edu.au\u003c/a\u003e),\nand Xiao Tan (\u003ca href=\"mailto:xiao.tan@uqconnect.edu.au\"\u003exiao.tan@uqconnect.edu.au\u003c/a\u003e) for issues, suggestions,\nand we are very welcome to collaboration opportunities.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1624098942.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "kavonrtep/SeqGrapheR",
    "latest_release": "v0.5.0.2.4",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-batch-connect---osc-rstudio-server\" class=\"anchor\" href=\"#batch-connect---osc-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBatch Connect - OSC RStudio Server\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bbf138f428dd98a7b779e572caebe1d8f6c369fb4f9ba270c27f4b29282e5530/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665725f7069747a65722e737667\" alt=\"GitHub Release\" data-canonical-src=\"https://img.shields.io/github/release/osc/bc_osc_rstudio_server_pitzer.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn interactive app designed for OSC OnDemand that launches an RStudio Server\nwithin an Pitzer batch job.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deprecated-application-warning\" class=\"anchor\" href=\"#deprecated-application-warning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeprecated application warning\u003c/h2\u003e\n\u003cp\u003eThis application no longer works.  It raises an exception when users attempt to submit jobs.\nThis is because we now have functionality to submit to multiple clusters and\n\u003ca href=\"https://github.com/OSC/bc_osc_rstudio_server\"\u003ethe generic application\u003c/a\u003e now submits\nto pitzer rendering this application useless.\u003c/p\u003e\n\u003cp\u003eFor historic versions, see the last released you can still view\n\u003ca href=\"https://github.com/OSC/bc_osc_rstudio_server_pitzer/tree/v0.3.0\"\u003ev0.3.0\u003c/a\u003e as it was the last\nworking version of this application.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617867243.0
  },
  {
    "data_format": 2,
    "description": "Singularity recipe files for ApsimX (https://github.com/APSIMInitiative/ApsimX)",
    "filenames": [
      "Singularity.2020.10.21.5755",
      "Singularity.2020.11.27.5887",
      "Singularity.2019.06.05.3920",
      "Singularity",
      "Singularity.2021.04.15.6139",
      "Singularity.2020.04.09.5012",
      "Singularity.2019.10.04.4236",
      "Singularity.2020.09.17.5665",
      "Singularity.2018.09.28.3099",
      "Singularity.2019.01.08.3392",
      "Singularity.2020.08.04.5350",
      "Singularity.2019.01.30.3436",
      "Singularity.2018.01.30.2253",
      "Singularity.2019.04.03.3693",
      "Singularity.2019.07.18.4025"
    ],
    "full_name": "powerPlant/apsimx-srf",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2271\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity recipe files for ApsimX, the next generation of the Agricultural Production Systems sIMulator (APSIM)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-maintainer-notes\" class=\"anchor\" href=\"#maintainer-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaintainer notes\u003c/h2\u003e\n\u003cp\u003eThis container downloads the ApsimX \u003ccode\u003edeb\u003c/code\u003e file built by the \u003ca href=\"https://github.com/APSIMInitiative/APSIM.Builds\"\u003eBOB Build Service\u003c/a\u003e at CSIRO and installs it on a Ubuntu:Bionic container.\u003c/p\u003e\n\u003cp\u003eThere are two useful endpoints from the Build Service:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://apsimdev.apsim.info/APSIM.Builds.Service/Builds.svc/GetLatestVersion\" rel=\"nofollow\"\u003eGet Latest Version\u003c/a\u003e: shows the full version number of the latest release\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://apsimdev.apsim.info/APSIM.Builds.Service/Builds.svc/GetURLOfLatestVersion?operatingSystem=Debian\" rel=\"nofollow\"\u003eGet URL of Latest Version\u003c/a\u003e: shows the download URL for the latest version of the specified OS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003eApsimSetup*.deb\u003c/code\u003e files are named using the \u003ca href=\"https://github.com/APSIMInitiative/APSIM.Builds/blob/21a8ac85a1d868a45f60a994a35a5d07dc04562a/APSIM.Builds.Service/Builds.svc.cs#L278\"\u003ebuild.issueNumber\u003c/a\u003e instead of the full version number, so we must hardcode the URLs inside the recipe files, while using the full version number for the recipe file names.\u003c/p\u003e\n\u003cp\u003eTo facilitate running the container, a \u003ccode\u003e/usr/local/bin/apsimmodels\u003c/code\u003e file is created as the entrypoint using \u003ccode\u003e/usr/local/bin/apsim\u003c/code\u003e as a template.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-container-specific-notes\" class=\"anchor\" href=\"#container-specific-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainer-specific notes\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eApsimX apparently needs \u003ccode\u003e/etc/localtime\u003c/code\u003e to run so we have to install \u003ccode\u003etzdata\u003c/code\u003e as a dependency. We configure our local timezone, but this can be overridden at run time by exporting the \u003ccode\u003eTZ\u003c/code\u003e environment variable.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1618541722.0
  },
  {
    "data_format": 2,
    "description": "Hybrid-FS: A planner for controlling hybrid systems specified in Functional STRIPS",
    "filenames": [
      "Singularity"
    ],
    "full_name": "miquelramirez/hybrid-fs",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-the-fs-functional-strips-planner\" class=\"anchor\" href=\"#the-fs-functional-strips-planner\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe FS Functional STRIPS planner\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003eFS\u003c/code\u003e is a classical planner that works with the Functional STRIPS planning language \u003ca href=\"#ref-geffner-fstrips-2000\"\u003e[Geffner, 2000]\u003c/a\u003e,\na modeling language based on the quantifier-free\nfragment of first-order logic that includes constant, function and predicate symbols, but no variable symbols. The increased expressiveness\nof the Functional STRIPS language with respect to propositional languages such as standard STRIPS (which is indeed subsumed by Functional STRIPS)\noften results in problem encodings which are more compact, more readable, have fewer ground actions\nand preserve the structural properties of the problem in a manner which allows the derivation of more effective heuristics.\u003c/p\u003e\n\u003cp\u003eAlong with the core of the Functional STRIPS language, the \u003ccode\u003eFS\u003c/code\u003e planner supports certain extensions which are useful both\nfrom the expressive \u003cem\u003eand\u003c/em\u003e the computational point of view. These include \u003cem\u003eexistential quantification\u003c/em\u003e,\n\u003cem\u003estate constraints\u003c/em\u003e, a fairly large library of \u003cem\u003eglobal constraints\u003c/em\u003e, and the possibility of using \u003cem\u003eexternally-defined symbols\u003c/em\u003e\nand \u003cem\u003ebuilt-in arithmetic symbols\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis documentation covers a number of practical issues related to the use of the \u003ccode\u003eFS\u003c/code\u003e planner. The planner, however, has\nbeen used and described in a number of academic publications that \u003ca href=\"http://gfrances.github.io/pubs/\" rel=\"nofollow\"\u003ecan be found here\u003c/a\u003e,\nthe most recent of which are \u003ca href=\"#ref-frances-modeling-2015\"\u003e[Franc\u00e8s and Geffner, 2015]\u003c/a\u003e and \u003ca href=\"#ref-frances-existential-2016\"\u003e[Franc\u00e8s and Geffner, 2016a]\u003c/a\u003e\nand \u003ca href=\"#ref-frances-effective-2016\"\u003e[Franc\u00e8s and Geffner, 2016b]\u003c/a\u003e.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#usage\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#credits\"\u003eCredits\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-references\"\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThe easiest way to use the planner is by \u003ca href=\"doc/installation.md\"\u003emanually compiling the planner source code\u003c/a\u003e.\nThis procedure is complemented by the \u003ca href=\"doc/hybrid.md\"\u003einstructions specific for setting up the hybrid module\u003c/a\u003e of \u003ccode\u003eFS\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-usage\"\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eYou can find a high-level overview of the planner usage options \u003ca href=\"doc/usage.md\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-credits\"\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003eFS\u003c/code\u003e planner is partially built upon the \u003ca href=\"http://www.lapkt.org\" rel=\"nofollow\"\u003eLightweight Automated Planning Toolkit\u003c/a\u003e\nand the PDDL parser from the \u003ca href=\"http://www.fast-downward.org\" rel=\"nofollow\"\u003eFast Downward\u003c/a\u003e distribution.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-references\"\u003e\u003c/a\u003eReferences\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca name=\"user-content-ref-frances-modeling-2015\"\u003eFranc\u00e8s, G., and Geffner, H. (2015)\u003c/a\u003e,\n\u003ca href=\"http://gfrances.github.io/pubs/2015-icaps-better-heuristics-more-expressive-languages/\" rel=\"nofollow\"\u003e\u003cem\u003eModeling and Computation in Planning: Better Heuristics from More Expressive Languages\u003c/em\u003e\u003c/a\u003e, ICAPS 2015.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca name=\"user-content-ref-frances-existential-2016\"\u003eFranc\u00e8s, G., and Geffner, H. (2016a)\u003c/a\u003e,\n\u003ca href=\"http://gfrances.github.io/pubs/2016-ijcai-existential-quantification-planning-csp/\" rel=\"nofollow\"\u003e\u003cem\u003eE-STRIPS: Existential Quantification in Planning and Constraint Satisfaction\u003c/em\u003e\u003c/a\u003e, IJCAI 2016.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca name=\"user-content-ref-frances-effective-2016\"\u003eFranc\u00e8s, G., and Geffner, H. (2016b)\u003c/a\u003e,\n\u003ca href=\"http://gfrances.github.io/pubs/2016-ijcai-effective-planning-more-expressive-languages/\" rel=\"nofollow\"\u003e\u003cem\u003eEffective Planning with More Expressive Languages\u003c/em\u003e\u003c/a\u003e, IJCAI 2016.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca name=\"user-content-ref-geffner-fstrips-2000\"\u003eGeffner, H. (2000)\u003c/a\u003e,\n\u003ca href=\"http://www.tecn.upf.es/~hgeffner/\" rel=\"nofollow\"\u003e\u003cem\u003eFunctional STRIPS: A more flexible lan-\nguage for planning and problem solving\u003c/em\u003e\u003c/a\u003e.\nIn Minker, J., ed., Logic-Based Artificial Intelligence. Kluwer. 187\u2013205.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [
      "cplusplus-14",
      "ai",
      "planning",
      "hybrid-systems",
      "kinodynamic-planning"
    ],
    "updated_at": 1570694552.0
  },
  {
    "data_format": 2,
    "description": "Container recipes for use with the U of A HPC resources",
    "filenames": [
      "Singularity.numba",
      "Singularity.keras",
      "Singularity.pytorch",
      "Singularity.dynet",
      "Singularity.im2markup",
      "Singularity.cuda9_py36",
      "Singularity.tensorflow",
      "Singularity.pytorch_skimage",
      "Singularity.caffe2",
      "Singularity.delphi",
      "Singularity.nvidia_docker",
      "Singularity.torch",
      "word2vec/Singularity.w2v",
      "openmnt/Singularity.opennmt"
    ],
    "full_name": "ml4ai/UA-hpc-containers",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ml4ai-lab-singularity-container-repository\" class=\"anchor\" href=\"#ml4ai-lab-singularity-container-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eML4AI Lab Singularity Container Repository\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2086\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository holds singularity containers that are commonly used by members of the ML4AI lab to run projects on the University of Arizona\u0027s high-performance computing environment.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSee Also\u003c/strong\u003e:\n\u003ca href=\"https://github.com/clulab/hpc-ml\"\u003ehttps://github.com/clulab/hpc-ml\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 20,
    "topics": [],
    "updated_at": 1617312549.0
  },
  {
    "data_format": 2,
    "description": "Research Computing Spring 2019 (IMSE 8410)",
    "filenames": [
      "examples/containers/Singularity"
    ],
    "full_name": "MiddelkoopT/RC-2019-Spring",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nipype-tutorial-notebooks\" class=\"anchor\" href=\"#nipype-tutorial-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNipype Tutorial Notebooks\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/miykael/nipype_tutorial/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/669c934f828c73340c0d591ed4b423ef3fa0193e787bfe385915e82dae5ed8fc/68747470733a2f2f636972636c6563692e636f6d2f67682f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f7374796c653d736869656c64\" alt=\"CircleCi\" data-canonical-src=\"https://circleci.com/gh/miykael/nipype_tutorial.svg?style=shield\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/issues/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ea29b9a6350d6278064569a97945097dcdeedf9e93740b62ef46df808891fd37/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub issues\" data-canonical-src=\"https://img.shields.io/github/issues/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/pulls/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/eb7044b2c212e415ec4669de3bb9767f22bfed317ade3070bac8d41ea2a71529/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d70722f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub pull-requests\" data-canonical-src=\"https://img.shields.io/github/issues-pr/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://GitHub.com/miykael/nipype_tutorial/graphs/contributors/\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7702816785d6120ca455fda7995bccb5bbdde3e3a92f859f27f866ad34bc55f2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6e7472696275746f72732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub contributors\" data-canonical-src=\"https://img.shields.io/github/contributors/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/commits/master\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fdcae12a957784eff34edadd6ded9a9a8cdf6354ce4d5c5b9d16727d838ecc23/68747470733a2f2f6769746875622d62617369632d6261646765732e6865726f6b756170702e636f6d2f636f6d6d6974732f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub Commits\" data-canonical-src=\"https://github-basic-badges.herokuapp.com/commits/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/miykael/nipype_tutorial/archive/master.zip\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fb9081bb8ee87986aea94736dd73ee86c56308df8e0b21ee9803cbe6976e3fab/68747470733a2f2f6769746875622d73697a652d62616467652e6865726f6b756170702e636f6d2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub size\" data-canonical-src=\"https://github-size-badge.herokuapp.com/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/r/miykael/nipype_tutorial/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3658dcdcaf69e757f1454f83966a15fcdf8b7bcb1d3b4427ffb4226668659eb6/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d69796b61656c2f6e69707970655f7475746f7269616c2e7376673f6d61784167653d32353932303030\" alt=\"Docker Hub\" data-canonical-src=\"https://img.shields.io/docker/pulls/miykael/nipype_tutorial.svg?maxAge=2592000\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://hits.dwyl.io/miykael/nipype_tutorial\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c19a46ac2503dae747aeea217a7a854e711a4c95b5814a8c85c59aa5c9920a61/687474703a2f2f686974732e6477796c2e696f2f6d69796b61656c2f6e69707970655f7475746f7269616c2e737667\" alt=\"GitHub HitCount\" data-canonical-src=\"http://hits.dwyl.io/miykael/nipype_tutorial.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is the Nipype Tutorial in Jupyter Notebook format. You can access the tutorial in two ways:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"https://miykael.github.io/nipype_tutorial/\" rel=\"nofollow\"\u003eNipype Tutorial Homepage\u003c/a\u003e: This website contains a static, read-only version of all the notebooks.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://miykael.github.io/nipype_tutorial/notebooks/introduction_docker.html\" rel=\"nofollow\"\u003eNipype Tutorial Docker Image\u003c/a\u003e: This guide explains how to use Docker to run the notebooks interactively on your own computer. The nipype tutorial docker image is the best interactive way to learn Nipype.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-feedback-help--support\" class=\"anchor\" href=\"#feedback-help--support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeedback, Help \u0026amp; Support\u003c/h1\u003e\n\u003cp\u003eIf you want to help with this tutorial or have any questions, feel free to fork the repo of the \u003ca href=\"https://github.com/miykael/nipype_tutorial\"\u003eNotebooks\u003c/a\u003e or interact with other contributors on the slack channel \u003ca href=\"https://brainhack.slack.com/messages/nipype/\" rel=\"nofollow\"\u003ebrainhack.slack.com/messages/nipype/\u003c/a\u003e. If you have any questions or found a problem, open a new \u003ca href=\"https://github.com/miykael/nipype_tutorial/issues\"\u003eissue on github\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-thanks-and-acknowledgment\" class=\"anchor\" href=\"#thanks-and-acknowledgment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks and Acknowledgment\u003c/h1\u003e\n\u003cp\u003eA huge thanks to \u003ca href=\"https://github.com/mwaskom\"\u003eMichael Waskom\u003c/a\u003e, \u003ca href=\"https://github.com/oesteban\"\u003eOscar Esteban\u003c/a\u003e, \u003ca href=\"https://github.com/chrisfilo\"\u003eChris Gorgolewski\u003c/a\u003e and \u003ca href=\"https://github.com/satra\"\u003eSatrajit Ghosh\u003c/a\u003e for their input to this tutorial! And a huge thanks to \u003ca href=\"https://github.com/djarecka/\"\u003eDorota Jarecka\u003c/a\u003e who updated this tutorial to Python 3 and is helping me with keeping this tutorial updated and running!\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1569468582.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.centos7.tbx-MG"
    ],
    "full_name": "ResearchIT/MolecularGraphicsToolbox",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-characterisationvl-software\" class=\"anchor\" href=\"#characterisationvl-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCharacterisationVL-Software\u003c/h1\u003e\n\u003cp\u003eThe purpose of this repository is for storing definition files to submit to \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you are new to Singularity containers, please refer to \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/user-guide/\u003c/a\u003e or a newer version of this documentation.\u003c/p\u003e\n\u003cp\u003eEach software package is located in its own folder. The files are tagged with the software name and version number or date of build. Please read below for the naming convention.\u003c/p\u003e\n\u003cp\u003eTo add software to the repository you will need to create a new branch. The new branch is the name of the software product. By convention, the new branch will be checked and merged into the master branch and then deleted.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-steps-to-add-a-software-package\" class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSteps to add a software package\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003eCreate a branch\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git branch \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMake a subdirectory for the software product.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ mkdir \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eAdd all the necessary files.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity definition file or installation script\u003c/li\u003e\n\u003cli\u003eReadme file including install and testing notes\u003c/li\u003e\n\u003cli\u003eDesktop files for adding to menus with necessary tags\u003c/li\u003e\n\u003cli\u003eFor full details, \u003ca href=\"template/README.md\"\u003eplease refer to the \u0027template\u0027 folder in this repository.\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eCommit all changes, including a helpful message\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git commit -m \"\u0026lt;software name\u0026gt; added as requested in support ticket\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003ePush to the remote repository. i.e. this one.\u003c/li\u003e\n\u003cli\u003eSubmit merge request\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\" class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNaming your Singularity definition file, Singularity Hub and Licensing\u003c/h2\u003e\n\u003cp\u003eFor all Singularity recipes where the software licensing permits redistribution, please use this naming convention:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   Singularity.applicationName_version\n   Singularity.applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is where Singularity Hub fits into the equation. There is a webhook between this repository and \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e. When a commit is merged into the master branch, Singularity Hub will build the container.\u003c/p\u003e\n\u003cp\u003eIf successfully built, the path to the container on Singularity Hub is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor software where licensing does not support redistribution, the container recipe can still be defined, but the container should not be built on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eAn example on how to handle this situation is the recipe for CCP-EM.\nThe \u003ca href=\"ccp-em/README.md\"\u003eREADME.md\u003c/a\u003e contains a section on Prerequisites. This section lists the required files to build the container. The license must be accepted by the end user to obtain them.\u003c/p\u003e\n\u003cp\u003ePrerequisite files should not be committed to this repository.\u003c/p\u003e\n\u003cp\u003eTo prevent Singularity Hub from attempting to build the container, we simply use a different recipe naming convention as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   applicationName_version.def\n   applicationName_version-cuda-cudaVersion.def\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUbuntu Base Images\u003c/h2\u003e\n\u003cp\u003eThe folder \u0027ubuntu-base-image\u0027 contains recipes for pre built base containers. These can be used as a starting point to aid/speed up the development of your container recipe.\u003c/p\u003e\n\u003cp\u003eThe current versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.\u003c/p\u003e\n\u003cp\u003eThese are available on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFor example: from the Graphviz Singularity.graphviz-2.40.1 recipe\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBootstrap: shub\nFrom:      Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese two lines, will tell Singularity to use the \u0027shub\u0027 bootstrap to obtain the \u00271804\u0027 ubuntu-base-image container from Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFrom here you just need to add the requirements to build a container for your required piece of software. Please see \u003ca href=\"graphviz/Singularity.graphviz-2.40.1\"\u003eSingularity.graphviz-2.40.1\u003c/a\u003e\nfor the full recipe.\u003c/p\u003e\n\u003cp\u003eThe current ubuntu-base-images include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning GUI applications on a non-GPU node\u003c/h2\u003e\n\u003cp\u003eThe applications in the Singularity container should run without the need for a dedicated GPU.\u003c/p\u003e\n\u003cp\u003eHowever, an X server needs to be running for this to work. On nodes with GPU, X Server is started with NVIDIA driver, and on non-GPU nodes, the X Server is started with MESA library.\u003c/p\u003e\n\u003cp\u003eX Server can be started during boot (for example, using \u003ccode\u003esystemctl set-default graphical.target\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eMake sure that VirtualGL package is installed in the container. The code below will download and install VirtualGL.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\ndpkg -i virtualgl_2.6.2_amd64.deb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe application startup script doesn\u0027t need to be modified, however, if the application needs to be manually started, then \u003ccode\u003evglrun\u003c/code\u003e needs to be appended before running the application. For example: \u003ccode\u003esingularity exec --nv -B /projects:/projects -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1582812783.0
  },
  {
    "data_format": 2,
    "description": "Pipeline for analysing M. tuberculosis nanopore reads and getting drug susceptibility information.",
    "filenames": [
      "containers/recipes/Singularity.mykrobe",
      "containers/recipes/Singularity.nanoporeqc"
    ],
    "full_name": "mbhall88/Longitude_pipeline",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003econtainers\u003c/h1\u003e\n\u003cp\u003erecipes of Singularity\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 2,
    "topics": [
      "nanopore",
      "tuberculosis",
      "bioinformatics-pipeline"
    ],
    "updated_at": 1581419079.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "config/Singularity",
      "scripts/unused/Singularity",
      "scripts/unused/Singularity_newhybrids"
    ],
    "full_name": "nealplatt/sH_hybridization",
    "latest_release": "v1.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://zenodo.org/badge/latestdoi/124456755\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4641d504e79e577f2add43b190e60f3910a1688ac8f26f972d799fd6f3f4b213/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f3132343435363735352e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/124456755.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/ambv/black\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\" alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium\" class=\"anchor\" href=\"#ancient-hybridization-and-adaptive-introgression-of-an-invadolysin-gene-in-schistosoma-haematobium\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAncient hybridization and adaptive introgression of an invadolysin gene in \u003cem\u003eSchistosoma haematobium\u003c/em\u003e.\u003c/h1\u003e\n\u003cp\u003eRoy N. Platt II, Marina McDew-White, Winka Le Clec\u0027h, Frederic D. Chevalier, Fiona Allan, Aidan M. Emery, Amadou Garba, Shaali M. Ame, Joanne P. Webster, David Rollinson, Bonnie L. Webster, Timothy J. C. Anderson.\u003c/p\u003e\n\u003cp\u003eThe parasitic blood fluke \u003cem\u003eSchistosoma\u003c/em\u003e \u003cem\u003ehaematobium\u003c/em\u003e causes urogenital schistosomiasis in humans and is a major cause of morbidity and mortality across sub-Saharan Africa. \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e can hybridize with closely-related livestock schistosomes, including \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ebovis\u003c/em\u003e, however the frequency, direction, age and genomic consequences of hybridization in nature are unknown. We sequenced 96 \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e exomes from Niger and the Zanzibar archipelago. We found evidence of an ancient, adaptive introgression event between Nigerien \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e and \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ebovis\u003c/em\u003e occurring 108-613 generations ago. Introgressed S. bovis alleles constitute 3.3-8.2% of Nigerien \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ehaematobium\u003c/em\u003e genomes. Some \u003cem\u003eS\u003c/em\u003e. \u003cem\u003ebovis\u003c/em\u003e alleles have reached high frequency and show signatures of directional selection; the strongest signal spans a single gene in the invadolysin gene family, an M8 metalloprotease associated with parasitic life-history traits.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-biorxiv-pre-print\" class=\"anchor\" href=\"#biorxiv-pre-print\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://doi.org/10.1101/539353\" rel=\"nofollow\"\u003ebioRxiv pre-print\u003c/a\u003e\n\u003c/h4\u003e\n\u003chr\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNOTES:\u003c/h3\u003e\n\u003cp\u003eAll analyses were conducted on a HPCC in a \u003ccode\u003esingularity\u003c/code\u003e container or in a \u003ccode\u003econda\u003c/code\u003e managed environment. The singularity recipe and conda environmental yaml are in the \u003ccode\u003econfig\u003c/code\u003e dir.\u003c/p\u003e\n\u003cp\u003eRaw code is found in the \u003ccode\u003escripts\u003c/code\u003e dir\u003c/p\u003e\n\u003cp\u003eData that is not readily available through the SRA is in the \u003ccode\u003edata\u003c/code\u003e dir.  These will be housed in an online repository (ex. Dryad), but provided here for documentation purposes.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1618309153.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "examples/arch/Singularity",
      "examples/asciinema/Singularity",
      "examples/busybox/Singularity",
      "examples/shub/Singularity",
      "examples/apps/Singularity",
      "examples/apps/Singularity.cowsay",
      "examples/ubuntu/Singularity",
      "examples/docker/Singularity",
      "examples/raspbian/Singularity",
      "examples/centos/Singularity",
      "examples/opensuse/Singularity",
      "examples/scientific/Singularity",
      "examples/self/Singularity"
    ],
    "full_name": "kernsuite-debian/singularity-container",
    "latest_release": null,
    "readme": "\u003cp\u003e_Please note recent changes in the github repo branch structure.  If you want\nto install a stable release of Singularity, please use a tag or a \u003ca href=\"https://github.com/singularityware/singularity/releases\"\u003erelease\ntarball\u003c/a\u003e.  If you are\na developer who would like to contribute to Singularity and you want to know\nwhich branch to submit your pull request to, please see notes on the branch\nreorganization \u003ca href=\"https://www.sylabs.io/2018/03/managing-singularity-branches/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePlease also note that 2.6.0 is expected to be the final feature release in the\n2.x series. While bug fixes may be added via point releases (for example 2.6.1)\nno new features releases (for example 2.7.0) are planned.\u003c/p\u003e\n\u003cp\u003ePull requests adding features to the 2.x series will no longer be reviewed.\u003cbr\u003e\nAny new features should be targeted to the master branch (which used to be\ncalled development-3.0)._\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/singularityware/singularity\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f9a86612d918b5d7b8615b4f1203222f491b2a672958652856370704a30742f9/68747470733a2f2f7472617669732d63692e6f72672f73696e67756c6172697479776172652f73696e67756c61726974792e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/singularityware/singularity.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"CONTRIBUTING.md\"\u003eGuidelines for Contributing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\".github/PULL_REQUEST_TEMPLATE.md\"\u003ePull Request Template\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"LICENSE.md\"\u003eProject License\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459\" rel=\"nofollow\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-singularity---enabling-users-to-have-full-control-of-their-environment\" class=\"anchor\" href=\"#singularity---enabling-users-to-have-full-control-of-their-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity - Enabling users to have full control of their environment.\u003c/h1\u003e\n\u003cp\u003eStarting a Singularity container \"swaps\" out the host\noperating system environment for one the user controls!\u003c/p\u003e\n\u003cp\u003eLet\u0027s say you are running Ubuntu on your workstation or server, but you\nhave an application which only runs on Red Hat Enterprise Linux 6.3.\nSingularity can instantly virtualize the operating system, without having\nroot access, and allow you to run that application in its native environment!\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h1\u003e\n\u003cp\u003eSingularity is a container platform focused on supporting \"Mobility of\nCompute\".\u003c/p\u003e\n\u003cp\u003eMobility of Compute encapsulates the development to compute model where\ndevelopers can work in an environment of their choosing and creation, and\nwhen the developer needs additional compute resources, this environment\ncan easily be copied and executed on other platforms. Additionally, as the\nprimary use case for Singularity is targeted towards computational portability.\nMany of the barriers to entry of other container solutions do not apply to\nSingularity, making it an ideal solution for users (both computational and\nnon-computational) and HPC centers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-the-container\" class=\"anchor\" href=\"#the-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe Container\u003c/h2\u003e\n\u003cp\u003eSingularity utilizes container images, which means when you enter and\nwork within the Singularity container, you are physically located inside\nof this image. The image grows and shrinks in real time as you install\nor delete files within the container. If you want to copy a container,\nyou copy the image.\u003c/p\u003e\n\u003cp\u003eUsing a single image for the container format has added advantages\nespecially within the context of HPC with large parallel file systems\nbecause all metadata operations within the container occur within the\ncontainer image (and not on the metadata server!).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mobility-of-compute\" class=\"anchor\" href=\"#mobility-of-compute\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMobility of Compute\u003c/h2\u003e\n\u003cp\u003eWith Singularity, developers who like to be able to easily control their\nown environment will love Singularity\u0027s flexibility. Singularity does not\nprovide a pathway for escalation of privilege (as do other container\nplatforms which are thus not appropriate for multi-tenant resources) so\nyou must be able to become root on the host system (or virtual machine)\nin order to modify the container.\u003c/p\u003e\n\u003cp\u003eA Singularity container can be launched in a variety of different ways\ndepending on what you wanted to do with it. A simple method might be to\nlaunch an interactive shell within the container image as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[gmk@centos7-x64 demo]$ singularity shell /tmp/Centos-7.img \ngmk@Centos-7.img demo\u0026gt; echo \"Hello from within the container\"\nHello from within the container\ngmk@Centos-7.img demo\u0026gt; whoami\ngmk\ngmk@Centos-7.img demo\u0026gt; \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd if you want to do the same thing as root:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[gmk@centos7-x64 demo]$ sudo singularity shell -w /tmp/Centos-7.img \nroot@Centos-7.img demo\u0026gt; whoami\nroot\nroot@Centos-7.img demo\u0026gt; \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003enote: By default, Singularity launches the container image in read-only\nmode (so it can be easily launched in parallel). The \u003ccode\u003e-w\u003c/code\u003e option used above\ntells Singularity to mount the image in read/write mode, such that root\ncan now make changes to the container.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, relevant file systems on your host are shared, automatically,\nwithin the context of your container. This can be demonstrated as\nfollows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[gmk@centos7-x64 demo]$ pwd\n/home/gmk/demo\n[gmk@centos7-x64 demo]$ echo \"world\" \u0026gt; hello\n[gmk@centos7-x64 demo]$ singularity shell /tmp/Centos-7.img \ngmk@Centos-7.img demo\u0026gt; pwd\n/home/gmk/demo\ngmk@Centos-7.img demo\u0026gt; cat hello\nworld\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce the developer has completed their environment, the image file can\nbe compressed and copied to any other system that has Singularity installed.\nIf you do not have root on that system, you will not be able to make any\nchanges to the image once on that system. But you will be able to use the\ncontainer and access the data and files outside the container as\neasily as you would on your development system or virtual machine.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-portability-of-singularity-container-images\" class=\"anchor\" href=\"#portability-of-singularity-container-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePortability of Singularity container images\u003c/h2\u003e\n\u003cp\u003eSingularity images are highly portable between Linux distributions (as\nlong as the binary format is the same). You can generate your image on\nDebian or CentOS, and run it on Mint or Slackware.\u003c/p\u003e\n\u003cp\u003eWithin a particular container, one can include their programs, data,\nscripts and pipelines and thus port a workflow to any other architecture\ncompatible Linux system or distribution.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bootstrapping-new-images\" class=\"anchor\" href=\"#bootstrapping-new-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBootstrapping new images\u003c/h2\u003e\n\u003cp\u003eGenerally, when bootstrapping an image from scratch, you must build it from\na compatible host. This is because you must use the distribution specific\ntools it comes with (e.g. Red Hat does not provide Debian\u0027s debootstrap by\ndefault). But once the image has been bootstrapped and includes the necessary\nbits to be self-hosting (e.g. YUM on CentOS and apt-get on Debian/Ubuntu) then\nthe process of managing the container can be implemented from within the\ncontainer.\u003c/p\u003e\n\u003cp\u003eThe process of building a bootstrap starts with a definition\nspecification. The definition file describes how you want the operating\nsystem to be built, what should go inside it and any additional\nmodifications necessary.\u003c/p\u003e\n\u003cp\u003eHere is an example of a very simple bootstrap definition file for CentOS:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBootStrap: yum\nOSVersion: 7\nMirrorURL: http://mirror.centos.org/centos-%{OSVERSION}/%{OSVERSION}/os/$basearch/\nInclude: yum\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce you have created your bootstrap definition, you can build your\nSingularity container image by first creating a blank image, and then\nbootstrapping using your definition file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[gmk@centos7-x64 demo]$ sudo singularity create /tmp/Centos-7.img\n[gmk@centos7-x64 demo]$ sudo singularity bootstrap /tmp/Centos-7.img centos.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there we can immediately start using the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img cat /etc/redhat-release \nCentOS Linux release 7.2.1511 (Core) \n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img python --version\nPython 2.7.5\n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-7.img python hello.py \nhello world\n[gmk@centos7-x64 demo]$ \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd if I do this same process again, while changing the \u003cstrong\u003eOSVersion\u003c/strong\u003e\nvariable in the bootstrap definition to \u003cstrong\u003e6\u003c/strong\u003e (where previously it was\nautomatically ascertained by querying the RPM database), we can\nessentially build a CentOS-6 image in exactly the same manner as\nabove. Doing so reveals this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-6.img cat /etc/redhat-release \nCentOS release 6.7 (Final)\n[gmk@centos7-x64 demo]$ singularity exec /tmp/Centos-6.img python --version\nPython 2.6.6\n[gmk@centos7-x64 demo]$ \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd as expected, the Python version we now see is what comes from by\ndefault in CentOS-6.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-cite-as\" class=\"anchor\" href=\"#cite-as\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCite as:\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe also have a Zenodo citation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer, Gregory M.. (2016). Singularity 2.1.2 - Linux application and environment\ncontainers for science. 10.5281/zenodo.60736\n\nhttp://dx.doi.org/10.5281/zenodo.60736\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-webpage\" class=\"anchor\" href=\"#webpage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWebpage\u003c/h1\u003e\n\u003cp\u003eWe have full documentation at \u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003ehttps://www.sylabs.io/docs/\u003c/a\u003e, and \u003ca href=\"http://www.github.com/singularityware/singularityware.github.io\"\u003ewelcome contributions\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1589975545.0
  },
  {
    "data_format": 2,
    "description": "singularity-recipe-share",
    "filenames": [
      "Singularity.tensorflow-gpu-1.12.0"
    ],
    "full_name": "lxwgcool/singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-coesra-singularity-canopy\" class=\"anchor\" href=\"#coesra-singularity-canopy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecoesra-singularity-canopy\u003c/h1\u003e\n\u003cp\u003eAuthor: Hoang Nguyen\nCreated: 22 July 2019\nThis will create a image with Singularity 2.5.1\u003c/p\u003e\n",
    "stargazers_count": 2,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1574278904.0
  },
  {
    "data_format": 2,
    "description": "Install methods for UPPMAX modules plus some helper scripts",
    "filenames": [
      "singularity_info/gapseq-RT-227932/Singularity.gapseq",
      "singularity_info/metaWRAP_1.3.2/Singularity.metaWRAP",
      "singularity_info/bonito/Singularity.bonito"
    ],
    "full_name": "UPPMAX/install-methods",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-module-installation-methods\" class=\"anchor\" href=\"#module-installation-methods\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eModule Installation Methods\u003c/h1\u003e\n\u003cp\u003eThis is a collection of READMEs generated during installation of software\napplications on Uppmax clusters.  It is incomplete in terms of modules\navailable on Uppmax, and the individual READMEs may also be incomplete in terms\nof what was actually done to install the modules.  We are publicising these in\nthe hopes that they can be helpful.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-workflow-of-a-basic-installation\" class=\"anchor\" href=\"#example-workflow-of-a-basic-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample workflow of a basic installation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone the install methods git repo (\u003ccode\u003egit clone https://github.com/UPPMAX/install-methods.git\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eAdd the repo to your \u003ccode\u003e$PATH\u003c/code\u003e and source the \u003ccode\u003euppmax_functions.sh\u003c/code\u003e file to get access to the functions.\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003erun_makeroom\u003c/code\u003e with at least \u003ccode\u003e-t\u003c/code\u003e and \u003ccode\u003e-v\u003c/code\u003e, to generate a \u003ccode\u003e.sh\u003c/code\u003e (\u003ccode\u003emakeroom_toolname_version.sh\u003c/code\u003e) file that will create the directory structure needed in \u003ccode\u003e/sw\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun the \u003ccode\u003e.sh\u003c/code\u003e file created in the directory you are standing to create the directory structure (\u003ccode\u003e/sw/category/toolname/\u003c/code\u003e and \u003ccode\u003e/sw/mf/common/category\u003c/code\u003e) and template files.\u003c/li\u003e\n\u003cli\u003ePut the source code for the program in \u003ccode\u003e/sw/category/toolname/version/src\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eCompile and/or install the tool in \u003ccode\u003e/sw/category/toolname/version/cluster/bin\u003c/code\u003e etc.\u003c/li\u003e\n\u003cli\u003eEdit the readme file, explaining how you did the installation, in \u003ccode\u003e/sw/category/toolname/toolname-version_install-README.md\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eEdit the template module file \u003ccode\u003e/sw/category/toolname/mf/version\u003c/code\u003e to do what you want when the module loads.\u003c/li\u003e\n\u003cli\u003eCopy the module file to the live location, \u003ccode\u003e/sw/mf/common/category/[section]/toolname\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003eall_mflink toolname version\u003c/code\u003e to create links for all clusters to the module file in \u003ccode\u003e/sw/mf/common/category/[section]/toolname\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003efixup /sw/category/toolname/version /sw/mf/common/category/[section]/toolname\u003c/code\u003e to make sure the ownership and permissions are ok.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-scripts\" class=\"anchor\" href=\"#scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eScripts\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003egather-READMEs.sh\u003c/code\u003e - bash script to scan installation directories, looking for\nREADME files having a particular filename format that we create during\ninstallation of tools\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003efixup\u003c/code\u003e - bash script fixing up permissions and group membership within\ninstallation trees; our local installation group is \u003ccode\u003esw\u003c/code\u003e. With the \u003ccode\u003e-g\u003c/code\u003e option,\nthis script will \u003ccode\u003echmod g+s\u003c/code\u003e directories in the tree, too.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003euppmax_functions.sh\u003c/code\u003e - bash helper functions for SLURM job viewing and various\nmodule-related tasks, mostly to do with setting up mf files for loading\nmodules; the latter require appexpert privileges.  Source these from \u003ccode\u003e.bashrc\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-directories\" class=\"anchor\" href=\"#installation-directories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation directories\u003c/h2\u003e\n\u003cp\u003eThe directories contain software installations in major subject areas.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-apps\" class=\"anchor\" href=\"#apps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eapps/\u003c/h3\u003e\n\u003cp\u003eGeneral applications.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-appsbioinfo\" class=\"anchor\" href=\"#appsbioinfo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eapps/bioinfo/\u003c/h3\u003e\n\u003cp\u003eBioinformatics applications.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-libs\" class=\"anchor\" href=\"#libs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elibs/\u003c/h3\u003e\n\u003cp\u003eLibraries.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-comp\" class=\"anchor\" href=\"#comp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ecomp/\u003c/h3\u003e\n\u003cp\u003eCompilers, interpreters, build tools.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-database-directories\" class=\"anchor\" href=\"#database-directories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDatabase directories\u003c/h2\u003e\n\u003cp\u003eThese directories cover installations of databases updated either manually, or via update scripts.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data_uppnex\" class=\"anchor\" href=\"#data_uppnex\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edata_uppnex/\u003c/h3\u003e\n\u003cp\u003eInstallation instructions for databases under \u003ccode\u003e/sw/data/uppnex/\u003c/code\u003e.  Database\ndirectories containing \u003ccode\u003e*-install-README.md\u003c/code\u003e files are updated manually.\nDatabase directories containing \u003ccode\u003e*-db-README.md\u003c/code\u003e files and scripts (currently,\n\u003ccode\u003eKraken\u003c/code\u003e, \u003ccode\u003ediamond_databases\u003c/code\u003e and \u003ccode\u003eRTG\u003c/code\u003e) are updated monthly via crontab entries.\u003c/p\u003e\n\u003cp\u003eBlast database updates are included here, and involve multiple scripts, crontab\nentries and a test directory.  These are updated monthly via crontab entries.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-data_other\" class=\"anchor\" href=\"#data_other\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edata_other/\u003c/h3\u003e\n\u003cp\u003eInstallation instructions for databases under other locations, currently just\n\u003ccode\u003eBUSCO\u003c/code\u003e lineage sets, which are kept in the module installation directory.\nThese are updated monthly via crontab entries.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1625128119.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "Transipedia/KaMRaT",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-kamrat\" class=\"anchor\" href=\"#kamrat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKaMRaT\u003c/h1\u003e\n\u003cp\u003ek-mers are substrings of a fixed length \u003cem\u003ek\u003c/em\u003e extracted from biological sequences.\u003c/p\u003e\n\u003cp\u003eTranscriptomics analysis based on k-mer counts allows to analyze RNA variation at nucleotide resolution, unlimited by a reference genome or transcriptome. Therefore, in principle, all changes in RNA sequences (splice variants, SNPs, novel transcripts) can be captured by k-mer analysis. However a challenge with k-mers is that there are too many of them. A typical bulk RNA-seq sample has in the order of 1e7 to 1e8 unique k-mers, producing a huge matrix when many samples are combined.\u003c/p\u003e\n\u003cp\u003eKaMRaT provides a set of tools for k-mer matrix reduction, for reducing k-mer number and extending k-mers to longer contigs.\u003c/p\u003e\n\u003cp\u003eThe name KaMRaT means \"k-mer Matrix Reduction Toolkit\", or \"k-mer Matrix, Really Tremendous !\".\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start-demos\" class=\"anchor\" href=\"#quick-start-demos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start: Demos\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eVariables\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eindir=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edemo-data/inputs\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\noutdir=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edemo-data/outputs\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsample_list=(sample1 sample2)\ndsgnfile=\u003cspan class=\"pl-smi\"\u003e$indir\u003c/span\u003e/rank-design.txt\nkmer_tab_path=\u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kmer-counts.tsv.gz\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003ek\u003c/em\u003e-mer matrix preparation\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Step 1: jellyfish count \u0026amp; dump\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e${sample_list[@]}\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e $sample_list contains list of considered sample names\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003edo\u003c/span\u003e\n\tjellyfish count -m 31 -s 1000000 -C -o \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e$s\u003c/span\u003e.jf -F 2 \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0026lt;(\u003c/span\u003ezcat \u003cspan class=\"pl-smi\"\u003e$indir\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e$s\u003c/span\u003e.R1.fastq.gz\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0026lt;(\u003c/span\u003ezcat \u003cspan class=\"pl-smi\"\u003e$indir\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e$s\u003c/span\u003e.R2.fastq.gz\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e\n\tjellyfish dump -c \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e$s\u003c/span\u003e.jf \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e sort -k 1 \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/\u003cspan class=\"pl-smi\"\u003e$s\u003c/span\u003e.txt\n\u003cspan class=\"pl-k\"\u003edone\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Step 2: joinCounts\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e -n \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etag\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e gzip -c \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$kmer_tab_path\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003es\u003c/span\u003e \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e${sample_list[@]}\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e $sample_list contains list of considered sample names\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003edo\u003c/span\u003e\n\t\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e -ne \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\\t\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$s\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e gzip -c \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$kmer_tab_path\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003edone\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e gzip -c \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$kmer_tab_path\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --bind /src:/des kamrat.sif joinCounts -r 1 -a 1 \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/\u003cspan class=\"pl-k\"\u003e*\u003c/span\u003e.txt \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e gzip -c \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$kmer_tab_path\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e no filter of recurrence\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eKaMRaT index\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kamrat.idx\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Make index for k-mer matrix with k=31, unstranded mode, and with a count per billion normalization\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --bind /src:/des kamrat.sif kamrat index -intab \u003cspan class=\"pl-smi\"\u003e$kmer_tab_path\u003c/span\u003e -outdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kamrat.idx -klen 31 -unstrand -nfbase 1000000000\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eKaMRaT rank-merge approach\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Select top 50% of relevant k-mers using ttest pi-value\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --bind /src:/des kamrat.sif kamrat rank -idxdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kamrat.idx -rankby ttest.pi -design \u003cspan class=\"pl-smi\"\u003e$indir\u003c/span\u003e/rank-design.txt -outpath \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/top-ranked-kmers.ttest-pi.bin -seltop 0.5\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Extend k-mers by tolerating overlap from 30nc to 15nc, intervened by Pearson distance \u0026lt;= 0.20, and with mean contig count\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --bind /src:/des kamrat.sif kamrat merge -idxdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kamrat.idx -overlap 30-15 -with \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/top-ranked-kmers.ttest-pi.bin -interv pearson:0.20 -outpath \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/contig-counts.ttest-pi.pearson20.tsv -withcounts mean\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eKaMRaT merge-rank approach\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Extend k-mers by tolerating overlap from 30nc to 15nc, intervened by Pearson distance \u0026lt;= 0.20, and with mean contig count\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --bind /src:/des kamrat.sif kamrat merge -idxdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kamrat.idx -overlap 30-15 -interv pearson:0.20 -outpath \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/contigs.pearson20.bin\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Select top 50% of relevant contigs using ttest pi-value\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --bind /src:/des kamrat.sif kamrat rank -idxdir \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/kamrat.idx -rankby ttest.pi -design \u003cspan class=\"pl-smi\"\u003e$indir\u003c/span\u003e/rank-design.txt -seltop 0.5 -with \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/contigs.pearson20.bin -outpath \u003cspan class=\"pl-smi\"\u003e$outdir\u003c/span\u003e/top-ranked-contigs.pearson20.ttest-pi.tsv -withcounts\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-typical-workflow-of-kamrat\" class=\"anchor\" href=\"#typical-workflow-of-kamrat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTypical Workflow of KaMRaT\u003c/h2\u003e\n\u003cp\u003eKaMRaT \u003cem\u003eper se\u003c/em\u003e is shown at the center of the workflow. It is a C++ program that takes as input a count matrix and produces another matrix as output.\nIn the workflow shown, KaMRaT is used for reducing a count matrix produced from a set of fastq files and producing a reduced matrix with features of interest with respect to conditions in the input sample-info file.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"./docs/workflow.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/workflow.png\" alt=\"workflow\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe feature matrix contains features in row and samples in column. Features can be \u003cem\u003ek\u003c/em\u003e-mers (for all modules) as well as other general features such as genes/transcripts (only for KaMRaT-index, -filter, and -rank). The feature counts can be either normalized or non-normalized.\u003c/p\u003e\n\u003cp\u003eThe \u003cem\u003ek\u003c/em\u003e-mer feature matrix can be constructed with the following possibilities:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ca href=\"./related-tools/prepare_kmer_table/Snakefile\"\u003eSnakefile\u003c/a\u003e provided with the project + \u003ca href=\"https://github.com/Transipedia/dekupl-joinCounts\"\u003eDE-kupl joinCounts\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/tlemane/kmtricks\"\u003eKmtricks\u003c/a\u003e software\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/Transipedia/dekupl-run\"\u003eDE-kupl\u003c/a\u003e\u0027s raw-counts.tsv or masked-counts.tsv matrices\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eA set of auxiliary tools to be used for upstream and downstream of kamrat are provided:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUpstream tools:\n\u003cul\u003e\n\u003cli\u003eA matrix generating module controlled by Snakemake which applying jellyfish and DE-kupl joinCounts module\u003c/li\u003e\n\u003cli\u003eA bash script for generating a submatrix by selecting from it a set of columns\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDownstream tools:\n\u003cul\u003e\n\u003cli\u003eA feature selection model with an R script applying ridge/lasso regressions and random forest classifier\u003c/li\u003e\n\u003cli\u003eA contig counting module implemented in C++ for estimating the counts of a list of contigs in an independent dataset; it also supports evaluation of sample count coherence among contig\u0027s compositional k-mers\u003c/li\u003e\n\u003cli\u003eA model evaluation module written in R taking a trained model and evaluating it with a feature count matrix and feature conditions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cdetails\u003e\n\u003csummary\u003eBuild from source\u003c/summary\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/mlpack/mlpack/releases/tag/3.3.2\"\u003eMLPack 3.3.2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.boost.org/doc/libs/1_74_0/libs/iostreams/doc/index.html\" rel=\"nofollow\"\u003eBoost-iostreams\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMLPack can be installed on \u003ca href=\"https://mlpack.org/doc/mlpack-3.3.2/doxygen/build.html\" rel=\"nofollow\"\u003eLinux/Mac\u003c/a\u003e, \u003ca href=\"https://mlpack.org/doc/mlpack-3.3.2/doxygen/build_windows.html\" rel=\"nofollow\"\u003eWindows\u003c/a\u003e, or via \u003ca href=\"https://anaconda.org/conda-forge/mlpack\" rel=\"nofollow\"\u003econda\u003c/a\u003e by following the corresponding links.\u003cbr\u003e\nIf you are installing MLPack with conda, please add the following line into your \u003ccode\u003e.bashrc\u003c/code\u003e file in the \u003ccode\u003ehome/\u003c/code\u003e directory before compiling KaMRaT:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LD_LIBRARY_PATH=/path_to_conda_env/mlpack/lib:\u003cspan class=\"pl-smi\"\u003e$LD_LIBRARY_PATH\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-clone-and-build\" class=\"anchor\" href=\"#clone-and-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eClone and Build\u003c/h3\u003e\n\u003cp\u003eFirstly, clone the repository:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone --recursive https://github.com/Transipedia/KaMRaT.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e KaMRaT\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you installed MLPack library with conda:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebash compile.bash /path_to_MLPack_conda_environment\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOtherwise, if you installed MLPack without conda:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebash compile.bash\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFinally, an executable binary file is available as \u003ccode\u003ebin/kamrat\u003c/code\u003e.\u003c/p\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003eUse singularity\u003c/summary\u003e\n\u003cp\u003eIf using KaMRaT inside singularity, only by pulling from docker hub is enough:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity build KaMRaT.sif docker://xuehl/kamrat:latest\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-general-information\" class=\"anchor\" href=\"#general-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGeneral Information\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sample-information-file\" class=\"anchor\" href=\"#sample-information-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSample Information File\u003c/h3\u003e\n\u003cp\u003eThe sample-info file is indicated by the option \u003ccode\u003e-smp-info\u003c/code\u003e. This file aims to indicate which columns in the k-mer count matrix should be considered as sample columns. Please do not put any header line in the file, since the columns are already defined by convention as below.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf the file contains only one column, it indicates sample names, and all samples are considered as the same condition\u003c/li\u003e\n\u003cli\u003eIf the file contains two columns, the first column corresponds to sample names, and the second corresponds to conditions (\u003cem\u003ee.g.\u003c/em\u003e tumor, normal)\u003c/li\u003e\n\u003cli\u003eIf the file is not provided, all columns in the matrix apart from the first one are considered as samples\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-input-count-matrix-for-kamrat\" class=\"anchor\" href=\"#input-count-matrix-for-kamrat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput Count Matrix for KaMRaT\u003c/h3\u003e\n\u003cp\u003eThe input count matrix should be in .tsv or .tsv.gz format, in which fields are separated by tabulations.\nIn the matrix, features are presented as rows, and samples as columns. The first column in matrix should always be the feature column (sequences or feature names).\u003cbr\u003e\n\"Features\" can be any quantified feature such as genes, k-mers or contigs. k-mers or contigs are represented by their own sequence.\nKaMRaT accepts extra columns representing non-count values, e.g. feature\u0027s p-value, score, etc. In this case, a smp-info file is mandatory for indicating which columns are the count columns.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-output-count-matrix-by-kamrat\" class=\"anchor\" href=\"#output-count-matrix-by-kamrat\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutput Count Matrix by KaMRaT\u003c/h3\u003e\n\u003cp\u003eThe output count matrix is also .tsv format table, where fields are separated by tabs.\u003cbr\u003e\nIn the matrix, the features are presented as rows, and the columns are in same order as the input.\u003cbr\u003e\nKaMRaT guarantees the information of output matrix is coherent with that of the input matrix. For KaMRaT-rank, though there are steps of count normalization, log transformation and standardization for score evaluation, the count values in output matrix are kept same as input (raw count).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eNote: if you use KaMRaT in command line, please remember to indicate the full path to KaMRaT binary file.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-kamrat-execution\" class=\"anchor\" href=\"#kamrat-execution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKaMRaT Execution\u003c/h3\u003e\n\u003cp\u003eWe recommande using KaMRaT within \u003ccode\u003esingularity\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -B /bind_src:/bind_des kamrat \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eCMD\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e [options] input_table \n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e \u0026lt;CMD\u0026gt; can be one of filter, mask, merge, rank\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe \u003ccode\u003e-B\u003c/code\u003e option is for binding disk partitions to singularity image, please check \u003ccode\u003esingularity\u003c/code\u003e helper for details:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e -h\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIt\u0027s also executable directly on command line:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e/path_to_KaMRaT_bin_dir/kamrat \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eCMD\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e [options] input_table \n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e \u0026lt;CMD\u0026gt; can be one of filter, mask, merge, rank\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn the following sections, we present under the situation of using KaMRaT in \u003ccode\u003esingularity\u003c/code\u003e.\u003cbr\u003e\nFor running it directly on command line, please replace the leading \u003ccode\u003esingularity exec -B /bind_src:/bind_des\u003c/code\u003e by the path to KaMRaT binary file.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-kamrat-helper\" class=\"anchor\" href=\"#kamrat-helper\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKaMRaT Helper\u003c/h3\u003e\n\u003cp\u003eKaMRaT\u0027s top-level helper is accessible by typing one of these commands:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e kamrat\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e kamrat -h\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e kamrat -help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHelpers of each KaMRaT modules are accessible via:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e \u0026lt;CMD\u0026gt; can be one from filter, mask, merge, rank #\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e kamrat \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eCMD\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e -h\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e kamrat \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eCMD\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e -help\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-kamrat-usage-by-module\" class=\"anchor\" href=\"#kamrat-usage-by-module\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKaMRaT Usage by Module\u003c/h3\u003e\n\u003cdetails\u003e\n\u003csummary\u003eindex: index feature count table on disk\u003c/summary\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e[USAGE]    kamrat index -intab STR -outdir STR [-klen INT -unstrand -nfbase INT]\n\n[OPTION]         -h, -help      Print the helper\n                 -intab STR     Input table for index, mandatory\n                 -outdir STR    Output index directory, mandatory\n                 -klen          k-mer length, mandatory if features are k-mer\n                                    if present, indexation will be switched to k-mer mode\n                 -unstrand      Unstranded mode, indexation with canonical k-mers\n                                    if present, indexation will be switched to k-mer mode\n                 -nfbase INT    Base for calculating normalization factor\n                                    normCount_ij \u0026lt;- INT * rawCount_ij / sum_i{rawCount_ij}\n                                    if not provided, input counts will not be normalized\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003efilter: filter feature by expression level\u003c/summary\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e[USAGE]    kamrat filter -idxdir STR -design STR [-upmin INT1:INT2 -downmax INT1:INT2 -reverse -outpath STR -withcounts]\n\n[OPTION]         -h,-help              Print the helper\n                 -idxdir STR           Indexing folder by KaMRaT index, mandatory\n                 -design STR           Path to filter design file, a table of two columns, mandatory\n                                           the first column indicate sample names\n                                           the second column should be either UP or DOWN (capital letters)\n                                               samples with UP will be considered as up-regulated samples\n                                               samples with DOWN will be considered as down-regulated samples\n                                               samples not given will be neutral (not considered for filter)\n                                               samples can also be all UP or all DOWN\n                 -upmin INT1:INT2      Up feature lower bound, [1:1, meaning no filter]\n                                           output features counting \u0026gt;= INT1 in \u0026gt;= INT2 UP-samples\n                 -downmax INT1:INT2    Down feature upper bound [inf:1, meaning no filter]\n                                           output features counting \u0026lt;= INT1 in \u0026gt;= INT2 DOWN-samples\n                 -reverse              Reverse filter, to remove eligible features [false]\n                 -outpath STR          Path to results after filter\n                                           if not provided, output to screen\n                 -withcounts           Output sample count vectors [false]\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003emask: mask k-mers from matrix\u003c/summary\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e[USAGE]    kamrat mask -idxdir STR -fasta STR [-reverse -outpath STR -withcounts]\n              \n[OPTION]         -h,-help         Print the helper\n                 -idxdir STR      Indexing folder by KaMRaT index, mandatory\n                 -fasta STR       Sequence fasta file as the mask, mandatory;\n                 -reverse         Reverse mask, to select the k-mers in sequence fasta file [false];\n                 -outpath STR     Path to extension results\n                                      if not provided, output to screen\n                 -withcounts      Output sample count vectors [false]\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003emerge: extend k-mers into contigs\u003c/summary\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e[USAGE]    kamrat merge -idxdir STR -overlap MAX-MIN [-with STR1[:STR2] -interv STR[:FLOAT] -min-nbkmer INT -outpath STR -withcounts STR]\n\n[OPTION]         -h,-help               Print the helper;\n                 -idxdir STR            Indexing folder by KaMRaT index, mandatory;\n                 -overlap MAX-MIN       Overlap range for extension, mandatory\n                                            MIN and MAX are integers, MIN \u0026lt;= MAX \u0026lt; k-mer length;\n                 -with STR1[:STR2]      File indicating k-mers to be extended (STR1) and rep-mode (STR2)\n                                            if not provided, all indexed k-mers are used for extension\n                                            in the file STR1, a supplementary column of rep-value can be provided\n                                            STR2 can be one of {min, minabs, max, maxabs} [min];\n                 -interv STR[:FLOAT]    Intervention method for extension [pearson:0.20]\n                                            can be one of {none, pearson, spearman, mac}\n                                            the threshold may follow a \u0027:\u0027 symbol;\n                 -min-nbkmer INT        Minimal length of extended contigs [0];\n                 -outpath STR           Path to extension results\n                                            if not provided, output to screen;\n                 -withcounts STR        Output sample count vectors, STR can be one of [mean, median]\n                                            if not provided, output without count vector\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003erank: rank features according to their association with sample conditions\u003c/summary\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e[USAGE]    kamrat rank -idxdir STR -count-mode STR -rankby STR -design STR [-with STR1[:STR2] -seltop NUM -outpath STR -withcounts]\n\n[OPTION]         -h,-help             Print the helper\n                 -idxdir STR          Indexing folder by KaMRaT index, mandatory\n                 -rankby STR          Ranking method, mandatory, can be one of:\n                                          ttest.padj      adjusted p-value of t-test between conditions\n                                          ttest.pi        \\u03C0-value of t-test between conditions\n                                          snr             signal-to-noise ratio between conditions\n                                          dids            DIDS score\n                                          lr:nfold        accuracy by logistic regression classifier\n                                          bayes:nfold     accuracy by naive Bayes classifier\n                                          svm:nfold       accuracy on SVM classifier\n                 -design STR          Path to file indicating sample-condition design\n                                          without header line, each row can be either:\n                                          sample name, sample condition\n                                          sample name, sample condition, sample batch (only for lrc, nbc, and svm)\n                 -with STR1[:STR2]    File indicating features to rank (STR1) and counting mode (STR2)\n                                          if not provided, all indexed features are used for ranking\n                                          STR2 can be one of [rep, mean, median]\n                 -seltop NUM          Select top ranked features\n                                          if NUM \u0026gt; 1, number of top features to select (should be integer)\n                                          if 0 \u0026lt; NUM \u0026lt;= 1, ratio of top features to select\n                                          if absent or NUM \u0026lt;= 0, output all features\n                 -outpath STR         Path to ranking result\n                                          if not provided, output to screen\n                 -withcounts          Output sample count vectors [false]\n\n[NOTE]     For ranking methods lrc, nbc, and svm, a univariate CV fold number (nfold) can be provided\n               if nfold = 0, leave-one-out cross-validation\n               if nfold = 1, without cross-validation, training and testing on the whole datset\n               if nfold \u0026gt; 1, n-fold cross-validation\n           For t-test ranking methods, a transformation log2(x + 1) is applied to sample counts\n           For SVM ranking, sample counts standardization is applied feature by feature\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n\u003csummary\u003equery: query sequences\u003c/summary\u003e\n\u003cpre lang=\"text\"\u003e\u003ccode\u003e[USAGE]    kamrat query -idxdir STR -fasta STR -toquery STR [-withabsent -outpath STR]\n\n[OPTION]         -h,-help         Print the helper\n                 -idxdir STR      Indexing folder by KaMRaT index, mandatory\n                 -fasta STR       Sequence fasta file, mandatory\n                 -toquery STR     Query method, mandatory, can be one of:\n                                      mean        mean count among all composite k-mers for each sample\n                                      median      median count among all composite k-mers for each sample\n                 -withabsent      Output also absent queries (count vector all 0) [default: false]\n                 -outpath STR     Path to extension results\n                                      if not provided, output to screen\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-softwarelibrary-citations\" class=\"anchor\" href=\"#softwarelibrary-citations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware/Library Citations\u003c/h2\u003e\n\u003cp\u003eArmadillo:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConrad Sanderson and Ryan Curtin. Armadillo: a template-based C++ library for linear algebra. Journal of Open Source Software, Vol. 1, pp. 26, 2016.\u003c/li\u003e\n\u003cli\u003eConrad Sanderson and Ryan Curtin. A User-Friendly Hybrid Sparse Matrix Class in C++. Lecture Notes in Computer Science (LNCS), Vol. 10931, pp. 422-430, 2018.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://www.boost.org/\" rel=\"nofollow\"\u003eBoost C++ Library\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eDE-kupl: Audoux, J., Philippe, N., Chikhi, R. et al. DE-kupl: exhaustive capture of biological variation in RNA-seq data through k-mer decomposition. Genome Biol 18, 243 (2017).\u003c/p\u003e\n\u003cp\u003eMLPack: R.R. Curtin, M. Edel, M. Lozhnikov, Y. Mentekidis, S. Ghaisas, S. Zhang. mlpack 3: a fast, flexible machine learning library. Journal of Open Source Software 3:26, 2018.\u003c/p\u003e\n\u003cp\u003eglmnet: Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. \"Regularization paths for generalized linear models via coordinate descent.\" Journal of statistical software 33.1 (2010): 1.\u003c/p\u003e\n\u003cp\u003erandomForest: Liaw, Andy, and Matthew Wiener. \"Classification and regression by randomForest.\" R news 2.3 (2002): 18-22.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1627211901.0
  },
  {
    "data_format": 2,
    "description": "Common scripts, libraries, and utilities for 2p experiments",
    "filenames": [
      "Singularity"
    ],
    "full_name": "deisseroth-lab/two-photon",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-two-photon\" class=\"anchor\" href=\"#two-photon\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etwo-photon\u003c/h1\u003e\n\u003cp\u003eThis repository contains utilities for analyzing 2p data:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#analysis-pipeline\"\u003eAnalysis Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#ripping-containers\"\u003eRipping Containers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-analysis-pipeline\" class=\"anchor\" href=\"#analysis-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnalysis Pipeline\u003c/h2\u003e\n\u003cp\u003eThe analysis pipeline consists of the following stages:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eraw2tiff: converts Bruker proprietary output format to a TIFF stack\u003c/li\u003e\n\u003cli\u003econvert: converts tiff and csv/text files to hdf5.\u003c/li\u003e\n\u003cli\u003epreprocess: detect and remove stim artefacts\u003c/li\u003e\n\u003cli\u003eqa: make QA plots to check stim artefact removal\u003c/li\u003e\n\u003cli\u003eanalyze: run suite2p, optionally combining multiple preprocessed datasets\u003c/li\u003e\n\u003cli\u003ebackup: back up input/intermediate/output data to a safe place\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h3\u003e\n\u003cp\u003eFirst, install the code. You can use \u003ca href=\"https://desktop.github.com/\"\u003eGitHub desktop\u003c/a\u003e, or use git on the command line. This only has to be done once.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/deisseroth-lab/two-photon.git\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNext, install the environment. You will need to install \u003ca href=\"https://docs.conda.io/en/latest/\" rel=\"nofollow\"\u003econda\u003c/a\u003e first. Then\nuse the following command from within the directory where you installed the repo above. This also only has\nto be done once.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda env create -f environment.yml -n two-photon\nconda activate two-photon\npip install -e \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e installs the 2p script (in editable mode, so you can update the code)\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-executing\" class=\"anchor\" href=\"#executing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecuting\u003c/h3\u003e\n\u003cp\u003eTo run the processing script, the environment needs to be activated. This needs to be done each time you start a\nnew terminal.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda activate two-photon\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe executable is called \u003ccode\u003e2p\u003c/code\u003e, and each stage is a different subcommand\nthat can be run. It is possible to run multiple stages by specifying\nmultiple subcommands.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-data-layout-and-global-flags\" class=\"anchor\" href=\"#data-layout-and-global-flags\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData Layout and Global Flags\u003c/h4\u003e\n\u003cp\u003eThe scripts required a strict layout of data, and assume the input data\nfollows a directory structure and filenaming that the Bruker scopes\ncreate. The data is setup in subdirectories of a \u003ccode\u003ebase-path\u003c/code\u003e, named\nby the stage and the \u003ccode\u003eacquisition\u003c/code\u003e name.\u003c/p\u003e\n\u003cp\u003eTo point the script to the correct location of of dataset,\nuse the following flags:\u003c/p\u003e\n\u003cpre lang=\"txt\"\u003e\u003ccode\u003e  --base-path PATH    Top-level storage for local data.  [required]\n  --acquisition TEXT  Acquisition sub-directory to process.  [required]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUsing the following global flags (meaning after \u003ccode\u003e2p\u003c/code\u003e but before other commands or flags):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003ewill use the following locations for the data. Note the expected location of the raw data.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003edata type\u003c/th\u003e\n\u003cth\u003elocation\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eRAWDATA, csv, xml, and env files from scope\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/raw/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etiff stacks\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/tiff/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003econverted hdf5 data\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/convert/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003epreprocess\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/preprocess/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eqa\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/qa/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eanalyze - suite2p output\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e/my/data/analyze/20210428M198/slm-001\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-command-raw2tiff\" class=\"anchor\" href=\"#command-raw2tiff\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: raw2tiff\u003c/h4\u003e\n\u003cp\u003eThe raw2tiff command runs the Bruker software to rip the RAWDATA into a tiff stack.\nThis is a Windows-only command, until the kinks of running on Linux are ironed out.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001\n    raw2tiff\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-convert\" class=\"anchor\" href=\"#command-convert\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: convert\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003econvert\u003c/code\u003e command converts the tiff stacks and voltage data to hdf5.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001 \\\n    convert --channel 3\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-preprocess\" class=\"anchor\" href=\"#command-preprocess\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: preprocess\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003epreprocess\u003c/code\u003e command performs processing like stim removal on the data. It should be\nrun even if there are no stim artefacts (in which case, no actual computation is done),\nso that downstream stages find the data in the correct place.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001 \\\n    preprocess --frame-channel-name=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eframe starts\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --stim-channel-name=respir\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample based on piezeo period:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/minoue2/2p_CNC/ \\\n    --acquisition Chris_210429/10263_920nm_PC250-300-001  \\\n    preprocess \\\n    --frame-channel-name=StartFrameResonant \\\n    --stim-channel-name=LEDSyncSignal \\\n    --piezo-period-frames=7 \\\n    --piezo-skip-frames=3\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-qa\" class=\"anchor\" href=\"#command-qa\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: qa\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eqa\u003c/code\u003e command makes some QA plots to understand if the stim effects are\nbeing correctly removed during preprocessing. It plots a number of frames\n(given by --max-frames) containing stims, showing the data before and after\nstim removal.\u003c/p\u003e\n\u003cp\u003eThis is an optional step.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /my/data \\\n    --acquisition 20210428M198/slm-001 \\\n    qa\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-analyze\" class=\"anchor\" href=\"#command-analyze\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: analyze\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003eanalyze\u003c/code\u003e command runs Suite2p on the preprocessed dataset.\u003c/p\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    analyze\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample of analyzing multiple acquisitions together:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    analyze --extra-acquisitions 20210428M198/slm-000\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample of using non-default Suite2p options file (json format):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    analyze --suite2p-params-file two_photon/ops_files/drinnedb.json\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-backup\" class=\"anchor\" href=\"#command-backup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand: backup\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003ebackup\u003c/code\u003e command copies the output of one or more stages to backup directory.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    backup \\\n    --backup-path /media/hdd1/oak/mount/two-photon/backup \\\n    --backup-stage raw,tiff\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e--backup_path\"\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-multiple-commands-at-once\" class=\"anchor\" href=\"#using-multiple-commands-at-once\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing multiple commands at once\u003c/h2\u003e\n\u003cp\u003eSeveral commands can be run in succession by adding each one to your command line with its\nnecessary flags.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e2p \\\n    --base-path /media/hdd0/two-photon/drinnenb/work \\\n    --acquisition 20210428M198/slm-001 \\\n    raw2tiff \\\n    convert --channel 3 \\\n    preprocess --stim-channel-name=respir \\\n    analyze --extra-acquisitions 20210428M198/slm-000\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ripping-containers\" class=\"anchor\" href=\"#ripping-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRipping Containers\u003c/h2\u003e\n\u003cp\u003eRipping is the process for converting a Bruker RAWDATA file into a set of TIFF files.\u003c/p\u003e\n\u003cp\u003eContainers exist to help run the ripping on any platform, but it has been found they\nperform sub-optimally and are 10-100x slower than ripping on a Windows machine using\nthe native ripper. It is advised NOT to use this yet.\u003c/p\u003e\n\u003cp\u003eThe lab has created \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e and\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e containers with the Bruker Prairie View software,\nwhich can be used to rip raw data computers with either set of container software installed.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ripping-via-singularity\" class=\"anchor\" href=\"#ripping-via-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRipping via Singularity\u003c/h3\u003e\n\u003cp\u003eIf you would like to run from a container on \u003ca href=\"https://www.sherlock.stanford.edu/\" rel=\"nofollow\"\u003eSherlock\u003c/a\u003e,\nthe lab keeps a copy available in $OAK/pipeline/bruker-rip/containers.\u003c/p\u003e\n\u003cp\u003eHere\u0027s a quick demo:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ mkdir -p \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test\n$ cp -r \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/pipeline/bruker-rip/sampledata/overview-023 \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test\n$ chmod -R u+w \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test/overview-023  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Write permissions needed to convert files.\u003c/span\u003e\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/users/\u003cspan class=\"pl-smi\"\u003e${USER}\u003c/span\u003e/test/overview-023\n$ singularity run --bind=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:/data \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/pipeline/bruker-rip/containers/bruker-rip.sif\n\nCopying wine environment.\n\nExecuting rip. One err and four fixme statements are OK.\n\n2020-11-16 17:25:43.859 rip:50 INFO Data created with Prairie version 5.4, using ripper: /apps/Prairie View 5.5/Utilities/Image-Block Ripping Utility.exe\n2020-11-16 17:25:43.861 rip:77 INFO Ripping from:\n /data/Cycle00001_Filelist.txt\n /data/CYCLE_000001_RAWDATA_000025\n2020-11-16 17:25:43.883 rip:123 INFO Watching \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e 3600 more seconds\n000d:err:menubuilder:init_xdg error looking up the desktop directory\n0031:fixme:ntdll:EtwEventRegister ({5eec90ab-c022-44b2-a5dd-fd716a222a15}, 0x5571000, 0x5582030, 0x5582050) stub.\n0031:fixme:ntdll:EtwEventSetInformation (deadbeef, 2, 0x557fd70, 43) stub\n0031:fixme:nls:GetThreadPreferredUILanguages 00000038, 0x4fccdb4, 0x4fccdd0 0x4fccdb0\n0031:fixme:nls:get_dummy_preferred_ui_language (0x38 0x4fccdb4 0x4fccdd0 0x4fccdb0) returning a dummy value (current locale)\n2020-11-16 17:25:53.889 rip:134 INFO   Found filelist files: None\n2020-11-16 17:25:53.889 rip:135 INFO   Found rawdata files: None\n2020-11-16 17:25:53.889 rip:136 INFO   Found this many tiff files: 1\n2020-11-16 17:25:53.889 rip:123 INFO Watching \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e 3590 more seconds\n2020-11-16 17:26:03.899 rip:134 INFO   Found filelist files: None\n2020-11-16 17:26:03.899 rip:135 INFO   Found rawdata files: None\n2020-11-16 17:26:03.899 rip:136 INFO   Found this many tiff files: 1\n2020-11-16 17:26:03.899 rip:139 INFO Detected ripping is \u003cspan class=\"pl-c1\"\u003ecomplete\u003c/span\u003e\n2020-11-16 17:26:13.909 rip:141 INFO Killing ripper\n2020-11-16 17:26:13.910 rip:143 INFO Ripper has been killed\n2020-11-16 17:26:14.912 rip:115 INFO cleaned up\u003cspan class=\"pl-k\"\u003e!\u003c/span\u003e\nX connection to :99 broken (explicit \u003cspan class=\"pl-c1\"\u003ekill\u003c/span\u003e or server shutdown).\nX connection to :99 broken (explicit \u003cspan class=\"pl-c1\"\u003ekill\u003c/span\u003e or server shutdown).\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHere\u0027s how to run on your own data. We request a node allocation using \u003ccode\u003esdev\u003c/code\u003e as\nlong-running jobs should not use login nodes.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e my/data/path\n$ sdev  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e May take some time to get a machine for development use\u003c/span\u003e\n$ singularity run --bind=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:/data \u003cspan class=\"pl-smi\"\u003e$OAK\u003c/span\u003e/pipeline/bruker-rip/containers/bruker-rip.sif\n\n[Similar output as above]\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd here\u0027s how to run a batch job, using the \u003ccode\u003erip.sbatch\u003c/code\u003e script from this\nrepository.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e my/data/path\n$ sbatch path/to/two-photon/rip.sbatch \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\nSubmitted batch job ABCDEFGH\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-ripping-via-docker\" class=\"anchor\" href=\"#ripping-via-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRipping via Docker\u003c/h3\u003e\n\u003cp\u003eYou can run on a device with Docker installed using the command below. The image\nwill be available locally if you\u0027ve build from source (see below), or it will be\nfetched from the the \u003ca href=\"https://code.stanford.edu/deisseroth-lab/bruker-rip\" rel=\"nofollow\"\u003eStanford GitLab\u003c/a\u003e. Contact \u003ca href=\"mailto:croat@stanford.edu\"\u003ecroat@stanford.edu\u003c/a\u003e if you need access.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ./rip_docker.sh \\\n    scr.svc.stanford.edu/deisseroth-lab/bruker-rip:20200903 \\\n    /path/to/data/with/filelist/and/rawdata/\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample run:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ ./rip_docker.sh \\\n    scr.svc.stanford.edu/deisseroth-lab/bruker-rip:20200903 \\\n    /media/hdd0/two-photon/sample/overview-023\nSetting up wine environment\n\nExecuting rip.  It is OK to see 1 err and 4 fixme statements \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e what follows\n\n2020-09-03 14:41:33.936 rip:50 INFO Ripping from:\n /data/Cycle00001_Filelist.txt\n /data/CYCLE_000001_RAWDATA_000025\n2020-09-03 14:41:33.940 rip:96 INFO Waiting \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish: 3600 seconds remaining\n000d:err:menubuilder:init_xdg error looking up the desktop directory\n0031:fixme:ntdll:EtwEventRegister ({5eec90ab-c022-44b2-a5dd-fd716a222a15}, 0xd441000, 0xd452030, 0xd452050) stub.\n0031:fixme:ntdll:EtwEventSetInformation (deadbeef, 2, 0xd44fd70, 43) stub\n0031:fixme:nls:GetThreadPreferredUILanguages 00000038, 0xdaacdb4, 0xdaacdd0 0xdaacdb0\n0031:fixme:nls:get_dummy_preferred_ui_language (0x38 0xdaacdb4 0xdaacdd0 0xdaacdb0) returning a dummy value (current locale)\n2020-09-03 14:41:43.951 rip:107 INFO   Found filelist files: None\n2020-09-03 14:41:43.951 rip:108 INFO   Found rawdata files: None\n2020-09-03 14:41:43.951 rip:109 INFO   Found this many tiff files: 1\n2020-09-03 14:41:43.951 rip:96 INFO Waiting \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e ripper to finish: 3590 seconds remaining\n2020-09-03 14:41:53.962 rip:107 INFO   Found filelist files: None\n2020-09-03 14:41:53.962 rip:108 INFO   Found rawdata files: None\n2020-09-03 14:41:53.962 rip:109 INFO   Found this many tiff files: 1\n2020-09-03 14:41:53.963 rip:112 INFO Detected ripping is \u003cspan class=\"pl-c1\"\u003ecomplete\u003c/span\u003e\n2020-09-03 14:42:03.973 rip:114 INFO Killing ripper\n2020-09-03 14:42:03.973 rip:116 INFO Ripper has been killed\n2020-09-03 14:42:04.975 rip:88 INFO cleaned up\u003cspan class=\"pl-k\"\u003e!\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building-containers\" class=\"anchor\" href=\"#building-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding Containers\u003c/h3\u003e\n\u003cp\u003eTo build all available containers, which will first build the Docker container, and then convert it\nto a Singularity container:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake build\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo build just the docker containers:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emake build_docker\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eView the \u003ca href=\"Makefile\"\u003eMakefile\u003c/a\u003e for additional targets, including targets to build just build specific containers.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1624938646.0
  },
  {
    "data_format": 2,
    "description": "Tools for XML submission",
    "filenames": [
      "Singularity"
    ],
    "full_name": "ddbj/submission-excel2xml",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-excel-and-container-images-for-dra-metadata-xml-submission\" class=\"anchor\" href=\"#excel-and-container-images-for-dra-metadata-xml-submission\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExcel and container images for DRA metadata XML submission\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u65e5\u672c\u8a9e\" class=\"anchor\" href=\"#%E6%97%A5%E6%9C%AC%E8%AA%9E\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u65e5\u672c\u8a9e\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u751f\u547d\u60c5\u5831\u30fbDDBJ \u30bb\u30f3\u30bf\u30fc\u003c/li\u003e\n\u003cli\u003e\u516c\u958b\u65e5: 2021-07-13\u003c/li\u003e\n\u003cli\u003eversion: v1.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission.html\" rel=\"nofollow\"\u003eDDBJ Sequence Read Archive (DRA)\u003c/a\u003e \u306b\u767b\u9332\u3059\u308b\u305f\u3081\u306e Submission\u3001Experiment \u3068 Run XML \u3092\u751f\u6210\u30fb\u30c1\u30a7\u30c3\u30af\u3059\u308b\u305f\u3081\u306e\u30a8\u30af\u30bb\u30eb\u3001Singularity \u3068 Docker \u30b3\u30f3\u30c6\u30ca\u3001\u53ca\u3073\u3001SRA xsd\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u5c65\u6b74\" class=\"anchor\" href=\"#%E5%B1%A5%E6%AD%B4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u5c65\u6b74\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e2021-07-13: v1.2 \u003ca href=\"https://github.com/ddbj/pub/tree/master/docs/dra#changes-to-common-xml-159-on-7-july-2021\"\u003exsd 1.5.9\u003c/a\u003e \u306b\u5bfe\u5fdc\u3002xsd \u3092 \u003ca href=\"https://github.com/ddbj/pub\"\u003epub\u003c/a\u003e \u304b\u3089\u53d6\u5f97\u3059\u308b\u3088\u3046\u306b\u5909\u66f4\u3002\u003c/li\u003e\n\u003cli\u003e2020-04-24: v1.1 \u521d\u7248\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\" class=\"anchor\" href=\"#%E3%83%80%E3%82%A6%E3%83%B3%E3%83%AD%E3%83%BC%E3%83%89\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u003c/h2\u003e\n\u003cp\u003esubmission-excel2xml \u30ec\u30dd\u30b8\u30c8\u30ea\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/ddbj/submission-excel2xml.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u30a8\u30af\u30bb\u30eb\u306b\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u8a18\u5165\" class=\"anchor\" href=\"#%E3%82%A8%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AB%E3%83%A1%E3%82%BF%E3%83%87%E3%83%BC%E3%82%BF%E3%82%92%E8%A8%98%E5%85%A5\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u30a8\u30af\u30bb\u30eb\u306b\u30e1\u30bf\u30c7\u30fc\u30bf\u3092\u8a18\u5165\u003c/h2\u003e\n\u003cp\u003e\u30e1\u30bf\u30c7\u30fc\u30bf\u3068\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u30a8\u30af\u30bb\u30eb\u306e \u0027Submission\u0027\u3001\u0027Experiment\u0027\u3001\u0027Run\u0027 \u3068 \u0027Run-file\u0027 \u30b7\u30fc\u30c8\u306b\u8a18\u5165\u3057\u307e\u3059\u3002\u30e1\u30bf\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u306f\u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission.html#metadata\" rel=\"nofollow\"\u003e\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u003c/a\u003e\u3068 \u0027Readme\u0027 \u30b7\u30fc\u30c8\u3092\u3054\u89a7\u304f\u3060\u3055\u3044\u3002\u003cbr\u003e\n\u0027example-0001_dra_metadata.xlsx\u0027 \u304c\u8a18\u5165\u4f8b\u306b\u306a\u308a\u307e\u3059\u3002\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-xml-\u3092\u751f\u6210-singularity\" class=\"anchor\" href=\"#xml-%E3%82%92%E7%94%9F%E6%88%90-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eXML \u3092\u751f\u6210: Singularity\u003c/h3\u003e\n\u003cp\u003eSingularity \u30a4\u30e1\u30fc\u30b8\u3092\u003ca href=\"https://drive.google.com/drive/u/3/folders/1Qrqpgjw_No5q6mO6rcihNwVCyMBVytzL\" rel=\"nofollow\"\u003e\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u003c/a\u003e\u3001\u3082\u3057\u304f\u306f\u3001\u4ee5\u4e0b\u306e\u624b\u9806\u3067\u30ed\u30fc\u30ab\u30eb\u74b0\u5883\u3067\u69cb\u7bc9\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd submission-excel2xml\nsudo singularity build excel2xml.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u30a8\u30af\u30bb\u30eb\u304b\u3089 Submission\u3001Experiment \u3068 Run XML \u3092\u751f\u6210\u3057\u307e\u3059\u3002\nD-way \u30a2\u30ab\u30a6\u30f3\u30c8 ID\u3001submission \u756a\u53f7\u3068 BioProject \u30a2\u30af\u30bb\u30c3\u30b7\u30e7\u30f3\u756a\u53f7\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003cp\u003e\u4f8b\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDRA submission id \u0027example-0001\u0027: -a example -i 0001\u003c/li\u003e\n\u003cli\u003eBioProject \u0027PRJDB7252\u0027 : -p PRJDB7252\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec excel2xml.simg excel2xml.rb -a example -i 0001 -p PRJDB7252 example-0001_dra_metadata.xlsx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u30a8\u30af\u30bb\u30eb\u304b\u3089\u4e09\u3064\u306e XML \u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexample-0001_Submission.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Experiment.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Run.xml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSubmission ID \u3092\u6307\u5b9a\u3057\u3066 XML \u3092\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3059\u3002XML \u306f submission-excel2xml \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u76f4\u4e0b\u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002SRA xsd \u30d5\u30a1\u30a4\u30eb\u306f build \u4e2d\u306b\u30b3\u30f3\u30c6\u30ca\u30fc\u5185\u306e /opt/submission-excel2xml/ \u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec excel2xml.simg validate_dra_meta.rb -a example -i 0001\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u3053\u3053\u3067\u306f xsd \u306b\u5bfe\u3059\u308b\u30c1\u30a7\u30c3\u30af\u3068\u6700\u4f4e\u9650\u306e\u30c1\u30a7\u30c3\u30af\u304c\u5b9f\u65bd\u3055\u308c\u307e\u3059\u3002\u003cbr\u003e\nDRA \u306e\u767b\u9332\u30b5\u30a4\u30c8\u3067\u306f\u3088\u308a\u8a73\u7d30\u306a\u30c1\u30a7\u30c3\u30af\u304c\u5b9f\u65bd\u3055\u308c\u308b\u305f\u3081\u3001\u30d1\u30b9\u3057\u305f XML \u304c\u767b\u9332\u904e\u7a0b\u3067\u30a8\u30e9\u30fc\u306b\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-xml-\u3092\u751f\u6210-docker\" class=\"anchor\" href=\"#xml-%E3%82%92%E7%94%9F%E6%88%90-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eXML \u3092\u751f\u6210: Docker\u003c/h3\u003e\n\u003cp\u003eDocker \u30a4\u30e1\u30fc\u30b8\u3092\u69cb\u7bc9\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd submission-excel2xml\nsudo docker build -t excel2xml .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u30a8\u30af\u30bb\u30eb\u304b\u3089 Submission\u3001Experiment \u3068 Run XML \u3092\u751f\u6210\u3057\u307e\u3059\u3002\u003cbr\u003e\nD-way \u30a2\u30ab\u30a6\u30f3\u30c8 ID\u3001submission \u756a\u53f7\u3001BioProject \u30a2\u30af\u30bb\u30c3\u30b7\u30e7\u30f3\u756a\u53f7\u3068\u30a8\u30af\u30bb\u30eb\u3092\u542b\u3080\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d5\u30eb\u30d1\u30b9\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003cp\u003e\u4f8b\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDRA submission id \u0027example-0001\u0027: -a example -i 0001\u003c/li\u003e\n\u003cli\u003eBioProject \u0027PRJDB7252\u0027 : -p PRJDB7252\u003c/li\u003e\n\u003cli\u003e\u0027path_to_excel_directory\u0027: \u30a8\u30af\u30bb\u30eb\u3092\u542b\u3080\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306e\u30d5\u30eb\u30d1\u30b9\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run -v /path_to_excel_directory:/data -w /data excel2xml excel2xml.rb -a example -i 0001 -p PRJDB7252 example-0001_dra_metadata.xlsx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u30a8\u30af\u30bb\u30eb\u304b\u3089\u4e09\u3064\u306e XML \u304c\u751f\u6210\u3055\u308c\u307e\u3059\u3002\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexample-0001_Submission.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Experiment.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Run.xml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSubmission ID \u3092\u6307\u5b9a\u3057\u3066 XML \u3092\u30c1\u30a7\u30c3\u30af\u3057\u307e\u3059\u3002XML \u306f submission-excel2xml \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u76f4\u4e0b\u306b\u914d\u7f6e\u3055\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002SRA xsd \u30d5\u30a1\u30a4\u30eb\u306f build \u4e2d\u306b\u30b3\u30f3\u30c6\u30ca\u30fc\u5185\u306e /opt/submission-excel2xml/ \u306b\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run -v /path_to_excel_directory:/data -w /data excel2xml validate_dra_meta.rb -a example -i 0001\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u3053\u3053\u3067\u306f xsd \u306b\u5bfe\u3059\u308b\u30c1\u30a7\u30c3\u30af\u3068\u6700\u4f4e\u9650\u306e\u30c1\u30a7\u30c3\u30af\u304c\u5b9f\u65bd\u3055\u308c\u307e\u3059\u3002\u003cbr\u003e\nDRA \u306e\u767b\u9332\u30b5\u30a4\u30c8\u3067\u306f\u3088\u308a\u8a73\u7d30\u306a\u30c1\u30a7\u30c3\u30af\u304c\u5b9f\u65bd\u3055\u308c\u308b\u305f\u3081\u3001\u30d1\u30b9\u3057\u305f XML \u304c\u767b\u9332\u904e\u7a0b\u3067\u30a8\u30e9\u30fc\u306b\u306a\u308b\u3053\u3068\u304c\u3042\u308a\u307e\u3059\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-\u30c1\u30a7\u30c3\u30af\u7d50\u679c\" class=\"anchor\" href=\"#%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF%E7%B5%90%E6%9E%9C\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u30c1\u30a7\u30c3\u30af\u7d50\u679c\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sra-xsd-\u306b\u5bfe\u3059\u308b-xml-\u30c1\u30a7\u30c3\u30af\" class=\"anchor\" href=\"#sra-xsd-%E3%81%AB%E5%AF%BE%E3%81%99%E3%82%8B-xml-%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSRA xsd \u306b\u5bfe\u3059\u308b XML \u30c1\u30a7\u30c3\u30af\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u30e1\u30bf\u30c7\u30fc\u30bf XML \u306f \u003ca href=\"https://github.com/ddbj/pub/tree/master/docs/dra/xsd/1-5\"\u003eSRA xsd\u003c/a\u003e \u306b\u5bfe\u3057\u3066\u30c1\u30a7\u30c3\u30af\u3055\u308c\u307e\u3059\u3002\u30e1\u30c3\u30bb\u30fc\u30b8\u306b\u5f93\u3063\u3066 XML \u3092\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-xml-\u306e\u5185\u5bb9\u30c1\u30a7\u30c3\u30af\" class=\"anchor\" href=\"#xml-%E3%81%AE%E5%86%85%E5%AE%B9%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eXML \u306e\u5185\u5bb9\u30c1\u30a7\u30c3\u30af\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eSubmission\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eError: Submission: \u516c\u958b\u4e88\u5b9a\u65e5\u304c\u904e\u53bb\u306e\u65e5\u4ed8\u003cbr\u003e\n\u5c06\u6765\u306e\u65e5\u4ed8\u3092\u6307\u5b9a\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExperiment \u3068 Run\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eError: Run: #{run_alias} Paired library only has one file.\u003cbr\u003e\n\u30da\u30a2\u30e9\u30a4\u30d6\u30e9\u30ea Experiment \u3067\u306f\u5c11\u306a\u304f\u3068\u3082\u4e8c\u3064\u306e\u914d\u5217\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb (\u4f8b\u3001R1.fastq \u3068 R2.fastq) \u304c\u542b\u307e\u308c\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u53c2\u7167\u95a2\u4fc2\u30c1\u30a7\u30c3\u30af\" class=\"anchor\" href=\"#%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E3%81%AE%E5%8F%82%E7%85%A7%E9%96%A2%E4%BF%82%E3%83%81%E3%82%A7%E3%83%83%E3%82%AF\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306e\u53c2\u7167\u95a2\u4fc2\u30c1\u30a7\u30c3\u30af\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eError: Run to Experiment reference error.\u003cbr\u003e\n\u5168\u3066\u306e Experiment \u304c Run \u304b\u3089\u53c2\u7167\u3055\u308c\u3066\u3044\u306a\u3044\u3002\u003cbr\u003e\nExperiment \u3092\u53c2\u7167\u3057\u3066\u3044\u306a\u3044 Run \u304c\u5b58\u5728\u3059\u308b\u3002\u003cbr\u003e\nRun \u304b\u3089\u53c2\u7167\u3055\u308c\u3066\u3044\u306a\u3044 Experiment \u304c\u5b58\u5728\u3059\u308b\u3002\n\u3053\u306e\u3088\u3046\u306a\u5834\u5408\u3001\u5168\u3066\u306e Run \u304c\u5168\u3066\u306e Experiment \u3092\u53c2\u7167\u3059\u308b\u3088\u3046\u306b\u4fee\u6b63\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u30e1\u30bf\u30c7\u30fc\u30bf\u30e2\u30c7\u30eb\u306f \u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission.html#metadata-objects\" rel=\"nofollow\"\u003eDRA Handbook\u003c/a\u003e \u3092\u53c2\u7167\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dra-\u30a6\u30a7\u30d6\u753b\u9762\u304b\u3089-xml-\u3092\u767b\u9332\u3059\u308b\" class=\"anchor\" href=\"#dra-%E3%82%A6%E3%82%A7%E3%83%96%E7%94%BB%E9%9D%A2%E3%81%8B%E3%82%89-xml-%E3%82%92%E7%99%BB%E9%8C%B2%E3%81%99%E3%82%8B\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDRA \u30a6\u30a7\u30d6\u753b\u9762\u304b\u3089 XML \u3092\u767b\u9332\u3059\u308b\u003c/h2\u003e\n\u003cp\u003e\u30e1\u30bf\u30c7\u30fc\u30bf XML \u3092\u767b\u9332\u3059\u308b\u524d\u306b\u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission.html#upload-sequence-data\" rel=\"nofollow\"\u003e\u767b\u9332\u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u306b\u914d\u5217\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3059\u003c/a\u003e\u3002D-way \u306b\u30ed\u30b0\u30a4\u30f3\u5f8c\u3001\u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission.html#create-metadata-in-xml-files\" rel=\"nofollow\"\u003eSubmission\u3001Experiment \u3068 Run XML \u3092 DRA \u767b\u9332\u30da\u30fc\u30b8\u3067\u3067\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u003c/a\u003e \u3057\u307e\u3059\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-github-\u3084-xml-\u751f\u6210\u65b9\u6cd5\u304c\u5206\u304b\u3089\u306a\u3044\u5834\u5408\" class=\"anchor\" href=\"#github-%E3%82%84-xml-%E7%94%9F%E6%88%90%E6%96%B9%E6%B3%95%E3%81%8C%E5%88%86%E3%81%8B%E3%82%89%E3%81%AA%E3%81%84%E5%A0%B4%E5%90%88\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGithub \u3084 XML \u751f\u6210\u65b9\u6cd5\u304c\u5206\u304b\u3089\u306a\u3044\u5834\u5408\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.ddbj.nig.ac.jp/files/submission/dra_metadata.xlsx\" rel=\"nofollow\"\u003eDRA \u30e1\u30bf\u30c7\u30fc\u30bf\u30a8\u30af\u30bb\u30eb\u003c/a\u003e \u3092\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u304b\u3089\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3001\u5185\u5bb9\u3092\u82f1\u8a9e\u3067\u8a18\u5165\u3057\u3001\u30e1\u30fc\u30eb (\u003ca href=\"mailto:trace@ddbj.nig.ac.jp\"\u003etrace@ddbj.nig.ac.jp\u003c/a\u003e) \u6dfb\u4ed8\u3067 DRA \u30c1\u30fc\u30e0\u306b\u304a\u9001\u308a\u304f\u3060\u3055\u3044\u3002\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nig-\u30b9\u30d1\u30b3\u30f3\u3067\u306e\u5b9f\u65bd\u65b9\u6cd5\" class=\"anchor\" href=\"#nig-%E3%82%B9%E3%83%91%E3%82%B3%E3%83%B3%E3%81%A7%E3%81%AE%E5%AE%9F%E6%96%BD%E6%96%B9%E6%B3%95\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNIG \u30b9\u30d1\u30b3\u30f3\u3067\u306e\u5b9f\u65bd\u65b9\u6cd5\u003c/h2\u003e\n\u003cp\u003e\u56fd\u7acb\u907a\u4f1d\u5b66\u7814\u7a76\u6240 \u751f\u547d\u60c5\u5831\u30fbDDBJ \u30bb\u30f3\u30bf\u30fc\u304c\u904b\u55b6\u3059\u308b \u003ca href=\"https://www.ddbj.nig.ac.jp/sc\" rel=\"nofollow\"\u003eNIG \u30b9\u30d1\u30b3\u30f3\u003c/a\u003e \u3067\u306f \u003ccode\u003e/lustre6/public/app/submission-excel2xml/\u003c/code\u003e\n\u306b Singularity \u30a4\u30e1\u30fc\u30b8\u304c\u8a2d\u7f6e\u3055\u308c\u3066\u3044\u307e\u3059\u3002\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3084 build \u4f5c\u696d\u3092\u3059\u308b\u3053\u3068\u306a\u304f\u3001\u30e1\u30bf\u30c7\u30fc\u30bf\u30a8\u30af\u30bb\u30eb\u30d5\u30a1\u30a4\u30eb\u304c\u3042\u308c\u3070 XML \u751f\u6210\u3084 XML \u306e\u30c1\u30a7\u30c3\u30af\u3092\u5b9f\u65bd\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u003cbr\u003e\n\u591a\u4ef6\u6570\u306e\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u304c\u30b9\u30d1\u30b3\u30f3\u306b\u3042\u308b\u5834\u5408\u3001\u30e1\u30bf\u30c7\u30fc\u30bf XML \u4f5c\u6210\u3001\u53ca\u3073\u3001\u30c7\u30fc\u30bf\u30d5\u30a1\u30a4\u30eb\u306e DRA \u30d5\u30a1\u30a4\u30eb\u53d7\u4ed8\u30b5\u30fc\u30d0 (ftp-private.ddbj.nig.ac.jp) \u3078\u306e\u8ee2\u9001\u3092\u30b9\u30d1\u30b3\u30f3\u4e0a\u3067\u5b8c\u7d50\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\u003c/p\u003e\n\u003cp\u003e\u30a8\u30af\u30bb\u30eb\u304b\u3089 Submission\u3001Experiment \u3068 Run XML \u3092\u751f\u6210\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec /lustre6/public/app/submission-excel2xml/excel2xml.simg excel2xml.rb -a example -i 0001 -p PRJDB7252 example-0001_dra_metadata.xlsx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eXML \u306e\u30c1\u30a7\u30c3\u30af\u3002\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec /lustre6/public/app/submission-excel2xml/excel2xml.simg validate_dra_meta.rb -a example -i 0001\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-english\" class=\"anchor\" href=\"#english\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnglish\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBioinformation and DDBJ Center\u003c/li\u003e\n\u003cli\u003erelease: 2020-07-13\u003c/li\u003e\n\u003cli\u003eversion: v1.2\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese files are Excel, Singularity and Docker container images and SRA xsd for generation and validation of Submission, Experiment and Run XMLs for \u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission-e.html\" rel=\"nofollow\"\u003eDDBJ Sequence Read Archive (DRA)\u003c/a\u003e submission.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-history\" class=\"anchor\" href=\"#history\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHistory\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e2021-07-13: v1.2 Update to \u003ca href=\"https://github.com/ddbj/pub/tree/master/docs/dra#changes-to-common-xml-159-on-7-july-2021\"\u003exsd 1.5.9\u003c/a\u003e. Download the xsd files from \u003ca href=\"https://github.com/ddbj/pub\"\u003epub\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003e2020-04-24: v1.1 Initial release\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h2\u003e\n\u003cp\u003eDownload the DDBJ submission-excel2xml repository.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/ddbj/submission-excel2xml.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-enter-metadata-in-the-excel\" class=\"anchor\" href=\"#enter-metadata-in-the-excel\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnter metadata in the excel\u003c/h2\u003e\n\u003cp\u003eEnter metadata and data files in the \u0027Submission\u0027, \u0027Experiment\u0027, \u0027Run\u0027 and \u0027Run-file\u0027 sheets of the excel.\u003cbr\u003e\nSee our \u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission-e.html#metadata\" rel=\"nofollow\"\u003ewebsite\u003c/a\u003e for metadata and \u0027Readme\u0027 sheet of the excel for details.\u003cbr\u003e\nSee \u0027example-0001_dra_metadata.xlsx\u0027 for example.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-generate-xmls-singularity\" class=\"anchor\" href=\"#generate-xmls-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate XMLs: Singularity\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://drive.google.com/drive/u/3/folders/1Qrqpgjw_No5q6mO6rcihNwVCyMBVytzL\" rel=\"nofollow\"\u003eDownload\u003c/a\u003e the Singularity image or build the Singularity image as follows.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd submission-excel2xml\nsudo singularity build excel2xml.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGenerate Submission, Experiment and Run XMLs from the excel.\u003cbr\u003e\nSpecify the D-way account ID, submission number and BioProject accession.\u003c/p\u003e\n\u003cp\u003eFor example,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDRA submission id \u0027example-0001\u0027: -a example -i 0001\u003c/li\u003e\n\u003cli\u003eBioProject \u0027PRJDB7252\u0027 : -p PRJDB7252\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec excel2xml.simg excel2xml.rb -a example -i 0001 -p PRJDB7252 example-0001_dra_metadata.xlsx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThree XMLs are generated from the excel.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexample-0001_Submission.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Experiment.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Run.xml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eValidate the XMLs by specifying the submission ID. The XML files must be under the submission-excel2xml directory. The SRA xsd files have been downloaded to /opt/submission-excel2xml/ from \u003ca href=\"https://github.com/ddbj/pub\"\u003epub\u003c/a\u003e in the container during the build.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec excel2xml.simg validate_dra_meta.rb -a example -i 0001\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlease note that this validator only performs xsd validation and minimum checks.\u003cbr\u003e\nThe XMLs are fully validated in the DRA web XML registration process,\nso the checked XMLs may be failed in the DRA submission system.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-generate-xmls-docker\" class=\"anchor\" href=\"#generate-xmls-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGenerate XMLs: Docker\u003c/h3\u003e\n\u003cp\u003eBuild the Docker image.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd submission-excel2xml\nsudo docker build -t excel2xml .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGenerate Submission, Experiment and Run XMLs from the excel.\u003cbr\u003e\nSpecify the D-way account ID, submission number, BioProject accession and full path of the directory which contains the excel.\u003c/p\u003e\n\u003cp\u003eFor example,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDRA submission id \u0027example-0001\u0027: -a example -i 0001\u003c/li\u003e\n\u003cli\u003eBioProject \u0027PRJDB7252\u0027 : -p PRJDB7252\u003c/li\u003e\n\u003cli\u003e\u0027path_to_excel_directory\u0027: full path of the directory which contains the excel.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run -v /path_to_excel_directory:/data -w /data excel2xml excel2xml.rb -a example -i 0001 -p PRJDB7252 example-0001_dra_metadata.xlsx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThree XMLs are generated from the excel.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eexample-0001_Submission.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Experiment.xml\u003c/li\u003e\n\u003cli\u003eexample-0001_Run.xml\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eValidate the XMLs by specifying the submission ID. The XML files must be under the submission-excel2xml directory. The SRA xsd files have been downloaded to /opt/submission-excel2xml/ from \u003ca href=\"https://github.com/ddbj/pub\"\u003epub\u003c/a\u003e in the container during the build.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo docker run -v /path_to_excel_directory:/data -w /data excel2xml validate_dra_meta.rb -a example -i 0001\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePlease note that this validator only performs xsd validation and minimum checks.\u003cbr\u003e\nThe XMLs are fully validated in the DRA web XML registration process,\nso the checked XMLs may be failed in the DRA submission system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-validation-results\" class=\"anchor\" href=\"#validation-results\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eValidation results\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-xml-validation-against-sra-xsd\" class=\"anchor\" href=\"#xml-validation-against-sra-xsd\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eXML validation against SRA xsd\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eMetadata XMLs are validated against \u003ca href=\"https://github.com/ddbj/pub/tree/master/docs/dra/xsd/1-5\"\u003erespective SRA xsd\u003c/a\u003e. Modify the XMLs according to the xsd validation messages.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-xml-content-check\" class=\"anchor\" href=\"#xml-content-check\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eXML content check\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eSubmission\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eError: Submission: Past hold date.\u003cbr\u003e\nSet the future hold date.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eExperiment and Run\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eError: Run: #{run_alias} Paired library only has one file.\u003cbr\u003e\nInclude at least two sequence data files (for example, R1.fastq and R2.fastq) for paired library Experiment.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-object-reference-check\" class=\"anchor\" href=\"#object-reference-check\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eObject reference check\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eError: Run to Experiment reference error.\u003cbr\u003e\nNot all Experiments are referenced by Runs.\u003cbr\u003e\nThere is Run(s) not referencing Experiment.\u003cbr\u003e\nThere is Experiment(s) not referenced by Run.\u003cbr\u003e\nModify metadata to make all Runs reference all Experiments.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee \u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission-e.html#metadata-objects\" rel=\"nofollow\"\u003ethe DRA Handbook\u003c/a\u003e for metadata model.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-submit-xmls-in-the-dra-web-interface\" class=\"anchor\" href=\"#submit-xmls-in-the-dra-web-interface\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSubmit XMLs in the DRA web interface\u003c/h2\u003e\n\u003cp\u003eBefore submitting the metadata XMLs, \u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission-e.html#upload-sequence-data\" rel=\"nofollow\"\u003eupload sequence data files to the submission directory\u003c/a\u003e.\u003cbr\u003e\nAfter logging in the D-way, \u003ca href=\"https://www.ddbj.nig.ac.jp/dra/submission-e.html#create-metadata-in-xml-files\" rel=\"nofollow\"\u003eupload the Submission, Experiment and Run XMLs in the XML upload area of the DRA submission\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-when-github-and-xml-generation-are-not-clear-for-you\" class=\"anchor\" href=\"#when-github-and-xml-generation-are-not-clear-for-you\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhen Github and XML generation are not clear for you\u003c/h2\u003e\n\u003cp\u003eDownload \u003ca href=\"https://www.ddbj.nig.ac.jp/files/submission/dra_metadata.xlsx\" rel=\"nofollow\"\u003eDRA metadata Excel\u003c/a\u003e from website, fill in and send it to the DRA team by Email (\u003ca href=\"mailto:trace@ddbj.nig.ac.jp\"\u003etrace@ddbj.nig.ac.jp\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nig-supercomputer\" class=\"anchor\" href=\"#nig-supercomputer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNIG SuperComputer\u003c/h2\u003e\n\u003cp\u003eThe singularity image is available at \u003ccode\u003e/lustre6/public/app/submission-excel2xml/\u003c/code\u003e in the \u003ca href=\"https://www.ddbj.nig.ac.jp/sc\" rel=\"nofollow\"\u003eNIG SuperComputer\u003c/a\u003e operated by Bioinformation and DDBJ Center, National Institute of Genetics. The SuperComputer user can readily generate XMLs from the metadata excel file and check the XMLs.\u003cbr\u003e\nThe user can create DRA metadata XMLs and transfer corresponding data files to the DRA file server (ftp-private.ddbj.nig.ac.jp) in the SuperComputer.\u003c/p\u003e\n\u003cp\u003eGenerate Submission, Experiment and Run XMLs from the excel.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec /lustre6/public/app/submission-excel2xml/excel2xml.simg excel2xml.rb -a example -i 0001 -p PRJDB7252 example-0001_dra_metadata.xlsx\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eValidate the XMLs.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec /lustre6/public/app/submission-excel2xml/excel2xml.simg validate_dra_meta.rb -a example -i 0001\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 8,
    "topics": [],
    "updated_at": 1626401578.0
  },
  {
    "data_format": 2,
    "description": "Useful scripts and tools related to alevin-fry",
    "filenames": [
      "docker/Singularity.def"
    ],
    "full_name": "COMBINE-lab/usefulaf",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-useful-utilities-for-single-cell-processing-with-alevin-fry\" class=\"anchor\" href=\"#useful-utilities-for-single-cell-processing-with-alevin-fry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful utilities for single-cell processing with alevin-fry\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/COMBINE-lab/alevin-fry\"\u003eAlevin-fry\u003c/a\u003e is a fast, accurate and memory-frugal tool for preprocessing single-cell and single-nucleus RNA-seq data.  You can read more about alevin-fry in \u003ca href=\"https://www.biorxiv.org/content/10.1101/2021.06.29.450377v1\" rel=\"nofollow\"\u003eits pre-print\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis respoistory contains scripts, functions and utilities that are useful for preparing data for processing with alevin-fry, as well as for reading alevin-fry data into other packages for downstream analysis.\u003c/p\u003e\n\u003cp\u003eThe different utilities are broken down in this repository by the language in which they are written (right now, Python, R and bash).  A brief listing of\nthe available utilities currently in the repository is:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-r-language\" class=\"anchor\" href=\"#r-language\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR language\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e \u2014 A script to build a spliced + intron (splici) ref for indexing and quantification with \u003ccode\u003ealevin-fry\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esplici.R\u003c/code\u003e \u2014 Contains the \u003ccode\u003emake_splici_txome\u003c/code\u003e function, which is the function called by the \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e wrapper script.  If you want to build a splici reference programatically in R code, you can use this function.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecellRangerLikeEmptyDrops.R\u003c/code\u003e \u2014 An implementation of the hybrid UMI count filtering and \u003ca href=\"https://github.com/MarioniLab/DropletUtils\"\u003e\u003ccode\u003eemptyDrops\u003c/code\u003e\u003c/a\u003e used by CellRanger (and subsequently by \u003ca href=\"https://github.com/alexdobin/STAR\"\u003eSTARsolo\u003c/a\u003e). This R implementation is a translation of the implemntation in STARsolo, which itself was reverse-engineered from CellRanger.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eload_fry.R\u003c/code\u003e \u2014 Contains a function to load \u003ccode\u003ealevin-fry\u003c/code\u003e output (including from USA mode quantification) into a \u003ca href=\"https://bioconductor.org/packages/release/bioc/html/SingleCellExperiment.html\" rel=\"nofollow\"\u003e\u003ccode\u003eSingleCellExperiment\u003c/code\u003e\u003c/a\u003e object.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python-language\" class=\"anchor\" href=\"#python-language\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython language\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eload_fry.py\u003c/code\u003e \u2014 Contains a Python function \u003ccode\u003eload_fry\u003c/code\u003e which is intended to load \u003ccode\u003ealevin-fry\u003c/code\u003e output (including from USA mode quantification) into a \u003ca href=\"https://github.com/theislab/scanpy\"\u003e\u003ccode\u003eScanpy\u003c/code\u003e\u003c/a\u003e object.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-bash\" class=\"anchor\" href=\"#bash\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBash\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eget_10x_permit_lists.sh\u003c/code\u003e \u2014 Provides a script to download the 10x chromium v2 or v3 permit lists.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esimpleaf\u003c/code\u003e \u2014 Provides a script to run the entire \u003ccode\u003esalmon -\u0026gt; alevin-fry (generate-permit-list \u0026gt; collate \u0026gt; quant)\u003c/code\u003e pipeline, though providing only a simplified set of options.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-simpleaf\" class=\"anchor\" href=\"#using-simpleaf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing simpleaf\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003esimpleaf\u003c/code\u003e script that resides in the \u003ccode\u003ebash\u003c/code\u003e subdirectory is intended to simply the running of \u003ccode\u003ealevin-fry\u003c/code\u003e in common usage scenarios.  By limiting some of the different options that can be set, it provides a streamlined way to build the splici reference and index in a single command, as well as to process an experiment from raw FASTQ files to a count matrix in a single command.\u003c/p\u003e\n\u003cp\u003eTo work properly, \u003ccode\u003esimpleaf\u003c/code\u003e has a few requirements.  First, it should be run from \u003cem\u003ewithin\u003c/em\u003e the \u003ccode\u003ebash\u003c/code\u003e subdirectory of this repository.  This is because it currently makes assumptions about the relative paths of the scripts \u003ccode\u003eget_10x_permit_lists.sh\u003c/code\u003e and \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e.  Additionally, the following environment variables are used within \u003ccode\u003esimpleaf\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eALEVIN_FRY_HOME\u003c/code\u003e \u003cstrong\u003eREQUIRED\u003c/strong\u003e \u2014 This directory will be used for persistent configuration and small file (\u0026lt;1G) storage between runs.  If you provide a directory and it doesn\u0027t exist, it will be created.  It is easiest to just set this in your enviornment globally so that the same home can be used over many runs without you having to provide the variable explicitly each time.  A good choice for this variable might be something like \u003ccode\u003e~/.alevin_fry_home\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eSALMON_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ccode\u003esalmon\u003c/code\u003e executable of version \u0026gt;= 1.5.1.  If not provided, the script will assume it can simply invoke \u003ccode\u003esalmon\u003c/code\u003e in the current enviornment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eFRY_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ccode\u003ealevin-fry\u003c/code\u003e executable of version \u0026gt;= 0.4.0.  If not provided, the script will assume it can simply invoke \u003ccode\u003ealevin-fry\u003c/code\u003e in the current enviornment.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eTIME_BIN\u003c/code\u003e \u003cstrong\u003eOPTIONAL\u003c/strong\u003e \u2014 This should provide the path to a \u003ca href=\"https://www.gnu.org/software/time/\" rel=\"nofollow\"\u003eGNU time\u003c/a\u003e executable; this is different from the shell \u003ccode\u003etime\u003c/code\u003e command, and on most linux systems exists at \u003ccode\u003e/usr/bin/time\u003c/code\u003e.  If this variable is not provided, the script will assume it can use \u003ccode\u003e/usr/bin/time\u003c/code\u003e.  On OSX systems, you should install GNU time explicitly.  This can be done with \u003ca href=\"https://anaconda.org/conda-forge/time\" rel=\"nofollow\"\u003econda\u003c/a\u003e or homebrew.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ccode\u003esimpleaf\u003c/code\u003e script has two sub-commands:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eindex\u003c/code\u003e \u2014 The \u003ccode\u003eindex\u003c/code\u003e command will take a reference genome FASTA and GTF as input, build a splici reference using the \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e script, and then build a sparse \u003ccode\u003esalmon\u003c/code\u003e index on the resulting reference. \u003cstrong\u003eNote\u003c/strong\u003e: The \u003ccode\u003eindex\u003c/code\u003e command requires the \u003ccode\u003eRscript\u003c/code\u003e executable to be in the path, as well as all of theR packages that are required by \u003ccode\u003ebuild_splici_ref.R\u003c/code\u003e. The relevant options (which you can obtain by running \u003ccode\u003e./simpleaf index -h\u003c/code\u003e) are:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{bash}\"\u003e\u003ccode\u003eUsage: ./simpleaf index [options]\n      options:\n       -f, --fasta REQUIRED genome reference FASTA file\n       -g, --gtf REQUIRED GTF file with gene annotations\n       -l, --rlen REQUIRED the target read length the index will be built for\n       -o, --output REQUIRED path to output directory (will be created if it doesn\u0027t exist)\n       -s, --spliced OPTIONAL path to FASTA file with extra spliced sequence to add to the index\n       -u, --unspliced OPTIONAL path to FASTA file with extra unspliced sequence to add to the index\n       -d, --dedup FLAG OPTIONAL deduplicate identical sequences inside the R script when building the splici reference\n       -t, --threads OPTIONAL number of threads to use when running [default: min(16, num cores)]\n       -h, --help display this help message\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003equant\u003c/code\u003e \u2014 The \u003ccode\u003equant\u003c/code\u003e command takes as input the index, reads, and relevant information about the experiment (e.g. chemistry), and runs all of the steps of the \u003ccode\u003ealevin-fry\u003c/code\u003e pipeline, from mapping with \u003ccode\u003esalmon\u003c/code\u003e through \u003ccode\u003equant\u003c/code\u003e with \u003ccode\u003ealevin-fry\u003c/code\u003e. The relevant options (which you can obtain by running \u003ccode\u003e./simpleaf quant -h\u003c/code\u003e) are:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre lang=\"{bash}\"\u003e\u003ccode\u003eUsage: ./simpleaf quant [options]\n      options:\n       -1, --r1 REQUIRED comma separated list of left reads\n       -2, --r2 REQUIRED comma separated list of right reads\n       -i, --index REQUIRED path to a (sparse or dense) salmon splici index\n       -o, --output REQUIRED path to output directory (will be created if it doesn\u0027t exist)\n       -f, --fmode REQUIRED permit list filter mode, one of {knee, k, unfilt, u}\n       -c, --chem REQUIRED chemistry of experiment, one of {v2, v3}\n       -r, --res REQUIRED resolution strategy for alevin-fry, one of {cr-like, cr-like-em}\n       -m, --t2g REQUIRED three-column txp-to-gene file to pass to alevin-fry quant command\n       -t, --threads OPTIONAL number of threads to use when running [default: min(16, num cores)]\n       -h, --help display this help message\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1626978037.0
  },
  {
    "data_format": 2,
    "description": "Code for Asking the Right Questions: Learning Interpretable Action Models Through Query Answering. AAAI 2021.",
    "filenames": [
      "dependencies/FD/misc/releases/19.12/Singularity.19.12",
      "dependencies/FD/misc/releases/20.06/Singularity.20.06",
      "dependencies/FD/misc/releases/19.06/Singularity.19.06",
      "dependencies/FD/misc/releases/latest/Singularity"
    ],
    "full_name": "AAIR-lab/AIA-AAAI21",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-agent-interrogation-algorithm-aia\" class=\"anchor\" href=\"#agent-interrogation-algorithm-aia\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAgent Interrogation Algorithm (AIA)\u003c/h1\u003e\n\u003cp\u003eThis repository contains the code for the paper:\u003c/p\u003e\n\u003cp\u003eAsking the Right Questions: Learning Interpretable Action Models through Query Answering.\u003cbr\u003e\n\u003ca href=\"https://pulkitverma.net\" rel=\"nofollow\"\u003ePulkit Verma\u003c/a\u003e,\n\u003ca href=\"https://marpally-raoshashank.netlify.app/\" rel=\"nofollow\"\u003eShashank Rao Marpally\u003c/a\u003e, and\n\u003ca href=\"http://siddharthsrivastava.net/\" rel=\"nofollow\"\u003eSiddharth Srivastava\u003c/a\u003e. \u003cbr\u003e\n35th AAAI Conference on Artificial Intelligence, 2021.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://aair-lab.github.io/Publications/vms_aaai21.pdf\" rel=\"nofollow\"\u003ePaper\u003c/a\u003e | \u003ca href=\"https://pulkitverma.net/assets/pdf/vms_aaai21/vms_aaai21_slides.pdf\" rel=\"nofollow\"\u003eSlides\u003c/a\u003e | \u003ca href=\"https://pulkitverma.net/assets/pdf/vms_aaai21/vms_aaai21_poster.pdf\" rel=\"nofollow\"\u003ePoster\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-directory-structure\" class=\"anchor\" href=\"#directory-structure\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDirectory Structure\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e|-- dependencies/\n|   |-- FD/\n|   |-- FF/\n|   |-- pddlgym/\n|   |-- VAL/\n|-- domains/\n|-- random_states/\n|-- results/\n|-- src/\n|   |-- agent.py\n|   |-- config.py\n|   |-- generate_random_states.py\n|   |-- main.py\n|   |-- interrogation/\n|   |-- lattice/\n|   |-- query/\n|   |-- sim/\n|   |-- utils/\n|-- README.md\n|-- LICENSE\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003edependencies: This directory includes the external software used to run the code. This includes FF, FD, VAL, and PDDLGym.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFF: \u003ca href=\"https://fai.cs.uni-saarland.de/hoffmann/ff/FF-v2.3.tgz\" rel=\"nofollow\"\u003ehttps://fai.cs.uni-saarland.de/hoffmann/ff/FF-v2.3.tgz\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFD: \u003ca href=\"https://github.com/aibasel/downward\"\u003ehttps://github.com/aibasel/downward\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePDDLGym: \u003ca href=\"https://github.com/tomsilver/pddlgym\"\u003ehttps://github.com/tomsilver/pddlgym\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eVAL: \u003ca href=\"https://github.com/KCL-Planning/VAL\"\u003ehttps://github.com/KCL-Planning/VAL\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003edependencies: Place all the domains in this directory. There must be a directory for each domain containing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003edomain.pddl (domain file for that domain), and\u003c/li\u003e\n\u003cli\u003einstances directory containing the problem files for that domain.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003erandom_states: This directory stores the set of states in serialized form. For each domain, there is a .pkl file containing 60 states approximately. These are generated using src/generate_random_states.py.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003esrc: This directory stores the source code for AIA. It contains 4 files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eagent.py: Contains the agent code.\u003c/li\u003e\n\u003cli\u003econfig.py: Declares the configuration parameters.\u003c/li\u003e\n\u003cli\u003egenerate_random_states.py: Generates the random states for each domain.\u003c/li\u003e\n\u003cli\u003emain.py : Contains the main driver code which runs the code end-to-end.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003esrc also contains code structured into following sub-directories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003einterrogation: Contains the AIA code.\u003c/li\u003e\n\u003cli\u003elattice: Contains the model and lattice classes.\u003c/li\u003e\n\u003cli\u003equery: Contains the plan outcome query code.\u003c/li\u003e\n\u003cli\u003esim: Simulator specific code. Contains a separate agent file for each simulator domain.\u003c/li\u003e\n\u003cli\u003eutils: Contains general utilities.\n\u003cul\u003e\n\u003cli\u003eutils/parser: Code based on \u003ca href=\"https://github.com/tomsilver/pddlgym\"\u003ePDDLGym\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eutils/translate: Code based on \u003ca href=\"https://github.com/aibasel/downward\"\u003eFD\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration\" class=\"anchor\" href=\"#configuration\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration\u003c/h2\u003e\n\u003cp\u003eConfiguration parameters are set in src/config.py\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFF_PATH, FD_PATH, and VAL_PATH stores the relative path of FF, FD, and VAL respectively.\u003c/li\u003e\n\u003cli\u003eNUM_PER_DOMAIN denotes how many instances per domain must be run. Keep minimum 2 for symbolic agent.\u003c/li\u003e\n\u003cli\u003ePLANNER specifies which planner to use. Set it to either FF or FD.\u003c/li\u003e\n\u003cli\u003eComment out either Symbolic Agent Settings or Simulator Agent Settings.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run\" class=\"anchor\" href=\"#how-to-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to Run\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eInstall the required software\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install -r requirements.txt \n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003eAdjust variables/paramters as needed in src/config.py.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun main.py\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003ecd src\npython3 main.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-common-installation-issues\" class=\"anchor\" href=\"#common-installation-issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommon Installation Issues\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eOpenCV (Tested on Ubuntu 18.04)\u003c/p\u003e\n\u003cp\u003eRefer to \u003ca href=\"https://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/\" rel=\"nofollow\"\u003ehttps://linuxize.com/post/how-to-install-opencv-on-ubuntu-18-04/\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFF:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ePlease install flex and bison for FF to compile.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOn newer versions of gcc (tested on gcc 10.2.0) please make the following changes:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003emain.c:150 : Comment out the gbracket_count definition\n\u003cpre\u003e\u003ccode\u003eint gbracket_count; --\u0026gt; /* int gbracket_count; */\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003erelax.c:111 : Define lcurrent_goals as static\n\u003cpre\u003e\u003ccode\u003eState lcurrent_goals; --\u0026gt; static State lcurrent_goals;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003esearch.c:110 : Define lcurrent_goals as static\n\u003cpre\u003e\u003ccode\u003eState lcurrent_goals; --\u0026gt; static State lcurrent_goals;\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePlease note that this is research code and not yet ready for public delivery,\nhence most parts are not documented.\u003c/p\u003e\n\u003cp\u003eIn case of any queries, please contact \u003ca href=\"mailto:verma.pulkit@asu.edu\"\u003everma.pulkit@asu.edu\u003c/a\u003e,\nor \u003ca href=\"mailto:smarpall@asu.edu\"\u003esmarpall@asu.edu\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003e@inproceedings{verma_2021_asking,\n    author = {Verma, Pulkit and Rao Marpally, Shashank and Srivastava, Siddharth},\n    title = {{Asking the Right Questions: Learning Interpretable Action Models Through Query Answering}},\n    booktitle = {Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)},\n    year={2021}\n}\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1623121132.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "envs/sauron/Singularity_remote.def"
    ],
    "full_name": "angelettilab/scMouseBcellFlu",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-scmousebcellflu\" class=\"anchor\" href=\"#scmousebcellflu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003escMouseBcellFlu\u003c/h1\u003e\n\u003cp\u003eThis is the repository associated with the publication \u003cem\u003eSingle cell BCR and RNA analysis after respiratory virus infection reveals spatiotemporal dynamics of antigen specific B cell response\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eThis repository contains the code and supporting files necessary to reproduce the analyses reported in the publication. In essence, the anlaysis workflow herein was done using the \u003ca href=\"https://github.com/NBISweden/sauron\"\u003eSauron\u003c/a\u003e analysis philosophy.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-single-cell-rna-seq-analysis\" class=\"anchor\" href=\"#single-cell-rna-seq-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingle-cell RNA-seq analysis\u003c/h2\u003e\n\u003cp\u003eTo re-run the analysis of the scRNA-Seq data, the necessary workflow and scripts can be found in the \u003ca href=\"scripts/scRNAseq_pipeline\"\u003e\u003ccode\u003escripts/scRNAseq_pipeline/\u003c/code\u003e\u003c/a\u003e subdirectory.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bcr-seq-analysis\" class=\"anchor\" href=\"#bcr-seq-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBCR seq analysis\u003c/h2\u003e\n\u003cp\u003eScripts necessary to reproduce the analysis of the B-cell receptor sequencing data can be found in the \u003ca href=\"scripts/VDJ_analysis\"\u003e\u003ccode\u003escripts/VDJ_analysis/\u003c/code\u003e\u003c/a\u003e subdirectory.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1623749451.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ctpelok77/fd-red-black-postipc2018",
    "latest_release": null,
    "readme": "\u003cp\u003eFast Downward is a domain-independent planning system.\u003c/p\u003e\n\u003cp\u003eFor documentation and contact information see \u003ca href=\"http://www.fast-downward.org/\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe following directories are not part of Fast Downward as covered by this\nlicense:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e./src/search/ext\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor the rest, the following license applies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eFast Downward is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License as published by the Free Software\nFoundation, either version 3 of the License, or (at your option) any later\nversion.\n\nFast Downward is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License along with\nthis program. If not, see \u0026lt;http://www.gnu.org/licenses/\u0026gt;.\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1622797950.0
  },
  {
    "data_format": 2,
    "description": "Singularity group project for EIPP 2019",
    "filenames": [
      "recipes/Singularity.snakemake",
      "recipes/Singularity.nanopolish",
      "recipes/Singularity.shellcheck",
      "recipes/Singularity.flye",
      "recipes/Singularity.fun",
      "recipes/Singularity.template",
      "recipes/Singularity.jupyter",
      "recipes/sandbox-dev/Singularity.nanopolish"
    ],
    "full_name": "mbhall88/eipp-2019-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-eipp-2019-singularity-group-project\" class=\"anchor\" href=\"#eipp-2019-singularity-group-project\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEIPP 2019 Singularity group project\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4457c96be6834fd67756b9c0eab298334a5b948ab2234fbea89648e221e66af1/68747470733a2f2f73796c6162732e696f2f6775696465732f322e362f61646d696e2d67756964652f5f7374617469632f6c6f676f2e706e67\" height=\"100\" data-canonical-src=\"https://sylabs.io/guides/2.6/admin-guide/_static/logo.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://camo.githubusercontent.com/0d44589c34845e74b1d32ae082d1f190828469fdc700fd026f3e4935eba669d2/68747470733a2f2f736369656e63652e736369656e63656d61672e6f72672f636f6e74656e742f7363692f3238372f353435372f313430312f46312e6d656469756d2e676966\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0d44589c34845e74b1d32ae082d1f190828469fdc700fd026f3e4935eba669d2/68747470733a2f2f736369656e63652e736369656e63656d61672e6f72672f636f6e74656e742f7363692f3238372f353435372f313430312f46312e6d656469756d2e676966\" height=\"100\" data-canonical-src=\"https://science.sciencemag.org/content/sci/287/5457/1401/F1.medium.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/3751\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#introduction-to-containers\"\u003eIntroduction to containers\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#tldr\"\u003etl;dr\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#what-can-i-do-with-a-container\"\u003eWhat can I do with a container?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#how-do-i-get-a-container\"\u003eHow do I get a container?\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#remote\"\u003eRemote\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#docker-hub\"\u003eDocker Hub\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#singularity-hub\"\u003eSingularity Hub\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#singularity-library\"\u003eSingularity Library\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#quay-and-biocontainers\"\u003eQuay and BioContainers\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#build-locally\"\u003eBuild locally\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#exercise-1\"\u003eExercise 1\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#task-1\"\u003eTask 1\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#task-2\"\u003eTask 2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#task-3\"\u003eTask 3\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#sandbox-development\"\u003eSandbox development\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#exercise-2\"\u003eExercise 2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#run-and-serving-applications\"\u003e\u003ccode\u003erun\u003c/code\u003e and serving applications\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#singularity-run\"\u003e\u003ccode\u003esingularity run\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#serving-applications\"\u003eServing applications\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#workflow-management-systems\"\u003eWorkflow management systems\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#programs-requiring-gpus\"\u003ePrograms requiring GPUs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#bonus\"\u003eBonus\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction-to-containers\" class=\"anchor\" href=\"#introduction-to-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction to containers\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tldr\" class=\"anchor\" href=\"#tldr\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etl;dr\u003c/h3\u003e\n\u003cp\u003eA container is a standard unit of software that packages up code and all its\ndependencies, so the application runs quickly and reliably from one computing\nenvironment to another. That includes files, environment variables, dependencies and\nlibraries.\u003c/p\u003e\n\u003cp\u003eFor those who would like more detailed information about what containers are, please\nrefer to \u003ca href=\"https://github.com/titansmc/singularity-training-2019/raw/master/1.-singularity-training-what-are-containers.odp\"\u003ethis fantastic slide deck from Josep Moscardo\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-what-can-i-do-with-a-container\" class=\"anchor\" href=\"#what-can-i-do-with-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat can I do with a container?\u003c/h2\u003e\n\u003cp\u003eIn it\u0027s most basic form, you can execute a software program, via a container, even\nthough you may not have that program installed on the system you are running it on.\u003c/p\u003e\n\u003cp\u003eExamples are the best teachers!\u003c/p\u003e\n\u003cp\u003eFirstly, let\u0027s clone this repository (and call it \u003ccode\u003eeipp-singularity\u003c/code\u003e) as we will use some files from it throughout this\nproject.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eproject=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eeipp-singularity\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\ngit clone https://github.com/mbhall88/eipp-2019-singularity.git \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$project\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$project\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow, there is a \u003ca href=\"https://samtools.github.io/hts-specs/SAMv1.pdf\" rel=\"nofollow\"\u003eBAM\u003c/a\u003e file in the repository that we sadly can\u0027t view as we do not have \u003ca href=\"https://github.com/samtools/samtools\"\u003e\u003ccode\u003esamtools\u003c/code\u003e\u003c/a\u003e installed (let\u0027s pretend). Thanks to Singularity we\ndon\u0027t have to worry about trying to install \u003ccode\u003esamtools\u003c/code\u003e and can instead use a pre-built container to view our BAM file with \u003ccode\u003esamtools\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eimg=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edocker://quay.io/biocontainers/samtools:1.9--h10a08f8_12\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$img\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e samtools view data/toy.bam\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eMagic \u003cg-emoji class=\"g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\u003e\u2728\u003c/g-emoji\u003e\u003c/p\u003e\n\u003cp\u003eSo what\u0027s going on here?\u003c/p\u003e\n\u003cp\u003eLet\u0027s work our way through the command.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#executing-commands\" rel=\"nofollow\"\u003e\u003ccode\u003esingularity exec\u003c/code\u003e\u003c/a\u003e tells Singularity to execute a given command inside a\ngiven container.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e\"$img\"\u003c/code\u003e specifies the container for Singularity to operate on. We will look at this component in more detail later.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003esamtools view data/toy.bam\u003c/code\u003e This is the command we want Singularity to execute inside the container. Notice how we can specify files that exist on our local file system?!\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-do-i-get-a-container\" class=\"anchor\" href=\"#how-do-i-get-a-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow do I get a container?\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-remote\" class=\"anchor\" href=\"#remote\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemote\u003c/h3\u003e\n\u003cp\u003eIn the above example, the container we used for \u003ccode\u003esamtools\u003c/code\u003e was remote.\u003c/p\u003e\n\u003cp\u003eRemote containers are containers that have been pre-built and stored in \"the cloud\".\nThere are many benefits to this kind of set up. Firstly, it makes sharing containers\neasy. Secondly, it saves users (and yourself) a lot of time in the future. As the\ncontainer is pre-built, we don\u0027t need to spend time waiting for the build to happen (more on this later). The only wait time we have is for the download of the remote\ncontainer to finish. Lastly, remote services are convenient for building images if we\ndon\u0027t have \u003ccode\u003esudo\u003c/code\u003e access on the machine we are using. We will look at building containers\nlocally very soon, but for now, it suffices to know that to build them locally, you need\n\u003ccode\u003esudo\u003c/code\u003e access.\u003c/p\u003e\n\u003cp\u003eNow you might have noticed in the example above that the \u003ca href=\"https://en.wikipedia.org/wiki/Uniform_Resource_Identifier\" rel=\"nofollow\"\u003eURI\u003c/a\u003e for the \u003ccode\u003esamtools\u003c/code\u003e\ncontainer has the work \u0027docker\u0027 in it. This is one of the coolest things about Singularity: \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/singularity_and_docker.html\" rel=\"nofollow\"\u003eit can convert Docker containers into Singularity containers\u003c/a\u003e! We now have\naccess to any Docker container \u003cem\u003eplus\u003c/em\u003e any Singularity container.\u003c/p\u003e\n\u003cp\u003eLet\u0027s take a look at some remote container registries in a little more detail and see\nhow we can use containers from them.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-docker-hub\" class=\"anchor\" href=\"#docker-hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://hub.docker.com/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eThe official registry for Docker containers. Let\u0027s search for \u003ca href=\"http://conda.pydata.org/miniconda.html\" rel=\"nofollow\"\u003e\u003ccode\u003eminiconda3\u003c/code\u003e\u003c/a\u003e on \u003ca href=\"https://hub.docker.com/\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e and select the option \u003ca href=\"https://hub.docker.com/r/continuumio/miniconda3\" rel=\"nofollow\"\u003e\u003ccode\u003econtinuumio/miniconda3\u003c/code\u003e\u003c/a\u003e. On the right, there is a section \u003cstrong\u003eDocker Pull Command\u003c/strong\u003e. It\nsays \u003ccode\u003edocker pull continuumio/miniconda3\u003c/code\u003e. If we were using Docker, this would be the\ncommand we would use to pull that container to our local machine. To use it in Singularity\nwe need to tweak it just a little. For \u003ccode\u003eminiconda3\u003c/code\u003e we would use the URI \u003ccode\u003edocker://continuumio/miniconda3\u003c/code\u003e. As we can see, you need to add \u003ccode\u003edocker://\u003c/code\u003e to the\nbeginning of the \u003ccode\u003erepository/tag\u003c/code\u003e.\u003cbr\u003e\nWe can go one step further and unlock another great benefit of using remote containers. We\u0027re reproducibility warriors, right?! Of course, we are. So let\u0027s be specific\nabout the version of \u003ccode\u003eminiconda3\u003c/code\u003e we want to use. On the \u003ca href=\"https://hub.docker.com/r/continuumio/miniconda3\" rel=\"nofollow\"\u003e\u003ccode\u003eminiconda3\u003c/code\u003e Docker Hub page\u003c/a\u003e, select the \u003ca href=\"https://hub.docker.com/r/continuumio/miniconda3/tags\" rel=\"nofollow\"\u003e\u003cstrong\u003eTags\u003c/strong\u003e\u003c/a\u003e heading. On this\npage, we see a whole bunch of different versions of \u003ccode\u003eminiconda3\u003c/code\u003e we can choose from. Any\nversion of this container that has been built is kept. If we wanted to use version \u003ccode\u003e4.6.14\u003c/code\u003e, then all we have to do is append this, with a colon, to our original URI\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker://continuumio/miniconda3:4.6.14\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow, as we saw earlier, we can directly execute a container from it\u0027s URI. However, it\nis likely you may want to use a container multiple times. In these circumstances, it is\nmore \"economical\" to pull a copy of the container onto our local machine, so we don\u0027t\nhave to try and retrieve it from the registry each time (images are usually cached though). To pull the \u003ccode\u003eminiconda3\u003c/code\u003e container from Docker Hub, we use Singularity\u0027s \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/quick_start.html#download-pre-built-images\" rel=\"nofollow\"\u003e\u003ccode\u003epull\u003c/code\u003e\u003c/a\u003e\ncommand and optionally specify a name.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull docker://continuumio/miniconda3:4.6.14\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe above command will pull the container into the current directory and name it \u003ccode\u003eminiconda3-4.6.14.sif\u003c/code\u003e. If we wanted to call it instead \u003ccode\u003eminiconda3.sif\u003c/code\u003e we would use the \u003ccode\u003e--name\u003c/code\u003e argument\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name miniconda3.sif docker://continuumio/miniconda3:4.6.14\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhen we want to use this image again in the future, rather than specifying the URI we\njust point Singularity at our local copy\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e miniconda3.sif \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003ecommand\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-singularity-hub\" class=\"anchor\" href=\"#singularity-hub\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eSet up and maintained by a collaboration between Stanford University and Singularity,\nSingularity Hub is Singularity\u0027s \"semi-official\" version of Docker Hub. We will dig\ninto how to set this up for yourself a little later in \u003ca href=\"#Exercise-1\"\u003eExercise 1\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAs with Docker Hub, we can search for containers uploaded by users and then use them in\nthe same way. However, it will ask us to log in using GitHub first. Login with your\nGitHub account and then search for \u003ca href=\"https://github.com/DaehwanKimLab/centrifuge\"\u003e\u003ccode\u003ecentrifuge\u003c/code\u003e\u003c/a\u003e. The first result should\nbe for \u003ca href=\"https://singularity-hub.org/collections/685\" rel=\"nofollow\"\u003e\u003ccode\u003embhall88/Singularity_recipes\u003c/code\u003e\u003c/a\u003e - click on this. This will take\nyou to a page listing all of the Singularity containers I maintain in a \u003ca href=\"https://github.com/mbhall88/Singularity_recipes\"\u003erecipes repository on GitHub\u003c/a\u003e. Scroll through these and look for the\n\u003ca href=\"https://singularity-hub.org/containers/5461\" rel=\"nofollow\"\u003e\u003ccode\u003ecentrifuge\u003c/code\u003e\u003c/a\u003e one and then click on the green \u003cstrong\u003eComplete\u003c/strong\u003e button.\nThe resulting screen will have the Build Specs (more on this soon) plus a bunch of\nbuild metrics. Additionally, at the top of this screen, you will see the core piece of\nthe URI that we need: \u003ccode\u003embhall88/Singularity_recipes:centrifuge\u003c/code\u003e. So to use this container,\nwe add the \u003ccode\u003eshub://\u003c/code\u003e scheme to the front.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003euri=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eshub://mbhall88/Singularity_recipes:centrifuge\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsingularity pull --name centrifuge.sif \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$uri\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e centrifuge.sif centrifuge --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eDue to Singularity Hub be generously hosted as no charge by Google Cloud, and also due\nto a recent malicious attack, it is \u003ca href=\"https://singularityhub.github.io/singularityhub-docs/docs/interact\" rel=\"nofollow\"\u003erecommended\u003c/a\u003e to \u003ccode\u003epull\u003c/code\u003e containers from Singularity and\nthen execute them, rather than running directly from the URI.\u003c/p\u003e\n\u003cp\u003eAgain, we can go one step further and specify a particular build of the container we\nwant to use. In the \u003cstrong\u003eBuild Metrics\u003c/strong\u003e section, there is a field called \u0027Version (file hash)\u0027. For reproducibility purposes, it is advisable to use this hash as it makes it\nclear to others who may read your code exactly which container you used. So to pull the\nlatest centrifuge container, we would do the following (\u003cstrong\u003edon\u0027t run this if you already\npulled the container above\u003c/strong\u003e).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ehash=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e13bc12f41b20001f17e6f8811dc3eeea\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nuri=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eshub://mbhall88/Singularity_recipes:centrifuge@\u003cspan class=\"pl-smi\"\u003e${hash}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsingularity pull --name centrifuge.sif \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$uri\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e centrifuge.sif centrifuge --help\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-singularity-library\" class=\"anchor\" href=\"#singularity-library\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://cloud.sylabs.io/library\" rel=\"nofollow\"\u003eSingularity Library\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eThis is the official container registry for Singularity. However, all images built on\nthis service are Singularity v3+ compatible. At EBI we only have Singularity v2.6, but\nEMBL Heidelberg\u0027s cluster does use Singularity v3+. This service works similarly to Singularity and Docker Hubs, using the scheme \u003ccode\u003elibrary://\u003c/code\u003e for its URIs.\u003c/p\u003e\n\u003cp\u003eOne additional feature that Singularity Library has is a \u003ca href=\"https://cloud.sylabs.io/builder\" rel=\"nofollow\"\u003eremote builder\u003c/a\u003e. This builder allows you to dump a recipe for a container, it will build the\ncontainer for you, and then you can download it on to your local machine. Very handy\nwhen working on a computer you do not have \u003ccode\u003esudo\u003c/code\u003e access on.\u003c/p\u003e\n\u003cp\u003eSee the slides \u003cem\u003ebelow\u003c/em\u003e \u003ca href=\"https://slides.com/mbhall88/remote-container-systems#/2/1\" rel=\"nofollow\"\u003ethis\u003c/a\u003e for more information about Singularity\nLibrary.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-quay-and-biocontainers\" class=\"anchor\" href=\"#quay-and-biocontainers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://quay.io/\" rel=\"nofollow\"\u003eQuay\u003c/a\u003e and \u003ca href=\"https://biocontainers.pro/\" rel=\"nofollow\"\u003eBioContainers\u003c/a\u003e\n\u003c/h4\u003e\n\u003cp\u003eQuay is a container registry for Docker and \u003ca href=\"https://coreos.com/rkt/\" rel=\"nofollow\"\u003erkt\u003c/a\u003e containers. We won\u0027t talk much\nabout this service outside how to use the BioContainers builds hosted on it.\u003c/p\u003e\n\u003cp\u003eBioContainers is an open-source and community-driven framework for reproducibility in\nbioinformatics\u003ca href=\"https://doi.org/10.1093/bioinformatics/btx192\" rel=\"nofollow\"\u003e\u003csup\u003e1\u003c/sup\u003e\u003c/a\u003e. They build and maintain containers for a large suite of bioinformatics\ntools. In particular, any tool that has a \u003ca href=\"https://bioconda.github.io/\" rel=\"nofollow\"\u003eBioconda\u003c/a\u003e recipe automatically has\na BioContainers image built and stored on Quay.\u003c/p\u003e\n\u003cp\u003eTo see an example of how to find and use these BioContainers images check out the slides\nbelow \u003ca href=\"https://slides.com/mbhall88/remote-container-systems#/4/1i\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFor more details on remote container systems, refer to \u003ca href=\"https://slides.com/mbhall88/remote-container-systems\" rel=\"nofollow\"\u003emy slides\u003c/a\u003e from a one-day\n\u003ca href=\"https://git.embl.de/grp-bio-it/singularity-training-2019\" rel=\"nofollow\"\u003eSingularity course\u003c/a\u003e I was involved in running at EMBL in early 2019.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-build-locally\" class=\"anchor\" href=\"#build-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild locally\u003c/h3\u003e\n\u003cp\u003eWe\u0027ve talked a lot about how to use containers that others have been kind enough to\nconstruct for us. But what happens if an image doesn\u0027t exist for the software tool you\nwant to use? Or if you want to combine multiple programs into a single container? You\nguessed it; we can build containers locally from definition/recipe files.\u003c/p\u003e\n\u003cp\u003eRather than reinvent the wheel, please refer to (and work your way through) \u003ca href=\"https://slides.com/mbhall88/making-containers#/\" rel=\"nofollow\"\u003ethese slides\u003c/a\u003e from the \u003ca href=\"https://git.embl.de/grp-bio-it/singularity-training-2019\" rel=\"nofollow\"\u003eSingularity course\u003c/a\u003e I was involved in running at EMBL in early 2019. Once you get to slide titled \u003ca href=\"https://slides.com/mbhall88/making-containers#/2/4\" rel=\"nofollow\"\u003e\"Playing in a sandbox with a shell\"\u003c/a\u003e you can move on to \u003ca href=\"#Exercise-1\"\u003eExercise 1\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e As the course was aimed at users of Singularity v3+ you will see the container\nextension \u003ccode\u003e.sif\u003c/code\u003e used. This was a new container file format introduced in v3 that is\nnot usable with v2. The container extension for v2 was \u003ccode\u003e.simg\u003c/code\u003e, so you may see this sometimes.\nFor instance, the cluster at EBI is still on v2 (the training VMs are v3). For those using\nthe Heidelberg cluster, your cluster has v3. Singularity v2 containers, with the \u003ccode\u003e.simg\u003c/code\u003e extension,\ncan be executed by Singularity v3. You will also find all of the recipe\nfiles in that presentation in the \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/tree/master/recipes\"\u003e\u003ccode\u003erecipes/\u003c/code\u003e\u003c/a\u003e directory of this repository.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-exercise-1\" class=\"anchor\" href=\"#exercise-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExercise 1\u003c/h2\u003e\n\u003cp\u003eForm two groups and complete the following tasks.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-task-1\" class=\"anchor\" href=\"#task-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTask 1\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://help.github.com/en/github/getting-started-with-github/fork-a-repo\"\u003eFork\u003c/a\u003e this repository on GitHub.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-task-2\" class=\"anchor\" href=\"#task-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTask 2\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://slides.com/mbhall88/remote-container-systems#/1/6\" rel=\"nofollow\"\u003eEnable Singularity Hub\u003c/a\u003e on your fork of this repository.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-task-3\" class=\"anchor\" href=\"#task-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTask 3\u003c/h3\u003e\n\u003cp\u003eEach group should choose one of the following two GitHub issues to close:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003esnakemake\u003c/code\u003e recipe: \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/1\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/26d3e148ca179ea5b34cb0255936905ed487432faa4027a512640b8f92a68ea7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f31\" alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/1\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eshellcheck\u003c/code\u003e recipe: \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/2\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a43a776c8cd5a471e5293ecd213c14f9452745fe9c75b850bd1986cf79d0d70a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f32\" alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/2\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sandbox-development\" class=\"anchor\" href=\"#sandbox-development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSandbox development\u003c/h2\u003e\n\u003cp\u003eDuring the previous exercise, you may have noticed that errors in your build recipe require you to rerun the build all over again. When installing simple programs, this isn\u0027t too costly. However, when we want to build more complicated containers, it becomes time-consuming to rerun the entire build continually. In this section, we will look at how we can use Singularity\u0027s \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/build_a_container.html#creating-writable-images-and-sandbox-directories\" rel=\"nofollow\"\u003e\u003ccode\u003e--sandbox\u003c/code\u003e\u003c/a\u003e option to speed up the container recipe development cycle.\u003c/p\u003e\n\u003cp\u003eSo what is a sandbox? Think of it as a directory that mimics the inside of a container. You can then start an interactive shell session in this sandbox and run commands in the same environment that they will run in when building the container. In this way, you can test out what commands you need to run to get your program(s) installed and executing correctly. This massively reduces your turnaround time for creating containers. In addition, as we make the sandbox writeable, any changes we make will stay saved.\u003c/p\u003e\n\u003cp\u003eLet\u0027s get into the sandbox and play!\u003c/p\u003e\n\u003cp\u003eCreate a new directory where we will do our sandbox development.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003emkdir sandbox-dev\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e sandbox-dev\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNext, we will use the \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/blob/master/recipes/Singularity.template\"\u003etemplate recipe\u003c/a\u003e in this repository to build our sandbox from.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build --sandbox playground ../recipes/Singularity.template\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou should now see a directory called \u003ccode\u003eplayground\u003c/code\u003e. I\u0027ve named the sandbox \u003ccode\u003eplayground\u003c/code\u003e, but you can name it whatever you want.\u003c/p\u003e\n\u003cp\u003eNow we will start an interactive shell within the sandbox/container image.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity shell --writable playground\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003eNote: If you don\u0027t use \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/build_a_container.html#writable\" rel=\"nofollow\"\u003e\u003ccode\u003e--writable\u003c/code\u003e\u003c/a\u003e you won\u0027t be able to install anything or do anything that changes the size of the container.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eYou should now see the prompt change to something like\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eSingularity playground:\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eIMPORTANT:\u003cbr\u003e\nThe directory \u003ccode\u003e/root\u003c/code\u003e from your local machine will be mounted in the sandbox. So anything you do in the sandbox in that directory will also be reflected in the \u003ccode\u003e/root\u003c/code\u003e directory locally.\nEnsure you move out of \u003ccode\u003e/root\u003c/code\u003e within the sandbox and do all of your work there. I tend to use \u003ccode\u003e/usr/local\u003c/code\u003e, but you could create a new directory altogether (but outside \u003ccode\u003e/root\u003c/code\u003e) e.g. \u003ccode\u003e/sandbox\u003c/code\u003e.\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e /usr/local\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow we\u0027ll try and \u003ca href=\"https://conda.io/projects/conda/en/latest/user-guide/install/macos.html#install-macos-silent\" rel=\"nofollow\"\u003einstall \u003ccode\u003econda\u003c/code\u003e\u003c/a\u003e inside the sandbox.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ewget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda3.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will give us: \u003ccode\u003ebash: wget: command not found\u003c/code\u003e. A perfect example of why these sandboxes\nare so useful. The OS installation is \u003cem\u003every\u003c/em\u003e minimal and doesn\u0027t include a lot of programs.\u003c/p\u003e\n\u003cp\u003eLet\u0027s install \u003ccode\u003ewget\u003c/code\u003e in our sandbox and try again.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eapt install -y wget\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda3.sh\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow we\u0027ll install \u003ccode\u003econda\u003c/code\u003e, specifying the prefix (\u003ccode\u003e-p\u003c/code\u003e) as a directory in \u003ccode\u003e/usr/local\u003c/code\u003e\ncalled \u003ccode\u003eminiconda\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebash miniconda3.sh -b -p \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epwd\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e/miniconda\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn order to run \u003ccode\u003econda\u003c/code\u003e now, we need to ensure it\u0027s binary is in our \u003ccode\u003ePATH\u003c/code\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PATH=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003erealpath miniconda/bin\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:\u003cspan class=\"pl-smi\"\u003e${PATH}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003eRemember from the \u003ca href=\"https://slides.com/mbhall88/making-containers#/1/7\" rel=\"nofollow\"\u003e\u003ccode\u003e%environment\u003c/code\u003e slide\u003c/a\u003e that when writing the recipe for\nthis \u003ccode\u003econda\u003c/code\u003e installation we would need to write the \u003ccode\u003eexport\u003c/code\u003e line as:\u003c/em\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eexport PATH=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003erealpath miniconda/bin\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e:\u003cspan class=\"pl-smi\"\u003e${PATH}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$SINGULARITY_ENVIRONMENT\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eLastly, we need to test \u003ccode\u003econda\u003c/code\u003e is executable.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003econda list\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn order to convert these commands into a recipe I generally keep a text file open where\nI paste (successful) commands into as I go so I don\u0027t have to search back through my\nshell history later.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-exercise-2\" class=\"anchor\" href=\"#exercise-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExercise 2\u003c/h2\u003e\n\u003cp\u003eSimilar to \u003ca href=\"#exercise-1\"\u003eExercise 1\u003c/a\u003e, form two groups (can be different groups) and put\nin a pull request each to close the following two issues:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eflye\u003c/code\u003e recipe: \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/3\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5bdb30d6ea7dece3a9a4cfc16de03ce988f6197b0363cb987ad5506c879a57eb/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f33\" alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/3\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003enanopolish\u003c/code\u003e recipe: \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/issues/4\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b1fb67d7045f5f2f26d02ed8c2d5b5423da330dfc0b19efa70dbfd53ca698f5f/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f64657461696c2f73746174652f6d6268616c6c38382f656970702d323031392d73696e67756c61726974792f34\" alt=\"GitHub issue/pull request detail\" data-canonical-src=\"https://img.shields.io/github/issues/detail/state/mbhall88/eipp-2019-singularity/4\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI chose more complicated programs this time so you can get some experience using a sandbox.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-and-serving-applications\" class=\"anchor\" href=\"#run-and-serving-applications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003erun\u003c/code\u003e and serving applications\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-run\" class=\"anchor\" href=\"#singularity-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003esingularity run\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThe \u003ca href=\"https://sylabs.io/guides/3.4/user-guide/cli/singularity_run.html\" rel=\"nofollow\"\u003e\u003ccode\u003erun\u003c/code\u003e\u003c/a\u003e directive will execute the \u003ca href=\"https://slides.com/mbhall88/making-containers#/1/10\" rel=\"nofollow\"\u003e\u003ccode\u003e%runscript\u003c/code\u003e\u003c/a\u003e and\npass along all arguments to this script. The \u003ccode\u003erun\u003c/code\u003e directive is handy for when you want\nto automate some common tasks using the programs installed within the container and be\nable to handle user options. Refer to \u003ca href=\"https://slides.com/mbhall88/making-containers#/1/10\" rel=\"nofollow\"\u003ethe slide on \u003ccode\u003e%runscript\u003c/code\u003e\u003c/a\u003e,\nfrom the earlier section on \u003ca href=\"#build-locally\"\u003ebuiding containers locally\u003c/a\u003e, for\nan example of using \u003ccode\u003esingularity run\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-serving-applications\" class=\"anchor\" href=\"#serving-applications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eServing applications\u003c/h3\u003e\n\u003cp\u003eIt is also possible to serve applications through a port from a container. As an example\nwe will build a container to run a \u003ca href=\"https://jupyter.org/\" rel=\"nofollow\"\u003e\u003ccode\u003ejupyter notebook\u003c/code\u003e\u003c/a\u003e that we can access on\nour local machine.\u003c/p\u003e\n\u003cp\u003eThe recipe to do this can be found in the \u003ccode\u003erecipe/\u003c/code\u003e directory as \u003ca href=\"https://github.com/mbhall88/eipp-2019-singularity/blob/master/recipes/Singularity.jupyter\"\u003e\u003ccode\u003eSingularity.jupyter\u003c/code\u003e\u003c/a\u003e.\nOf particular interest for this example, see the \u003ccode\u003e%runscript\u003c/code\u003e section.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e%runscript\n    PORT=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${1\u003cspan class=\"pl-k\"\u003e:-\u003c/span\u003e8888}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    \u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eStarting notebook...\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    \u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eOpen browser to localhost:\u003cspan class=\"pl-smi\"\u003e${PORT}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e /usr/local/bin/jupyter notebook  --ip=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e*\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e --port=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$PORT\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e --no-browser\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe take the first option passed by the user and store it in a variable \u003ccode\u003ePORT\u003c/code\u003e, or use \u003ccode\u003e8888\u003c/code\u003e\nif nothing is given. We print some logging to the screen with \u003ccode\u003eecho\u003c/code\u003e and then start\na \u003ccode\u003ejupyter\u003c/code\u003e session, passing the \u003ccode\u003ePORT\u003c/code\u003e to \u003ccode\u003ejupyter\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eLet\u0027s build this image and then fire it up.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build jupyter.sif recipes/Singularity.jupyter\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e we will use the default port 8888\u003c/span\u003e\nsingularity run jupyter.sif  \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou should get some output from \u003ccode\u003ejupyter\u003c/code\u003e indicating it has started running the notebook\nand providing a location, which should look something like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e[I 11:40:28.948 NotebookApp] Serving notebooks from local directory: /home/vagrant/container-dev\n[I 11:40:28.949 NotebookApp] The Jupyter Notebook is running at:\n[I 11:40:28.949 NotebookApp] http://dev-vm:8888/?token=c8fe88de778120e5ccd42850d6d13712e27b125b0481d5b0\n[I 11:40:28.949 NotebookApp]  or http://127.0.0.1:8888/?token=c8fe88de778120e5ccd42850d6d13712e27b125b0481d5b0\n[I 11:40:28.949 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 11:40:28.953 NotebookApp]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCopy the URL (either one), and paste it into a web browser. You should now see the home\npage for the notebook. Select the example notebook at \u003ccode\u003enotebooks/plot.ipynb\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eRun the two cells in the notebook, and you should see some toy data plotted.\u003c/p\u003e\n\u003cp\u003eThis is quite a simple use case for serving applications. You can do far more complicated\nthings like \u003ca href=\"https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\" rel=\"nofollow\"\u003erunning an RStudio server\u003c/a\u003e from a container and access it locally.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-workflow-management-systems\" class=\"anchor\" href=\"#workflow-management-systems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow management systems\u003c/h2\u003e\n\u003cp\u003eContainers and workflow management systems (WMSs), such as \u003ccode\u003esnakemake\u003c/code\u003e and \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003e\u003ccode\u003enextflow\u003c/code\u003e\u003c/a\u003e,\nare a match made in heaven. Containers add a crucial layer of reproducibility to these systems.\u003c/p\u003e\n\u003cp\u003eThough this is not a project to teach you how to use WMSs, I would\nencourage you to take a look at \u003ca href=\"https://slides.com/mbhall88/singularity-and-workflow-management-systems#/\" rel=\"nofollow\"\u003ethis short slide deck\u003c/a\u003e from the Singularity course I ran\nas it shows you how easy it is to integrate Singularity containers into WMSs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-programs-requiring-gpus\" class=\"anchor\" href=\"#programs-requiring-gpus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrograms requiring GPUs\u003c/h2\u003e\n\u003cp\u003eSingularity also provides the ability to utilise GPU cards, without needing to install\nthe GPU drivers into your container. Currently, it can only use NVIDIA GPUs. To allow a\ncontainer to use the local GPU card and drivers all you need to do it pass the\n\u003ca href=\"https://sylabs.io/guides/2.6/user-guide/appendix.html#a-gpu-example\" rel=\"nofollow\"\u003e\u003ccode\u003e--nv\u003c/code\u003e\u003c/a\u003e option. For example, to get a python shell with the GPU version of \u003ccode\u003etensorflow\u003c/code\u003e\navailable, you would run the following (on a machine with an NVIDIA GPU).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e --nv docker://tensorflow/tensorflow:latest-gpu python\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bonus\" class=\"anchor\" href=\"#bonus\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBonus\u003c/h2\u003e\n\u003cp\u003eIf you have gotten to this point, then have a go at creating a container for a piece of\nsoftware you have had difficulties installing in the past. Alternatively, you could try\nand reduce the size of the containers we have already produced by using \u003ca href=\"https://www.alpinelinux.org/\" rel=\"nofollow\"\u003eAlpine\u003c/a\u003e as the\nbase OS.\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [
      "singularity",
      "containers",
      "bioinformatics",
      "phd",
      "embl-ebi",
      "embl"
    ],
    "updated_at": 1626495910.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "Lizhen0909/LSHVec",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-lshvec-a-vector-representation-of-dna-sequences-using-locality-sensitive-hashing\" class=\"anchor\" href=\"#lshvec-a-vector-representation-of-dna-sequences-using-locality-sensitive-hashing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLSHVec: A Vector Representation of DNA Sequences Using Locality Sensitive Hashing\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-summary\" class=\"anchor\" href=\"#summary\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSummary\u003c/h2\u003e\n\u003cp\u003eLSHVec is a k-mer/sequence embedding/classfication software which extends \u003ca href=\"https://fasttext.cc/\" rel=\"nofollow\"\u003eFastText\u003c/a\u003e . It applies LSH (Locality Sensitive Hashing) to reduce the size of k-mer vocabulary and improve the performance of embedding.\u003c/p\u003e\n\u003cp\u003eBesides building from source code, LSHVec can run using docker or singularity.\u003c/p\u003e\n\u003cp\u003ePlease refer to \u003ca href=\"https://www.biorxiv.org/content/10.1101/726729v1\" rel=\"nofollow\"\u003eA Vector Representation of DNA Sequences Using Locality Sensitive Hashing\u003c/a\u003e for the idea and experiments.\u003c/p\u003e\n\u003cp\u003eThere are also some pretained models that can be used, please see \u003ca href=\"https://github.com/Lizhen0909/PyLSHvec/blob/master/README.md\"\u003ePyLSHvec\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cp\u003eHere is the environment I worked on.  Other versions may also work. Python 3 should work, but I don\u0027t use it a lot.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLinux, gcc with C++11\u003c/li\u003e\n\u003cli\u003ePython 2.7 or Python 3.6 or 3.7\n\u003cul\u003e\n\u003cli\u003ejoblib 0.12.4\u003c/li\u003e\n\u003cli\u003etqdm 4.28.1\u003c/li\u003e\n\u003cli\u003enumpy 1.15.0\u003c/li\u003e\n\u003cli\u003epandas 0.23.4\u003c/li\u003e\n\u003cli\u003esklearn 0.19.1 (only for evaluation)\u003c/li\u003e\n\u003cli\u003eMulticoreTSNE (only for visualization)\u003c/li\u003e\n\u003cli\u003ecython 0.28.5\u003c/li\u003e\n\u003cli\u003ecsparc (included)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-from-source\" class=\"anchor\" href=\"#build-from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild from Source\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eclone from git\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egit clone https://LizhenShi@bitbucket.org/LizhenShi/lshvec.git\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecd lshvec\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003einstall csparc which wraps a c version of k-mer generator I used in another project\u003c/p\u003e\n\u003cp\u003efor python 2.7\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epip install pysparc-0.1-cp27-cp27mu-linux_x86_64.whl\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eor for python 3.6\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epip install pysparc-0.1-cp36-cp36m-linux_x86_64.whl\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eor for python 3.7\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epip install pysparc-0.1-cp37-cp37m-linux_x86_64.whl\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003emake\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003emake\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-jupyter-notebook-examples\" class=\"anchor\" href=\"#jupyter-notebook-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJupyter Notebook Examples\u003c/h2\u003e\n\u003cp\u003eA toy example, which is laptop friendly and should finish in 10 minutes,  can be found in \u003ca href=\"notebook/Tutorial_Toy_Example.ipynb\"\u003eTutorial_Toy_Example.ipynb\u003c/a\u003e. Because of randomness the result may be different.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"notebook/Tutorial_Toy_Example.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"notebook/Tutorial_Toy_Example.png\" alt=\"Tutorial_Toy_Example\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA practical example which uses ActinoMock Nanopore data can be found at \u003ca href=\"notebook/Tutorial_ActinoMock_Nanopore.ipynb\"\u003eTutorial_ActinoMock_Nanopore.ipynb\u003c/a\u003e. The notebook ran on a 16-core 64G-mem node and took a few hours (I think 32G mem should work too).\u003c/p\u003e\n\u003cp\u003e\u200b\t\t\t\t\t\t \u003ca href=\"notebook/Tutorial_ActinoMock_Nanopore.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"notebook/Tutorial_ActinoMock_Nanopore.png\" alt=\"Tutorial_ActinoMock_Nanopore\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-command-line-options\" class=\"anchor\" href=\"#command-line-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand line options\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fastqtoseqpy\" class=\"anchor\" href=\"#fastqtoseqpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003efastqToSeq.py\u003c/h3\u003e\n\u003cp\u003econvert a fastq file to a seq file\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython fastqToSeq.py -i \u0026lt;fastq_file\u0026gt; -o \u0026lt;out seq file\u0026gt; -s \u0026lt;1 to shuffle, 0 otherwise\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hashseqpy\" class=\"anchor\" href=\"#hashseqpy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ehashSeq.py\u003c/h3\u003e\n\u003cp\u003eEncode reads in a seq file use an encoding method.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython hashSeq.py -i \u0026lt;seq_file\u0026gt; --hash \u0026lt;fnv or lsh\u0026gt; -o \u0026lt;outfile\u0026gt; [-k \u0026lt;kmer_size\u0026gt;] [--n_thread \u0026lt;n\u0026gt;] [--hash_size \u0026lt;m\u0026gt;] [--batch_size \u0026lt;n\u0026gt;] [--bucket \u0026lt;n\u0026gt;] [--lsh_file \u0026lt;file\u0026gt;] [--create_lsh_only]\n\n  --hash_size \u0026lt;m\u0026gt;:        only used by lsh which defines 2^m bucket.\n  --bucket \u0026lt;n\u0026gt;:           number of bucket for hash trick, useless for onehot.\n   \t\t\t\t          For fnv and lsh it limits the max number of words.\n   \t\t\t\t          For lsh the max number of words is min(2^m, n).\n  --batch_size \u0026lt;b\u0026gt;:       how many reads are processed at a time. A small value uses less memory.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-lshvec\" class=\"anchor\" href=\"#lshvec\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003elshvec\u003c/h3\u003e\n\u003cp\u003ePlease refer to \u003ca href=\"https://fasttext.cc/docs/en/options.html\" rel=\"nofollow\"\u003efasttext options\u003c/a\u003e.  However note that options of \u003ccode\u003ewordNgrams\u003c/code\u003e, \u003ccode\u003eminn\u003c/code\u003e,\u003ccode\u003emaxn\u003c/code\u003e does not work with lshvec.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-of-docker-run\" class=\"anchor\" href=\"#example-of-docker-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample of Docker Run\u003c/h2\u003e\n\u003cp\u003ePull from docker hub:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull lizhen0909/lshvec:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAssume \u003ccode\u003edata.fastq\u003c/code\u003e file is in folder \u003ccode\u003e/path/in/host\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003econvert fastq to a seq file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -v /path/in/host:/host lshvec:latest bash -c \"cd /host \u0026amp;\u0026amp; fastqToSeq.py  -i data.fastq -o data.seq\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecreate LSH:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -v /path/in/host:/host lshvec:latest bash -c \"cd /host \u0026amp;\u0026amp; hashSeq.py -i data.seq --hash lsh -o data.hash -k 15\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003erun lshvec:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -v /path/in/host:/host lshvec:latest bash -c \"cd /host \u0026amp;\u0026amp; lshvec skipgram -input data.hash -output model\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-of-singularity-run\" class=\"anchor\" href=\"#example-of-singularity-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample of Singularity Run\u003c/h2\u003e\n\u003cp\u003eWhen running using Singularity, it is probably in an HPC environment. The running is similar to docker. However depending on the version of singularity, commands and paths might be different, especially from 2.x to 3.x. Here is an example for version 2.5.0.\u003c/p\u003e\n\u003cp\u003eAlso it is better to specify number of threads, otherwise max number of cores will be used which is not desired in HPC environment.\u003c/p\u003e\n\u003cp\u003ePull from docker hub:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name lshvec.sif shub://Lizhen0909/LSHVec\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePut \u003ccode\u003edata.fastq\u003c/code\u003e file is in host \u003ccode\u003e/tmp\u003c/code\u003e,  since Singularity automatically mount \u003ccode\u003e/tmp\u003c/code\u003e folder.\u003c/p\u003e\n\u003cp\u003econvert fastq to a seq file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run /path/to/lshvec.sif bash -c \"cd /tmp \u0026amp;\u0026amp; fastqToSeq.py  -i data.fastq -o data.seq\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ecreate LSH:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run /path/to/lshvec.sif bash -c \"cd /tmp \u0026amp;\u0026amp; hashSeq.py -i data.seq --hash lsh -o data.hash -k 15 --n_thread 12\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003erun lshvec:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run /path/to/lshvec.sif bash -c \"cd /tmp \u0026amp;\u0026amp; lshvec skipgram -input data.hash -output model -thread 12\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-questions\" class=\"anchor\" href=\"#questions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuestions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003elshvec\u003c/code\u003e gets stuck at \u003ccode\u003eRead xxxM words\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSearch \u003ccode\u003eMAX_VOCAB_SIZE\u003c/code\u003e in the source code and change it to a bigger one.  When a word\u0027s index is bigger than that number, a loop is carried to query it, which is costly. The number is 30M in FastText which is good for languages. But it is too small for k-mers. The number has been already increased to 300M in FastSeq. But for large and/or high-error-rate data, it may be still not enough.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eI have big data\u003c/p\u003e\n\u003cp\u003ehashSeq reads all data into memory to sample k-mers for hyperplanes. If data is too big it may not fit into memory. One can\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTry sampling. DNA reads generally have high coverage. Such high coverage may not be necessary.\u003c/li\u003e\n\u003cli\u003eOr use \u003ccode\u003ecreate_hash_only\u003c/code\u003e to create lsh on a small (sampled) data; then split your data into multiple files and run hashSeq with \u003ccode\u003elsh_file\u003c/code\u003e option on many nodes.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecore dumped when hashing\u003c/p\u003e\n\u003cp\u003eError like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eterminate called after throwing an instance of \u0027std::out_of_range\u0027\nwhat(): map::at\nAborted (core dumped)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003emostly because a sequence contains characters other than ACGTN. So please convert non-ACGT characters to N\u0027s.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eInherit license from FastText which is BSD License\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 1,
    "topics": [
      "locality-sensitive-hashing",
      "sequence-vector",
      "classfication"
    ],
    "updated_at": 1624367592.0
  },
  {
    "data_format": 2,
    "description": "Singularity configurations for R and pbdR packages.",
    "filenames": [
      "pbdR/pbdR/openmpi/Singularity.1.0-1",
      "pbdR/pbdR/mpich/Singularity.1.0-1",
      "pbdR/pbdR-minimal/openmpi/Singularity.1.0-1",
      "pbdR/pbdR-minimal/mpich/Singularity.1.0-1",
      "R/rstudio-server/Singularity.1.1.456",
      "R/rstudio/Singularity.1.1.456",
      "R/r/Singularity.3.5.1",
      "R/jupyter/Singularity",
      "R/r-minimal/Singularity.3.5.1"
    ],
    "full_name": "RBigData/singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-r-and-pbdr-singularity-recipes\" class=\"anchor\" href=\"#r-and-pbdr-singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR and pbdR Singularity Recipes\u003c/h1\u003e\n\u003cp\u003eSingularity recipes for R and pbdR.\u003c/p\u003e\n\u003cp\u003eBuild requirements:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.sylabs.io/\" rel=\"nofollow\"\u003esingularity\u003c/a\u003e \u0026gt;= 2.3\u003c/li\u003e\n\u003cli\u003eModify the \u003ccode\u003emake -j\u003c/code\u003e line of each recipe to your liking.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1571942199.0
  },
  {
    "data_format": 2,
    "description": "Singularity Recipe for QIIME 2",
    "filenames": [
      "Singularity.2019.10",
      "Singularity.2019.4",
      "Singularity.2017.12",
      "Singularity",
      "Singularity.2018.2",
      "Singularity.2018.6"
    ],
    "full_name": "ResearchIT/qiime2",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipe-for-qiime2\" class=\"anchor\" href=\"#singularity-recipe-for-qiime2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipe for QIIME2\u003c/h1\u003e\n\u003cp\u003eThis repo contains recipe run \u003ca href=\"https://qiime2.org\" rel=\"nofollow\"\u003eqiime2\u003c/a\u003e within a\n\u003ca href=\"https://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container, which can be built\nusing \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eVersions:\n2017.12 - QIIME2-2017.12 installed on CentOS 7\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to Use:\u003c/h2\u003e\n\u003cp\u003esingularity run shub://ResearchIT/qiime2 --help\u003c/p\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 6,
    "topics": [
      "qiime",
      "singularity"
    ],
    "updated_at": 1576035067.0
  },
  {
    "data_format": 2,
    "description": "SC17 tutorial - \"HPC via HTTP: Portable, Scalable Computing using App Containers and the Agave API\"",
    "filenames": [
      "content/images/funwave-tvd/Singularity"
    ],
    "full_name": "agaveplatform/SC17-container-tutorial",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpc-via-http-portable-scalable-computing-using-app-containers-and-the-agave-api\" class=\"anchor\" href=\"#hpc-via-http-portable-scalable-computing-using-app-containers-and-the-agave-api\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPC via HTTP: Portable, Scalable Computing using App Containers and the Agave API\u003c/h1\u003e\n\u003cp\u003eSupercomputing matters. So does user experience. Standing between the mainstream adoption of supercomputing and a new generation of users is the reality that the entry cost to using these systems, both in terms of dollars and in time spent learning the technology, has not significantly changed in the last 20 years. The rise of cloud computing only complicates the learning curve further. Over the last 6 years, the authors have been addressing this gap through the development of a Science-as-a-Service platform enabling users to go from their desktop, to their local data center, to the cloud, and back without sacrificing their existing tool chain or user experience.\u003c/p\u003e\n\u003cp\u003eIn this tutorial, we combine best practices and lessons learned while on-boarding the last 70k new users to TACC\u2019s data center through the Agave Platform. Participants will walk through the process of scaling their application from a local environment to the Jetstream academic cloud and to a high performance computing system at the Texas Advanced Computing Center. They will learn to use multiple container technologies to harmonize app execution between cloud and HPC resources, and they will learn to use modern APIs to orchestrate job execution, capture provenance information, and foster collaboration.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-preview\" class=\"anchor\" href=\"#preview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreview\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"http://www.youtube.com/watch?v=hVnIrjn_aBI\" title=\"HPC via HTTP: Portable, Scalable Computing using App Containers and the Agave API\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c65f2550bc90ecabb4429c52335a19f58b324a579f4cb9a3f92dfcab4bc7391f/687474703a2f2f696d672e796f75747562652e636f6d2f76692f68566e49726a6e5f6142492f6d617872657364656661756c742e6a7067\" alt=\"Intro Video\" data-canonical-src=\"http://img.youtube.com/vi/hVnIrjn_aBI/maxresdefault.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-schedule\" class=\"anchor\" href=\"#schedule\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSchedule\u003c/h1\u003e\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003cth\u003eTime\u003c/th\u003e\n    \u003cth\u003ePresenterr\u003c/th\u003e\n    \u003cth\u003eTopic\u003c/th\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e08:30 - 08:45\u003c/td\u003e\n    \u003ctd\u003eJohn, Steve\u003c/td\u003e\n    \u003ctd\u003e[Introductions](01%20Introduction.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e08:45 - 09:05\u003c/td\u003e\n    \u003ctd\u003eRion\u003c/td\u003e\n    \u003ctd\u003e[Agave Overview](02%20Agave%20Overview.pdf)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e09:05 - 09:15\u003c/td\u003e\n    \u003ctd\u003eKathy\u003c/td\u003e\n    \u003ctd\u003e[Jupyter, Sanbox, and Logging In](03%20Jupyter%2C%20Sandboxes%2C%20and%20Logging%20In.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e09:15 - 09:30\u003c/td\u003e\n    \u003ctd\u003eSteve\u003c/td\u003e\n    \u003ctd\u003e[Code, Build, and Test](04%20Code%20Build%20and%20Test.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e09:30 - 10:00\u003c/td\u003e\n    \u003ctd\u003eRion, John\u003c/td\u003e\n    \u003ctd\u003e[Hands on with Agave](05%20Hands%20on%20with%20Agave.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e10:00 - 10:30\u003c/td\u003e\n    \u003ctd\u003e--\u003c/td\u003e\n    \u003ctd\u003eBreak\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e10:30 - 11:00\u003c/td\u003e\n    \u003ctd\u003eSteve,John\u003c/td\u003e\n    \u003ctd\u003e[Docker and Singularity](06%20Docker%20and%Singularity.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e11:00 - 11:15\u003c/td\u003e\n    \u003ctd\u003eRion\u003c/td\u003e\n    \u003ctd\u003e[Automation an Benchmarking](07%20Automation%20and%20Benchmarking.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e11:15 - 11:45\u003c/td\u003e\n    \u003ctd\u003eKathy, Rion\u003c/td\u003e\n    \u003ctd\u003e[Packaging, publishing, and Portability](08%20Packaging%20publishing%20and%20Portability.ipynb)\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd\u003e11:45 - 12:00\u003c/td\u003e\n    \u003ctd\u003eSteve, John\u003c/td\u003e\n    \u003ctd\u003e[Future Directions and Homework)[09%20Future%20Directions%20and%20Homework.ipynb]\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e01: \u003ca href=\"01-Requirements-and-Preparation.md\"\u003eRequirements and Preparation\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e02: \u003ca href=\"02-Installation-and-Infrastructure.md\"\u003eInstallation and Infrastructure\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e03: \u003ca href=\"03-Auth-Notebooks-and-Web-Console.md\"\u003eAuth, Notebooks, and the Web Interface\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e04: \u003ca href=\"04-SciOps-and-Sample-Application.md\"\u003eSciOps and our Sample Application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e05: \u003ca href=\"05-Code-Build-and-Run-Locally.md\"\u003eCode, Build, and Run Locally\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e06: \u003ca href=\"06-Containerize-Existing-Applications.md\"\u003eContainerize Existing Applications\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e07: \u003ca href=\"07-Automation-Registries-and-App-Catalogues\"\u003eAutomation, Registries, and App Catalogues\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eAgave\u003c/li\u003e\n\u003cli\u003eCI/CD\u003c/li\u003e\n\u003cli\u003eImage publishing\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003e08: \u003ca href=\"\"\u003eScaling and Portability\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003eImage caching\u003c/li\u003e\n\u003cli\u003eRuntime environments\u003c/li\u003e\n\u003cli\u003eData scheduling\u003c/li\u003e\n\u003cli\u003eReproducibility anti-patterns\u003c/li\u003e\n\u003c/ul\u003e\n\u003cul\u003e\n\u003cli\u003e09: \u003ca href=\"\"\u003eViewing simulation results, sharing, provenance\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e10: \u003ca href=\"\"\u003ePackaging and Publishing Experiments\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e11: \u003ca href=\"\"\u003eBenchmarking and Performance Considerations\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e12: \u003ca href=\"\"\u003eFunctions, Microcodes, and Exascale\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e13: \u003ca href=\"\"\u003eHomework an Further Reading\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e90: \u003ca href=\"90-Appendix-A.md\"\u003eAppendix A\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e99: \u003ca href=\"99-References.md\"\u003eReferences\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 3,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1529379287.0
  },
  {
    "data_format": 2,
    "description": "An oil land-spill and overland flow simulator for pipeline rupture events",
    "filenames": [
      "Singularityfiles/Singularity.v1.0.dev3",
      "Singularityfiles/Singularity.v1.0.dev1",
      "Singularityfiles/Singularity.v1.0.dev2",
      "Singularityfiles/Singularity.v1.0",
      "Singularityfiles/Singularity.v0.1.trusty",
      "Singularityfiles/Singularity.v0.1.bionic",
      "Singularityfiles/Singularity.v1.0.dev4"
    ],
    "full_name": "barbagroup/geoclaw-landspill",
    "latest_release": "v1.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-geoclaw-landspill\" class=\"anchor\" href=\"#geoclaw-landspill\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egeoclaw-landspill\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/barbagroup/geoclaw-landspill/raw/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8ccf186e7288af6d88a1f6a930c0fcc4e7a8a9936b34e07629d815d1eab4d977/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d425344253230332d2d436c617573652d626c75652e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/License-BSD%203--Clause-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.com/barbagroup/geoclaw-landspill\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7f22540fd3a0f3b9a8d8a57ead3744961fd8b2d9edd257d02e4a8e0ae1d6d7a6/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f636f6d2f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6d61737465723f6c6162656c3d5472617669732532304349\" alt=\"Travis CI\" data-canonical-src=\"https://img.shields.io/travis/com/barbagroup/geoclaw-landspill/master?label=Travis%20CI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/barbagroup/geoclaw-landspill/actions?query=workflow%3ACI\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/91894ad74ed7c9cc23b3f2fb08cc1ea432c8ed6830abd92e489e7f59ac619ab3/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f43492f6d61737465723f6c6162656c3d476974487562253230416374696f6e2532304349\" alt=\"GitHub Action CI\" data-canonical-src=\"https://img.shields.io/github/workflow/status/barbagroup/geoclaw-landspill/CI/master?label=GitHub%20Action%20CI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/036ff156f41dafdb919e29a22cd5aa00a7f0ded742b75831240ea25fe720350e/68747470733a2f2f6a6f73732e7468656f6a2e6f72672f7061706572732f66623762303132373939613730633962346335356562346262306633366639372f7374617475732e737667\" alt=\"status\" data-canonical-src=\"https://joss.theoj.org/papers/fb7b012799a70c9b4c55eb4bb0f36f97/status.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/barbagroup/geoclaw-landspill\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9e96d790dd4b2cdbdea7b49eff75628a95ae5cbd3c8f5b7bc902b7f9b603b149/68747470733a2f2f616e61636f6e64612e6f72672f626172626167726f75702f67656f636c61772d6c616e647370696c6c2f6261646765732f696e7374616c6c65722f636f6e64612e737667\" alt=\"Conda\" data-canonical-src=\"https://anaconda.org/barbagroup/geoclaw-landspill/badges/installer/conda.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eNote: if looking for content of \u003ccode\u003egeoclaw-landspill-cases\u003c/code\u003e, please checkout tag\n\u003ccode\u003ev0.1\u003c/code\u003e. This repository has been converted to a fully working solver package.\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003egeoclaw-landspill\u003c/em\u003e is a package for running oil overland flow simulations for\napplications in pipeline risk management. It includes a numerical solver and\nsome pre-/post-processing utilities.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"./doc/sample.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./doc/sample.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThe numerical solver is a modified version of\n\u003ca href=\"http://www.clawpack.org/geoclaw.html\" rel=\"nofollow\"\u003eGeoClaw\u003c/a\u003e.\nGeoClaw solves full shallow-water equations. We added several new features and\nutilities to it and make it usable to simulate the overland flow from pipeline\nruptures. These features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eadding point sources to mimic the rupture points\u003c/li\u003e\n\u003cli\u003eadding evaporation models\u003c/li\u003e\n\u003cli\u003eadding Darcy-Weisbach bottom friction models with land roughness\u003c/li\u003e\n\u003cli\u003eadding temperature-dependent viscosity\u003c/li\u003e\n\u003cli\u003erecording detail locations and time of oil flowing into in-land waterbodies\u003c/li\u003e\n\u003cli\u003edownloading topography and hydrology data automatically (the US only)\u003c/li\u003e\n\u003cli\u003egenerating CF-1.7 compliant NetCDF files\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"doc/deps_install_tests.md\"\u003eDependencies, installation, and tests\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"doc/usage.md\"\u003eUsage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"doc/configuration.md\"\u003eConfiguration file: \u003ccode\u003esetrun.py\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"cases/README.md\"\u003eExample cases\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"doc/container.md\"\u003eContainers: Docker and Singularity\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cp\u003eWe only maintain compatibility with Linux. Though using \u003ccode\u003epip\u003c/code\u003e or building from\nsource may still work in Mac OS or Windows (e.g., through WSL), we are not able\nto help with the installation issues on these two systems.\u003c/p\u003e\n\u003cp\u003eBeyond this quick start, to see more details, please refer to the\n\u003ca href=\"#documentation\"\u003edocumentation\u003c/a\u003e section.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-installation\" class=\"anchor\" href=\"#1-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Installation\u003c/h3\u003e\n\u003cp\u003eThe fast way to install \u003cem\u003egeoclaw-landspill\u003c/em\u003e is through\n\u003ca href=\"https://www.anaconda.com/\" rel=\"nofollow\"\u003eAnaconda\u003c/a\u003e\u0027s \u003ccode\u003econda\u003c/code\u003e command. The following command\ncreates a conda environment (called \u003ccode\u003elandspill\u003c/code\u003e) and installs the package and\ndependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda create \\\n    -n landspill -c barbagroup -c conda-forge \\\n    python=3.8 geoclaw-landspill\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen use \u003ccode\u003econda activate landspill\u003c/code\u003e or\n\u003ccode\u003esource \u0026lt;conda installation prefix\u0026gt;/bin/activate landspill\u003c/code\u003e to activate the\nenvironment. Type \u003ccode\u003egeoclaw-landspill --help\u003c/code\u003e in the terminal to see if\n\u003cem\u003egeoclaw-landspill\u003c/em\u003e is correctly installed.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-running-an-example-case\" class=\"anchor\" href=\"#2-running-an-example-case\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Running an example case\u003c/h3\u003e\n\u003cp\u003eTo run an example case under the folder \u003ccode\u003ecases\u003c/code\u003e, users have to clone this\nrepository. We currently don\u0027t maintain another repository for cases. After\ncloning this repository, run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ geoclaw-landspill run \u0026lt;path to an example case folder\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor example, to run \u003ccode\u003eutal-flat-maya\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ geoclaw-landspill run ./cases/utah-flat-maya\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUsers can use environment variable \u003ccode\u003eOMP_NUM_THREADS\u003c/code\u003e to control how many CPU\nthreads the simulation should use for OpenMP parallelization.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-creating-a-cf-compliant-netcdf-raster-file\" class=\"anchor\" href=\"#3-creating-a-cf-compliant-netcdf-raster-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Creating a CF-compliant NetCDF raster file\u003c/h3\u003e\n\u003cp\u003eAfter a simulation is done, users can convert flow depth in raw simulation data\ninto a CF-compliant NetCDF raster file. For example,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ geoclaw-landspill createnc ./case/utah-flat-maya\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eReplace \u003ccode\u003e./cases/utah-flat-maya\u003c/code\u003e with the path to another desired case.\u003c/p\u003e\n\u003cp\u003eQGIS and ArcGIS should be able to read the resulting NetCDF raster file.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-third-party-codes-and-licenses\" class=\"anchor\" href=\"#third-party-codes-and-licenses\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThird-party codes and licenses\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eamrclaw: \u003ca href=\"https://github.com/clawpack/amrclaw\"\u003ehttps://github.com/clawpack/amrclaw\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/amrclaw/blob/ee85c1fe178ec319a8403503e779d3f8faf22840/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003egeoclaw: \u003ca href=\"https://github.com/clawpack/geoclaw\"\u003ehttps://github.com/clawpack/geoclaw\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/geoclaw/blob/3593cb1b418fd52739c186a8845a288037c8f575/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003epyclaw: \u003ca href=\"https://github.com/clawpack/pyclaw\"\u003ehttps://github.com/clawpack/pyclaw\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/pyclaw/blob/a85a01a5f20be1a18dde70b7bb37dc1cdcbd0b26/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eclawutil: \u003ca href=\"https://github.com/clawpack/clawutil\"\u003ehttps://github.com/clawpack/clawutil\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/clawutil/blob/116ffb792e889fbf0854d7ac599657039d7b1f3e/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eriemann: \u003ca href=\"https://github.com/clawpack/riemann\"\u003ehttps://github.com/clawpack/riemann\u003c/a\u003e\n(\u003ca href=\"https://github.com/clawpack/riemann/blob/597824c051d56fa0c8818e00d740867283329b24/LICENSE\"\u003eBSD 3-Clause License\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eSee \u003ca href=\"CONTRIBUTING.md\"\u003eCONTRIBUTING.md\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003ePi-Yueh Chuang: \u003ca href=\"mailto:pychuang@gwu.edu\"\u003epychuang@gwu.edu\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 5,
    "topics": [
      "geoclaw",
      "overland-flow",
      "pipeline",
      "shallow-water-equations",
      "pipeline-ruptures",
      "land-spill"
    ],
    "updated_at": 1623343615.0
  },
  {
    "data_format": 2,
    "description": "Singularity containers with common radio transient search software.  ",
    "filenames": [
      "Singularity.gpu",
      "Singularity",
      "Singularity.cpu",
      "Singularity.arm"
    ],
    "full_name": "josephwkania/radio_transients",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-radio_transients\" class=\"anchor\" href=\"#radio_transients\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/88694793dd1e428a0aab6788e9bbd21141580f62cbc4d6310bbcc74fde83ab22/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff5d91d6824296a5d7ffaad36635c5ef0688d2c0e21e50ccc94735fe8387faa7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f666f726b732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"Forks\" data-canonical-src=\"https://img.shields.io/github/forks/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6e6ccae69f7f4df8c46d4d56b7e36d27fd932cc463a486a3111796543c271ab9/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f73746172732f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"Stars\" data-canonical-src=\"https://img.shields.io/github/stars/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d5708d5c2bcd6f7d5f3565d9e75135e1eaa086ff847e198c14d187c5612f8203/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6a6f73657068776b616e69612f726164696f5f7472616e7369656e74733f7374796c653d666c61742d737175617265\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/josephwkania/radio_transients?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://cloud.sylabs.io/library/josephwkania/radio_transients/radio_transients\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5a9c47f4c4e2f587278d94eea8fe905a930df7dd78bc5258f5d15cc1981348f8/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f486f737465642d53796c6162732d477265656e2e737667\" alt=\"Sylabs\" data-canonical-src=\"https://img.shields.io/badge/Hosted-Sylabs-Green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThese are my Singularity Recipes for common radio transient software.\nThere are three containers\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-radio_transients-1\" class=\"anchor\" href=\"#radio_transients-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients\u003c/h3\u003e\n\u003cp\u003eContains everything (CPU+GPU)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCUDA 10.2\nFETCH          https://github.com/devanshkv/fetch  -- In Conda environment `FE`\nheimdall       https://sourceforge.net/p/heimdall-astro/wiki/Use/\n- dedisp       https://github.com/ajameson/dedisp\nhtop           https://htop.dev/\niqrm_apollo    https://gitlab.com/kmrajwade/iqrm_apollo\njupyterlab     https://jupyter.org/\nPRESTO         https://www.cv.nrao.edu/~sransom/presto/\npsrdada        http://psrdada.sourceforge.net/\npsrdada-python https://github.com/TRASAL/psrdada-python\npsrcat         https://www.atnf.csiro.au/people/pulsar/psrcat/download.html\npysigproc      https://github.com/devanshkv/pysigproc\nriptide        https://github.com/v-morello/riptide\nsigproc        https://github.com/SixByNine/sigproc\nTempo          http://tempo.sourceforge.net/\nRFIClean       https://github.com/ymaan4/RFIClean\nYAPP           https://github.com/jayanthc/yapp\nyour           https://github.com/thepetabyteproject/your\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGet with\n\u003ccode\u003esingularity pull --arch amd64 library://josephwkania/radio_transients/radio_transients:latest\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e*One of FETCH\u0027s dependencies causes PRESTO\u0027s Python scripts to fail.\nThis necessitated putting them in different environments.\nEverything except for PRESTO is in \u003ccode\u003eRT\u003c/code\u003e, which is loaded by default.\nPRESTO is in \u003ccode\u003ePE\u003c/code\u003e, in the shell you can activate this\nwith \u003ccode\u003econda activate PE\u003c/code\u003e. If you need access outside the container,\nyou should use radio_transients:cpu, which has PRESTO in the default environment.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-radio_transients_cpu\" class=\"anchor\" href=\"#radio_transients_cpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients_cpu\u003c/h3\u003e\n\u003cp\u003eContains CPU based programs\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehtop\niqrm_apollo\njupyterlab   \nPRESTO\npsrcat\npysigproc\nriptide\nsigproc\nTempo \nRFIClean\nYAPP  \nyour\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGet with\n\u003ccode\u003esingularity pull --arch amd64 library://josephwkania/radio_transients/radio_transients:cpu\u003c/code\u003e\u003cbr\u003e\nThere is an arm version \u003ccode\u003eSingularity.arm\u003c/code\u003e,\n\u003ccode\u003esingularity pull --arch arm library://josephwkania/radio_transients/radio_transients:arm\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-radio_transients_gpu\" class=\"anchor\" href=\"#radio_transients_gpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eradio_transients_gpu\u003c/h3\u003e\n\u003cp\u003eContains gpu based programs\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eCUDA 10.2\nFETCH      \njupyterlab\nheimdall\n- dedisp\nhtop \npsrdada \npsrdada-python\nyour\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGet with\n\u003ccode\u003esingularity pull --arch amd64 library://josephwkania/radio_transients/radio_transients:gpu\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-use\" class=\"anchor\" href=\"#how-to-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to use\u003c/h3\u003e\n\u003cp\u003eYour \u003ccode\u003e$HOME\u003c/code\u003e automatically gets mounted.\nYou can mount a directory with \u003ccode\u003e-B /dir/on/host:/mnt\u003c/code\u003e, which will mount \u003ccode\u003e/dir/on/host\u003c/code\u003e to \u003ccode\u003e/mnt\u003c/code\u003e in the container.\u003c/p\u003e\n\u003cp\u003eFor the gpu processes, you must pass \u003ccode\u003e--nv\u003c/code\u003e when running singularity.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity shell --nv -B /data:/mnt radio_transients_gpu.sif\u003c/code\u003e\nwill mount \u003ccode\u003e/data\u003c/code\u003e to \u003ccode\u003e/mnt\u003c/code\u003e, give you GPU access, and drop you into the interactive shell.\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity exec --nv -B /data:/mnt radio_transients_gpu.sif your_heimdall.py -f /mnt/data.fil\u003c/code\u003e\nwill mount \u003ccode\u003e/data\u003c/code\u003e to \u003ccode\u003e/mnt\u003c/code\u003e, give you GPU access, and run your_heimdall.py without entering the container.\u003c/p\u003e\n\u003cp\u003eAll the Python scripts are installed in a Conda environment \u003ccode\u003eRT\u003c/code\u003e, this environment is automatically loaded.\u003c/p\u003e\n\u003cp\u003eYou can see the commits and corresponding dates by running \u003ccode\u003esingularity inspect radio_transients.sif\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sylabs-cloud\" class=\"anchor\" href=\"#sylabs-cloud\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSylabs Cloud\u003c/h3\u003e\n\u003cp\u003eThese are built on a E5 v3 family machine and uploaded to Sylabs Cloud at\n\u003ca href=\"https://cloud.sylabs.io/library/josephwkania/radio_transients/radio_transients\" rel=\"nofollow\"\u003ehttps://cloud.sylabs.io/library/josephwkania/radio_transients/radio_transients\u003c/a\u003e\nThey where last built on 21-Jun-2021\u003c/p\u003e\n\u003cp\u003eIf your processor your processor is significantly older than this, you may run into problems with\nthe older processor not having the whole instruction set needed. In this case, you should build\nuse singularity to build the image locally.\u003c/p\u003e\n\u003cp\u003eAn archival version of these (built 25-April-2021) are on Singularity Hub at:\n\u003ca href=\"https://singularity-hub.org/collections/5231\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/5231\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5231\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-improvements\" class=\"anchor\" href=\"#improvements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImprovements\u003c/h3\u003e\n\u003cp\u003eIf you come across bug or have suggestions for improvements, let me know or submit a pull request.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h3\u003e\n\u003cp\u003eTo Kshitij Aggarwal for bug reports and suggestions.\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 2,
    "topics": [
      "pulsars",
      "fast-radio-bursts",
      "psrdada",
      "radio-astronomy"
    ],
    "updated_at": 1624333236.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity.v0.0.5",
      "Singularity.v0.0.5c",
      "Singularity.v0.0.5a",
      "Singularity.v0.0.5e",
      "Singularity.v0.0.3b",
      "Singularity.0.0.2f",
      "Singularity.v0.0.5b",
      "Singularity.0.0.1d",
      "Singularity.0.0.2",
      "Singularity.v0.0.5g",
      "Singularity",
      "Singularity.0.0.2c",
      "Singularity.v0.0.5f",
      "Singularity.0.0.2d",
      "Singularity.v0.0.5d",
      "Singularity.0.0.1b",
      "Singularity.v0.0.4",
      "Singularity.0.0.2e",
      "Singularity.0.0.2b",
      "Singularity.v0.0.3c",
      "Singularity.v0.0.3a",
      "Singularity.0.0.2a",
      "Singularity.v0.0.3",
      "Singularity.v0.0.4c",
      "Singularity.v0.05a",
      "Singularity.0.0.2h",
      "Singularity.v0.0.3d",
      "Singularity.0.0.2g",
      "Singularity.v0.0.5h",
      "Singularity.0.0.1c"
    ],
    "full_name": "khanlab/tar2bids",
    "latest_release": "v0.0.5g",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-tar2bids\" class=\"anchor\" href=\"#tar2bids\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003etar2bids\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003eRuns dicom tarball(s) to BIDS conversion using heudiconv\nUsage tar2bids  \u0026lt;optional flags\u0026gt;   \u0026lt;in tar file(s)\u0026gt;\n\n Assumes tar files are generated by cfmm2tar or dicom2tar (to find PatientName)\n If this is not the case, please use the -T option.\n\n Optional flags (must appear before required arguments):\n\t-P \u0026lt;patient name search string\u0026gt; :  default:  *_{subject}\n\t-T \u0026lt;tar name search string\u0026gt; :  uses tarfile name instead of PatientName to search, e.g. \u0027{subject}\u0027 if \u0026lt;subject\u0026gt;.tar\n\t-o \u0026lt;output_dir\u0026gt; : default=./bids\n\t-N \u0026lt;num parallel cores\u0026gt; : default=0  (max cores)\n\t-h \u0026lt;heuristic.py\u0026gt; : default=cfmm_bold_rest.py\n\t-w \u0026lt;tempdir\u0026gt;  (--tempdir in heudiconv)\n\t-O \"\u0026lt;additional heudiconv options\u0026gt;\" : default=\n\n Available heuristic files:\n\tcfmm_base.py\n\tcfmm_bold_rest.py\n\tcfmm_PS_PRC_3T.py\n\tEPL14A_GE_3T.py\n\tEPL14B_3T.py\n\tGEvSE.py\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1624334230.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "yngvem/ntnu-analysis",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-code-for-autodelineation-experiments-on-mri-data\" class=\"anchor\" href=\"#code-for-autodelineation-experiments-on-mri-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCode for autodelineation experiments on MRI data\u003c/h1\u003e\n\u003cp\u003eStart by running \u003ccode\u003esetup.sh\u003c/code\u003e to download the singularity container\nThen, submit slurm jobs like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esbatch slurm.sh json/dice/dwi.json dwi_dice 200\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhich will load the setup from the \u003ccode\u003ejson/dice/dwi.json\u003c/code\u003e file, train for 200 epochs\nand store the results in the folder \u003ccode\u003e$HOME/logs/ntnu/dwi_dice/\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAlternatively, if your cluster does not have slurm installed, simply omit the \u003ccode\u003esbatch\u003c/code\u003e\npart of the call above, thus running\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./slurm.sh json/dice/dwi.json dwi_dice 200\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1614353943.0
  },
  {
    "data_format": 2,
    "description": "A BIDSapp for automated preprocessing of EEG data.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "C0C0AN/EEGprep",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-status\" class=\"anchor\" href=\"#singularity-status\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Status\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4930\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h1\u003e\n\u003cp\u003eThis repository includes a definition file for a singularity container \u003cem\u003eand\u003c/em\u003e instructions for starting up an instance on CENTOS in a HPC environment.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setting-up-the-singularity-instance\" class=\"anchor\" href=\"#setting-up-the-singularity-instance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetting up the Singularity Instance\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eSSH to the server\u003c/li\u003e\n\u003cli\u003eRun the following lines to create a new directory in the scratch drive and pull the desired container from the Singularity Hub (or other source):\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /panasas/scratch/grp-adamw/singularity/$USER\ncd /panasas/scratch/grp-adamw/singularity/$USER;\nsingularity pull -F shub://AdamWilsonLab/singularity-geospatial-r\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if you are downloading from github, use something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /panasas/scratch/grp-adamw/singularity/$USER;\nwget -O singularity-geospatial-r_latest.sif https://github.com/AdamWilsonLab/singularity-geospatial-r/releases/download/0.0.1/AdamWilsonLab-singularity-geospatial-r.latest.sif\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eCreate symlinks to singularity folder in project storage to prevent disk space problems in the home directory.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003emkdir -p /projects/academic/adamw/singularity/$USER/.singularity\nln -s /projects/academic/adamw/singularity/$USER/.singularity .singularity\n\n# Symlinks for RStudio\nmkdir -p /projects/academic/adamw/rstudio/$USER/rstudio\nmv .local/share/rstudio /projects/academic/adamw/rstudio/$USER/\n\nmkdir -p ~/.local/share\nln -s /projects/academic/adamw/rstudio/$USER/rstudio ~/.local/share/rstudio\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eRun the \u003ca href=\"https://github.com/AdamWilsonLab/singularity-geospatial-r/blob/main/singularity_start.sh\"\u003esingularity_start.sh\u003c/a\u003e script to start up a singularity instance. You can just copy paste the code into the terminal.  This includes a few system specific settings for the Buffalo CCR.  This should only need to be done once (as long as the instance keeps running, server is not restarted, etc.).  If the instance stops for any reason, you\u0027ll need to rerun this script.  You can confirm it\u0027s running with \u003ccode\u003esingularity instance list\u003c/code\u003e or by checking \u003ccode\u003ehtop\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-connecting-to-rstudio\" class=\"anchor\" href=\"#connecting-to-rstudio\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConnecting to RStudio\u003c/h2\u003e\n\u003cp\u003eAfter running the steps above, you should be able to do just the following to begin working.  If the server restarts you will need to re-run step 4 above.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eConnect to the instance via SSH with port Forwarding.  You will need to be on campus or connected via VPN.  See notes below for *nix and windows.\u003c/li\u003e\n\u003cli\u003eOpen RStudio at localhost:8787 in your local browser and login with user/password from #4 above.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container-geospatial-r\" class=\"anchor\" href=\"#singularity-container-geospatial-r\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Container: Geospatial R\u003c/h2\u003e\n\u003cp\u003eThis container builds upon the \u003ca href=\"https://hub.docker.com/r/rocker/geospatial\" rel=\"nofollow\"\u003erocker geospatial container\u003c/a\u003e, which I ported to \u003ca href=\"https://singularity-hub.org/collections/4908\" rel=\"nofollow\"\u003eSingularity here\u003c/a\u003e.  This repository/collection then \u003ca href=\"https://github.com/AdamWilsonLab/singularity-geospatial-r/blob/main/Singularity.latest\"\u003eadds additional packages in this file\u003c/a\u003e.  That\u0027s the file to modify if you want to add more linux packages, etc.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-connecting-via-ssh\" class=\"anchor\" href=\"#connecting-via-ssh\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConnecting via SSH\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-nix-systems-mac-and-linux\" class=\"anchor\" href=\"#nix-systems-mac-and-linux\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e*NIX systems (Mac and Linux)\u003c/h2\u003e\n\u003cp\u003eUse terminal to ssh to the server as explained in \u003ca href=\"https://github.com/AdamWilsonLab/singularity-geospatial-r/blob/main/singularity_start.sh\"\u003esingularity_start.sh\u003c/a\u003e.\nAdd something like the following to your .ssh/config file to simplify connecting with port forwarding via ssh.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eHost rserver\nHostName HOST\nLocalForward 8787 HOST:PORT_NUMBER\nUser adamw\nForwardX11 yes\nForwardAgent yes\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-windows\" class=\"anchor\" href=\"#windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWindows\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-putty-instructions\" class=\"anchor\" href=\"#putty-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePuTTY Instructions\u003c/h3\u003e\n\u003cp\u003eOn Windows you will need to use PuTTY or a similar terminal program.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIn PuTTY, enter the server address (host name) and \"22\" (port) on the \"Session\" tab.\u003c/li\u003e\n\u003cli\u003eOn the \"SSH/Tunnels\" tab, enter the port number of the rsession  under \u201cSource port\u201d and type in HOST:PORT (replace with the actual server IP address + the port number) as the destination address. Then, click \"Add\".\u003c/li\u003e\n\u003cli\u003eConnect and login as usual in the terminal.\u003c/li\u003e\n\u003cli\u003ePoint the web browser to \u003ccode\u003ehttp://localhost:PORT\u003c/code\u003e (where PORT is the port number)\" and log in with the user name and the previously generated password.\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-todos\" class=\"anchor\" href=\"#todos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTODOs\u003c/h1\u003e\n\u003col\u003e\n\u003cli\u003eSeparate container from startup and monitor script\u003c/li\u003e\n\u003cli\u003eSwitch to a docker image\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-development-notes\" class=\"anchor\" href=\"#development-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment Notes\u003c/h1\u003e\n\u003cp\u003eI started with \u003ca href=\"https://github.com/nickjer/singularity-rstudio/blob/master/.travis.yml\"\u003enickjer\u0027s very helpful example\u003c/a\u003e and updated it to pull from the geospatial version of the versioned rocker stack instead of the repository based R.  This should make it easier to keep up to date.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-errors\" class=\"anchor\" href=\"#errors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eErrors\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-unable-to-connect-to-service\" class=\"anchor\" href=\"#unable-to-connect-to-service\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUnable to connect to service\u003c/h3\u003e\n\u003cp\u003eThis error can appear in the web browser when connecting via localhost.  This can be caused by RStudio not being able to write session files in the right place.  Confirm that:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eThe directory \u003ccode\u003e/projects/academic/adamw/rstudio/$USER/rstudio\u003c/code\u003e exists\u003c/li\u003e\n\u003cli\u003eand is linked to \u003ccode\u003e~/.local/share/rstudio\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-could-not-acquire-revocation-list-file-lock\" class=\"anchor\" href=\"#could-not-acquire-revocation-list-file-lock\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCould not acquire revocation list file lock\u003c/h3\u003e\n\u003cp\u003eThe error \"Could not acquire revocation list file lock\" resolved with help from \u003ca href=\"https://www.gitmemory.com/issue/rocker-org/rocker-versioned/213/726807289\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-database-error-7\" class=\"anchor\" href=\"#database-error-7\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003edatabase error 7\u003c/h3\u003e\n\u003cp\u003eStarting in early 2021, something changed that resulted in the following error when starting a new instance:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eERROR database error 7 (sqlite3_statement_backend::loadOne: attempt to write a readonly database) [description: Could not delete expired revoked cookies from the database, description: Could not read revoked cookies from the database]; OCCURRED AT virtual rstudio::core::Error rstudio::core::database::Connection::execute(rstudio::core::database::Query\u0026amp;, bool*) src/cpp/core/Database.cpp:480; LOGGED FROM: int main(int, char* const*) src/cpp/server/ServerMain.cpp:729\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI solved this by binding an address outside the container to \u003ccode\u003e/var/lib/rstudio-server\u003c/code\u003e when starting the instance as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--bind $RSTUDIO_DB:/var/lib/rstudio-server\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003ccode\u003e$RSTUDIO_DB\u003c/code\u003e is just a path outside the container.  I got this idea from \u003ca href=\"https://community.rstudio.com/t/permissions-related-to-upgrade-to-rstudio-server-open-source-1-4/94256/3\" rel=\"nofollow\"\u003ethis post\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local-rocker-updates\" class=\"anchor\" href=\"#local-rocker-updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal rocker updates\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003edocker run -d -p 8787:8787 -e PASSWORD=really_clever_password -v ~/Documents:~/Documents rocker/rstudio\u003c/code\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-useful-links\" class=\"anchor\" href=\"#useful-links\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful Links\u003c/h1\u003e\n\u003cp\u003eA few links I found useful while developing this container\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\" rel=\"nofollow\"\u003ehttps://divingintogeneticsandgenomics.rbind.io/post/run-rstudio-server-with-singularity-on-hpc/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://hub.docker.com/r/rocker/geospatial\" rel=\"nofollow\"\u003ehttps://hub.docker.com/r/rocker/geospatial\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://singularity-hub.org/collections/4930\" rel=\"nofollow\"\u003ehttps://singularity-hub.org/collections/4930\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://pawseysc.github.io/singularity-containers/23-web-rstudio/index.html\" rel=\"nofollow\"\u003ehttps://pawseysc.github.io/singularity-containers/23-web-rstudio/index.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.rocker-project.org/use/singularity/\" rel=\"nofollow\"\u003ehttps://www.rocker-project.org/use/singularity/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003ehttps://github.com/grst/rstudio-server-conda/issues/3\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1606593422.0
  },
  {
    "data_format": 2,
    "description": "The Recommender Engine for Intelligent Transient Tracking",
    "filenames": [
      "Singularity"
    ],
    "full_name": "refitt/refitt",
    "latest_release": "0.18.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pytorch-singularity\" class=\"anchor\" href=\"#pytorch-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003epytorch-singularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/4939\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository contains Singularity definition files used for PyTorch development in the Sinzlab.\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 2,
    "topics": [
      "science",
      "astronomy",
      "distributed-systems",
      "machine-learning",
      "citizen-science",
      "open-source",
      "python"
    ],
    "updated_at": 1625632115.0
  },
  {
    "data_format": 2,
    "description": "Data generation, model training and evaluation pipelines for the cold-start setting.",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "MI-911/cold-start-framework",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-cold-start-framework\" class=\"anchor\" href=\"#cold-start-framework\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCold-start Framework\u003c/h1\u003e\n\u003cp\u003eData partitioning, model training and evaluation pipelines for the cold-start setting.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eWe have fully dockerized an evaluation pipeline, from downloading the most recent dataset to conducting interviews.\nThe pipeline was developed using Docker version 19.03.5-ce.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cp\u003eFrom a clean slate, run the pipeline by running the script \u003ccode\u003escripts/run_pipeline.sh\u003c/code\u003e. The pipeline will:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDownload the latest stable MindReader version and the related entities.\u003c/li\u003e\n\u003cli\u003ePartition the downloaded dataset into training (warm-start) and testing (cold-start).\u003c/li\u003e\n\u003cli\u003eRun all models on the partitioned dataset.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eWe recommend running the entire pipeline initially.\nFollowing this, one can run the experiments alone by running \u003ccode\u003escripts/run_interview.sh\u003c/code\u003e.\nNote that if changes are made to the code, the base image should be rebuilt by running \u003ccode\u003escripts/build_base.sh\u003c/code\u003e.\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 3,
    "topics": [
      "pipeline",
      "recommender-system",
      "cold-start",
      "evaluation-pipelines",
      "dataset",
      "interview",
      "hacktoberfest"
    ],
    "updated_at": 1607268817.0
  },
  {
    "data_format": 2,
    "description": "Face Recognition from Oak Ridge (FaRO) provides a well-defined server-client interface to a some of the best open source face recognition projects on the web. ",
    "filenames": [
      "services/rcnn/Singularity"
    ],
    "full_name": "ORNL/faro",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-faro-readme\" class=\"anchor\" href=\"#faro-readme\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFARO: Readme\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eFace Recognition from Oak Ridge (FaRO) provides a well-defined server-client\ninterface to some of the best open source face recognition projects on the\nweb.  The intention is to support an open platform for face recognition research\nand to provide a well-defined and modern baseline for face recognition accuracy.\u003cbr\u003e\nWhile many universities and independent developers have released high quality\nface recognition models, they often lack many useful features such as\nconfiguration management, easy to use interfaces, deployment tools, backend\ndatabases, and analysis tools that FaRO provides.\u003c/p\u003e\n\u003cp\u003eIn our research we have found that there are many high quality and open source\nface analysis and recognition algorithms available for research; however,\nend-to-end systems that can support larger systems or that can be retrained for niche\napplications are lacking. We hope FARO can fill some of those needs.\u003c/p\u003e\n\u003cp\u003eThe primary goals of this project are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCreate an easy to use foundation that can support complex face recognition systems.\u003c/li\u003e\n\u003cli\u003eProvide well-defined benchmark algorithms.\u003c/li\u003e\n\u003cli\u003eAllow for algorithm improvements via open source software and models and to support improvements using techniques like transfer learning.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eFaRO is designed as a client/server system to accomodate the need for high speed GPU\nhardware to support deep learning face processing.  GRPC calls are used to communicate\nwith the server components which allows the clients to be written in many languages and\nimplemented on a varity of computationally limited platforms such as cellphones or biometric\ncollection devices.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publications\" class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublications\u003c/h2\u003e\n\u003cp\u003eIf you use FARO for publications please cite as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e@misc{bolme2019faro,\n    title={{FaRO}: {FA}ce {R}ecognition From {O}ak ridge},\n    author={David S. Bolme and David C. Cornett III and Nisha Srinivas},\n    year={2019},\n    howpublished={https://github.com/ORNL/faro}\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-system-requirements\" class=\"anchor\" href=\"#system-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSystem Requirements:\u003c/h2\u003e\n\u003cp\u003eMany FaRO services should run nicely on limited hardware resources.  As we\nintegrate more deep learning algorithms, those may require GPUs and additional\nhardware.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSoftware: python3, virtualenv, cmake, wget\u003c/li\u003e\n\u003cli\u003ePython Libraries: see requirements.txt\u003c/li\u003e\n\u003cli\u003eNVidia GPU with 8GB of Ram - GTX Titan X/1070/1080 or better\u003c/li\u003e\n\u003cli\u003envidia-docker2 - supporting Cuda 9.0\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eThis is intended to get Dlib algorithm up and running quickly.  This is a good\nplace to start and will allow you to test the FaRO interface.  A few\ndependencies may be needed on a fresh Ubuntu installation including: cmake,\npython2, and python3.  The install scripts will download and install many other\ndependencies in the user directory as well as some large machine learning\nmodels.  To get some initial dependencies install:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo apt install cmake\n$ sudo apt install python2-dev\n$ sudo apt install python3-dev\n$ sudo apt install virtualenv\n$ sudo apt install wget\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFirst build the client environment and compile the proto interfaces.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ./build-env-universal.sh\n#For Mac users run - $echo \"export PYTHONPATH=`pwd`/src:$PYTHONPATH\" \u0026gt;\u0026gt; \"$HOME/.bash_profile\" - after running build-env-universal.sh\nif using virtualenv,\n    $ source env_faro_server/bin/activate\n\nif using conda,\n    $ source activate env_faro_server\n    or\n    $ conda activate env_faro_server\n\n$ ./build-proto.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn one terminal run the Dlib service.  When you do this for the first time it\nwill create a \"faro-storage\" directory and will download and extract the machine\nlearning models.  At the end it will print out messages for each started worker:\n\"Worker N Started.\"  By default the service is started on port localhost:50030.\u003c/p\u003e\n\u003cp\u003eIf using virtualenv,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source env_faro_server/bin/activate\n$ cd services/dlib\n$ ./run-dlib.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf using conda,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source activate env_faro_server or conda activate env_faro_server\n$ cd services/dlib\n$ ./run_dlib.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe VGG2Resnet model can also be run using similar commands, but only run one\nservice at a time unless you carefully configure the ports and check available\nmemory, etc.\u003c/p\u003e\n\u003cp\u003eIf using virtualenv,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source env_faro_server/bin/activate\n$ cd services/vggface2\n$ ./run-vgg2.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf using conda,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source activate env_faro_server or conda activate env_faro_server\n$ cd services/vggface2\n$ ./run_vgg2.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSimilarly, InsightFace algorithms can be executed using similar commands.\nFace detection is performed using RetinaFace and features are extracted using ArcFace.\nCurrently, InsightFace works only with 1 GPU and worker.\u003c/p\u003e\n\u003cp\u003eIf using virtualenv,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source env_faro_server/bin/activate \n$ cd services/arcface\n$ ./run_arcface.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf using conda,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source activate env_faro_server or conda activate env_faro_server    \n$ cd services/arcface\n$ ./run_arcface.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn a second terminal run client applications. For this you can use either the\n\"env_faro\" or \"env_faro_server\" environments.  Test scripts are available in\nthe test directory to test the workings of the different functionalities in FaRO.\u003c/p\u003e\n\u003cp\u003eTo test the scripts,\u003c/p\u003e\n\u003cp\u003eIf using virtualenv,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source env_faro/bin/activate\n$ cd tests\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf using conda,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ source activate env_faro or conda activate env_faro\n$ cd tests\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo test the detect functionality on images execute,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$./test_detect.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo test the detect functionality in videos execute,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$./test_detect_videos.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-with-pip\" class=\"anchor\" href=\"#install-with-pip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall With PIP\u003c/h2\u003e\n\u003cp\u003eThis is a simple way to add FaRO to the environment.  It should install everything needed to run client api calls, but it may not provide all the configurations or models needed to run services.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ pip install git+https://github.com/ORNL/faro.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-a-service-command-line\" class=\"anchor\" href=\"#run-a-service-command-line\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun a Service Command Line\u003c/h2\u003e\n\u003cp\u003eStarting python services can be done with a simple command line.  This will start the service specifying the port, the number of workers, and the algorithm.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ python -m faro.FaceService --port=localhost:50030 --worker-count=2 --algorithm=dlib\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-the-client-api\" class=\"anchor\" href=\"#using-the-client-api\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the Client API\u003c/h2\u003e\n\u003cp\u003eExamples can be found in the Notebooks directory.  The best place to start is the \u003ca href=\"https://github.com/ORNL/faro/blob/master/Notebooks/FaRO%20Client%20Usage.ipynb\"\u003eFaRO Client Usage notebook\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cp\u003eFaRO_Client_Face_Detection_Video_and_Images.ipynb\u003c/p\u003e\n\u003cp\u003eThe client can access the services using the FaRO command line interface. The CLI includes the following functions/commands\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#client environment has to be activated\n$ cd bin\n$ ./faro \n\nusage : ./faro \u0026lt;command\u0026gt; --help\nlist the commands to be used\nCommands:\n    flist - List the faces in a gallery.\n    detectExtract - Run face detection and template extraction.\n    glist - List the galleries on the service.\n    test - Process a probe and gallery directory and produce a distance matrix.\n    extractOnly - Only run face extraction and attribute extraction.\n    enroll - Extract faces and enroll faces in a gallery.\n    search - Search images for faces in a gallery.\n    detect - Only run face detection.\n    \n#to run detect command and find its input options execute,\n$./faro detect --help\n\nUsage: ./faro command [OPTIONS] [image] [image_directory] [video] [...]\n\nRun detection on a collection of images.\n\nOptions:\n  --version             show program\u0027s version number and exit\n  -h, --help            show this help message and exit\n  -v, --verbose         Print out more program information.\n  -n MAX_IMAGES, --max-images=MAX_IMAGES\n                        Process at N images and then stop.\n  --maximum-size=MAX_SIZE\n                        If too large, images will be scaled to have this\n                        maximum size. Default=1920\n\n  Detector Options:\n    Configuration for the face detector.\n\n    -d DETECTIONS_CSV, --detections-csv=DETECTIONS_CSV\n                        Save detection data to the file.\n    -a ATTRIBUTES_CSV, --attributes-csv=ATTRIBUTES_CSV\n                        Save attributes data to the file.\n    --detect-log=DETECT_LOG\n                        A directory for detection images.\n    --face-log=FACE_LOG\n                        A directory for faces.\n    -b, --best          Detect the \u0027best\u0027 highest scoring face in the image.\n    --detect-thresh=DETECT_THRESH\n                        The threshold for a detection.\n    --min-size=MIN_SIZE\n                        Faces with a height less that this will be ignored.\n    --attribute-filter=ATTRIBUTE_FILTER\n                        A comma separated list of filters example: \u0027Male\u0026gt;0.5\u0027\n\n  Connection Options:\n    Control the connection to the FaRO service.\n\n    --max-async=MAX_ASYNC\n                        The maximum number of asyncronous call to make at a\n                        time. Default=8\n    --max-message-size=MAX_MESSAGE_SIZE\n                        Maximum GRPC message size. Set to -1 for unlimited.\n                        Default=67108864\n    -p DETECT_PORT, --port=DETECT_PORT\n                        The port used for the recognition service.\n    --detect-port=DETECT_PORT\n                        The port used for the recognition service.\n    --recognition-port=REC_PORT\n                        The port used for the recognition service.\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Help\u003c/h2\u003e\n\u003cp\u003eWe currently have limited resources to support FaRO but will do our best to provide support.  If you encounter\nproblems please submit tickets to the issues list so that they can be properly tracked.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ORNL/faro/issues\"\u003ehttps://github.com/ORNL/faro/issues\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eWe would also like to see new features or fixes submitted as pull requests.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ORNL/faro/pulls\"\u003ehttps://github.com/ORNL/faro/pulls\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 9,
    "topics": [],
    "updated_at": 1626235710.0
  },
  {
    "data_format": 2,
    "description": "Batch Connect - OSC RStudio Server",
    "filenames": [
      "Singularity"
    ],
    "full_name": "OSC/bc_osc_rstudio_server",
    "latest_release": "v0.19.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-batch-connect---osc-rstudio-server\" class=\"anchor\" href=\"#batch-connect---osc-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBatch Connect - OSC RStudio Server\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a1b9af067a60a648ea0b5068b19ea4a14b09e232574dac90c50903719ef5cc5b/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6f73632f62635f6f73635f7273747564696f5f7365727665722e737667\" alt=\"GitHub Release\" data-canonical-src=\"https://img.shields.io/github/release/osc/bc_osc_rstudio_server.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAn interactive app designed for OSC OnDemand that launches an RStudio Server\nwithin an Owens batch job.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eThis Batch Connect app requires the following software be installed on the\n\u003cstrong\u003ecompute nodes\u003c/strong\u003e that the batch job is intended to run on (\u003cstrong\u003eNOT\u003c/strong\u003e the\nOnDemand node):\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.tacc.utexas.edu/research-development/tacc-projects/lmod\" rel=\"nofollow\"\u003eLmod\u003c/a\u003e 6.0.1+ or any other \u003ccode\u003emodule restore\u003c/code\u003e and \u003ccode\u003emodule load \u0026lt;modules\u0026gt;\u003c/code\u003e based\nCLI used to load appropriate environments within the batch job before\nlaunching the RStudio Server.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003ewithout Singularity\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e 3.3.2+ (earlier versions are untested but may work for you)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.rstudio.com/products/rstudio-server/\" rel=\"nofollow\"\u003eRStudio Server\u003c/a\u003e 1.0.136+ (earlier versions are untested by may work for you)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://proot-me.github.io/\" rel=\"nofollow\"\u003ePRoot\u003c/a\u003e 5.1.0+ (used to setup fake bind mount)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eor with Singularity\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e 2.4.2+\u003c/li\u003e\n\u003cli\u003eA Singularity image similar to \u003ca href=\"https://www.singularity-hub.org/collections/463\" rel=\"nofollow\"\u003enickjer/singularity-rstudio\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCorresponding module to launch the above Singularity image (see\n\u003ca href=\"https://github.com/nickjer/singularity-rstudio/blob/master/example_module/\"\u003eexample_module\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eUse git to clone this app and checkout the desired branch/version you want to\nuse:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003escl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git clone \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003erepo\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git checkout \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etag/branch\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou will not need to do anything beyond this as all necessary assets are\ninstalled. You will also not need to restart this app as it isn\u0027t a Passenger\napp.\u003c/p\u003e\n\u003cp\u003eTo update the app you would:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003edir\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git fetch\nscl \u003cspan class=\"pl-c1\"\u003eenable\u003c/span\u003e git19 -- git checkout \u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003etag/branch\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAgain, you do not need to restart the app as it isn\u0027t a Passenger app.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eFork it ( \u003ca href=\"https://github.com/OSC/bc_osc_rstudio_server/fork\"\u003ehttps://github.com/OSC/bc_osc_rstudio_server/fork\u003c/a\u003e )\u003c/li\u003e\n\u003cli\u003eCreate your feature branch (\u003ccode\u003egit checkout -b my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCommit your changes (\u003ccode\u003egit commit -am \u0027Add some feature\u0027\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003ePush to the branch (\u003ccode\u003egit push origin my-new-feature\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eCreate a new Pull Request\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDocumentation, website content, and logo is licensed under\n\u003ca href=\"https://creativecommons.org/licenses/by/4.0/\" rel=\"nofollow\"\u003eCC-BY-4.0\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCode is licensed under MIT (see LICENSE.txt)o\u003c/li\u003e\n\u003cli\u003eRStudio, Shiny and the RStudio logo are all registered trademarks of RStudio.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 4,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1626113067.0
  },
  {
    "data_format": 2,
    "description": "example builder for Singularity containers using Circle Continuous Integration, circle-ci",
    "filenames": [
      "Singularity",
      "Singularity.tag"
    ],
    "full_name": "singularityhub/circle-ci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-builder-circle-ci\" class=\"anchor\" href=\"#singularity-builder-circle-ci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Builder Circle-CI\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\".circleci/sregistry-circle.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\".circleci/sregistry-circle.png\" alt=\".circleci/sregistry-circle.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/singularityhub/circle-ci\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bb4face391e298efe97092a4f2484374be5bc661342e9575c47b82ada2df4772/68747470733a2f2f636972636c6563692e636f6d2f67682f73696e67756c61726974796875622f636972636c652d63692e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/singularityhub/circle-ci.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is a simple example of how you can achieve:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eversion control of your recipes\u003c/li\u003e\n\u003cli\u003eversioning to include image hash \u003cem\u003eand\u003c/em\u003e commit id\u003c/li\u003e\n\u003cli\u003ebuild of associated container and\u003c/li\u003e\n\u003cli\u003epush to a storage endpoint\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efor a reproducible build workflow.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should this be managed via Github?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGithub, by way of easy integration with continuous integration, is an easy way\nto have a workflow set up where multiple people can collaborate on a container recipe,\nthe recipe can be tested (with whatever testing you need), discussed in pull requests,\nand then finally pushed to your storage of choice or Singularity Registry.\nImportantly, you don\u0027t need to give your entire team manager permissions\nto the registry. An encrypted credential that only is accessible to\nadministrators can do the push upon merge of a discussed change.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should I use this instead of a service?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou could use a remote builder, but if you do the build in a continuous integration\nservice you get complete control over it. This means everything from the version of\nSingularity to use, to the tests that you run for your container. You have a lot more\nfreedom in the rate of building, and organization of your repository, because it\u0027s you\nthat writes the configuration. Although the default would work for most, you can\nedit the build, setup, and circle configuration file in the\n\u003ca href=\".circleci\"\u003e.circleci\u003c/a\u003e folder to fit your needs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003cp\u003eAdd your Singularity recipes to this repository, and edit the build commands in\nthe \u003ca href=\".circleci/build.sh\"\u003ebuild.sh\u003c/a\u003e file. This is where you can specify endpoints\n(Singularity Registry, Dropbox, Google Storage, AWS) along with container names\n(the uri) and tag. You can build as many recipes as you like, just add another line!\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e                               \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e recipe relative to repository base\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .circleci/build.sh Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .circleci/build.sh --uri collection/container --tag tacos --cli google-storage Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .circleci/build.sh --uri collection/container --cli google-drive Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .circleci/build.sh --uri collection/container --cli globus Singularity\u003c/span\u003e\n  - \u003cspan class=\"pl-s\"\u003e/bin/bash .circleci/build.sh --uri collection/container --cli registry Singularity\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor each client that you use, required environment variables (e.g., credentials to push,\nor interact with the API) must be defined in the (encrypted) Travis environment. To\nknow what variables to define, along with usage for the various clients, see\nthe \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclient specific pages\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-detailed-started\" class=\"anchor\" href=\"#detailed-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDetailed Started\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-0-fork-this-repository\" class=\"anchor\" href=\"#0-fork-this-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e0. Fork this repository\u003c/h3\u003e\n\u003cp\u003eYou can clone and tweak, but it\u0027s easiest likely to get started with our example\nfiles and edit them as you need.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-get-to-know-circleci\" class=\"anchor\" href=\"#1-get-to-know-circleci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Get to Know CircleCi\u003c/h3\u003e\n\u003cp\u003eWe will be working with \u003ca href=\"https://www.circleci.com\" rel=\"nofollow\"\u003eCircle CI\u003c/a\u003e. You can see\nexample builds for this \u003ca href=\"https://circleci.com/gh/singularityhub/circle-ci\" rel=\"nofollow\"\u003erepository here\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCircle offers \u003ca href=\"https://support.circleci.com/hc/en-us/articles/115015481128-Scheduling-jobs-cron-for-builds-\" rel=\"nofollow\"\u003escheduled builds\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eCircleCI also offers \u003ca href=\"https://circleci.com/docs/enterprise/gpu-configuration/\" rel=\"nofollow\"\u003eGPU Builders\u003c/a\u003e if you want/need that sort of thing.\u003c/li\u003e\n\u003cli\u003eIf you don\u0027t want to use the \u003ca href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003esregistry\u003c/a\u003e to push to Google Storage, Drive, Globus, Dropbox, or your personal Singularity Registry, CircleCI will upload your artifacts directly to your \u003ca href=\"https://circleci.com/docs/2.0/deployment-integrations/#section=deployment\" rel=\"nofollow\"\u003edeployment\u003c/a\u003e location of choice.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-add-your-recipes\" class=\"anchor\" href=\"#2-add-your-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Add your Recipe(s)\u003c/h3\u003e\n\u003cp\u003eFor the example here, we have a single recipe named \"Singularity\" that is provided\nas an input argument to the \u003ca href=\".circleci/build.sh\"\u003ebuild script\u003c/a\u003e. You could add another\nrecipe, and then of course call the build to happen more than once.\nThe build script will name the image based on the recipe, and you of course\ncan change this. Just write the path to it (relative to the repository base) in\nyour \u003ca href=\".circleci/config.yml\"\u003e.circleci/config.yml\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-configure-singularity\" class=\"anchor\" href=\"#3-configure-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Configure Singularity\u003c/h3\u003e\n\u003cp\u003eThe recipe uses the \u003ca href=\"https://circleci.com/orbs/registry/orb/singularity/singularity\" rel=\"nofollow\"\u003eSingularity Orb\u003c/a\u003e to install your chosen version of Singularity. If you want to change the version, just adjust\nthe parameter here:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e  - \u003cspan class=\"pl-ent\"\u003ebuild\u003c/span\u003e:\n      \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eSingularity 3.2.1 - Python 3\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003esingularity\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e3.2.1\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003esingularity-3\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003etrue\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe basic steps to \u003ca href=\".circleci/setup.sh\"\u003esetup\u003c/a\u003e the build are the following:\u003c/p\u003e\n\u003cp\u003eWe also install the \u003ca href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003esregistry client\u003c/a\u003e\nthat allows you to issue a command like \"sregistry push ...\" to upload a finished\nimage to one of your cloud / storage endpoints. In this basic example, the push won\u0027t happen,\nand you will just build an image using the CI.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-configure-the-build\" class=\"anchor\" href=\"#4-configure-the-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Configure the Build\u003c/h3\u003e\n\u003cp\u003eThe basic steps for the \u003ca href=\".circleci/build.sh\"\u003ebuild\u003c/a\u003e are the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRunning build.sh with no inputs will default to a recipe called \"Singularity\" in the base of the repository. You can provide an argument to point to a different recipe path, always relative to the base of your repository.\u003c/li\u003e\n\u003cli\u003eIf you want to define a particular unique resource identifier for a finished container (to be uploaded to your storage endpoint) you can do that with \u003ccode\u003e--uri collection/container\u003c/code\u003e. If you don\u0027t define one, a robot name will be generated.\u003c/li\u003e\n\u003cli\u003eYou can add \u003ccode\u003e--uri\u003c/code\u003e to specify a custom name, and this can include the tag, OR you can specify \u003ccode\u003e--tag\u003c/code\u003e to go along with a name without one. It depends on which is easier for you.\u003c/li\u003e\n\u003cli\u003eIf you add \u003ccode\u003e--cli\u003c/code\u003e then this is telling the build script that you have defined the \u003ca href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"nofollow\"\u003eneeded environment variables\u003c/a\u003e for your \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclient of choice\u003c/a\u003e and you want successful builds to be pushed to your storage endpoint. See \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003ehere\u003c/a\u003e for a list of current client endpoints, or roll your own!\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee the \u003ca href=\".circleci/config.yml\"\u003econfig.yml\u003c/a\u003e for examples of this build.sh command (commented out). If there is some cloud service that you\u0027d like that is not provided, please \u003ca href=\"https://www.github.com/singularityhub/sregistry-cli/issues\"\u003eopen an issue\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-5-connect-to-ci\" class=\"anchor\" href=\"#5-connect-to-ci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e5. Connect to CI\u003c/h3\u003e\n\u003cp\u003eIf you go to your \u003ca href=\"https://circleci.com/dashboard\" rel=\"nofollow\"\u003eCircle Dashboard\u003c/a\u003e you can usually select a Github organization (or user) and then the repository, and then click the toggle button to activate it to build on commit --\u0026gt; push.\u003c/p\u003e\n\u003cp\u003eThat\u0027s it for the basic setup! At this point, you will have a continuous integration service that will build your container from a recipe each time that you push. The next step is figuring out where you want to put the finished image(s), and we will walk through this in more detail.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-storage\" class=\"anchor\" href=\"#storage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStorage!\u003c/h2\u003e\n\u003cp\u003eOnce the image is built, where can you put it? An easy answer is to use the\n\u003ca href=\"https://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003eSingularity Global Client\u003c/a\u003e and\nchoose \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eone of the many clients\u003c/a\u003e\nto add a final step to push the image. You then use the same client to pull the\ncontainer from your host. Once you\u0027ve decided which endpoints you want to push to,\nyou will need to:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSave the credentials / other environment variables that your client needs (see the client settings page linked in the sregistry docs above) to your \u003ca href=\"https://circleci.com/docs/2.0/env-vars/\" rel=\"nofollow\"\u003erepository settings\u003c/a\u003e where they will be encrypted and in the environment.\u003c/li\u003e\n\u003cli\u003eAdd a line to your \u003ca href=\".circleci/config.yml\"\u003e.circleci/config.yml\u003c/a\u003e to do an sregistry push action to the endpoint(s) of choice. We have provided some (commented out) examples to get you started.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eRemember that you can also take advantage of deployment options that CircleCI offers, or do any other action that you might want for the reproducibility or archive of metadata of your builds. We save the build folder as an artifact to the repository, but the containers might be too big to do this.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-advanced-usage\" class=\"anchor\" href=\"#advanced-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdvanced Usage\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThis setup can work as an analysis node as well! Try setting up a \u003ca href=\"https://support.circleci.com/hc/en-us/articles/115015481128-Scheduling-jobs-cron-for-builds-\" rel=\"nofollow\"\u003escheduled build\u003c/a\u003e to build a container that processes some information feed, and you have a regularly scheduled task.\u003c/li\u003e\n\u003cli\u003erun builds in parallel and test different building environments. You could try building the \"same\" container across different machine types and see if you really do get the same thing :)\u003c/li\u003e\n\u003cli\u003eYou can also do other sanity checks like testing if the container runs as you would expect, etc.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 2,
    "topics": [
      "singularity",
      "builder",
      "circle-ci",
      "singularity-ci"
    ],
    "updated_at": 1561654292.0
  },
  {
    "data_format": 2,
    "description": "BLADE: Bayesian Log-normAl DEconvolution for enhanced in silico microdissection of bulk gene expression data",
    "filenames": [
      "Singularity"
    ],
    "full_name": "tgac-vumc/BLADE",
    "latest_release": null,
    "readme": "\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://github.com/tgac-vumc/BLADE/blob/master/logo_final_small.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"254\" height=\"281\" src=\"https://github.com/tgac-vumc/BLADE/raw/master/logo_final_small.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-blade-bayesian-log-normal-deconvolution\" class=\"anchor\" href=\"#blade-bayesian-log-normal-deconvolution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBLADE: Bayesian Log-normAl DEconvolution\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://www.python.org/downloads/release/python-360/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8e26ba5220a7019a30342315ff5cc4989f91e698fdfe73a41476dd57524385d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e362d626c75652e737667\" alt=\"Python 3.6\" data-canonical-src=\"https://img.shields.io/badge/python-3.6-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://badge.fury.io/py/BLADE-Deconvolution\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/09a5fb429236fe0ed8858f2cd7a0d424aa6a58af4fa0bdfda86aa97e8409c118/68747470733a2f2f62616467652e667572792e696f2f70792f424c4144452d4465636f6e766f6c7574696f6e2e737667\" alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/BLADE-Deconvolution.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/4861\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://mybinder.org/v2/gh/tgac-vumc/BLADE/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\" alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBLADE (Bayesian Log-normAl DEconvolution) was designed to jointly estimate cell type composition and gene expression profiles per cell type in a single-step while accounting for the observed gene expression variability in single-cell RNA-seq data.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://github.com/tgac-vumc/BLADE/blob/master/framework.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"100%\" height=\"100%\" src=\"https://github.com/tgac-vumc/BLADE/raw/master/framework.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp\u003eBLADE framework. To construct a prior knowledge of BLADE, we used single-cell sequencing data. Cell are subject to phenotyping, clustering and differential gene expression analysis. Then, for each cell type, we retrieve average expression profiles (red cross and top heatmap), and standard deviation per gene (blue circle and bottom heatmap). This prior knowledge is then used in the hierarchical Bayesian model (bottom right) to deconvolute bulk gene expression data.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-demo-notebook-is-available-here-you-can-also-run-the-demo-using-binder\" class=\"anchor\" href=\"#demo-notebook-is-available-here-you-can-also-run-the-demo-using-binder\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo notebook is available \u003ca href=\"https://github.com/tgac-vumc/BLADE/blob/master/jupyter/BLADE%20-%20Demo%20script.ipynb\"\u003ehere\u003c/a\u003e. You can also run the demo using \u003ca href=\"https://mybinder.org/v2/gh/tgac-vumc/BLADE/master\" rel=\"nofollow\"\u003eBinder\u003c/a\u003e.\u003c/h4\u003e\n\u003cp\u003eNote that for the testing on Binder, parallel processing has to be disabled by setting \u003ccode\u003eNjob\u003c/code\u003e to 1. BLADE significantly performs better with high number of cores, epecially when \u003ccode\u003eNsample\u003c/code\u003e, \u003ccode\u003eNgene\u003c/code\u003e and \u003ccode\u003eNcell\u003c/code\u003e is high. In case of Binder, we recommend the following setting:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eNcell=3\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eNgene=50\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eNsample=10\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt takes about 30 minutes to complete the demo execution on Binder.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-system-requirements\" class=\"anchor\" href=\"#system-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSystem Requirements\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hardware-requirements\" class=\"anchor\" href=\"#hardware-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHardware Requirements\u003c/h3\u003e\n\u003cp\u003eBLADE can run on the minimal computer spec, such as Binder (1 CPU, 2GB RAM on Google Cloud), when data size is small. However, BLADE can significantly benefit from the larger amount of CPUs and RAM. Empirical Bayes procedure of BLADE runs independent optimization procedure that can be parallelized. In our evaluation, we used a computing node with the following spec:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e40 threads (Xeon 2.60GHz)\u003c/li\u003e\n\u003cli\u003e128 GB RAM\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-os-requirements\" class=\"anchor\" href=\"#os-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOS Requirements\u003c/h3\u003e\n\u003cp\u003eThe package development version is tested on Linux operating systems. (CentOS 7 and Ubuntu 16.04).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-pip\" class=\"anchor\" href=\"#using-pip\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing pip\u003c/h3\u003e\n\u003cp\u003eThe python package of BLADE is available on pip.\nYou can simply (takes only \u0026lt;1min):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip install BLADE_Deconvolution\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe tested BLADE with \u003ccode\u003epython =\u0026gt; 3.6\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-conda\" class=\"anchor\" href=\"#using-conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Conda\u003c/h3\u003e\n\u003cp\u003eOne can create a conda environment contains BLADE and also other dependencies to run \u003ca href=\"https://github.com/tgac-vumc/BLADE/blob/master/jupyter/BLADE%20-%20Demo%20script.ipynb\"\u003eDemo\u003c/a\u003e.\nThe environment definition is in \u003ca href=\"https://github.com/tgac-vumc/BLADE/environment.yml\"\u003eenvironment.yml\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1-installing-miniconda-3\" class=\"anchor\" href=\"#step-1-installing-miniconda-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1: Installing Miniconda 3\u003c/h3\u003e\n\u003cp\u003eFirst, please open a terminal or make sure you are logged into your Linux VM. Assuming that you have a 64-bit system, on Linux, download and install Miniconda 3 with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\nbash Miniconda3-latest-Linux-x86_64.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOn MacOS X, download and install with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl https://repo.continuum.io/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -o Miniconda3-latest-MacOSX-x86_64.sh\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2-create-a-conda-environment\" class=\"anchor\" href=\"#step-2-create-a-conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2: Create a conda environment\u003c/h3\u003e\n\u003cp\u003eYou can install all the necessary dependency using the following command (may takes few minutes).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create --file environment.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen, the \u003ccode\u003eBLADE\u003c/code\u003e environment can be activate by:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda activate BLADE\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-singularity\" class=\"anchor\" href=\"#using-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing Singularity\u003c/h3\u003e\n\u003cp\u003eIf you have Singularity, you can simply pull the singularity container with all dependency resolved (in few minutes, depends on the network speed).\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://tgac-vumc/BLADE\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview-of-blade\" class=\"anchor\" href=\"#overview-of-blade\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview of BLADE\u003c/h2\u003e\n\u003cp\u003eIn the BLADE package, you can load the following functions and modules.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eBLADE\u003c/code\u003e: A class object contains core algorithms of \u003ccode\u003eBLADE\u003c/code\u003e. Users can reach internal variables (\u003ccode\u003eNu\u003c/code\u003e, \u003ccode\u003eOmega\u003c/code\u003e, and \u003ccode\u003eBeta\u003c/code\u003e) and functions for calculating objective functions (ELBO function) and gradients with respect to the variational parameters. There also is an optimization function (\u003ccode\u003eBLADE.Optimize()\u003c/code\u003e) for performing L-BFGS optimization. Though this is the core, we also provide a more accessible function (\u003ccode\u003eBLADE_framework\u003c/code\u003e) that performs deconvolution. See below to obtain the current estimate of cellualr fractions, gene expression profiles per cell type and per sample:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eExpF(self.Beta)\u003c/code\u003e : returns a \u003ccode\u003eNsample\u003c/code\u003e by \u003ccode\u003eNgene\u003c/code\u003e matrix contains estimated fraction of each cell type in each sample.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eself.Nu\u003c/code\u003e: a \u003ccode\u003eNsample\u003c/code\u003e by \u003ccode\u003eNgene\u003c/code\u003e by \u003ccode\u003eNcell\u003c/code\u003e multidimensional array contains estimated gene expression levels of each gene in each cell type for each sample.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003enumpy.mean(self.Nu,0)\u003c/code\u003e: To obtain a estimated gene expression profile per cell type, we can simply take an average across the samples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eFramework\u003c/code\u003e: A framework based on the \u003ccode\u003eBLADE\u003c/code\u003e class module above. Users need to provide the following input/output arguments.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eInput arguments\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eX\u003c/code\u003e: a \u003ccode\u003eNgene\u003c/code\u003e by \u003ccode\u003eNcell\u003c/code\u003e matrix contains average gene expression profiles per cell type (a signature matrix) in log-scale.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003estdX\u003c/code\u003e: a \u003ccode\u003eNgene\u003c/code\u003e by \u003ccode\u003eNcell\u003c/code\u003e matrix contains standard deviation per gene per cell type (a signature matrix of gene expression variability).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eY\u003c/code\u003e: a \u003ccode\u003eNgene\u003c/code\u003e by \u003ccode\u003eNsample\u003c/code\u003e matrix contains bulk gene expression data. This should be in linear-scale data without log-transformation.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eInd_Marker\u003c/code\u003e: Index for marker genes. By default, \u003ccode\u003e[True]*Ngene\u003c/code\u003e (all genes used without filtering). For the genes with \u003ccode\u003eFalse\u003c/code\u003e they are excluded in the first phase (Empirical Bayes) for finidng the best hyperparameters.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eInd_sample\u003c/code\u003e: Index for the samples used in the first phase (Empirical Bayes). By default, \u003ccode\u003e[True]*Nsample\u003c/code\u003e (all samples used).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAlphas\u003c/code\u003e, \u003ccode\u003eAlpha0s\u003c/code\u003e, \u003ccode\u003eKappa0s\u003c/code\u003e and \u003ccode\u003eSYs\u003c/code\u003e: all possible hyperparameters considered in the phase of Empirical Bayes. A default parameters are offered as described in the manuscript (to appear): \u003ccode\u003eAlphas=[1,10]\u003c/code\u003e, \u003ccode\u003eAlpha0s=[0.1, 1, 5]\u003c/code\u003e, \u003ccode\u003eKappa0s=[1,0.5,0.1]\u003c/code\u003e and \u003ccode\u003eSYs=[1,0.3,0.5]\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNrep\u003c/code\u003e: Number of repeat for evaluating each parameter configuration in Empirical Bayes phase. By default, \u003ccode\u003eNrep=3\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNrepfinal\u003c/code\u003e: Number of repeated optimizations for the final parameter set. By default, \u003ccode\u003eNrepfinal=10\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eNjob\u003c/code\u003e: Number of jobs executed in parallel. By default, \u003ccode\u003eNjob=10\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eOutput values\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003efinal_obj\u003c/code\u003e: A final \u003ccode\u003eBLADE\u003c/code\u003e object with optimized variational parameters and hyperparameters.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ebest_obj\u003c/code\u003e: The best object form Empirical Bayes step. If no genes and samples are filtered, \u003ccode\u003ebest_obj\u003c/code\u003e is the same as \u003ccode\u003efinal_obj\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ebest_set\u003c/code\u003e: A list contains the hyperparameters selected in the Empirical Bayes step.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eAll_out\u003c/code\u003e: A list of \u003ccode\u003eBLADE\u003c/code\u003e objects from the Empirical Bayes step.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eBLADE_job\u003c/code\u003e/\u003ccode\u003eOptimize\u003c/code\u003e: Internal functions used by \u003ccode\u003eFramework\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1625660549.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ctpelok77/kstar",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-welcome-to-the-page-of-k-planner----a-state-of-the-art-top-k-planner-integrating-the-k-algorithm-into-fast-downward\" class=\"anchor\" href=\"#welcome-to-the-page-of-k-planner----a-state-of-the-art-top-k-planner-integrating-the-k-algorithm-into-fast-downward\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWelcome to the page of K* planner -- a state of the art Top-k planner integrating the K* algorithm into Fast Downward.\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e## Suggested build for 64bit\n\n./build.py release64\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e# ./fast-downward.py --build release64 \u0026lt;domain_file\u0026gt; \u0026lt;problem_file\u0026gt; --search \"kstar(heuristic,k=\u0026lt;number-of-plans\u0026gt;)\"\n\n./fast-downward.py --build release64 examples/gripper/domain.pddl examples/gripper/prob01.pddl --search \"kstar(blind(),k=100)\"\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eheurisitic\u003c/em\u003e:  any heuristic provided by Fast Downward\u003cbr\u003e\n(\u003ca href=\"http://www.fast-downward.org/Doc/Heuristic\" rel=\"nofollow\"\u003ehttp://www.fast-downward.org/Doc/Heuristic\u003c/a\u003e).\u003cbr\u003e\n\u003cstrong\u003eDisclaimer\u003c/strong\u003e: Optimality of K* is only guaranteed with an admissible and consistent heuristic.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h3\u003e\n\u003cp\u003eMichael Katz, Shirin Sohrabi, Octavian Udrea and Dominik Winterer\u003cbr\u003e\n\u003cstrong\u003eA Novel Iterative Approach to Top-k Planning\u003c/strong\u003e \u003ca href=\"https://www.aaai.org/ocs/index.php/ICAPS/ICAPS18/paper/download/17749/16971\" rel=\"nofollow\"\u003e[pdf]\u003c/a\u003e \u003ca href=\"/top_k.bib\"\u003e[bib]\u003c/a\u003e\u003cbr\u003e\n\u003cem\u003eIn ICAPS 2018\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h3\u003e\n\u003cp\u003eFor questions and comments please get in touch with Michael Katz (\u003ca href=\"mailto:michael.katz1@ibm.com\"\u003emichael.katz1@ibm.com\u003c/a\u003e).\u003c/p\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1617563735.0
  },
  {
    "data_format": 2,
    "description": "Popular Deep RL algorithms implemented in PyTorch",
    "filenames": [
      "Singularity"
    ],
    "full_name": "jkulhanek/deep-rl-pytorch",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-deep-rl-pytorch\" class=\"anchor\" href=\"#deep-rl-pytorch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeep RL PyTorch\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2581\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repo contains implementation of popular Deep RL algorithms. Furthermore it contains unified interface for training and evaluation with unified model saving and visualization. It can be used as a good starting point when implementing new RL algorithm in PyTorch.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting started\u003c/h2\u003e\n\u003cp\u003eIf you want to base your algorithm on this repository, start by installing it as a package\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip install git+https://github.com/jkulhanek/deep-rl-pytorch.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you want to run attached experiments yourself, feel free to clone this repository.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/jkulhanek/deep-rl-pytorch.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAll dependencies are prepared in a docker container. If you have nvidia-docker enabled, you can use this image. To pull and start the image just run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run --runtime=nvidia --net=host -it kulhanek/deep-rl-pytorch:latest bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFrom there, you can either clone your own repository containing your experiments or clone this one.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-concepts\" class=\"anchor\" href=\"#concepts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConcepts\u003c/h2\u003e\n\u003cp\u003eAll algorithms are implemented as base classes. In your experiment your need to subclass from those base classes. The \u003ccode\u003edeep_rl.core.AbstractTrainer\u003c/code\u003e class is used for all trainers and all algorithms inherit this class. Each trainer can be wrapped in several wrappers (classes extending \u003ccode\u003edeep_rl.core.AbstractWrapper\u003c/code\u003e). Those wrappers are used for saving, logging, terminating the experiment and etc. All experiments should be registered using \u003ccode\u003e@deep_rl.register_trainer\u003c/code\u003e decorator. This decorator than wraps the trainer with default wrappers. This can be controlled by passing arguments to the decorator. All registered trainers (experiments) can be run by calling \u003ccode\u003edeep_rl.make_trainer(\u0026lt;\u0026lt;name\u0026gt;\u0026gt;).run()\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-implemented-algorithms\" class=\"anchor\" href=\"#implemented-algorithms\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eImplemented algorithms\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-a2c\" class=\"anchor\" href=\"#a2c\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA2C\u003c/h3\u003e\n\u003cp\u003eA2C is a synchronous, deterministic variant of Asynchronous Advantage Actor Critic (A3C) [2] which according to OpenAI [1] gives equal performance. It is however more efficient for GPU utilization.\u003c/p\u003e\n\u003cp\u003eStart your experiment by subclassing \u003ccode\u003edeep_rl.a2c.A2CTrainer\u003c/code\u003e.\nSeveral models are included in \u003ccode\u003edeep_rl.a2c.model\u003c/code\u003e. You may want to use at least some helper modules contained in this package when designing your own experiment.\u003c/p\u003e\n\u003cp\u003eIn most of the models, initialization is done according to [3].\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-asynchronous-advantage-actor-critic-a3c-2\" class=\"anchor\" href=\"#asynchronous-advantage-actor-critic-a3c-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAsynchronous Advantage Actor Critic (A3C) [2]\u003c/h3\u003e\n\u003cp\u003eThis implementation uses multiprocessing. It comes with two optimizers - RMSprop and Adam.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-actor-critic-using-kronecker-factored-trust-region-acktr-1\" class=\"anchor\" href=\"#actor-critic-using-kronecker-factored-trust-region-acktr-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eActor Critic using Kronecker-Factored Trust Region (ACKTR) [1]\u003c/h3\u003e\n\u003cp\u003eThis is an improvement of A2C described in [1].\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-experiments\" class=\"anchor\" href=\"#experiments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExperiments\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eComming soon\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cp\u003eThose packages must be installed before using the framework for your own algorithm:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOpenAI baselines (can be installed by running \u003ccode\u003epip install git+https://github.com/openai/baselines.git\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003ePyTorch\u003c/li\u003e\n\u003cli\u003eVisdom (\u003ccode\u003epip install visdom\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eGym (\u003ccode\u003epip install gym\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eMatPlotLib\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThose packages must be installed prior running experiments:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeepMind Lab\u003c/li\u003e\n\u003cli\u003eGym[atari]\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sources\" class=\"anchor\" href=\"#sources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSources\u003c/h2\u003e\n\u003cp\u003eThis repository is based on work of several other authors. We would like to express our thanks.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/openai/baselines/tree/master/baselines\"\u003ehttps://github.com/openai/baselines/tree/master/baselines\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/a2c_ppo_acktr\"\u003ehttps://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/a2c_ppo_acktr\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/miyosuda/unreal\"\u003ehttps://github.com/miyosuda/unreal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/openai/gym\"\u003ehttps://github.com/openai/gym\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-references\" class=\"anchor\" href=\"#references\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReferences\u003c/h2\u003e\n\u003cp\u003e[1] Wu, Y., Mansimov, E., Grosse, R.B., Liao, S. and Ba, J., 2017. Scalable trust-region method for deep reinforcement learning using kronecker-factored approximation. In Advances in neural information processing systems (pp. 5279-5288).\u003c/p\u003e\n\u003cp\u003e[2] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement learning. In International conference on machine learning (pp. 1928-1937).\u003c/p\u003e\n\u003cp\u003e[3] Saxe, A.M., McClelland, J.L. and Ganguli, S., 2013. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120.\u003c/p\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1626571356.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "csiro-crop-informatics/nextflow-embl-abr-webinar",
    "latest_release": "v1.2",
    "readme": "\u003cp\u003eThis repository contains information for the EMBL-ABR webinar on \"Nextflow: Scalable, Sharable and Reproducible Computational Workflows across Clouds and Clusters\" presented by Rad Suchecki on 14th March 2019.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-webinar-details\" class=\"anchor\" href=\"#webinar-details\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWebinar details\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e\nLarge analysis workflows are fragile ecosystems of software tools, scripts and dependencies. This complexity commonly makes these workflows not only irreproducible but sometimes even not re-runnable outside their original development environment. Nextflow is a reactive workflow framework and a domain specific programming language which follows the dataflow paradigm and offers an alternative, and arguably superior, approach to developing, executing and sharing pipelines.\u003c/p\u003e\n\u003cp\u003eIn this webinar we will follow the steps required for developing sharable, version controlled, container-backed workflows, which can be seamlessly executed across different environments from a laptop to cluster to cloud. We will do this by leveraging Nextflow\u2019s integration with code and container image hosting services such as GitHub and Docker Hub, and out of the box support for various HPC cluster schedulers and the Amazon AWS cloud.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDate/time:\u003c/strong\u003e Thursday 14 March 2019 13:00-14:00 AEDT /12:00-13:00 AEST\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003ePresenter:\u003c/strong\u003e \u003ca href=\"https://orcid.org/0000-0003-4992-9497\" rel=\"nofollow\"\u003eRad Suchecki\u003c/a\u003e, CSIRO Crop Bioinformatics and Data Science\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://twitter.com/bioinforad\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/059d6c1e6596889bce7982a4745bea213207aae7fa1cd8a3053ed1e6b3f5190f/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f62696f696e666f7261642e7376673f7374796c653d736f6369616c\" alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/bioinforad.svg?style=social\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRegistration:\u003c/strong\u003e \u003cdel\u003e\u003ca href=\"https://attendee.gotowebinar.com/register/8408436403729692931\" rel=\"nofollow\"\u003ehttps://attendee.gotowebinar.com/register/8408436403729692931\u003c/a\u003e\u003c/del\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eVideo link:\u003c/strong\u003e \u003ca href=\"https://www.youtube.com/channel/UC5WlFNBSfmt3e8Js8o2fFqQ\" rel=\"nofollow\"\u003eEMBL-ABR YouTube Channel\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.youtube.com/watch?v=lqm-VV5dOgk\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/72039949253c34d78d283c828b267bdbec41479ad5b39928f41704a9182e4f5c/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6c716d2d565635644f676b2f687164656661756c742e6a7067\" alt=\"Nextflow Webinar Video\" data-canonical-src=\"http://img.youtube.com/vi/lqm-VV5dOgk/hqdefault.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSlides\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://csiro-crop-informatics.github.io/nextflow-embl-abr-webinar/nextflow-embl-abr.html\" rel=\"nofollow\"\u003ehttps://csiro-crop-informatics.github.io/nextflow-embl-abr-webinar/nextflow-embl-abr.html\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-data-for-the-webinar\" class=\"anchor\" href=\"#data-for-the-webinar\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData for the Webinar\u003c/h1\u003e\n\u003cp\u003eFor the purpose of demonstrating a Nextflow workflow in reasonable time, we will use the dataset used in \u003ca href=\"https://github.com/UofABioinformaticsHub/2019_EMBL-ABR_Snakemake_webinar#data-for-the-webinar\"\u003ethis Snakemake webinar\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-tutorial\" class=\"anchor\" href=\"#tutorial\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTutorial\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"nextflow-tutorial.md\"\u003enextflow-tutorial.md\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1568158524.0
  },
  {
    "data_format": 2,
    "description": "various singularity recipes for FSL",
    "filenames": [
      "Singularity.5.0.9",
      "Singularity.6.0.2-Cuda8",
      "Singularity.6.0.3",
      "Singularity.6.0.1",
      "Singularity.5.0.11",
      "Singularity.6.0.0",
      "Singularity.6.0.2-Cuda8-xtract_viewer",
      "Singularity.5.0.10",
      "Singularity.5-Cuda8",
      "Singularity.6.0.4-Cuda8",
      "Singularity.6.0.4",
      "Singularity.6.0.2"
    ],
    "full_name": "MPIB/singularity-fsl",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/702\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# Download a (versioned) container\nsingularity pull shub://MPIB/singularity-fsl:6.0.4\n\n# Run it\nsingularity exec singularity-fsl_6.0.4.sif fslmaths\nsingularity exec --nv singularity-fsl_6.0.4.sif eddy_cuda9.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-fsl\" class=\"anchor\" href=\"#fsl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFSL\u003c/h2\u003e\n\u003cp\u003eProject Home: \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/\" rel=\"nofollow\"\u003ehttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThese are containers primarily used at the MPI for Human Development.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cuda\" class=\"anchor\" href=\"#cuda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCuda\u003c/h2\u003e\n\u003cp\u003eStarting with Singularity 6.0.2 we include Nvidia CUDA through Debian backports repositories.\nMake sure your Nvidia driver on the host \u003ca href=\"https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility\" rel=\"nofollow\"\u003esupports it\u003c/a\u003e and add the \u003ccode\u003e--nv\u003c/code\u003e flag with singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-note\" class=\"anchor\" href=\"#note\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNote\u003c/h2\u003e\n\u003cp\u003ePlease be aware of FSL\u0027s strict license regarding non-commercial use.\u003c/p\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 3,
    "topics": [
      "containers",
      "science"
    ],
    "updated_at": 1622785432.0
  },
  {
    "data_format": 2,
    "description": "Nextflow Pipeline for the analysis of Double Progressive Alignment (DPA)",
    "filenames": [
      "singularity/Singularity",
      "singularity/.ipynb_checkpoints/Singularity-checkpoint"
    ],
    "full_name": "evanfloden/dpa-analysis",
    "latest_release": "v0.2.6",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fast-and-accurate-large-multiple-sequence-alignments-using-root-to-leave-regressive-computation\" class=\"anchor\" href=\"#fast-and-accurate-large-multiple-sequence-alignments-using-root-to-leave-regressive-computation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFast and accurate large multiple sequence alignments using root-to-leave regressive computation\u003c/h1\u003e\n\u003cp\u003eThis repository contains data, documentation, analysis and Nextflow workflow for the manuscript \"Fast and accurate large multiple sequence alignments using root-to-leave regressive computation\".\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-for-details-on-how-to-use-the-regressive-multiple-sequence-alignment-method-see-the-t-coffee-documentation\" class=\"anchor\" href=\"#for-details-on-how-to-use-the-regressive-multiple-sequence-alignment-method-see-the-t-coffee-documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor details on how to use the Regressive Multiple Sequence Alignment method, see the \u003ca href=\"https://tcoffee.readthedocs.io/en/latest/tcoffee_quickstart_regressive.html\" rel=\"nofollow\"\u003eT-Coffee documentation\u003c/a\u003e.\u003c/h4\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h3\u003e\n\u003cp\u003eThis workflow was written by Evan Floden (\u003ca href=\"https://github.com/evanfloden\"\u003eevanfloden\u003c/a\u003e) and\nEdgar(\u003ca href=\"https://github.com/edgano\"\u003eedgano\u003c/a\u003e) at the \u003ca href=\"http://www.crg.eu\" rel=\"nofollow\"\u003eCenter for Genomic Regulation (CRG)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe authors who contributed to the analysis and manuscript are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eEdgar Garriga Nogales\u003c/li\u003e\n\u003cli\u003ePaolo Di Tommaso\u003c/li\u003e\n\u003cli\u003eCedrik Magis\u003c/li\u003e\n\u003cli\u003eIonas Erb\u003c/li\u003e\n\u003cli\u003eHafid Laayouni\u003c/li\u003e\n\u003cli\u003eFyodor Kondrashov\u003c/li\u003e\n\u003cli\u003eEvan Floden\u003c/li\u003e\n\u003cli\u003eCedric Notredame\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-notebooks\" class=\"anchor\" href=\"#notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotebooks\u003c/h3\u003e\n\u003cp\u003eThis repository contains a series of \u003ca href=\"http://jupyter.org/\" rel=\"nofollow\"\u003eJupyter Notebooks\u003c/a\u003e that contain\nthe steps for replicating the analysis, tables and figures in the manuscript.\u003c/p\u003e\n\u003cp\u003eThe index jupyter notebook can be found \u003ca href=\"notebook/00_StartHere.ipynb\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe notebook executes the pipeline, some steps of which require a lot of resources.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pipeline\" class=\"anchor\" href=\"#pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline\u003c/h3\u003e\n\u003cp\u003eThe pipeline for generating trees, alignments and performing the evaluations is built using\n\u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e, a workflow tool to run tasks across\nmultiple compute infrastructures in a very portable manner. It comes with a docker container\nmaking installation trivial and results highly reproducible.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pipeline-quick-start\" class=\"anchor\" href=\"#pipeline-quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Quick Start\u003c/h3\u003e\n\u003cp\u003eMake sure you have either docker/singularity installed or the required dependencies listed\nin the last section.\u003c/p\u003e\n\u003cp\u003eInstall the Nextflow runtime by running the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ curl -fsSL get.nextflow.io | bash\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen done, you can launch the pipeline execution by entering the command shown below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run evanfloden/dpa-analysis\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBy default the pipeline is executed against the provided example dataset.\nCheck the \u003cem\u003ePipeline parameters\u003c/em\u003e  section below to see how enter your data on the program\ncommand line.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h3\u003e\n\u003cp\u003eAll the methods above are available in a \u003ca href=\"http://www.docker.com\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e image on DockerHub \u003ca href=\"https://hub.docker.com/r/cbcrg/regressive-msa/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e and the image is tested to be compatible with the \u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe container also contains test data consisting of protein sequences, reference alignments and trees in the directory \u003ccode\u003e/test_data\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo launch the container interactively with Docker run:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003edocker run cbcrg/regressive-msa\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTo launch the container interactivly with Singularity run:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esingularity shell docker://cbcrg/regressive-msa\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pipeline-parameters\" class=\"anchor\" href=\"#pipeline-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline parameters\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---seqs\" class=\"anchor\" href=\"#--seqs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--seqs\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSpecifies the location of the input \u003cem\u003efasta\u003c/em\u003e file(s).\u003c/li\u003e\n\u003cli\u003eMultiple files can be specified using the usual wildcards (*, ?), in this case make sure to surround the parameter string\nvalue by single quote characters (see the example below)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ nextflow run evanfloden/dpa-analysis --seqs \u0027/home/seqs/*.fasta\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will handle each fasta file as a seperate sample.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---refs\" class=\"anchor\" href=\"#--refs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--refs\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSpecifies the location of the reference \u003cem\u003ealigned fasta\u003c/em\u003e file(s).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---trees\" class=\"anchor\" href=\"#--trees\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--trees\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSpecifies the location of input tree file(s).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---align_method\" class=\"anchor\" href=\"#--align_method\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--align_method\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSpecifies which alignment methods should be used.\u003c/li\u003e\n\u003cli\u003eOptions include: \"CLUSTALO,MAFFT-FFTNS1,MAFFT-SPARSECORE,MAFFT-GINSI,PROBCONS,UPP\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---tree_method\" class=\"anchor\" href=\"#--tree_method\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--tree_method\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSpecifies which guide-tree / clustering methods should be used.\u003c/li\u003e\n\u003cli\u003eOptions include: \"CLUSTALO,MAFFT_PARTTREE\"\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---regressive_align\" class=\"anchor\" href=\"#--regressive_align\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--regressive_align\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eFlag to generate regressive MSAs.\u003c/li\u003e\n\u003cli\u003eSee \u003ccode\u003etemplates/dpa_align\u003c/code\u003e for the specific commands executed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---stardard_align\" class=\"anchor\" href=\"#--stardard_align\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--stardard_align\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eFlag to perform standard MSAs.\u003c/li\u003e\n\u003cli\u003eStandard MSA is alignment where the guide-tree is provided as input.\u003c/li\u003e\n\u003cli\u003eSee \u003ccode\u003etemplates/std_align\u003c/code\u003e for the specific commands executed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---default_align\" class=\"anchor\" href=\"#--default_align\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--default_align\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eFlag to perform default MSAs.\u003c/li\u003e\n\u003cli\u003eDefault MSA is alignment where the alignment software uses an internally generated guide-tree.\u003c/li\u003e\n\u003cli\u003eSee \u003ccode\u003etemplates/default_align\u003c/code\u003e for the specific commands executed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---evaluate\" class=\"anchor\" href=\"#--evaluate\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--evaluate\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eFlag to perform evaluation of the alignments.\u003c/li\u003e\n\u003cli\u003eRequires reference sequences to be provided with the \u003ccode\u003e--refs\u003c/code\u003e parameter.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---buckets\" class=\"anchor\" href=\"#--buckets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--buckets\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eList of bucket sizes or maximum size of the subMSAs in the regressive proceedure.\u003c/li\u003e\n\u003cli\u003eDefault value is \"1000\" sequences.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---output\" class=\"anchor\" href=\"#--output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--output\u003c/code\u003e\n\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eLocation of the results.\u003c/li\u003e\n\u003cli\u003eDefault locations is \u003ccode\u003eresults\u003c/code\u003e directory.\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1620129144.0
  },
  {
    "data_format": 2,
    "description": "Analysis pipeline for ATACseq data using Nextflow",
    "filenames": [
      "Singularity"
    ],
    "full_name": "DoaneAS/atacflow",
    "latest_release": null,
    "readme": "\n\u003ch1\u003e\n\u003ca id=\"user-content-atacflow\" class=\"anchor\" href=\"#atacflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAtacFlow\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-analysis-pipeline-for-atac-seq-data-using-nextflow\" class=\"anchor\" href=\"#analysis-pipeline-for-atac-seq-data-using-nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnalysis pipeline for ATAC-seq data using Nextflow\u003c/h2\u003e\n\u003cp\u003eThis pipeline inspired by and based on the \u003ca href=\"https://www.encodeproject.org/atac-seq/\" rel=\"nofollow\"\u003eENCODE ATAC-seq processubg pipeline\u003c/a\u003e and\nthe \u003cem\u003eprototype\u003c/em\u003e ATAC-seq pipeline\ndeveloped by \u003ca href=\"https://github.com/kundajelab/atac_dnase_pipelines\"\u003eAnshul Kundaje\u0027s lab\u003c/a\u003e at Stanford University\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eInstall \u003ca href=\"https://www.nextflow.io\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eClone repository\n\u003cul\u003e\n\u003cli\u003eusing nextflow: \u003ccode\u003enextflow clone DoaneAS/atacflow ./\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eor using git: \u003ccode\u003egit clone https://github.com/DoaneAS/atacflow.git\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInstall conda dependencies:\n\u003cpre\u003e\u003ccode\u003econda update conda\nconda env create --file requirements.atacFlow.yml\nconda env create --file deep.yml\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-setup-data\" class=\"anchor\" href=\"#setup-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup data\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eATAC-seq reads go in \u003ccode\u003edata/\u0026lt;Sample\u0026gt;/*_001.fastq.gz\u003c/code\u003e\n\u003cul\u003e\n\u003cli\u003eConcatenate read pairs per sample \u003ccode\u003eparallel -j8 \u0027./bin/catlanes.sh {}\u0027 ::: data/Sample*\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eCreate sample index: \u003ccode\u003epython bin/makeIndex.py\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-execution\" class=\"anchor\" href=\"#execution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecution\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003enextflow run -with-trace -with-dag flow.html main.nf --index sampleIndex.csv --genome hg38\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003esupported genomes on panda WCM cluster:  hg38, mm10\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 1,
    "topics": [],
    "updated_at": 1623367033.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "PGP-UK/GenomeChronicler",
    "latest_release": "0.91",
    "readme": "\u003cpre\u003e\u003ccode\u003e #####                                         #####                                                            \n#     # ###### #    #  ####  #    # ######    #     # #    # #####   ####  #    # #  ####  #      ###### #####  \n#       #      ##   # #    # ##  ## #         #       #    # #    # #    # ##   # # #    # #      #      #    # \n#  #### #####  # #  # #    # # ## # #####     #       ###### #    # #    # # #  # # #      #      #####  #    # \n#     # #      #  # # #    # #    # #         #       #    # #####  #    # #  # # # #      #      #      #####  \n#     # #      #   ## #    # #    # #         #     # #    # #   #  #    # #   ## # #    # #      #      #   #  \n #####  ###### #    #  ####  #    # ######     #####  #    # #    #  ####  #    # #  ####  ###### ###### #    #  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/3664\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-welcome\" class=\"anchor\" href=\"#welcome\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWelcome\u003c/h1\u003e\n\u003cp\u003eThis is the repository for Genome Chronicler, the Personal Genome Project United Kingdom (PGP-UK) genomic report generation scripts.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h1\u003e\n\u003cp\u003eAfter cloning this repository, run the SetupMeFirst.sh script in your local system to retrieve the extra files needed to run the pipeline (around 10GB, so too big for git).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-input-files\" class=\"anchor\" href=\"#input-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInput files\u003c/h1\u003e\n\u003cp\u003eThe main script (GenomeChronicler_mainDruid.pl) needs a BAM file as input, and optionally can also use a VEP generated summary html file, if variants have already been called on the data and summaries are to be produced.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h1\u003e\n\u003cp\u003eTo handle the myriad dependencies present in this pipeline, it is avaliable through Singularity Hub as a singularity container (see badge at top of the page).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-easy-start-using-singularity\" class=\"anchor\" href=\"#easy-start-using-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEasy Start using Singularity\u003c/h1\u003e\n\u003cp\u003eIf you don\u0027t already have singularity on your system, or want to know more about it, head to their userguide at: \u003ca href=\"https://sylabs.io/guides/3.1/user-guide/\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.1/user-guide/\u003c/a\u003e\nWhile Singularity is not needed to run GenomeChronicler, it does make setup much easier.\u003c/p\u003e\n\u003cp\u003eFor a manual installation without Singularity, please follow the steps in the %post section of the Singularity file in this repository, to install all the dependencies.\u003c/p\u003e\n\u003cp\u003eDownloading pre-packaged GenomeChronicler from SingularityHub\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://PGP-UK/GenomeChronicler\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGetting some test data (NA12878 from ENA, pre-mapped to GRCh38, and the respective reference)\u003c/p\u003e\n\u003cpre lang=\"wget\"\u003e\u003ccode\u003esingularity exec GenomeChronicler_latest.sif wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/data/CEU/NA12878/alignment/NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram\n\nsingularity exec GenomeChronicler_latest.sif wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eConverting data to BAM format\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec GenomeChronicler_latest.sif samtools view -T GRCh38_full_analysis_set_plus_decoy_hla.fa -b -o NA12878wxs.bam NA12878.alt_bwamem_GRCh38DH.20150718.CEU.low_coverage.cram\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eRunning GenomeChronicler on the data\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run GenomeChronicler_latest.sif --bamFile=NA12878wxs.bam \n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-command-line-options\" class=\"anchor\" href=\"#command-line-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand Line Options\u003c/h1\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"center\"\u003eOption\u003c/th\u003e\n\u003cth align=\"center\"\u003eRequirement\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e--bamFile\u003c/td\u003e\n\u003ctd align=\"center\"\u003eREQUIRED\u003c/td\u003e\n\u003ctd\u003eThe path to a BAM file that has been preprocessed through markDuplicates and VariantQualityScoreRecalibration. This can be obtained by running the first step of the Sarek nextflow pipeline, or through other means that do respect the general principles of the GATK Variation Calling Best Practices workflow. Note that no variation calling is needed to run GenomeChronicler.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e--vepFile\u003c/td\u003e\n\u003ctd align=\"center\"\u003eOPTIONAL\u003c/td\u003e\n\u003ctd\u003eFor the summary tables to appear in the report, a VEP summary HTML file must be provided. This will likely be generated if the data is from whole genome sequencing and variants were called (e.g. by running all the germline calling steps of the Sarek nextflow pipeline or other GATK Best Practices based workflow). If this isn\u0027t provided, summary tables and plots will automatically be excluded from the final report.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e--resultsDir\u003c/td\u003e\n\u003ctd align=\"center\"\u003eOPTIONAL\u003c/td\u003e\n\u003ctd\u003eFor setting the absolute path of the results folder to be produced when running GenomeChronicler.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e--customTemplate\u003c/td\u003e\n\u003ctd align=\"center\"\u003eOPTIONAL\u003c/td\u003e\n\u003ctd\u003eFor customising the output report, set this variable to the path of a custom LaTeX file to act as a template for the report. The default templates bundled with this software can also be found in the project github page.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"center\"\u003e--GATKthreads\u003c/td\u003e\n\u003ctd align=\"center\"\u003eOPTIONAL\u003c/td\u003e\n\u003ctd\u003eNumber of threads to use for the GATK genotyping steps of this processing pipeline.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n",
    "stargazers_count": 5,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1617920191.0
  },
  {
    "data_format": 2,
    "description": "Collection of Singularity build files and scripts to create them for popular Linux Distributions",
    "filenames": [
      "definitions/steak/Singularity.STEAK-20190912-foss-2019b-Python-2.7.16-envmod-centos7",
      "definitions/salmon/Singularity.Salmon-1.3.0-gompi-2020a-envmod-debian9",
      "definitions/salmon/Singularity.Salmon-1.0.0-gompi-2019a-envmod-debian9",
      "definitions/salmon/Singularity.Salmon-1.1.0-gompi-2019b-envmod-debian9",
      "definitions/salmon/Singularity.Salmon-1.2.1-gompi-2019b-envmod-debian9",
      "definitions/gemma/Singularity.GEMMA-0.98.1-foss-2018b-envmod-centos7",
      "definitions/gemma/Singularity.GEMMA-0.98.1-foss-2018b-envmod-debian9",
      "definitions/bwa/Singularity.BWA-0.7.17-foss-2018b-envmod-debian9",
      "definitions/samtools/Singularity.SAMtools-1.10-GCC-9.3.0-envmod-debian10",
      "definitions/plink/Singularity.PLINK-2.00a2.3LM-x86_64-lmod",
      "definitions/plink/Singularity.PLINK-2.00a2.3LM-x86_64-envmod-debian9",
      "definitions/plink/Singularity.PLINK-2.00-alpha2-x86_64-envmod-debian9",
      "definitions/mirtk/Singularity.mirtk-2.0.0-foss-2020a-Python-3.8.2-envmod-centos7",
      "definitions/tabix/Singularity.tabix-0.2.6-GCCcore-7.3.0-envmod-debian9",
      "definitions/bowtie/Singularity.Bowtie-1.2.3-foss-2018b-envmod-debian9",
      "definitions/HTSeq/Singularity.HTSeq-0.11.3-foss-2020b-centos-7-envmod",
      "definitions/R/Singularity.R-3.6.2-foss-2020a-envmod-debian9",
      "definitions/R/Singularity.R-4.0.0-foss-2020a-envmod-debian9",
      "definitions/R/Singularity.R-3.6.2-foss-2019b-envmod-debian9",
      "definitions/R/Singularity.R-3.6.0-foss-2018b-envmod-debian9",
      "definitions/R/Singularity.R-3.6.3-foss-2020a-envmod-debian9",
      "definitions/RSEM/Singularity.RSEM-1.3.3-foss-2019b-centos-7-envmod",
      "definitions/foss/Singularity.foss-2018a-envmod-centos7",
      "definitions/foss/Singularity.foss-2020a-envmod-debian9",
      "definitions/ruby/Singularity.Ruby-2.7.1-GCCcore-8.3.0-envmod-debian10",
      "definitions/ruby/Singularity.Ruby-2.7.1-GCCcore-8.3.0-envmod-debian9",
      "definitions/gcc/Singularity.GCC-7.3.0-2.30-envmod-centos7",
      "definitions/gcc/Singularity.GCC-9.3.0-envmod-centos7",
      "definitions/gcc/Singularity.GCC-9.2.0-2.32-envmod-centos7",
      "definitions/gcc/Singularity.GCC-8.3.0-envmod-debian9",
      "definitions/gcc/Singularity.GCC-9.3.0-envmod-debian9",
      "definitions/gcc/Singularity.GCC-9.3.0-envmod-debian10",
      "definitions/vcftools/Singularity.VCFtools-0.1.15-foss-2018a-Perl-5.26.1-envmod-centos7",
      "definitions/gcc-core/Singularity.GCCcore-7.3.0-envmod-debian9",
      "definitions/fastqtl/Singularity.FastQTL-2.184-foss-2018b-envmod-centos7",
      "definitions/bcftools/Singularity.BCFtools-1.10.2-GCC-8.3.0-envmod-debian9",
      "definitions/bcftools/Singularity.BCFtools-1.10.2-GCC-8.3.0-envmod-debian10",
      "definitions/bcftools/Singularity.BCFtools-1.3-foss-2016b-envmod-centos7",
      "definitions/meme/Singularity.MEME-5.1.1-foss-2019b-Perl-5.30.0-Python-3.7.4-envmod-debian10"
    ],
    "full_name": "sassy-crick/Singularity-Easybuild",
    "latest_release": "v1.0.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-easybuild\" class=\"anchor\" href=\"#singularity-easybuild\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity-Easybuild\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-description\" class=\"anchor\" href=\"#description\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDescription:\u003c/h2\u003e\n\u003cp\u003eCollection of Singularity definition files and scripts to create them for popular Linux Distributions\u003c/p\u003e\n\u003cp\u003eThe definitions folder contains the successful Singularity Definition files, tested with version 3.5.3 and 3.7.1, whereas the scripts folder contains the scripts to create the Singularity definition files which are based on EasyBuild. This version is using EasyBuild version 4.4.1.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements:\u003c/h2\u003e\n\u003cp\u003eYou will need to have a Linux environment and Singularity installed in it.\nIf you don\u0027t have Linux, please use Vagrant to set up a virtual Linux environment.\u003c/p\u003e\n\u003cp\u003eFurthermore, if you want to build the containers, you either need to have \u003ccode\u003efakeroot\u003c/code\u003e installed and configured so it can be used as normal user, or have \u003ccode\u003esudo\u003c/code\u003e installed. The latter is required if you want to open up containers and re-build them again.\u003c/p\u003e\n\u003cp\u003eAs the software inside the containers is built using Easybuild, you will need to know the names of the Easybuild Configuration files, e.g. GCC-9.3.0.eb.\nThus, it is probably best to install the easybuild-easyconfig files like this in a separate folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ git clone https://github.com/easybuilders/easybuild-easyconfigs.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand search the easybuild/easyconfigs folder for the name of the EasyBuild Configuration files you want to use. You only need the name, not the content of the files.\u003c/p\u003e\n\u003cp\u003eThe version of EasyBuild  is now fixed with this release. If you require a specific version, simply change inside the Singularity definition file this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install easybuild==4.4.0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epip3 install easybuild\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich will install the latest EasyBuild version.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-depreciated-versions\" class=\"anchor\" href=\"#depreciated-versions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDepreciated versions:\u003c/h2\u003e\n\u003cp\u003eAs \u003ccode\u003ePython2\u003c/code\u003e is depreciated, the containers are using the \u003ccode\u003ePython3\u003c/code\u003e version for as their default system version. Note:  This is different from the Python versions EasyBuild will install and should not be mixed up.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage:\u003c/h2\u003e\n\u003cp\u003eUsing the scripts is simple. Go into the \u003ccode\u003escripts\u003c/code\u003e folder and run the installation script \u003ccode\u003einstall.sh\u003c/code\u003e This will \u003cem\u003eeither\u003c/em\u003e install the scripts in your \u003ccode\u003e~/bin\u003c/code\u003e folder as sym-links, or create the sym-links in the folder where you are running the script from. We advice you to install the script in the \u003ccode\u003e~/bin\u003c/code\u003e folder so they are in your \u003ccode\u003ePATH\u003c/code\u003e environment. If you don\u0027t want to do this, we recommend to install the sym-links in a different folder from where you have downloaded the GitHub files from. Please note the usage of sym-links. Thus, if you do any changes in the folder where you downloaded the GitHub repository to, these changes will be carried over. If, for example, you were to delete that folder, the installation is broken.\u003c/p\u003e\n\u003cp\u003eDuring the installation, you will given a number of choices regarding whether you want to \u003cem\u003ebuild\u003c/em\u003e or \u003cem\u003ecreate\u003c/em\u003e the definition files, which Linux distribution and version you want to use and if you want to use Lmod or the environment-module system.\u003c/p\u003e\n\u003cp\u003eYou can then execute for example, assuming the links are in your \u003ccode\u003ePATH\u003c/code\u003e environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ container-create-debian11-envmodules.sh GCC-9.3.0.eb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can supply a second script as well, which could be one you have created. This script will be\nread into the Singularity Build file. So in our example we would get a file called \u003ccode\u003eSingularity.GCC-9.3.0-envmod-debian11\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eIf you want to build the container and additionally a sandbox, you could use this instead:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ container-build-debian11-envmodules.sh GCC-9.3.0.eb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSo in our example we would get a file called \u003ccode\u003eSingularity.GCC-9.3.0-envmod-debian11\u003c/code\u003e, next to the Singularity container called \u003ccode\u003eGCC-9.3.0-envmod-debian11.sif\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIf you don\u0027t want to or cannot use the automatic build process, you can build the container like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build GCC-9.3.0-envmod-debian10.sif Singularity.GCC-9.3.0-envmod-debian10\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eEqually, if you want to install software on top of the existing container manually, simply do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo singularity build --sandbox GCC-9.3.0-envmod-debian10.sif Singularity.GCC-9.3.0-envmod-debian10\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee the example below for a complete build of R-4.0.0 in two steps: We first build the toolchain container (foss-2020a) and inside the container we build R-4.0.0. This approach allows us to create our own complete environment for building complete pipelines as well.\u003c/p\u003e\n\u003cp\u003eIf you want to have your name and email address included in the Singularity definition file, just create this file:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e~\u003c/span\u003e/.singularity/sing-eb.conf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn empty file will mean these values are not included in the Singularity definition file. If you want to include your name and email address, simply add it. Likewise, you can pin the version of EasyBuild you want to use, like 4.4.0 in this example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ename=\"Your Name\"\nemail=\"email@address\"\neb_version=\"EasyBuild-version\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand replace \"Your Name\" and \"email@address\" and the \"EasyBuild-version\" you want to use accordingly.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-build\" class=\"anchor\" href=\"#example-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample build:\u003c/h2\u003e\n\u003cp\u003eThis would be the complete sequence to build a container with the FOSS-2020a tool chain from EasyBuild, unpack the container and build for example R-4.0.0 inside the container. Of course you could to that all in one go as well. We are using the CentOS7 OS in this example:\u003c/p\u003e\n\u003cp\u003eWe first create the Singularity Definition File. As we don\u0027t need to add a separate EasyBuild configuration file we say \u0027n\u0027 here:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ container-create-centos8-envmodules.sh foss-2020a.eb\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eDo we need a second Easybuild recipe (y/N)?: n\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ ls\nSingularity.foss-2020a-centos-8-envmod\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe now build the Singularity container. Note that the command \u0027singularity\u0027 needs to be in the\nPATH of root:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build foss-2020a-centos-8-envmod.sif Singularity.foss-2020a-centos-8-envmod\n\n$ ls\nSingularity.foss-2020a-envmod-centos7 foss-2020a-envmod-centos7.sif \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow that we got a container, we can unpack it so we can add software to it.\nFirst we unpack:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ sudo singularity build --sandbox foss-2020a-centos-8-envmod foss-2020a-centos-8-envmod.sif\n$ ls\nSingularity.foss-2020a-centos-8-envmod foss-2020a-centos-8-envmod.sif foss-2020a-centos-8-envmod\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we enter the container. The \u0027-w\u0027 flag means we can write to the pseudo-chroot environment:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo singularity shell -w foss-2020a-centos-8-envmod \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe become the easybuild user and install the software. We first fetch all the source files. This\nis sometimes a problem due to flaky Internet connections. We then, in a second step, build the\nsoftware. This step can take some time but is done fully automatic. Once build, we exit the\ncontainer again:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e su -l easybuild\u003c/span\u003e\n[easybuild]$ eb --fetch R-4.0.0-foss-2020a.eb\n[easybuild]$ eb  R-4.0.0-foss-2020a.eb\n[easybuild]$ \u003cspan class=\"pl-c1\"\u003eexit\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e exit\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eOne step we need to do here as root is, to change the environment file so the new module will be loaded. This will be towards the bottom of this file:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo vi foss-2020a-centos-8-envmod/environment\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFinally, we build the Singularity container:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ sudo singularity build  R-4.0.0-foss-2020a-centos-8-envmod .sif foss-2020a-centos-8-envmod \n\n$ ls\nSingularity.foss-2020a-centos-8-envmod  foss-2020a-centos-8-envmod .sif foss-2020a-centos-8-envmod  R-4.0.0-foss-2020a-centos-8-envmod .sif \u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow you can run R-4.0.0 on a different system like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ singularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e R-4.0.0-foss-2020a-centos-8-envmod.sif R\nR\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWe have introduced a more automated way of building containers. Thus, instead of doing all of the steps above manually, let the computer do it for you. Instead of using a \u0027create\u0027 script, we are simply using a \u0027build\u0027 script, like this for example:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ container-build-centos8-envmodules.sh foss-2020a.eb\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou will be asked whether or not to build the sandbox in the same run.\u003c/p\u003e\n\u003cp\u003eWe would recommend to build a generic container like \u003ccode\u003efoss-2020b\u003c/code\u003e for example and then use the provided sandbox to add software to it.\u003c/p\u003e\n\u003cp\u003eFor more details about what you can do with Singularity please refer to their home page.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgement\" class=\"anchor\" href=\"#acknowledgement\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgement:\u003c/h2\u003e\n\u003cp\u003eThis work would not be possible without EasyBuild,  I am greateful to the project and the community for their help.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-links\" class=\"anchor\" href=\"#links\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinks:\u003c/h2\u003e\n\u003cp\u003eSingularity: \u003ca href=\"https://sylabs.io/guides/3.5/admin-guide/installation.html\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/admin-guide/installation.html\u003c/a\u003e\u003cbr\u003e\nVagrant: \u003ca href=\"https://www.vagrantup.com/intro/getting-started\" rel=\"nofollow\"\u003ehttps://www.vagrantup.com/intro/getting-started\u003c/a\u003e\u003cbr\u003e\nEasybuild: \u003ca href=\"https://easybuild.readthedocs.io/en/latest\" rel=\"nofollow\"\u003ehttps://easybuild.readthedocs.io/en/latest\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUpdated: 7.6.2021\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626469302.0
  },
  {
    "data_format": 2,
    "description": "In this training course you will find theory and practice material for introducing yourself to wgs analysis for bacterial, including outbreak investigation.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "BU-ISCIII/bacterial_wgs_training",
    "latest_release": "ISCIII2018",
    "readme": "\u003cp\u003e\u003ca href=\"https://circleci.com/gh/BU-ISCIII/bacterial_wgs_training\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0f0038e6bcac192fb10dace1b0ea3475aa34f39f969123b37fdc8730f1f845b0/68747470733a2f2f636972636c6563692e636f6d2f67682f636972636c6563692f636972636c6563692d646f63732e7376673f7374796c653d736869656c64\" alt=\"CircleCI Build Status\" data-canonical-src=\"https://circleci.com/gh/circleci/circleci-docs.svg?style=shield\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://www.gnu.org/licenses/gpl-3.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9e54064fb698af20a2b6089b4f16ec3e31f31f72b47f15a5bb215bfd2e41d1b2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d626c75652e737667\" alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://discuss.circleci.com\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4595d2a3dbf792d4810e309e7cf08e0aeecdd155a48934cc46a625ce669280e7/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6d6d756e6974792d436972636c654349253230446973637573732d3334333433342e737667\" alt=\"CircleCi Community\" data-canonical-src=\"https://img.shields.io/badge/community-CircleCI%20Discuss-343434.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://nextflow.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/36a03a9b995f400d6adfcfda96e16b0b61f0d0ae8e859aa8acde1162d6517bfe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6e657874666c6f772d253345302e32392e302d677265656e2e737667\" alt=\"Nextflow version\" data-canonical-src=\"https://img.shields.io/badge/nextflow-%3E0.29.0-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://sci-f.github.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1ee06357ac79da293d08136619bdf903a80f520229e0916813d4a6eca768a963/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f46696c6573797374656d2d536369656e74696669632d627269676874677265656e2e737667\" alt=\"Scif\" data-canonical-src=\"https://img.shields.io/badge/Filesystem-Scientific-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-bacterial-wgs-training\" class=\"anchor\" href=\"#bacterial-wgs-training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBacterial WGS training\u003c/h1\u003e\n\u003cp\u003eIn this training course you will find theory and practice material for introducing yourself to wgs analysis for bacterial, including outbreak investigation.\u003c/p\u003e\n\u003cp\u003eThe material includes slides with theory concepts and a bunch of practical exercises using nextflow and singularity, focusing on the interpretation of results.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-slides\" class=\"anchor\" href=\"#slides\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSlides\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-day-1\" class=\"anchor\" href=\"#day-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDay 1\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"exercises/00_SetUp.md\"\u003e\u003cstrong\u003eExercise 0\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/00_Setup.pdf\"\u003eDownload pdf\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eTalk 1:\u003c/strong\u003e \u003ca href=\"slides/talk2/curso_SeqGenBac_session1.2_linux.pdf\"\u003eLinux environment review.\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"exercises/01_LinuxBasicCommands.md\"\u003e\u003cstrong\u003eExercise 1\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/01_LinuxBasicCommands.pdf\"\u003eDownload pdf\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eTalk 2 and 3:\u003c/strong\u003e \u003ca href=\"slides/talk1/20210628_3ED_curso_SeqGenBac_session1.1-2_Introduccion_ICuesta.pdf\"\u003eMassive sequencing of bacterial genomes. State-of-the-art. \u0026amp; Bacterial genomes sequencing. Applications.\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-day-2\" class=\"anchor\" href=\"#day-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDay 2\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 4:\u003c/strong\u003e \u003ca href=\"slides/talk3/curso_SeqGenBac_ChangingComputingParadigm.pdf\"\u003eThe computing revolution in Biosciences. Nextflow and Singularity introduction.\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"exercises/02_NextflowSingularity.md\"\u003e\u003cstrong\u003eExercise 2\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/02_LinuxNextflowSingularity.pdf\"\u003eDownload pdf\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 5:\u003c/strong\u003e \u003ca href=\"slides/talk5/curso_SeqGenBac_session2.2_quality_assesment.pdf\"\u003eQuality analysis and control of HTS data\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 6:\u003c/strong\u003e \u003ca href=\"slides/talk6/curso_SeqGenBac_session2.3_assembly.pdf\"\u003eBacterial genomes assembly\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"exercises/02_QualityAndAssembly.md\"\u003e\u003cstrong\u003eExercise 3\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/02_QualityAndAssembly.pdf\"\u003eDownload pdf\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-day-3\" class=\"anchor\" href=\"#day-3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDay 3\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 7:\u003c/strong\u003e \u003ca href=\"slides/talk7/curso_SeqGenBac_session3.1_MappingAndVariantCalling.pdf\"\u003eMapping against reference genome and Variant Calling.\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 8:\u003c/strong\u003e \u003ca href=\"slides/talk8/curso_SeqGenBac_session3.2_SNPMatrixAndPhylogenetics.pdf\"\u003eSNP matrix and phylogenetics.\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"exercises/03_outbreakSNP.md\"\u003e\u003cstrong\u003eExercise 4\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/03_outbreakSNP.pdf\"\u003eDownload pdf\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-day-4\" class=\"anchor\" href=\"#day-4\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDay 4\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 9:\u003c/strong\u003e \u003ca href=\"slides/talk9/curso_SeqGenBac_session4.1_tipificacion-gen-by-gene_ICuesta.pdf\"\u003eTyping based on allelic profile or gene-by-gene\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTalk 10:\u003c/strong\u003e \u003ca href=\"slides/talk10/curso_SeqGenBac_session4.2_GeneByGenevsSNPs_v2.pdf\"\u003eGene-by-gene WGS analysis\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"exercises/04_outbreakcgMLST.md\"\u003e\u003cstrong\u003eExercise 5\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/04_outbreakcgMLST.pdf\"\u003eDownload pdf\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-day-5\" class=\"anchor\" href=\"#day-5\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDay 5\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eTalk 11:\u003c/strong\u003e \u003ca href=\"slides/talk11/curso_SeqGenBac_session5.1_annotation.pdf\"\u003eSequence annotation\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eTalk 12:\u003c/strong\u003e [Genome characterization, Resistance and Virulence genes]\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"exercises/05_annotation.md\"\u003e\u003cstrong\u003eExercise 6\u003c/strong\u003e\u003c/a\u003e -- \u003ca href=\"exercises/05_annotation.pdf\"\u003eDownload pdf\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 8,
    "topics": [
      "wgs",
      "genome",
      "sequencing",
      "bacterial-genomes",
      "ngs-analysis",
      "outbreak-detection",
      "outbreaks",
      "nextflow",
      "bacterial-wgs-training",
      "wgs-analysis"
    ],
    "updated_at": 1625220082.0
  },
  {
    "data_format": 2,
    "description": "Modify C++ test coverage reports to show uninstantiated templates",
    "filenames": [
      "Singularity"
    ],
    "full_name": "emilydolson/force-cover",
    "latest_release": "v2.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-force-cover\" class=\"anchor\" href=\"#force-cover\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eForce-cover\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/emilydolson/force-cover/actions/workflows/tests.yml\"\u003e\u003cimg src=\"https://github.com/emilydolson/force-cover/actions/workflows/tests.yml/badge.svg\" alt=\"Build\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://codecov.io/gh/emilydolson/force-cover\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c930205a7ee3eef3e56759777419a15908deea8038f1bf0a91c39ef8e6da97cc/68747470733a2f2f636f6465636f762e696f2f67682f656d696c79646f6c736f6e2f666f7263652d636f7665722f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/emilydolson/force-cover/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://camo.githubusercontent.com/cd7aa456de7148b9cd70c63dab27300c5ae46df596766aa915c226c27c590490/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f656d696c79646f6c736f6e2f666f7263652d636f7665722e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cd7aa456de7148b9cd70c63dab27300c5ae46df596766aa915c226c27c590490/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f656d696c79646f6c736f6e2f666f7263652d636f7665722e737667\" alt=\"GitHub release\" data-canonical-src=\"https://img.shields.io/github/release/emilydolson/force-cover.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://github.com/emilydolson/force-cover/issues\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f5054ffcd4245c10d3ec85ef059e07aacf787b560f83ad4aec2236364437d097/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e747269627574696f6e732d77656c636f6d652d627269676874677265656e2e7376673f7374796c653d666c6174\" alt=\"contributions welcome\" data-canonical-src=\"https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://singularity-hub.org/collections/3916\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eGetting accurate test coverage information about C++ code containing templates is challenging; uninstantiated templates don\u0027t make it into the compiled binary, so compilers don\u0027t instrument them for coverage tracking (i.e. if you never use a template the compiler thinks it isn\u0027t runnable code and doesn\u0027t count it as lines that should be covered). Since templates with no test coverage are likely to never get instantiated this results in overly accurate test coverage metrics.\u003c/p\u003e\n\u003cp\u003eForce-cover is a set of tools for dealing with this problem. It consists of two parts:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea C++ program (built with Clang Libtooling) that reads your C++ code, finds the templates, and sticks comments before and after them to indicate that they should be covered.\u003c/li\u003e\n\u003cli\u003ea python program that looks at the final test coverage output, finds the macros, and adjusts the file as necessary to indicate that uncovered template code should be counted as uncovered code.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-requirements\" class=\"anchor\" href=\"#requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRequirements\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePython (any version)\u003c/li\u003e\n\u003cli\u003eclang (version 7+) (for version 6, use \u003ca href=\"https://github.com/emilydolson/force-cover/releases/tag/v1.5\"\u003ethis release of force-cover\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003elibclang-dev (version 7+ - must be same version as clang)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTheoretically force-cover should work on any operating system, but it\u0027s currently only been tested on Ubuntu and Linux Mint.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eYou can install the requirements on Ubuntu-flavored Linux with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo apt install -y clang llvm-dev libclang-dev\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can build force-cover by cloning this repo and running Make inside it:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/emilydolson/force-cover.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e force-cover\nmake\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will create the force_cover executable. No additional work is needed to set up the Python script.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h3\u003e\n\u003cp\u003eIf you have multiple versions of clang or llvm on your computer, the Make command may fail. You may be able to fix this by changing the default version as described at the bottom of \u003ca href=\"https://blog.kowalczyk.info/article/k/how-to-install-latest-clang-6.0-on-ubuntu-16.04-xenial-wsl.html\" rel=\"nofollow\"\u003ethis page\u003c/a\u003e. Alternatively, you can modify the Makefile to include absolute paths to the installation location. Set LLVM_SRC_PATH equal to the path to your llvm installation location (e.g. \u003ccode\u003e/usr/lib/llvm-11\u003c/code\u003e). Uncomment the \u003ccode\u003eLLVM_CONFIG := $(LLVM_BIN_PATH)/llvm-config\u003c/code\u003e line and comment out the line above it.\u003c/p\u003e\n\u003cp\u003eAlternately, save yourself a trip through install hell by using a containerized environment a la \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e!\nBuild from our handy-dandy Singularity recipe (\u003ccode\u003esudo singularity build force-cover.simg Singularity\u003c/code\u003e) or grab a pre-built container from SingularityHub (\u003ccode\u003esingularity pull --name \"force-cover.simg\" shub://emilydolson/force-cover\u003c/code\u003e).\nThen, hop on to an interactive shell by \u003ccode\u003esingularity shell force-cover.simg\u003c/code\u003e.\nCowabunga!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start-guide\" class=\"anchor\" href=\"#quick-start-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick-start guide\u003c/h2\u003e\n\u003cp\u003eHere is the basic sequence of commands you need to execute to use force-cover with LLVM Source-Based coverage (the recommended approach):\u003c/p\u003e\n\u003cpre lang=\"none\"\u003e\u003ccode\u003e./force_cover [C++ code file to be evaluated] -- [any flags you would pass to the compiler when compiling this program] \u0026gt; [name of file to store modified code in]\nclang++ -fprofile-instr-generate -fcoverage-mapping -O0 -fno-inline -fno-elide-constructors [.cpp file] -o [executable name]\n[run executable]\nllvm-profdata merge default.profraw -o default.profdata\nllvm-cov show [executable name] -instr-profile=default.profdata \u0026gt; coverage.txt\npython fix_coverage.py coverage.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExample (using included example.cc file):\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./force_cover examples/example.cc -- --language c++ -std=c++11 \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e examples/example_with_template_coverage_info.cc\nclang++ -fprofile-instr-generate -fcoverage-mapping -O0 -fno-inline -fno-elide-constructors examples/example_with_template_coverage_info.cc -o example\n./example\nllvm-profdata merge default.profraw -o default.profdata\nllvm-cov show ./example -instr-profile=default.profdata \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e coverage.txt\npython fix_coverage.py coverage.txt\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-force-cover-in-detail\" class=\"anchor\" href=\"#using-force-cover-in-detail\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing force-cover (in detail)\u003c/h2\u003e\n\u003cp\u003eThe workflow for using force-cover is as follows:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRun all of your C++ code through the force_cover C++ program to insert comments.\u003c/li\u003e\n\u003cli\u003eCompile your program using appropriate flags for your compiler to indicate that you want to measure test coverage on this program\u003c/li\u003e\n\u003cli\u003eRun your program\u003c/li\u003e\n\u003cli\u003eRun your coverage program\u003c/li\u003e\n\u003cli\u003eRun the python script on the coverage program\u0027s output\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn theory, this should be possible with a variety of compilers and code coverage programs. Thus far, I have only tested it with LLVM Source Based coverage. If you have tested it and found that it worked with a different toolchain, let me know so I can add it to this documentation!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1-run-force_cover-on-your-code\" class=\"anchor\" href=\"#step-1-run-force_cover-on-your-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1: Run force_cover on your code\u003c/h3\u003e\n\u003cp\u003eThe syntax for running the force_cover C++ program is:\u003c/p\u003e\n\u003cpre lang=\"none\"\u003e\u003ccode\u003e./force_cover [C++ code file to be evaluated] -- [any flags you would pass to the compiler when compiling this program]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor instance, to run it on the example you could use:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./force_cover examples/example.cc -- --language c++ -std=c++11\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBy default, it prints the modified version of the code to stdout. In order to compile programs using the modified code, you\u0027ll need to pipe this new code to a file. For instance:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./force_cover examples/example.cc -- --language c++ -std=c++11 \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e examples/example_with_template_coverage_info.cc\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor larger code-bases, one option is to make a copy of your code, rewrite all of the files in the copy, and use those files to compile your tests. This can be achieved with a few lines of bash code. For instance, let\u0027s say you\u0027re writing a header-only library and all of the headers live in a directory called \u003ccode\u003esource\u003c/code\u003e. You could run the following code:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ecp -r \u003cspan class=\"pl-c1\"\u003esource\u003c/span\u003e coverage_source\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003efilename\u003c/span\u003e \u003cspan class=\"pl-k\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e`\u003c/span\u003efind ../coverage_source -name \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e*.h\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e`\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003edo\u003c/span\u003e\n    ./force_cover \u003cspan class=\"pl-smi\"\u003e$filename\u003c/span\u003e -- -I../coverage_source --language c++ -std=c++14 \u003cspan class=\"pl-k\"\u003e|\u003c/span\u003e xargs -0 \u003cspan class=\"pl-c1\"\u003eecho\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003e$filename\u003c/span\u003e.temp\n    mv \u003cspan class=\"pl-smi\"\u003e$filename\u003c/span\u003e.temp \u003cspan class=\"pl-smi\"\u003e$filename\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003edone\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThen when you go to compile your tests for coverage, instead of including \u003ccode\u003esource\u003c/code\u003e you would include \u003ccode\u003ecoverage_source\u003c/code\u003e (i.e. replace \u003ccode\u003e-Isource\u003c/code\u003e with \u003ccode\u003e-Icoverage_source\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eIf you are running tests on a continuous integration platform you may choose to skip the step of copying the code to a different directory. Just be aware that \u003cstrong\u003ethis is dangerous because it will overwrite your code\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2-compile-your-program\" class=\"anchor\" href=\"#step-2-compile-your-program\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2: Compile your program\u003c/h3\u003e\n\u003cp\u003eIn order to get coverage information, you need to compile your program with coverage instrumentation turned on. This can be achieved by passing a few flags to the compiler. In LLVM, there are a number of different systems of coverage instrumentation. The one I have had by far the most luck with is Source Based coverage, which can be enabled with the \u003ccode\u003e-fprofile-instr-generate\u003c/code\u003e and  \u003ccode\u003e-fcoverage-mapping\u003c/code\u003e flags. The other version, which mirrors GCC\u0027s gcov system, sometimes optimizes unused class methods out of the binary, preventing them from getting appropriately flagged as not covered.\u003c/p\u003e\n\u003cp\u003eSome other useful flags to prevent the compiler from making optimizations that hide uncovered code are: \u003ccode\u003e-O0 -fno-inline -fno-elide-constructors\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eSo your compilation step will probably look something like:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eclang++ -fprofile-instr-generate -fcoverage-mapping -O0 -fno-inline -fno-elide-constructors examples/example_with_template_coverage_info.cc -o example\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNote that Source Based coverage is only available in clang. Theoretically, the tools in this repo should work on code instrumented in other ways but, as mentioned before, it hasn\u0027t been tested on them.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-3-run-your-program\" class=\"anchor\" href=\"#step-3-run-your-program\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 3: Run your program\u003c/h3\u003e\n\u003cp\u003eThe most straightforward step! Run your program so that the coverage instrumentation can record which lines were executed.\u003c/p\u003e\n\u003cp\u003eFor instance:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e./example\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-4-extract-coverage-information\" class=\"anchor\" href=\"#step-4-extract-coverage-information\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 4: Extract coverage information\u003c/h3\u003e\n\u003cp\u003eNow that you\u0027ve run your program, coverage data exists but it\u0027s probably not in an easy-to-interpret form. You\u0027ll have to run a program to extract it. For LLVM Source Based coverage, that will look like:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ellvm-profdata merge default.profraw -o default.profdata\nllvm-cov show ./example -instr-profile=default.profdata \u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e coverage.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis processes the raw coverage data and then compares that information to the executable to generate a report indicating the number of time each line was executed. Specifically, the format should look like this:\u003c/p\u003e\n\u003cpre lang=\"none\"\u003e\u003ccode\u003e[line_number] |     [times_line_executed]|  [code from source file]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhatever compiler and tools you used, you need to end up with data in this format for step 5 to work. Fortunately, it seems to be a relatively common format (Note: if anyone knows the actual name of this format, send me a PR! I wrote this tool because I needed it and thought others might too, not because I\u0027m some kind of code coverage expert).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-5-run-fix_coveragepy\" class=\"anchor\" href=\"#step-5-run-fix_coveragepy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 5: Run fix_coverage.py\u003c/h3\u003e\n\u003cp\u003eFor the final step, run fix_coverage.py on your output file from the previous step. \u003cstrong\u003eNote that this will overwrite your output file\u003c/strong\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython fix_coverage.py coverage.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis script will go through and find all of the regions that are erroneously being excluded from coverage analysis and modify the coverage file to indicate that they should be covered but are not.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-6-profit\" class=\"anchor\" href=\"#step-6-profit\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 6: Profit!\u003c/h3\u003e\n\u003cp\u003eTa-da! You have code coverage data that includes uninstantiated templates! You can look at the file directly, or pass it along to a service like \u003ca href=\"https://codecov.io\" rel=\"nofollow\"\u003ecodecov\u003c/a\u003e that will give you a more user-friendly way to examine your coverage (codecov\u0027s documentation on using llvm-cov isn\u0027t super clear, but it will accept files in this format with names matching the pattern \u003ccode\u003ecoverage*.txt\u003c/code\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-caveats\" class=\"anchor\" href=\"#caveats\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCaveats\u003c/h2\u003e\n\u003cp\u003eCode coverage is a flawed metric. Just because a line of code is executed doesn\u0027t mean it\u0027s being rigorously tested. This is especially true for templates, since different instantiations of the same template could be wildly different from each other. That\u0027s the whole reason uninstantiated templates don\u0027t get included in the binary in the first place: template definitions only have a meaning with an appropriate set of arguments. Force-cover can increase the accuracy of your code coverage and alert you to uninstantiated templates, but it can\u0027t guarantee that your tests are actually good.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bugs-contributions\" class=\"anchor\" href=\"#bugs-contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBugs? Contributions?\u003c/h2\u003e\n\u003cp\u003eOpen an issue or send me a PR! I\u0027m not an expert on this stuff, so I\u0027m sure there are myriad ways force-cover could be better. I welcome all contributions. The code is pretty succinct, so hopefully it\u0027s not too overwhelming to wade into.\u003c/p\u003e\n\u003cp\u003eIn particular I would love to receive:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdditional rules for \u003ccode\u003evalidate_line\u003c/code\u003e in \u003ccode\u003efix_coverage.py\u003c/code\u003e. Its goal is to detect lines that should not be marked as potentially coverable (e.g. lines containing only comments). I wrote some very basic rules, but I\u0027m sure there are a bunch of edge cases it\u0027s missing.\u003c/li\u003e\n\u003cli\u003eImprovements to the AST matching rules in \u003ccode\u003eforce_cover.cpp\u003c/code\u003e. I\u0027m sure there are edge cases that they\u0027re currently missing. Also in general they\u0027re a little overzealous at this point (in mostly harmless ways).\u003c/li\u003e\n\u003cli\u003eThere is probably a smoother way to do all of this (e.g. one that doesn\u0027t require both a pre-processing step and a post-processing step). Potential options (some of which I tried and gave up on):\n\u003cul\u003e\n\u003cli\u003eAutomatically add code that instantiates templates. Problem: you need to know what types to instantiate them with.\u003c/li\u003e\n\u003cli\u003eDetect uninstantiated templates and replace them with an equivalent number of lines of non-templated code. Problem: detecting uninstantiated templates is non-trivial.\u003c/li\u003e\n\u003cli\u003eDitch the preprocessing script and let Python find templates in the coverage output. Problem: probably requires parsing C++ in Python (although there are Python bindings for clang libtools... they\u0027re just really poorly documented).\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 1,
    "topics": [
      "test-coverage",
      "llvm-cov",
      "libtooling"
    ],
    "updated_at": 1625146655.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "fiji/Singularity.fiji",
      "jupyter-fileuploadtool/Singularity.jupyter-fileuploadtool_20201211",
      "datalad/Singularity.datalad_0.13.3",
      "pyprismatic/Singularity.pyprismatic_1_2_1-cuda-11.0",
      "paraview/Singularity.paraview_5.6.0-cuda-9.0",
      "crisprcas/Singularity.crisprcas",
      "fsl/Singularity.fsl",
      "cloudstor/Singularity.cloudstor-2.4.1",
      "colmap/Singularity.colmap_3.6-dev.3",
      "colmap/Singularity.colmap_3.5",
      "caffe/Singularity.caffe_1.0",
      "anaconda3/Singularity.anaconda3_5.3.0-cuda-11.0.3",
      "anaconda3/Singularity.anaconda3_5.3.0",
      "atom/Singularity.atom_1.45.0",
      "atom/Singularity.atom_1.39.1",
      "darknet/Singularity.darknet_yolo_v3-cuda-9.0",
      "ilastik/Singularity.ilastik_1.3.3post3",
      "libertem/Singularity.libertem-v0.7.0",
      "libertem/Singularity.libertem-v0.4.0",
      "libertem/Singularity.libertem-v0.6.0",
      "libertem/Singularity.libertem-v0.4.1",
      "libertem/Singularity.libertem-21-May-2019",
      "libertem/Singularity.libertem-v0.5.1",
      "libertem/Singularity.libertem-v0.5.0",
      "libertem/Singularity.libertem-v0.2.2",
      "haystack/Singularity.haystack_bio_v0_5_0",
      "eman/Singularity.eman_2.91",
      "eman/Singularity.eman_2.3",
      "eman/Singularity.eman_2.22",
      "eman/Singularity.eman_2.9",
      "eman/Singularity.eman_2.3.1",
      "globus-cli/Singularity.globus-cli-v2.0.0",
      "amide/Singularity.amide-1.0.5",
      "cistem/Singularity.cisTEM-1.0.0-beta",
      "git-annex/Singularity.git-annex.6.20180227",
      "cellprofiler/Singularity.cellprofiler_3.1.5",
      "cellprofiler/Singularity.cellprofiler_3.1.9",
      "argos/Singularity.argos_3.0.0-beta53",
      "argos/Singularity.argos_3.0.0-beta52",
      "mrtrix3tissue/Singularity.mrtrix3tissue-5.2.8",
      "mydata-python/Singularity.mydata-python_20200603",
      "cryolo/Singularity.cryolo_v1_6_1",
      "cryolo/Singularity.cryolo_v1_0_0",
      "cryolo/Singularity.cryolo_v1_5_6",
      "cryolo/Singularity.cryolo_v1_0_4",
      "mango/Singularity.mango_4.0.1",
      "R/Singularity.R_4.0.5",
      "octave/Singularity.octave-4.2.2",
      "cvmfs-client/Singularity.cvmfs-client",
      "meshlab/Singularity.meshlab-2019.03-cuda-9.0",
      "openrefine/Singularity.openrefine-3.1",
      "openmodelica/Singularity.openmodelica_1.14.2-cuda-10.1",
      "dragondisk/Singularity.dragondisk_v1_0_5",
      "ashs/Singularity.ashs_2.0.0",
      "omero-insight/Singularity.1804",
      "connectome-workbench/Singularity.connectome-workbench_1.4.2",
      "ants/Singularity.ants_2.3.4",
      "ants/Singularity.ants_2.3.1",
      "globus-connect-personal/Singularity.globus-connect-personal_latest",
      "ariba/Singularity.ariba_2.14.4",
      "ariba/Singularity.ariba_2.12.1",
      "jupyter-ml/Singularity.jupyter-ml_20201120",
      "jupyter-ml/Singularity.jupyter-ml_20210415",
      "mantid/Singularity.mantid_v_3_13_0",
      "caffe-unet/Singularity.caffe-unet_1.0",
      "matlab/Singularity.MATLAB_SAMPLE",
      "volview/Singularity.VolView_3.4-cuda-9.0",
      "imagej/Singularity.imagej_1.50e",
      "quit/Singularity.quit_2.0.2",
      "bidscoin/Singularity.bidscoin_3",
      "imblproc/Singularity.imblproc",
      "gimp/Singularity.gimp_2.8",
      "gimp/Singularity.gimp_2.8.22",
      "omero.insight/Singularity.omero_5.5.10",
      "bids-validator/Singularity.bids-validator-1.3.1",
      "bids-validator/Singularity.bids-validator-1.2.2",
      "ubuntu-base-image/Singularity.1804-cuda10.1",
      "ubuntu-base-image/Singularity.1804",
      "ubuntu-base-image/Singularity.2004",
      "ubuntu-base-image/Singularity.1804-cuda9",
      "ubuntu-base-image/Singularity.2004-cuda11.0",
      "deeplabcut/Singularity.latest",
      "imagemagick/Singularity.imagemagick-7.0.8-68",
      "apex/Singularity.apex_master",
      "3dslicer/Singularity.3dslicer_4.10.2",
      "3dslicer/Singularity.3dslicer_4.8.1",
      "imod/Singularity.imod_v4_9_9",
      "graphviz/Singularity.graphviz-2.40.1",
      "dristhi/Singularity.dristhi_2.6.4",
      "mydata/Singularity.mydata_0.9.2-1",
      "mrtrix/Singularity.mrtrix_3_beta",
      "cytoscape/Singularity.cytoscape_3.8.0",
      "octopus/Singularity.octopus_8.4_parallel",
      "octopus/Singularity.octopus_8.4"
    ],
    "full_name": "Characterisation-Virtual-Laboratory/CharacterisationVL-Software",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-characterisationvl-software\" class=\"anchor\" href=\"#characterisationvl-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCharacterisationVL-Software\u003c/h1\u003e\n\u003cp\u003eThe purpose of this repository is for storing definition files to submit to \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub.\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you are new to Singularity containers, please refer to \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003ehttps://sylabs.io/guides/3.5/user-guide/\u003c/a\u003e or a newer version of this documentation.\u003c/p\u003e\n\u003cp\u003eEach software package is located in its own folder. The files are tagged with the software name and version number or date of build. Please read below for the naming convention.\u003c/p\u003e\n\u003cp\u003eTo add software to the repository you will need to create a new branch. The new branch is the name of the software product. By convention, the new branch will be checked and merged into the master branch and then deleted.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-steps-to-add-a-software-package\" class=\"anchor\" href=\"#steps-to-add-a-software-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSteps to add a software package\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository\u003c/li\u003e\n\u003cli\u003eCreate a branch\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git branch \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eMake a subdirectory for the software product.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ mkdir \u0026lt;software name\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eAdd all the necessary files.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSingularity definition file or installation script\u003c/li\u003e\n\u003cli\u003eReadme file including install and testing notes\u003c/li\u003e\n\u003cli\u003eDesktop files for adding to menus with necessary tags\u003c/li\u003e\n\u003cli\u003eFor full details, \u003ca href=\"template/README.md\"\u003eplease refer to the \u0027template\u0027 folder in this repository.\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eCommit all changes, including a helpful message\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e$ git commit -m \"\u0026lt;software name\u0026gt; added as requested in support ticket\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"6\"\u003e\n\u003cli\u003ePush to the remote repository. i.e. this one.\u003c/li\u003e\n\u003cli\u003eSubmit merge request\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-naming-your-singularity-definition-file-singularity-hub-and-licensing\" class=\"anchor\" href=\"#naming-your-singularity-definition-file-singularity-hub-and-licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNaming your Singularity definition file, Singularity Hub and Licensing\u003c/h2\u003e\n\u003cp\u003eFor all Singularity recipes where the software licensing permits redistribution, please use this naming convention:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   Singularity.applicationName_version\n   Singularity.applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis is where Singularity Hub fits into the equation. There is a webhook between this repository and \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e. When a commit is merged into the master branch, Singularity Hub will build the container.\u003c/p\u003e\n\u003cp\u003eIf successfully built, the path to the container on Singularity Hub is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version\n  singularity pull shub://Characterisation-Virtual-Laboratory/CharacterisationVL-Software:applicationName_version-cuda-cudaVersion\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor software where licensing does not support redistribution, the container recipe can still be defined, but the container should not be built on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eAn example on how to handle this situation is the recipe for CCP-EM.\nThe \u003ca href=\"ccp-em/README.md\"\u003eREADME.md\u003c/a\u003e contains a section on Prerequisites. This section lists the required files to build the container. The license must be accepted by the end user to obtain them.\u003c/p\u003e\n\u003cp\u003ePrerequisite files should not be committed to this repository.\u003c/p\u003e\n\u003cp\u003eTo prevent Singularity Hub from attempting to build the container, we simply use a different recipe naming convention as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e   applicationName_version.def\n   applicationName_version-cuda-cudaVersion.def\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-ubuntu-base-images\" class=\"anchor\" href=\"#ubuntu-base-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUbuntu Base Images\u003c/h2\u003e\n\u003cp\u003eThe folder \u0027ubuntu-base-image\u0027 contains recipes for pre built base containers. These can be used as a starting point to aid/speed up the development of your container recipe.\u003c/p\u003e\n\u003cp\u003eThe current versions are built using Ubuntu 18.04 LTS, plus Cuda 9 or Cuda 10.1 if required.\u003c/p\u003e\n\u003cp\u003eThese are available on Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFor example: from the Graphviz Singularity.graphviz-2.40.1 recipe\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eBootstrap: shub\nFrom:      Characterisation-Virtual-Laboratory/CharacterisationVL-Software:1804\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese two lines, will tell Singularity to use the \u0027shub\u0027 bootstrap to obtain the \u00271804\u0027 ubuntu-base-image container from Singularity Hub.\u003c/p\u003e\n\u003cp\u003eFrom here you just need to add the requirements to build a container for your required piece of software. Please see \u003ca href=\"graphviz/Singularity.graphviz-2.40.1\"\u003eSingularity.graphviz-2.40.1\u003c/a\u003e\nfor the full recipe.\u003c/p\u003e\n\u003cp\u003eThe current ubuntu-base-images include Python, VirtualGL and TurboVNC plus Cuda if indicated in the name.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-gui-applications-on-a-non-gpu-node\" class=\"anchor\" href=\"#running-gui-applications-on-a-non-gpu-node\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning GUI applications on a non-GPU node\u003c/h2\u003e\n\u003cp\u003eThe applications in the Singularity container should run without the need for a dedicated GPU.\u003c/p\u003e\n\u003cp\u003eHowever, an X server needs to be running for this to work. On nodes with GPU, X Server is started with NVIDIA driver, and on non-GPU nodes, the X Server is started with MESA library.\u003c/p\u003e\n\u003cp\u003eX Server can be started during boot (for example, using \u003ccode\u003esystemctl set-default graphical.target\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eMake sure that VirtualGL package is installed in the container. The code below will download and install VirtualGL.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget https://swift.rc.nectar.org.au/v1/AUTH_810/CVL-Singularity-External-Files/virtualgl_2.6.2_amd64.deb\n\ndpkg -i virtualgl_2.6.2_amd64.deb\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe application startup script doesn\u0027t need to be modified, however, if the application needs to be manually started, then \u003ccode\u003evglrun\u003c/code\u003e needs to be appended before running the application. For example: \u003ccode\u003esingularity exec --nv -B /projects:/projects -B /scratch:/scratch /usr/local/chimerax/0.8/chimerax.sif vglrun ChimeraX\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/1396\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1627344846.0
  },
  {
    "data_format": 2,
    "description": "Quality control plotting for long reads",
    "filenames": [
      "Singularity"
    ],
    "full_name": "mbhall88/pistis",
    "latest_release": "v0.3.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-pistis\" class=\"anchor\" href=\"#pistis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePistis\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-quality-control-plotting-for-long-reads\" class=\"anchor\" href=\"#quality-control-plotting-for-long-reads\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuality control plotting for long reads.\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://pypi.python.org/pypi/pistis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0d377fd7c4560ba9ce5e50da718cfcda6af8bfe6e63362d9c8741335e20fec6c/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f7069737469732e737667\" alt=\"PyPI status\" data-canonical-src=\"https://img.shields.io/pypi/v/pistis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.org/mbhall88/pistis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/460505d13dbbc44006c446a195f753c22160192229624c04b719693986845945/68747470733a2f2f7472617669732d63692e6f72672f6d6268616c6c38382f7069737469732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/mbhall88/pistis.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/mbhall88/pistis/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a5606fdcd10a7afc202cdcc307f242a27a106834bebba2be192225e4315fb774/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f6d6268616c6c38382f7069737469732e737667\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/mbhall88/pistis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://twitter.com/mbhall88\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/899e87a3d856d3491f29644236afe87260be498a45240bd9acde07d48634d9fd/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f6d6268616c6c38382e7376673f7374796c653d736f6369616c266c6f676f3d74776974746572266c6162656c3d466f6c6c6f77\" alt=\"Twitter Follow\" data-canonical-src=\"https://img.shields.io/twitter/follow/mbhall88.svg?style=social\u0026amp;logo=twitter\u0026amp;label=Follow\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/2402\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis package provides plotting designed to give you an idea of how your long read\nsequencing data looks. It was conceived of and developed with nanopore reads in\nmind, but there is no reason why PacBio reads can\u0027t be used.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epip3 install pistis\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can also use \u003ccode\u003epip\u003c/code\u003e if you are running with python2.\u003cbr\u003e\nOr using a virtual\nenvironment manager such as \u003ca href=\"https://conda.io/docs/\" rel=\"nofollow\"\u003econda\u003c/a\u003e or\n\u003ca href=\"https://docs.pipenv.org/\" rel=\"nofollow\"\u003epipenv\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou should now be able to run \u003ccode\u003epistis\u003c/code\u003e from the command line\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis --help\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h3\u003e\n\u003cp\u003eThere is a built image maintained with this repository that can be used. For the latest release you can use the URI \u003ccode\u003eshub://mbhall88/pistis\u003c/code\u003e\u003cbr\u003e\nFor example\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity \u003cspan class=\"pl-c1\"\u003eexec\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eshub://mbhall88/pistis\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e pistis --help\nsingularity pull --name pistis.simg \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eshub://mbhall88/pistis\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003cp\u003eThe main use case for \u003ccode\u003epistis\u003c/code\u003e is as a command-line interface (CLI), but it can also be\nused in an interactive way, such as with a \u003ca href=\"https://jupyter.org/\" rel=\"nofollow\"\u003eJupyter Notebook\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-cli-usage\" class=\"anchor\" href=\"#cli-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCLI Usage\u003c/h4\u003e\n\u003cp\u003eAfter installing and running the help menu you should see the following usage\noptions\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epistis -h\n\nUsage: pistis [OPTIONS]\n\n  A package for sanity checking (quality control) your long read data.\n  Feed it a fastq file and in return you will receive a PDF with four plots:\n\n          1. GC content histogram with distribution curve for sample.\n\n          2. Jointplot showing the read length vs. phred quality score for\n          each         read. The interior representation of this plot can be\n          altered with the         --kind option.\n\n          3. Box plot of the phred quality score at positional bins across\n          all reads. The reads are binned into read positions 1, 2, 3, 4, 5,\n          6, 7, 8, 9, 10, 11-20, 21-50, 51-100, 101-200, 201-300. Plots from\n          the start of reads.\n\n          4. Same as 3, but plots from the end of the read.\n\n  Additionally, if you provide a BAM/SAM file a histogram of the read\n  percent identity will be added to the report.\n\nOptions:\n  -f, --fastq PATH                Fastq file to plot. This can be gzipped.\n  -o, --output PATH               Path to save the plot PDF as. If name is not\n                                  specified, will use the name of the fastq\n                                  (or bam) file with .pdf extension.\n  -k, --kind [kde|scatter|hex]    The kind of representation to use for the\n                                  jointplot of quality score vs read length.\n                                  Accepted kinds are \u0027scatter\u0027, \u0027kde\u0027\n                                  (default), or \u0027hex\u0027. For examples refer to h\n                                  ttps://seaborn.pydata.org/generated/seaborn.\n                                  jointplot.html\n  --log_length / --no_log_length  Plot the read length as a log10\n                                  transformation on the quality vs read length\n                                  plot\n  -b, --bam PATH                  SAM/BAM file to produce read percent\n                                  identity histogram from.\n  -d, --downsample INTEGER        Down-sample the sequence files to a given\n                                  number of reads. Set to 0 for no\n                                  subsampling. Default: 50000\n  -h, --help                      Show this message and exit.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote the \u003ccode\u003e--downsample\u003c/code\u003e option is set to 50000 by default. That is, \u003ccode\u003epistis\u003c/code\u003e will\nonly plot 50000 reads (sampled from a uniform distribution). You can set this to\n0 if you want to plot every read, or select another number of your choosing. Be aware\nthat if you try to plot too many reads you may run into memory issues, so try\ndownsampling if this happens.\u003c/p\u003e\n\u003cp\u003eThere are three different use cases - currently - for producing plots:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eFastq only\u003c/strong\u003e - This will return four plots:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA distribution plot of the GC content for each read.\u003c/li\u003e\n\u003cli\u003eA bivariate jointplot with read length on the y-axis and mean read quality\nscore on the x-axis.\u003c/li\u003e\n\u003cli\u003eTwo boxplots that show the distribution of quality scores at select positions\nand positional ranges. One plot shows the scores from the beginning of the\nread and the other from the end of the read.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo use \u003ccode\u003epistis\u003c/code\u003e in this way you just need a fastq file.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis -f /path/to/my.fastq -o /save/as/report.pdf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThis will save the four plots to a file called \u003ccode\u003ereport.pdf\u003c/code\u003e in directory \u003ccode\u003e/save/as/\u003c/code\u003e.\nIf you don\u0027t provide a \u003ccode\u003e--output/-o\u003c/code\u003e option the file will be saved in the current\ndirectory with the basename of the fastq file. So in the above example it would be\nsaved as \u003ccode\u003emy.pdf\u003c/code\u003e.\u003cbr\u003e\nIf you would prefer the read lengths in the bivariate plot of read length vs.\nmean quality score then you can indicate this like so\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis -f /path/to/my.fastq -o /save/as/report.pdf --no_log_length\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAdditionally, you can change the way the data is represented in the bivariate plot.\nThe default is a kernel density estimation plot (as in the below image), however you can\nchoose to use a \u003ca href=\"https://seaborn.pydata.org/generated/seaborn.jointplot.html\" rel=\"nofollow\"\u003ehex bin or scatter plot version instead\u003c/a\u003e.\nIn the running example, to use a scatter plot you would run the following\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis -f /path/to/my.fastq -o /save/as/report.pdf --kind scatter\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can also provide a \u003ccode\u003egzip\u003c/code\u003eed fastq file without any extra steps\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis -f /path/to/my.fastq.gz -o /save/as/report.pdf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eExamples\u003c/strong\u003e\u003cbr\u003e\nGC content:\u003cbr\u003e\n\u003ca href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_gc_plot.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_gc_plot.png\" alt=\"gc content plot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eRead length vs. mean read quality score:\u003cbr\u003e\n\u003ca href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_v_len.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_v_len.png\" alt=\"read length vs quality plot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBase quality from the start of each read:\u003cbr\u003e\n\u003ca href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_start.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_start.png\" alt=\"base quality from start plot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eBase quality from the end of each read:\u003cbr\u003e\n\u003ca href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_qual_end.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_qual_end.png\" alt=\"base quality from end plot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eFastq and BAM/SAM\u003c/strong\u003e - This will return the above four plots, plus a distribution\nplot of each read\u0027s percent identity with the reference it is aligned to in the\n[BS]AM file. Reads which are flagged as supplementary or secondary are not included.\nThe plot also includes a dashed vertical red line indicating the median\npercent identity.\u003cbr\u003e\nNote: If using a BAM file, it must be sorted and indexed (i.e \u003ccode\u003e.bai\u003c/code\u003e file). See \u003ca href=\"http://www.htslib.org/doc/samtools.html\" rel=\"nofollow\"\u003e\u003ccode\u003esamtools\u003c/code\u003e\u003c/a\u003e\nfor instructions on how to do this.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis -f /path/to/my.fastq  -b /path/to/my.bam -o /save/as/report.pdf\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or\u003c/span\u003e\npistis -f /path/to/my.fastq  -b /path/to/my.sam -o /save/as/report.pdf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eExample\u003c/strong\u003e\u003cbr\u003e\nDistribution of aligned read percent identity:\u003cbr\u003e\n\u003ca href=\"https://github.com/mbhall88/pistis/blob/master/docs/imgs/pistis_perc_id.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/mbhall88/pistis/raw/master/docs/imgs/pistis_perc_id.png\" alt=\"percent identity plot\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eBAM/SAM only\u003c/strong\u003e - At this stage you will receive only the distribution\nplot of each read\u0027s percent identity with the reference it is aligned to. In a\nfuture release I aim to allow you to also get the other four fastq-only plots.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epistis -b /path/to/my.bam -o /save/as/report.pdf\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAs with the fastq-only method, if you don\u0027t provide a \u003ccode\u003e--output/-o\u003c/code\u003e option the file will be saved in the current\ndirectory with the basename of the [BS]AM file. So in the above example it would be\nsaved as \u003ccode\u003emy.pdf\u003c/code\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-usage-in-a-development-environment\" class=\"anchor\" href=\"#usage-in-a-development-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage in a development environment\u003c/h4\u003e\n\u003cp\u003eIf you would like to use \u003ccode\u003epistis\u003c/code\u003e within a development environment such as a\n\u003ccode\u003ejupyter notebook\u003c/code\u003e or just a plain ol\u0027 python shell then take a look at \u003ca href=\"https://github.com/mbhall88/pistis/blob/master/examples/example_usage.ipynb\"\u003ethis example notebook\u003c/a\u003e\nfor all the details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-credits\" class=\"anchor\" href=\"#credits\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCredits\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThis package was created with \u003ca href=\"https://github.com/audreyr/cookiecutter\"\u003eCookiecutter\u003c/a\u003e and the \u003ca href=\"https://github.com/audreyr/cookiecutter-pypackage\"\u003e\u003ccode\u003eaudreyr/cookiecutter-pypackage\u003c/code\u003e project template\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThe two test data files (fastq and BAM) that I have used in this repository were\ntaken from \u003ca href=\"https://github.com/wdecoster/nanotest\"\u003eWouter De Coster\u0027s \u003ccode\u003enanotest\u003c/code\u003e repository\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eWhich in turn comes from \u003ca href=\"http://lab.loman.net/2017/03/09/ultrareads-for-nanopore/\" rel=\"nofollow\"\u003eNick Loman and Josh Quick\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThe example plots in this \u003ccode\u003eREADME\u003c/code\u003e were made using the entire fastq of basecalled\nreads from the experiment in that \u003ca href=\"http://lab.loman.net/2017/03/09/ultrareads-for-nanopore/\" rel=\"nofollow\"\u003eblog on \"whale hunting\"\u003c/a\u003e.\u003c/li\u003e\n\u003cli\u003eThe plot for the BAM file was obtained by running \u003ccode\u003epistis\u003c/code\u003e on a BAM file generated\nby mapping the fastq file to \u003cem\u003eE. coli\u003c/em\u003e reference \u003ca href=\"https://www.ncbi.nlm.nih.gov/nuccore/NC_000913.3\" rel=\"nofollow\"\u003eNC_000913.3\u003c/a\u003e\nusing Heng Li\u0027s \u003ca href=\"https://github.com/lh3/minimap2\"\u003e\u003ccode\u003eminimap2\u003c/code\u003e\u003c/a\u003e and \u003ccode\u003e-x map-ont\u003c/code\u003e option.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h1\u003e\n\u003cp\u003eIf you would like to contribute to this package you are more than welcome.\u003cbr\u003e\n\u003cstrong\u003ePlease read through the \u003ca href=\"https://github.com/mbhall88/pistis/blob/master/CONTRIBUTING.rst\"\u003econtributing guidelines\u003c/a\u003e first\u003c/strong\u003e.\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 2,
    "topics": [
      "nanopore",
      "oxford-nanopore",
      "bioinformatics",
      "bioinformatics-analysis",
      "plotting",
      "quality-control",
      "pacbio"
    ],
    "updated_at": 1626495440.0
  },
  {
    "data_format": 2,
    "description": "This docker and singularity image bundles the tgv-qsm algorithm with bet2, dcm2niix and provides a complete QSM processing pipeline.",
    "filenames": [
      "Singularity.tgvqsm_amd",
      "Singularity.tgvqsm"
    ],
    "full_name": "CAIsr/qsm",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-qsm-pipeline\" class=\"anchor\" href=\"#qsm-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQSM Pipeline\u003c/h1\u003e\n\u003cp\u003eThis docker and singularity image provides the tgv-qsm algorithm (\u003ca href=\"http://www.neuroimaging.at/pages/qsm.php\" rel=\"nofollow\"\u003ehttp://www.neuroimaging.at/pages/qsm.php\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eIf you use this image, this is the reference to cite describing the QSM algorithm:\nLangkammer, C; Bredies, K; Poser, BA; Barth, M; Reishofer, G; Fan, AP; Bilgic, B; Fazekas, F; Mainero; C; Ropele, S\nFast Quantitative Susceptibility Mapping using 3D EPI and Total Generalized Variation.\nNeuroimage. 2015 May 1;111:622-30. doi: 10.1016/j.neuroimage.2015.02.041. PubMed\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-if-you-are-looking-for-a-full-qsm-pipeline-including-dicom-conversion-qsm-solution-image-segmentation-atlas-building\" class=\"anchor\" href=\"#if-you-are-looking-for-a-full-qsm-pipeline-including-dicom-conversion-qsm-solution-image-segmentation-atlas-building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIf you are looking for a full QSM pipeline including dicom conversion, QSM solution, image segmentation, atlas building\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/QSMxT/QSMxT\"\u003ehttps://github.com/QSMxT/QSMxT\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-the-image-in-singularity\" class=\"anchor\" href=\"#using-the-image-in-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the image in singularity\u003c/h1\u003e\n\u003cp\u003einstalling singularity will depend on your operating system, here an exampe for a debian based system\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y \\\n    build-essential \\\n    uuid-dev \\\n    libgpgme-dev \\\n    squashfs-tools \\\n    libseccomp-dev \\\n    wget \\\n    pkg-config \\\n    git \\\n    cryptsetup-bin\n\nwget https://golang.org/dl/go1.15.2.linux-amd64.tar.gz\n\ntar -C /usr/local -xzf go1.15.2.linux-amd64.tar.gz\n\nexport PATH=$PATH:/usr/local/go/bin\n\nexport VERSION=3.6.3 \u0026amp;\u0026amp; # adjust this as necessary \\\n    wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz \u0026amp;\u0026amp; \\\n    tar -xzf singularity-${VERSION}.tar.gz \u0026amp;\u0026amp; \\\n    cd singularity\n\n\n./mconfig \u0026amp;\u0026amp; \\\n    make -C ./builddir \u0026amp;\u0026amp; \\\n    sudo make -C ./builddir install\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethen you can download and run the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/NeuroDesk/transparent-singularity tgvqsm_1.0.0_20210317\ncd tgvqsm_1.0.0_20210317\n./run_transparent_singularity.sh tgvqsm_1.0.0_20210317\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethis will download the image, unpack it and provide a wrapper script for starting tgv_qsm:\u003c/p\u003e\n\u003cp\u003eThe wrapper script can be started using\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./tgv_qsm\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr you can open a shell into the container:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e singularity shell tgvqsm_1.0.0_20210317.*\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eyou can also bind a different directory to your image (e.g. bind /data from your host to /data in your singularity image)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell --bind /data:/data/ tgvqsm_1.0.0_20210317.*\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere is an example for a single echo QSM processing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edcm2niix -o ./ -f magnitude GR_M_5_QSM_p2_1mmIso_TE20/\ndcm2niix -o ./ -f phase GR_P_6_QSM_p2_1mmIso_TE20/\n\nbet2 magnitude.nii magnitude_bet2\n\ntgv_qsm \\\n  -p phase.nii \\\n  -m magnitude_bet2_mask.nii.gz \\\n  -f 2.89 \\\n  -t 0.02 \\\n  -s \\\n  -o qsm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe -s option will scale the phase correctly if the phase dicom values are between -2048 and 2048 (should be default on Siemens VD and VE platforms). On the VB platform the phase is between 0 and 4096, so omit the -s option and scale the phase between -pi and pi:\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-the-image-in-docker\" class=\"anchor\" href=\"#using-the-image-in-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the image in docker\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull vnmd/tgvqsm_1.0.0:20210317\nsudo docker run -it -v $PWD:/data vnmd/tgvqsm_1.0.0:20210317\n\ncd /data\ndcm2niix -o ./ -f magnitude GR_M_5_QSM_p2_1mmIso_TE20/\ndcm2niix -o ./ -f phase GR_P_6_QSM_p2_1mmIso_TE20/\n\nbet2 magnitude.nii magnitude_bet2\n\ntgv_qsm -p phase.nii -m magnitude_bet2_mask.nii.gz -f 2.89 -t 0.02 -s -o qsm\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-optimizing-for-your-cpu\" class=\"anchor\" href=\"#optimizing-for-your-cpu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptimizing for your CPU\u003c/h1\u003e\n\u003cp\u003eBy default, QSM is compiled with the \u003ccode\u003e-O3 -march=x86-64\u003c/code\u003e which should provide a good balance between speed and portability. If you know what CPU you\u0027re going to be using you can compile with that instruction set to improve performance (e.g. \u003ccode\u003e-march=ivybridge\u003c/code\u003e for Intel Ivy Bridge CPUs, \u003ccode\u003e-march=native\u003c/code\u003e for whatever CPU you\u0027re currently on). If you would like maximum portability, you can recompile omitting the \u003ccode\u003e-march\u003c/code\u003e flag altogether.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-using-tgv_qsm-in-windows-subsystem-for-linux-example-debian-based-system\" class=\"anchor\" href=\"#using-tgv_qsm-in-windows-subsystem-for-linux-example-debian-based-system\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing tgv_qsm in Windows Subsystem for Linux (example: Debian based system)\u003c/h1\u003e\n\u003cp\u003eWSL 1.0 doesn\u0027t support singularity or docker containers (but WSL 2.0 will). But it is possible to directly install TGV QSM in a miniconda environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt install wget unzip gcc\nwget https://repo.anaconda.com/miniconda/Miniconda2-4.6.14-Linux-x86_64.sh\nbash Miniconda2-4.6.14-Linux-x86_64.sh\n(install, accept agreement with yes, after install source bash again:)\nbash\nconda install -c anaconda cython==0.25.2\nconda install numpy\nconda install pyparsing\n(make sure pip is not your system pip, but the one in miniconda: which pip)\npip install scipy==0.17.1 nibabel==2.1.0\nwget http://www.neuroimaging.at/media/qsm/TGVQSM-plus.zip\nunzip TGVQSM-plus.zip\ncd TGVQSM-master-011045626121baa8bfdd6633929974c732ae35e3\npython setup.py install\ncd test_data\ntgv_qsm  -p epi3d_test_phase.nii.gz -m epi3d_test_mask.nii.gz -f 2.89 -t 0.027 -o epi3d_test_QSM\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-adding-fsl-to-wsl-ubuntu-1804\" class=\"anchor\" href=\"#adding-fsl-to-wsl-ubuntu-1804\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdding fsl to WSL Ubuntu 18.04\u003c/h1\u003e\n\u003cpre\u003e\u003ccode\u003ewget -O- http://neuro.debian.net/lists/bionic.us-ca.full | sudo tee /etc/apt/sources.list.d/neurodebian.sources.list\nsudo apt-key adv --recv-keys --keyserver hkp://pool.sks-keyservers.net:80 0xA5D32F012649A5A9\nsudo apt-get update\nsudo apt-get install fsl-5.0-core\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eadd \". /etc/fsl/5.0/fsl.sh\" to the end of your .profile file\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1615972979.0
  },
  {
    "data_format": 2,
    "description": "Code for the Optimisation of ID\u0027s using Python and Opt-AI",
    "filenames": [
      "Singularity",
      "Singularity.env-v2"
    ],
    "full_name": "DiamondLightSource/Opt-ID",
    "latest_release": "v2.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://travis-ci.org/DiamondLightSource/Opt-ID\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/54cbf520664efa3f8fc3298323da50593160dd744d2ec6bd5de8ed8dd3593e0d/68747470733a2f2f7472617669732d63692e6f72672f4469616d6f6e644c69676874536f757263652f4f70742d49442e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/DiamondLightSource/Opt-ID.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e  \u003ca href=\"https://coveralls.io/github/DiamondLightSource/Opt-ID?branch=master\u0026amp;service=github\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/dc50340a825cc5da0454649fde18840b6c0ec2d3b4dd91c5e8319d2319850548/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f4469616d6f6e644c69676874536f757263652f4f70742d49442f62616467652e7376673f6272616e63683d6d617374657226736572766963653d676974687562\" alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/DiamondLightSource/Opt-ID/badge.svg?branch=master\u0026amp;service=github\" style=\"max-width:100%;\"\u003e\u003c/a\u003e  \u003ca href=\"https://scrutinizer-ci.com/g/DiamondLightSource/Opt-ID/?branch=master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c0b7776aa669724907a66bc7d335a00b5606e9f3dc41409d189185f47b791cbb/68747470733a2f2f7363727574696e697a65722d63692e636f6d2f672f4469616d6f6e644c69676874536f757263652f4f70742d49442f6261646765732f7175616c6974792d73636f72652e706e673f623d6d6173746572\" alt=\"Scrutinizer Code Quality\" data-canonical-src=\"https://scrutinizer-ci.com/g/DiamondLightSource/Opt-ID/badges/quality-score.png?b=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://doi.org/10.5281/zenodo.3968577\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5bb0569d502774e80da711f971a82ba8beb8a3decdc75b595131dd5a79f03bf9/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e333936383537372e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3968577.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://singularity-hub.org/collections/4728\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-opt-id\" class=\"anchor\" href=\"#opt-id\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpt-ID\u003c/h1\u003e\n\u003cp\u003eCode for the Optimisation of ID\u0027s using Python and Opt-AI\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview-of-how-to-use-opt-id\" class=\"anchor\" href=\"#overview-of-how-to-use-opt-id\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview of how to use Opt-ID\u003c/h2\u003e\n\u003cp\u003eOpt-ID is run is by providing:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea main configuration file in YAML format which contains all the various\nparameters for the sort/shim job\u003c/li\u003e\n\u003cli\u003ean existing directory in which output data will be written to\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are two main flags, \u003ccode\u003e--sort\u003c/code\u003e and \u003ccode\u003e--shim\u003c/code\u003e, to run sort and shim jobs. The\nidea is that using either of these flags in conjunction with the YAML config\nfile will go through and run all the scripts that are used to produce\nintermediate files and pass them around appropriately, so then there\u0027s only one\ncommand needed to be executed to run a sort or shim job, and the YAML config\nfile is the single source of all the parameter information used for that\nparticular job.\u003c/p\u003e\n\u003cp\u003eThere are several other processes that Opt-ID provides that are desired to be\ndone after a sort/shim but don\u0027t require the sequence of scripts that a\nsort/shim job does (for example, the use of \u003ccode\u003ecompare.py\u003c/code\u003e to compare a shimmed\ngenome to the original genome), so the \u003ccode\u003e--sort\u003c/code\u003e and \u003ccode\u003e--shim\u003c/code\u003e flags aren\u0027t able\nto provide these sorts of processes. To do so, there are several shell scripts\nthat are autogenerated when a sort or shim job is run that can be executed.\nThese scripts run Opt-ID in the particular way that is needed to perform the\nprocess, without the user needing to worry about extra configuration on top of\nthe YAML file.\u003c/p\u003e\n\u003cp\u003eTaking the \u003ccode\u003ecompare.py\u003c/code\u003e example previously mentioned, a script would be\nautogenerated after a shim job called \u003ccode\u003ecompare_shim.sh\u003c/code\u003e that can be passed any\nshimmed genome file in the data directory, and it will take care of calling\nOpt-ID in the particular way it needs to in order to run the \u003ccode\u003ecompare.py\u003c/code\u003e script\nwith the appropriate parameters. More details on how to use these autogenerated\nshell scripts are below in the \"Using the autogenerated shell scripts\" section.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-data-directory\" class=\"anchor\" href=\"#data-directory\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eData directory\u003c/h2\u003e\n\u003cp\u003eThe data outputted by Opt-ID is split roughly into two categories:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elarge files such as \u003ccode\u003e.h5\u003c/code\u003e files\u003c/li\u003e\n\u003cli\u003esmaller files such as \u003ccode\u003e.json\u003c/code\u003e, \u003ccode\u003e.mag\u003c/code\u003e, \u003ccode\u003e.sh\u003c/code\u003e files\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe smaller files get written to the directory passed as the second parameter to\nOpt-ID, so if OptID was passed \u003ccode\u003e/home/FedID/my_dir\u003c/code\u003e then the smaller files would\nget written to \u003ccode\u003e/home/FedID/my_dir\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eThe larger files get written to a directory within \u003ccode\u003e/dls/tmp/FedID\u003c/code\u003e whose path\nis based on the user\u0027s FedID and also the name of the data directory passed to\nOpt-ID. The name of the directory created in \u003ccode\u003e/dls/tmp/FedID\u003c/code\u003e will be the name\nof the very last directory in the path passed to Opt-ID. For example, if the\npath \u003ccode\u003e/home/FedID/my_dir\u003c/code\u003e is passed to Opt-ID, then the directory\n\u003ccode\u003e/dls/tmp/FedID/my_dir\u003c/code\u003e will be created. Symlinks are then created in\n\u003ccode\u003e/home/FedID/my_dir\u003c/code\u003e to point to the larger files inside\n\u003ccode\u003e/dls/tmp/FedID/my_dir\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eOne reason behind having two separate directories containing different data\nfiles is due to the large size of the \u003ccode\u003e.h5\u003c/code\u003e files produced by Opt-ID and not\nhaving the space to put them just anywhere in the filesystem (\u003ccode\u003e/dls/tmp\u003c/code\u003e has\nmuch more available space than, for example, the home directory associated to a\nFedID). Another reason is that the automatic deletion of files in \u003ccode\u003e/dls/tmp\u003c/code\u003e can\nbe used to do some automatic periodic cleanup of old, large files.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-intended-usage\" class=\"anchor\" href=\"#intended-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntended usage\u003c/h3\u003e\n\u003cp\u003eThe intended usage of this dual-directory structure is that the smaller files\nare written to somewhere away from \u003ccode\u003e/dls/tmp\u003c/code\u003e so then they\u0027re not deleted\nperiodically and can be referred to later if needed, whilst the larger files are\nwritten to the user\u0027s directory in \u003ccode\u003e/dls/tmp\u003c/code\u003e so then they \u003cem\u003eare\u003c/em\u003e deleted\nperiodically. Therefore, it is advised that the directory provided to Opt-ID is\nnot a directory in \u003ccode\u003e/dls/tmp/FedID\u003c/code\u003e; this is not only because of potential\ndeletion of the smaller files, but also because passing a directory in\n\u003ccode\u003e/dls/tmp/FedID\u003c/code\u003e can cause some confusion regarding the directory that is\nsubsequently created by Opt-ID in \u003ccode\u003e/dls/tmp/FedID\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eIn particular, it is advised that the directory passed to Opt-ID is one within\n\u003ccode\u003e/dls/technical/id\u003c/code\u003e\u003c/strong\u003e, as this is where output data from other Opt-ID jobs has\ntypically been placed.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example-directory-structures\" class=\"anchor\" href=\"#example-directory-structures\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample directory structures\u003c/h3\u003e\n\u003cp\u003eFor example, if the directory \u003ccode\u003e/dls/technical/id/test/\u003c/code\u003e is passed to Opt-ID, the\nexpected directory structures right after having run a sort job on a cluster is\ngiven below:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e/dls/technical/id/test/\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003etest_sort.json\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etest_sort.mag\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etest_sort.h5 -\u0026gt; /dls/tmp/FedID/test/test_sort.h5\u003c/code\u003e (symlink to a file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003egenerate_report.sh\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003erestart_sort.sh\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003elogfiles/\u003c/code\u003e (directory)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003egenomes -\u0026gt; /dls/tmp/FedID/test/genomes/\u003c/code\u003e (symlink to a directory)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eprocess_genome_output -\u0026gt; /dls/tmp/FedID/test/process_genome_output/\u003c/code\u003e\n(symlink to a directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e/dls/tmp/FedID/test/\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003etest_sort.h5\u003c/code\u003e (file, the symlink \u003ccode\u003e/dls/technical/id/test/test_sort.h5\u003c/code\u003e points\nto this file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003egenomes/\u003c/code\u003e (directory, the symlink \u003ccode\u003e/dls/technical/id/test/genomes\u003c/code\u003e points to\nthis directory)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eprocess_genome_output/\u003c/code\u003e (directory, the symlink\n\u003ccode\u003e/dls/technical/id/test/process_genome_output\u003c/code\u003e points to this directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs another example, for the same directory being passed but instead a shim job\nbeing run on a cluster, the expected directory structures right after the job\nare:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e/dls/technical/id/test/\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003etest_shim.json\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etest_shim.mag\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etest_shim.h5 -\u0026gt; /dls/tmp/FedID/test/test_shim.h5\u003c/code\u003e (symlink to a file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003egenerate_report.sh\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecompare_shim.sh\u003c/code\u003e (file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003elogfiles/\u003c/code\u003e (directory)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eshimmed_genomes -\u0026gt; /dls/tmp/FedID/test/shimmed_genomes/\u003c/code\u003e (symlink to a\ndirectory)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eprocess_genome_output -\u0026gt; /dls/tmp/FedID/test/process_genome_output/\u003c/code\u003e\n(symlink to a directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e/dls/tmp/FedID/test/\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003etest_shim.h5\u003c/code\u003e (file, the symlink \u003ccode\u003e/dls/technical/id/test/test_shim.h5\u003c/code\u003e points\nto this file)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eshimmed_genomes/\u003c/code\u003e (directory, the symlink\n\u003ccode\u003e/dls/technical/id/test/shimmed_genomes\u003c/code\u003e points to this directory)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eprocess_genome_output/\u003c/code\u003e (directory, the symlink\n\u003ccode\u003e/dls/technical/id/test/process_genome_output\u003c/code\u003e points to this directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that the filenames \u003ccode\u003etest_sort.*\u003c/code\u003e and \u003ccode\u003etest_shim.*\u003c/code\u003e are just placeholders\nand have been chosen only for illustrative purposes, these files can be named as\ndesired in the YAML config file.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-preliminary-steps-to-be-able-to-run-opt-id\" class=\"anchor\" href=\"#preliminary-steps-to-be-able-to-run-opt-id\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreliminary steps to be able to run Opt-ID\u003c/h2\u003e\n\u003cp\u003eA process that is not done by Opt-ID is the transfer of magnet information in\nthe Excel files provided by the supplier to \u003ccode\u003e.sim\u003c/code\u003e files. To do so, from the\nExcel files supplied by the supplier, create tab delimited \u003ccode\u003e.sim\u003c/code\u003e files of\nmagnetisation. This is a manual procedure done only on Windows. Note that,\ncurrently, Opt-ID requires the magnet names in the \u003ccode\u003e.sim\u003c/code\u003e files to have leading\nzeros that pad out the name to 3 digits. For example, instead of \u00271\u0027 it should\nbe \u0027001\u0027.\u003c/p\u003e\n\u003cp\u003eTo get the code, clone the Opt-ID repo to the desired place in the filesystem.\nTo set up the environment for running Opt-ID on a Linux machine, in a terminal\nrun the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load python/3\nmodule load global/cluster\nexport PYTHONPATH=$PYTHONPATH:/path/to/Opt-ID\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhere \u003ccode\u003e/path/to/Opt-ID\u003c/code\u003e is the path to the root directory of the cloned repo.\n(There is a change to how \u003ccode\u003epython\u003c/code\u003e is used to run the code which is detailed in\nthe next section, and so the third command is to enable \u003ccode\u003epython\u003c/code\u003e to find the\ncode in the repo).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-opt-id-with-the-python-command\" class=\"anchor\" href=\"#running-opt-id-with-the-python-command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Opt-ID with the \u003ccode\u003epython\u003c/code\u003e command\u003c/h2\u003e\n\u003cp\u003eThe main script that is used for running Opt-ID is \u003ccode\u003eIDSort/src/optid.py\u003c/code\u003e. It\nshould be run using the syntax \u003ccode\u003epython -m IDSort.src.optid\u003c/code\u003e as opposed to\n\u003ccode\u003epython /path/to/Opt-ID/IDSort/src/optid.py\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-different-options-that-opt-id-can-be-run-with\" class=\"anchor\" href=\"#different-options-that-opt-id-can-be-run-with\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDifferent options that Opt-ID can be run with\u003c/h2\u003e\n\u003cp\u003eThere are two sets of flags from which one flag from each set is mandatory to be\npassed to Opt-ID, and the rest are optional and have sensible default values if\nthey are not provided.\u003c/p\u003e\n\u003cp\u003eThe mandatory sets of flags are\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003e--sort\u003c/code\u003e vs \u003ccode\u003e--shim\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--cluster-on\u003c/code\u003e vs \u003ccode\u003e--cluster-off\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ewhere only one flag from each bullet point should be provided.\u003c/p\u003e\n\u003cp\u003eExamples of running Opt-ID with the bare mininum flags and parameters it needs\nare:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m IDSort.src.optid --sort --cluster-on /path/to/yaml /path/to/data/dir\npython -m IDSort.src.optid --shim --cluster-off /path/to/yaml /path/to/data/dir\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content---sort-and---shim\" class=\"anchor\" href=\"#--sort-and---shim\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--sort\u003c/code\u003e and \u003ccode\u003e--shim\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThese are used for specifying what type of job is desired.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content---cluster-on-and---cluster-off\" class=\"anchor\" href=\"#--cluster-on-and---cluster-off\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--cluster-on\u003c/code\u003e and \u003ccode\u003e--cluster-off\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThese are used for specifying whether the job is run on the local machine or\nsubmitted to run on a cluster.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---num-threads---queue-and---node-os\" class=\"anchor\" href=\"#--num-threads---queue-and---node-os\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--num-threads\u003c/code\u003e, \u003ccode\u003e--queue\u003c/code\u003e, and \u003ccode\u003e--node-os\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eThese are used in conjunction with \u003ccode\u003e--cluster-on\u003c/code\u003e. Some examples of using these\nflags would be\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m IDSort.src.optid --sort --cluster-on --node-os rhel7 /path/to/yaml /path/to/data/dir\npython -m IDSort.src.optid --shim --cluster-on --queue low.q /path/to/yaml /path/to/data/dir\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content---seed-and---seed-value\" class=\"anchor\" href=\"#--seed-and---seed-value\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--seed\u003c/code\u003e and \u003ccode\u003e--seed-value\u003c/code\u003e\n\u003c/h4\u003e\n\u003cp\u003eThese are used in conjunction with \u003ccode\u003e--cluster-off\u003c/code\u003e. \u003ccode\u003e--seed\u003c/code\u003e is used to specify\nthat the random number generator (RNG) should be seeded and thus produce the\nsame output across multiple runs with the same parameters. \u003ccode\u003e--seed-value\u003c/code\u003e is\nspecified if a particular value to seed the RNG is desired (by default its value\nis 1). Some examples of using these flags would be\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m IDSort.src.optid --sort --cluster-off --seed /path/to/yaml /path/to/data/dir\npython -m IDSort.src.optid --shim --cluster-off --seed --seed-value 30 /path/to/yaml /path/to/data/dir\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-yaml-config-files\" class=\"anchor\" href=\"#yaml-config-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eYAML config files\u003c/h2\u003e\n\u003cp\u003eThe YAML config files contain the parameters used by the various scripts that\nOpt-ID runs. The top-level sections of the YAML config files are the script\nnames minus the \u003ccode\u003e.py\u003c/code\u003e and the subsections are the different parameters passed to\nthat particular script. For the most part, the subsection names are exactly the\nsame as the script parameters they\u0027re associated to, for example, the\n\u003ccode\u003eid_setup.py\u003c/code\u003e script has a \u003ccode\u003e--periods\u003c/code\u003e flag, and the YAML subsection\ncorresponding to that parameter is \u003ccode\u003eid_setup.periods\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eA few exceptions exist to try and be more descriptive with what the parameter\nis, for example, \u003ccode\u003eprocess_genome.py\u003c/code\u003e refers to the files it\u0027s given as elements\nof the \u003ccode\u003eargs\u003c/code\u003e list, but in the YAML the corresponding subsection for a shim job\nis \u003ccode\u003eprocess_genome.readable_genome_file\u003c/code\u003e which is hopefully a more useful\ndescription.\u003c/p\u003e\n\u003cp\u003eExamples of YAML config files can be found in the \u003ccode\u003eIDSort/example_configs\u003c/code\u003e\ndirectory. There are some placeholder values in these config files that aren\u0027t\nvalid values for their associated section in the YAML, and the following\nsections detail the changes that need to be made to the example config files to\nget them in a state ready to run a job.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sort-config-example\" class=\"anchor\" href=\"#sort-config-example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSort config example\u003c/h3\u003e\n\u003cp\u003eThere are three values that need to be changed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emagnets.hmags\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emagnets.hemags\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emagnets.htmags\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTheir values should be absolute paths to any \u003ccode\u003e.sim\u003c/code\u003e files of the relevant type.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-shim-config-example\" class=\"anchor\" href=\"#shim-config-example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eShim config example\u003c/h3\u003e\n\u003cp\u003eThere are five values that need to be changed:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emagnets.hmags\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emagnets.hemags\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emagnets.htmags\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eprocess_genome.readable_genome_file\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003empi_runner_for_shim_opt.bfield_filename\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe first three are the same as in the sort config example. The value of\n\u003ccode\u003eprocess_genome.readable_genome_file\u003c/code\u003e should be an absolute path to the \u003ccode\u003e.inp\u003c/code\u003e\nfile that is used to start the shim job from. The value of\n\u003ccode\u003empi_runner_for_shim_opt.bfield_filename\u003c/code\u003e should be an absolute path to the\n\u003ccode\u003e.h5\u003c/code\u003e file that is converted from \u003ccode\u003e.bfield\u003c/code\u003e files that are produced by igor.\u003c/p\u003e\n\u003cp\u003eNote that, currently, the use of the \u003ccode\u003eigor2h5.py\u003c/code\u003e script hasn\u0027t yet been\nintegrated into the YAML configuration file for Opt-ID, so the process of\nconverting \u003ccode\u003e.bfield\u003c/code\u003e data into \u003ccode\u003e.h5\u003c/code\u003e data is one that needs to be done by\nmanually executing the \u003ccode\u003eigor2h5.py\u003c/code\u003e script (or by any other means) prior to\nrunning a shim job with Opt-ID.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-the-autogenerated-shell-scripts\" class=\"anchor\" href=\"#using-the-autogenerated-shell-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the autogenerated shell scripts\u003c/h2\u003e\n\u003cp\u003eAll the autogenerated scripts can be executed from anywhere in the filesystem,\nit\u0027s not necessary for the current working directory to be the same directory\nthat the script is in.\u003c/p\u003e\n\u003cp\u003eDue to the facts that\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethese scripts are generated on a job-by-job basis and are only meant to be run\nfor the particular data within the directory the scripts are in\u003c/li\u003e\n\u003cli\u003ethe structure of the data directories are fixed and known in advance\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ewhen it comes to passing parameters to these scripts they are aware of the\nspecific directories that the files they\u0027re expecting should be in, so only\nfilenames need to be given to them and not absolute or even relative filepaths.\nConcrete examples are given below in the \u003ccode\u003egenerate_report.sh\u003c/code\u003e and\n\u003ccode\u003ecompare_shim.sh\u003c/code\u003e sections that hopefully explain in more detail how to pass\nparameters to these scripts.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-generate_reportsh\" class=\"anchor\" href=\"#generate_reportsh\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003egenerate_report.sh\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThis script is used to create a report with some useful data visualisation in a\nPDF file. For a sort job it can be passed multiple \u003ccode\u003e.genome\u003c/code\u003e and \u003ccode\u003e.inp\u003c/code\u003e files,\nand for a shim job it can be passed multiple \u003ccode\u003e.h5\u003c/code\u003e files that are associated to\nthe \"full genomes\" (as opposed to the smaller-sized \"compare genomes\") in the\nshim output.\u003c/p\u003e\n\u003cp\u003eFor a sort job, Opt-ID will look in both the \u003ccode\u003egenomes/\u003c/code\u003e and\n\u003ccode\u003eprocess_genome_output/\u003c/code\u003e directories for the given \u003ccode\u003e.genome\u003c/code\u003e and \u003ccode\u003e.inp\u003c/code\u003e files,\nand for a shim job Opt-ID will look in the \u003ccode\u003eshimmed_genomes/\u003c/code\u003e directory for the\ngiven \u003ccode\u003e.h5\u003c/code\u003e files. Therefore, the parameters passed to \u003ccode\u003egenerate_report.sh\u003c/code\u003e\nshould only be the filenames and not filepaths.\u003c/p\u003e\n\u003cp\u003eFor example, for a sort job, the correct way to pass a genome and a \u003ccode\u003e.inp\u003c/code\u003e file\nto the script would be\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/generate_report.sh foo.genome bar.inp\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eas opposed to\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/generate_report.sh genomes/foo.genome process_genome_output/bar.inp\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnother example: for a shim job, the correct way to pass \u003ccode\u003e.h5\u003c/code\u003e files to the\nscript would be\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/generate_report.sh foo.h5 bar.h5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eas opposed to\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/generate_report.sh shimmed_genomes/foo.h5 shimmed_genomes/bar.h5\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAn optional \u003ccode\u003e--report-filename\u003c/code\u003e flag can be passed before the files to specify\nthe name of the PDF file, and genome reports are stored in the \u003ccode\u003egenome_reports/\u003c/code\u003e\ndirectory within the directory passed to Opt-ID. Report filenames should have a\n\u003ccode\u003e.pdf\u003c/code\u003e extension to enable a simple check between the report filename parameter\nand \u003ccode\u003e.genome\u003c/code\u003e/\u003ccode\u003e.inp\u003c/code\u003e file parameters that follow it. The \u003ccode\u003e--report-filename\u003c/code\u003e\noption can be omitted and in that case the report filename will be a\nconcatenation of all the filenames passed with an underscore character \"_\" as\nthe separator between the filenames.\u003c/p\u003e\n\u003cp\u003eAn example of using the \u003ccode\u003e--report-filename\u003c/code\u003e flag is\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/generate_report --report-filename report.pdf foo.genome bar.inp\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-restart_sortsh\" class=\"anchor\" href=\"#restart_sortsh\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003erestart_sort.sh\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThis script requires no parameters and can be run simply as\n\u003ccode\u003e/path/to/restart_sort.sh\u003c/code\u003e, Opt-ID will take care of loading the YAML config of\nthe previous sort job and will use all the same flags and paramters as the\noriginal sort job. One example is that if the original sort job was run on a\ncluster, so will the restart-sort job, and another example is that the same\n\u003ccode\u003e.json\u003c/code\u003e, \u003ccode\u003e.mag\u003c/code\u003e and \u003ccode\u003e.h5\u003c/code\u003e (lookup table) files from the original sort job will\nbe reused in the restart-sort job instead of being regenerated.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-compare_shimsh\" class=\"anchor\" href=\"#compare_shimsh\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003ecompare_shim.sh\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThis can be passed a single \u003ccode\u003e.genome\u003c/code\u003e file that is in the \u003ccode\u003eshimmed_genomes/\u003c/code\u003e\ndirectory and it will generate a human readable diff between the original and\nshimmed genomes that will be written to the \u003ccode\u003eshim_diffs/\u003c/code\u003e directory. It\u0027s not\nnecessary to pass the original genome to this script, Opt-ID will take care of\nfinding it so only the shimmed genome needs to be given as a parameter.\u003c/p\u003e\n\u003cp\u003eSimilarly to what \u003ccode\u003egenerate_report.sh\u003c/code\u003e does, \u003ccode\u003ecompare_shim.sh\u003c/code\u003e will look in the\n\u003ccode\u003eshimmed_genomes/\u003c/code\u003e directory so only filenames should be passed to it and not\nfilepaths. An example of using this script would be:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/compare_shim.sh foo.genome\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAn optional \u003ccode\u003e--diff-filename\u003c/code\u003e flag can be passed before the shimmed genome file\nto specify the filename of the human readable diff. Currently Opt-ID appends a\n\u003ccode\u003e.txt\u003c/code\u003e extension to the filename so it\u0027s not necessary to put that in the\nparameter. Again, similarly to what \u003ccode\u003egenerate_report.sh\u003c/code\u003e does, if this flag is\nomitted then the diff filename is a concatenation of the original genome and\nshimmed genome filenames with an underscore character as the separator, and then\nalso prepended with \u003ccode\u003eshim_\u003c/code\u003e. For example, if the original genome is \u003ccode\u003efoo.genome\u003c/code\u003e\nand the shimmed genome is \u003ccode\u003ebar.genome\u003c/code\u003e, then if the \u003ccode\u003e--diff-filename\u003c/code\u003e flag is\nomitted then the diff filename would be \u003ccode\u003eshim_foo.genome_bar.genome.txt\u003c/code\u003e. An\nexample of using the \u003ccode\u003e--diff-filename\u003c/code\u003e flag is\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/path/to/compare_shim.sh --diff-filename my_shim foo.genome\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-hidden-options-of-opt-id\" class=\"anchor\" href=\"#hidden-options-of-opt-id\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\"Hidden\" options of Opt-ID\u003c/h2\u003e\n\u003cp\u003eThere are several options that Opt-ID has but are only meant to be used by the\nautogenerated shell scripts and not intended to be invoked directly by a user;\ntherefore, these options aren\u0027t of much interest to users and only of potential\ninterest to developers. The following are just some useful notes to any\ndevelopers viewing this document:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethese options are related to those kinds of processes that a user would want\nto do that aren\u0027t full sort/shim jobs that were referred to in the \"Overview\nof how to use Opt-ID\" section of this document\u003c/li\u003e\n\u003cli\u003ethese options are all used by the autogenerated shell scripts that were also\nreferred to in the \"Overview of how to use Opt-ID\" section, hence why the\nusers need not directly use them, the autogenerated scripts should take care\nof using these \"hidden options\" where necessary\u003c/li\u003e\n\u003cli\u003ethese are also processes that are done after a sort/shim, so they assume the\nexistence of a YAML config that has already been used for the sort/shim job,\nas well as any output data from a sort/shim job\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content---generate-report\" class=\"anchor\" href=\"#--generate-report\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--generate-report\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThis option starts off the process of using the\n\u003ccode\u003eIDSort/src/genome_report_template.ipynb\u003c/code\u003e file to generate a Jupyter notebook\nfile, and then running it to produce a PDF report.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content---restart-sort\" class=\"anchor\" href=\"#--restart-sort\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--restart-sort\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThis option starts off the process of reusing the same YAML config file that was\nused for the sort job to get all the parameters used for the original sort job,\nand then running Opt-ID to generate genomes from an initial population as\nopposed to generating genomes from scratch.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content---compare-shim\" class=\"anchor\" href=\"#--compare-shim\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003e--compare-shim\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eThis option starts off the process of comparing the given shimmed genome to the\noriginal genome that was used to start the shim job.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-the-tests\" class=\"anchor\" href=\"#running-the-tests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning the tests\u003c/h2\u003e\n\u003cp\u003eNavigate to the root directory of the Opt-ID repo:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /path/to/Opt-ID\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run all the tests:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m pytest IDSort/test/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo run a particular test in the \u003ccode\u003etest/\u003c/code\u003e directory, it can be specified in the\npath in the above command. For example, to run \u003ccode\u003eIDSort/test/magnets_test.py\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m pytest IDSort/test/magnets_test.py\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://codescene.io/projects/6289/jobs/latest-successful/results\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f3827d47125aaed62ec3276ebe498b2f14e96da020a3a3c25000597585019c5a/68747470733a2f2f636f64657363656e652e696f2f70726f6a656374732f363238392f7374617475732e737667\" alt=\"\" data-canonical-src=\"https://codescene.io/projects/6289/status.svg\" style=\"max-width:100%;\"\u003e Get more details at \u003cstrong\u003ecodescene.io\u003c/strong\u003e.\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 6,
    "subscribers_count": 8,
    "topics": [],
    "updated_at": 1609847329.0
  },
  {
    "data_format": 2,
    "description": "Scripts to run dask and jupyter lab on Singularity using the pangeo-notebook image",
    "filenames": [
      "Singularity.pangeo-notebook"
    ],
    "full_name": "pbranson/pangeo-hpc-singularity",
    "latest_release": null,
    "readme": "\u003cp\u003eThis repository provides some boiler plate scripts for running \u0027pangeo\u0027 python ecosystem using singularity containers.\u003c/p\u003e\n\u003cp\u003eSteps are:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eObtain docker image curated at \u003ca href=\"https://github.com/pangeo-data/pangeo-stacks\"\u003ehttps://github.com/pangeo-data/pangeo-stacks\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull pangeo/pangeo-notebook\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe pangeo-notebook has a pretty diverse set of libraries for most cloud,\ndask, zarr, netCDF, analysis type tasks.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e(Optional) Obtain docker image curated at \u003ca href=\"https://github.com/pangeo-data/pangeo-stacks\"\u003ehttps://github.com/pangeo-data/pangeo-stacks\u003c/a\u003e\nIf you need to customise, see minimal example in Dockerfile and requirements.txt and description here:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e(Deprecated) \u003ca href=\"https://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants\"\u003ehttps://github.com/pangeo-data/pangeo-stacks#customize-images-with-the--onbuild-variants\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e(\u003cstrong\u003eUse this since 27-07-2020\u003c/strong\u003e) \u003ca href=\"https://github.com/pangeo-data/pangeo-docker-images\"\u003ehttps://github.com/pangeo-data/pangeo-docker-images\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThen you would build a custom image along the lines of:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake pangeo-notebook\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConvert docker image to singularity with a command such as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity -d build pangeo-latest.sif docker-daemon://pangeo/pangeo-notebook:master\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy the created \u003ccode\u003epangeo-latest.sif\u003c/code\u003e singularity image to somewhere accessible on the HPC filesystem and edit the \u003ccode\u003econtainer=\u003c/code\u003e and \u003ccode\u003escheduler_file=\u003c/code\u003e variables in the \u003ccode\u003estart_jupyter.slurm\u003c/code\u003e and \u003ccode\u003estart_worker.slurm\u003c/code\u003e scripts to point to the singularity image and the shared filesystem location to write the scheduler details, respectively.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStart the jupyter lab and dask-scheduler, the first parameter is the working path you want to use for jupyter lab:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch start_jupyter.slurm $MYGROUP\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis starts a scheduler and jupyterlab with 2 cores each and 8GB/core memory. These can be edited in the #SBATCH headers, also note you can set the default directory for jupyterlab with the notebook_dir which is the parameter passed to start_jupyter.slurm.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eStart dask-workers (where n is the number of workers you want - these are configures for \u0026lt; 2 hour wall time limit so that they use the \u003ccode\u003eh2\u003c/code\u003e queue):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch -n 10 start_worker.slurm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ealso note that this input argument to dask-worker \u003ccode\u003e--local-directory $LOCALDIR\u003c/code\u003e tells the worker the path to local disk storage on the node which can be used for spilling data, but not all HPC nodes/centres have attached local storage. Currently this is disabled.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSee instruction printed to the slurm-######.out log file for connecting to the jupyter session running on the compute node, something like:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh -N -l pbranson -L 8888:compute-node123:8888 hpc-login.host.com\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand take note of the randomly generated token printed to the slurm-######.out log file. You will need that to login to Jupyterlab.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo connect to the dask-scheduler from a notebook use the following snippet:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport os\nfrom distributed import Client\nclient=Client(scheduler_file=os.environ[\u0027MYSCRATCH\u0027] + \u0027/scheduler.json\u0027)\nclient\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eView the scheduler bokeh dashboard at \u003ca href=\"http://localhost:8888/proxy/8787/status\" rel=\"nofollow\"\u003ehttp://localhost:8888/proxy/8787/status\u003c/a\u003e. This can also be entered into the Jupyterlab dask widget as \u003ccode\u003e/proxy/8787/status\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs a little cheat in jupyter lab I open up a terminal and then do\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003essh localhost\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto connect to the host running the jupyter container - this gives you access to the slurm job scheduler from that terminal and you can start workers  in there with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch start_worker.slurm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlso note that the dask worker specifications used in the \u003ccode\u003estart_worker.slurm\u003c/code\u003e script are based of the slurm environment variables, so you can alter the worker specification using the \u003ccode\u003e#SBATCH\u003c/code\u003e directives:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#SBATCH --ntasks=20\n#SBATCH --cpus-per-task=2\n#SBATCH --mem-per-cpu=10G\n#SBATCH --time=0:30:00\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor at the command line when you submit the script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esbatch -n 4 -c 4 --mem-per-cpu=16G start_worker.slurm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich would start 4 workers with 4 cores per worker and 16*4 = 64GB memory per dask-worker.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 7,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1624890978.0
  },
  {
    "data_format": 2,
    "description": "Target exome sequencing analysis for NYU NGS580 gene panel",
    "filenames": [
      "containers/variant-calling-0.0.2/Singularity.variant-calling-0.0.2",
      "containers/manta-1.5.0/Singularity.manta-1.5.0",
      "containers/R-3.2.3/Singularity.R-3.2.3",
      "containers/trimmomatic-0.36/Singularity.trimmomatic-0.36",
      "containers/bwa-0.7.17-sambamba-0.6.8/Singularity.bwa-0.7.17-sambamba-0.6.8",
      "containers/python-3.6/Singularity.python-3.6",
      "containers/reporting-3.4.3/Singularity.reporting-3.4.3",
      "containers/varscan-2.4.3/Singularity.varscan-2.4.3",
      "containers/R-3.5.1/Singularity.R-3.5.1",
      "containers/R-3.4.3/Singularity.R-3.4.3",
      "containers/bcftools-1.3/Singularity.bcftools-1.3",
      "containers/multiqc-1.5/Singularity.multiqc-1.5",
      "containers/deconstructSigs-1.8.0/Singularity.deconstructSigs-1.8.0",
      "containers/bedtools-2.27.1/Singularity.bedtools-2.27.1",
      "containers/cnvkit-0.9.0/Singularity.cnvkit-0.9.0",
      "containers/samtools-1.7/Singularity.samtools-1.7",
      "containers/sambamba-0.6.8/Singularity.sambamba-0.6.8",
      "containers/fastqc-0.11.7/Singularity.fastqc-0.11.7",
      "containers/strelka-2.9.10/Singularity.strelka-2.9.10",
      "containers/cnv_facets-0.14.0/Singularity.cnv_facets-0.14.0",
      "containers/delly2-0.7.7/Singularity.delly2-0.7.7",
      "containers/pindel-0.2.5b9/Singularity.pindel-0.2.5b9",
      "containers/annovar-150617/Singularity.annovar-150617",
      "containers/lofreq-2.1.3/Singularity.lofreq-2.1.3",
      "containers/bwa-0.7.17/Singularity.bwa-0.7.17",
      "containers/multiqc-1.4/Singularity.multiqc-1.4",
      "containers/python-2.7/Singularity.python-2.7",
      "containers/base/Singularity.base",
      "containers/sambamba-0.6.6/Singularity.sambamba-0.6.6",
      "containers/sambamba-0.6.6/Singularity.sambamba-0.6.6.old",
      "containers/msisensor-0.2/Singularity.msisensor-0.2",
      "containers/IGV-2.4.10/Singularity.IGV-2.4.10",
      "containers/htslib-1.7/Singularity.htslib-1.7",
      "containers/bedtools-2.26.0/Singularity.bedtools-2.26.0",
      "containers/cnvkit-0.9.5/Singularity.cnvkit-0.9.5",
      "containers/variant-calling-0.0.1/Singularity.variant-calling-0.0.1"
    ],
    "full_name": "NYU-Molecular-Pathology/NGS580-nf",
    "latest_release": "19.10.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-ngs580-nf\" class=\"anchor\" href=\"#ngs580-nf\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNGS580-nf\u003c/h1\u003e\n\u003cp\u003eTarget exome analysis for 580 gene panel (NGS580)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e Details listed here may change during development\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis pipeline is designed to run targeted exome analysis on Illumina Next-Gen sequencing genomic data, in support of the NGS580 cancer diagnostic panel for NYU\u0027s Molecular Pathology Department.\u003c/p\u003e\n\u003cp\u003eThis pipeline starts from paired-end fastq data (\u003ccode\u003e.fastq.gz\u003c/code\u003e), and is meant to accompany the output from the Illumina demultiplexing pipeline listed here: \u003ca href=\"https://github.com/NYU-Molecular-Pathology/demux-nf\"\u003ehttps://github.com/NYU-Molecular-Pathology/demux-nf\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe NGS580-nf analysis workflow includes read trimming, QC, alignment, variant calling, annotation, and reporting, along with many other steps.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h2\u003e\n\u003cp\u003eSome key pipeline components included in this repository:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ebin\u003c/code\u003e: directory of custom scripts used throughout the pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003econtainers\u003c/code\u003e: directory of container recipes (Docker, Singularity) for use with the pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eexample\u003c/code\u003e: directory of example samplesheets, etc., to show the format used with this pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003etargets\u003c/code\u003e: directory of target region .bed files included with the pipeline for typical analyses\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eMakefile\u003c/code\u003e: A Makefile with recipes for configuring, starting, and managing the pipeline. This is meant to be the main interface between the end-user and the pipeline. The Makefile should be reviewed as-needed to familiarize yourself with the methods and configurations that are meant to be used for running and managing the pipeline.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003emain.nf\u003c/code\u003e: the main Nextflow pipeline script\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003enextflow.config\u003c/code\u003e: configuration file for the main Nextflow pipeline script\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e.config.json\u003c/code\u003e: a template for the required \u003ccode\u003econfig.json\u003c/code\u003e file used in the pipeline, shows the default pipeline settings that are meant to be easily modified by the end-user and used within the pipeline for data processing.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eannovar_db.nf\u003c/code\u003e, \u003ccode\u003ecnv-pool.nf\u003c/code\u003e, \u003ccode\u003ehapmap-pool.nf\u003c/code\u003e, \u003ccode\u003eref.nf\u003c/code\u003e: workflows for generating and downloading extra reference files used in the main pipeline.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eref\u003c/code\u003e: default location for the storage of reference files (not used on NYU Big Purple HPC)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-items\" class=\"anchor\" href=\"#pipeline-items\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Items\u003c/h2\u003e\n\u003cp\u003eSome key components that are created during setup, configuration, and execution of the pipeline:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esamples.analysis.tsv\u003c/code\u003e: the main samplesheet definig input items for the pipeline (described below)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003econfig.json\u003c/code\u003e: configuration file used for pipeline settings (see \u003ccode\u003e.config.json\u003c/code\u003e template for example)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eoutput\u003c/code\u003e: analysis output files published by the Nextflow pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ework\u003c/code\u003e: Nextflow temporary directories for execution of pipeline tasks\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003etrace.txt\u003c/code\u003e, \u003ccode\u003enextflow.html\u003c/code\u003e, \u003ccode\u003etimeline.html\u003c/code\u003e, \u003ccode\u003e.nextflow.log\u003c/code\u003e: Nextflow execution logs and reports\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003elogs\u003c/code\u003e: directory for pipeline execution logs\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-setup\" class=\"anchor\" href=\"#setup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSetup\u003c/h1\u003e\n\u003cp\u003eThis repository should first be cloned from GitHub:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/NYU-Molecular-Pathology/NGS580-nf.git\ncd NGS580-nf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eOnce a copy of the repo is made, it can be used to \"deploy\" new copies of the workflow in a pre-configured state\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-reference-data\" class=\"anchor\" href=\"#reference-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eReference Data\u003c/h2\u003e\n\u003cp\u003eNextflow pipelines have been included for downloading required reference data, including ANNOVAR reference databases. You can run them with the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake setup\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-hapmap-pool-bam\" class=\"anchor\" href=\"#hapmap-pool-bam\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHapMap Pool .bam\u003c/h3\u003e\n\u003cp\u003eA negative control HapMap pool .bam file can be prepared using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake hapmap-pool\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRequires \u003ccode\u003esamples.hapmap.tsv\u003c/code\u003e file specifying the .bam files to be combined (example included at \u003ccode\u003eexample/samples.hapmap.tsv\u003c/code\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis file is typically built from multiple HapMap samples previously aligned by this pipeline. For demonstration purposes, you can provide any .bam and .bai files.\u003c/p\u003e\n\u003cp\u003eThe HapMap Pool files to be used in the pipeline should be set under the \u003ccode\u003eHapMapBam\u003c/code\u003e and \u003ccode\u003eHapMapBai\u003c/code\u003e keys of \u003ccode\u003econfig.json\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-cnv-pool\" class=\"anchor\" href=\"#cnv-pool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCNV Pool\u003c/h3\u003e\n\u003cp\u003eA control normal sample .cnn file for CNV calling can be prepared using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake cnv-pool\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRequires \u003ccode\u003esamples.cnv.tsv\u003c/code\u003e file specifying the .bam files to be used (example included at \u003ccode\u003eexample/samples.cnv.tsv\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis file is typically built from .bam files of specially chosen normal tissue sequencing samples previously aligned by this pipeline. For demonstration purposes, you can create the .cnn file from any desired .bam file. Note that the targets .bed file used to create the .cnn file must match the targets used in the rest of the pipeline.\u003c/p\u003e\n\u003cp\u003eThe .cnn file to be used in the pipeline should be set under the \u003ccode\u003eCNVPool\u003c/code\u003e key in \u003ccode\u003econfig.json\u003c/code\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h2\u003e\n\u003cp\u003eThe \u003ccode\u003econtainers\u003c/code\u003e directory contains instructions and recipes for building the Docker and Singularity containers used in the pipeline.\u003c/p\u003e\n\u003cp\u003eDocker is typically used for local container development, while Singularity containers are used on the NYU Big Purple HPC cluster. The current pipeline configuration for Big Purple uses \u003ccode\u003e.simg\u003c/code\u003e files stored in a common location on the file system.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003eThe pipeline is designed to start from demultiplexed paired end \u003ccode\u003e.fastq.gz\u003c/code\u003e files, with sample ID, tumor ID, and matched normal ID associations defined for each set of R1 and R2 .fastq file using a file \u003ccode\u003esamples.analysis.tsv\u003c/code\u003e (example included at \u003ccode\u003eexample/samples.analysis.tsv\u003c/code\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deployment\" class=\"anchor\" href=\"#deployment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeployment\u003c/h2\u003e\n\u003cp\u003eThe easiset way to use the pipeline is to \"deploy\" a new instance of it based on output from the demultiplexing pipeline \u003ca href=\"https://github.com/NYU-Molecular-Pathology/demux-nf\"\u003e\u003ccode\u003edemux-nf\u003c/code\u003e\u003c/a\u003e. This will automatically propagate configurations and information from the demultiplexing output.\u003c/p\u003e\n\u003cp\u003eThe pipeline can also deploy a new, pre-configured copy of itself using the included \u003ccode\u003edeploy\u003c/code\u003e recipe:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake deploy PRODDIR=/path/to/NGS580_analyses RUNID=Name_for_analysis FASTQDIR=/path/to/fastq_files\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eAn optional argument \u003ccode\u003eDEMUX_SAMPLESHEET\u003c/code\u003e can be used to provide a specially formatted demultiplexing samplesheet to be used for extracting extra sample information (example included at \u003ccode\u003eexample/demux-SampleSheet.csv\u003c/code\u003e; note the extra columns labeling tumor-normal pair IDs, used later).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-create-config\" class=\"anchor\" href=\"#create-config\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Config\u003c/h2\u003e\n\u003cp\u003eA file \u003ccode\u003econfig.json\u003c/code\u003e is required to hold settings for the pipeline. It should be created using the built-in methods:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake config RUNID=my_run_ID FASTQDIR=/path/to/fastqs\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake config RUNID=my_run_ID FASTQDIRS=\u0027/path/to/fastqs1 /path/to/fastqs2\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecp .config.json config.json\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand then simply edit the new \u003ccode\u003econfig.json\u003c/code\u003e and update the items to match your pipeline settings.\u003c/p\u003e\n\u003cp\u003eOnce created, the \u003ccode\u003econfig.json\u003c/code\u003e file can be updated manually as needed. The template and default values can be viewed in the included \u003ccode\u003e.config.json\u003c/code\u003e file.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003econfig.json\u003c/code\u003e should be generated automatically if you used \u003ccode\u003emake deploy\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-create-samplesheet\" class=\"anchor\" href=\"#create-samplesheet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreate Samplesheet\u003c/h2\u003e\n\u003cp\u003eA samplesheet file \u003ccode\u003esamples.analysis.tsv\u003c/code\u003e is required in order to define the input samples and their associated .fastq files (example included at \u003ccode\u003eexample/samples.analysis.tsv\u003c/code\u003e). Create a samplesheet, based on the config file, using the built-in methods:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake samplesheet\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eNote that this uses the values previously saved in \u003ccode\u003econfig.json\u003c/code\u003e to create the samplesheet\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sample-pairs-optional\" class=\"anchor\" href=\"#sample-pairs-optional\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSample Pairs (Optional)\u003c/h3\u003e\n\u003cp\u003eThe NGS580-nf pipeline has special processing for tumor-normal pairs. These pairs should be defined in the \u003ccode\u003esamples.analysis.tsv\u003c/code\u003e file, by listing the matched Normal sample for each applicable sample.\u003c/p\u003e\n\u003cp\u003eIn order to update \u003ccode\u003esamples.analysis.tsv\u003c/code\u003e automatically with these sample pairs, an extra samplesheet can be provided with the tumor-normal pairs.\u003c/p\u003e\n\u003cp\u003eCreate a \u003ccode\u003esamples.tumor.normal.csv\u003c/code\u003e samplesheet (example included at \u003ccode\u003eexample/samples.tumor.normal.csv\u003c/code\u003e) with the tumor-normal groupings for your samples, and update the original samplesheet with it by running the following script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython update-samplesheets.py --tumor-normal-sheet samples.tumor.normal.csv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf a demultiplexing samplesheet with extra tumor-normal pairs information was supplied (see example: \u003ccode\u003eexample/demux-SampleSheet.csv\u003c/code\u003e), then it can be used to update the samplesheet with pairs information with the following recipe:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake pairs PAIRS_SHEET=demux-SampleSheet.csv PAIRS_MODE=demux\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003cp\u003eThe pipeline includes an auto-run functionality that attempts to determine the best configuration to use for NYU phoenix and Big Purple HPC clusters:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will run the pipeline in the current session.\u003c/p\u003e\n\u003cp\u003eIn order to run the pipeline in the background as a job on NYU\u0027s Big Purple HPC, you should instead use the \u003ccode\u003esubmit\u003c/code\u003e recipe:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake submit SUBQ=fn_medium\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhere \u003ccode\u003eSUBQ\u003c/code\u003e is the name of the SLURM queue you wish to use.\u003c/p\u003e\n\u003cp\u003eRefer to the \u003ccode\u003eMakefile\u003c/code\u003e for more run options.\u003c/p\u003e\n\u003cp\u003eDue to the scale of the pipeline, a \"local\" run option is not currently configured, but can be set up easily based on the details shown in the Makefile and \u003ccode\u003enextflow.config\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-extra-parameters\" class=\"anchor\" href=\"#extra-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExtra Parameters\u003c/h3\u003e\n\u003cp\u003eYou can supply extra parameters for Nextflow by using the \u003ccode\u003eEP\u003c/code\u003e variable included in the Makefile, like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run EP=\u0027--runID 180320_NB501073_0037_AH55F3BGX5\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-demo\" class=\"anchor\" href=\"#demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo\u003c/h3\u003e\n\u003cp\u003eA demo dataset can be loaded using the following command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake demo\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003echeckout a demo dataset\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ecreate a \u003ccode\u003esamples.analysis.tsv\u003c/code\u003e samplesheet for the analysis\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eYou can then proceed to run the analysis with the commands described above.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-more-functionality\" class=\"anchor\" href=\"#more-functionality\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMore Functionality\u003c/h2\u003e\n\u003cp\u003eExtra functions included in the Makefile for pipeline management include:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-make-clean\" class=\"anchor\" href=\"#make-clean\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003emake clean\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eRemoves all Nextflow output except for the most recent run. Use \u003ccode\u003emake clean-all\u003c/code\u003e to remove all pipeline outputs.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-make-record-presome_prefix_\" class=\"anchor\" href=\"#make-record-presome_prefix_\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003emake record PRE=some_prefix_\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003e\"Records\" copies of the most recent pipeline run\u0027s output logs, configuration, Nextflow reports, etc.. Useful for recording analyses that failed or had errors in order to debug. Include the optional argument \u003ccode\u003eTASK\u003c/code\u003e to specify a Nextflow \u003ccode\u003ework\u003c/code\u003e directory to include in the records (example: \u003ccode\u003emake record PRE=error_something_broke_ TASK=e9/d9ff34\u003c/code\u003e).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-make-kill\" class=\"anchor\" href=\"#make-kill\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003emake kill\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eAttempts to cleanly shut down a pipeline running on a remote host e.g. inside a SLURM HPC compute job. Note that you can also use \u003ccode\u003escancel\u003c/code\u003e to halt the parent Nextflow pipeline job as well.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-make-fix-permissions-make-fix-group\" class=\"anchor\" href=\"#make-fix-permissions-make-fix-group\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003emake fix-permissions\u003c/code\u003e, \u003ccode\u003emake fix-group\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eAttempts to fix usergroup and permissions issues that may arise on shared systems with multiple users. Be sure to use the extra argument \u003ccode\u003eUSERGROUP=somegroup\u003c/code\u003e to specify the usergroup to update to.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-make-finalize-work-rm\" class=\"anchor\" href=\"#make-finalize-work-rm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003emake finalize-work-rm\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eExamines the \u003ccode\u003etrace.txt\u003c/code\u003e output from the most recent completed pipeline run in order to determine while subdirectories in the Nextflow \u003ccode\u003ework\u003c/code\u003e dir are no longer needed, and then deletes them. Can delete multiple subdirs in parallel when run with \u003ccode\u003emake finalize-work-rm -j 20\u003c/code\u003e e.g. specifying to delete 20 at a time, etc.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-software\" class=\"anchor\" href=\"#software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware\u003c/h1\u003e\n\u003cp\u003eDeveloped under Centos 6, RHEL 7, macOS 10.12\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ebash\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGNU \u003ccode\u003emake\u003c/code\u003e, standard GNU tools\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePython 2/3\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eJava 8+ for Nextflow\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDocker/Singularity as needed for containers\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 8,
    "subscribers_count": 2,
    "topics": [
      "nextflow",
      "pipeline",
      "docker",
      "singularity",
      "exome",
      "exome-sequencing-analysis"
    ],
    "updated_at": 1599538191.0
  },
  {
    "data_format": 2,
    "description": "definition files and wrapper scripts used by NIH HPC staff to install user-facing apps on the Biowulf cluster",
    "filenames": [
      "systems-biology/cellphonedb/2.1.7/cellphonedb.def",
      "systems-biology/cellphonedb/2.1.2/cellphonedb.def",
      "utilities/datalad/0.13.0rc2/datalad.def",
      "utilities/vcf2db/2020.09.14/vcf2db.def",
      "utilities/atom/1.13.1/atom.def",
      "utilities/pdf2svg/0.2.3/pdf2svg.def",
      "utilities/visidata/2.2/visidata.def",
      "utilities/gdc-client/1.5.0/gdc-client.def",
      "utilities/whatshap/0.18/whatshap.def",
      "utilities/snp-sites/2.4.1/snp-sites.def",
      "utilities/sysbench/1.0.20/sysbench.def",
      "utilities/sysbench/1.0.11/sysbench.def",
      "utilities/longshot/0.3.5/longshot.def",
      "utilities/ariba/2.14.4/ariba.def",
      "utilities/uropa/3.5.0/uropa.def",
      "utilities/pyega3/3.3.0/pyega3.def",
      "utilities/xvfb/1.19.6/xvfb.def",
      "mass-spectrometry/maxquant/1.6.7.0/maxquant.def",
      "mass-spectrometry/maxquant/1.6.17.0/maxquant.def",
      "mass-spectrometry/maxquant/1.6.3.3/maxquant.def",
      "linkage-phylogenetics/bali-phy/3.5/bali-phy.def",
      "linkage-phylogenetics/gubbins/2.3.4/gubbins.def",
      "structural-biology/pymol/2.4.0/pymol.def",
      "structural-biology/pymol/2.3.0/pymol_2.3.0.def",
      "structural-biology/parsnip/20180507/parsnip.def",
      "structural-biology/rdock/2013.1/rdock.def",
      "computational-chemistry/ampl/f35623d4/ampl.def",
      "high-throughput-sequencing/maggie/0.3.4/maggie.def",
      "high-throughput-sequencing/surpi/1.0.67/surpi.def",
      "high-throughput-sequencing/salmon/1.4.0/salmon.def",
      "high-throughput-sequencing/xengsort/28762aac/xengsort.def",
      "high-throughput-sequencing/rmats/4.0.2/rmats.def",
      "high-throughput-sequencing/medaka/1.0.3/medaka.def",
      "high-throughput-sequencing/medaka/1.2.0/medaka.def",
      "high-throughput-sequencing/medaka/0.12.1/medaka.def",
      "high-throughput-sequencing/sicer/2-1.0.2/sicer.def",
      "high-throughput-sequencing/hicexplorer/3.5.1/hicexplorer.def",
      "high-throughput-sequencing/raremetal/4.15.1/raremetal.def",
      "high-throughput-sequencing/canvas/1.40/canvas.def",
      "high-throughput-sequencing/bamliquidator/1.3.8/bamliquidator.def",
      "high-throughput-sequencing/taiji/1.2.0/taiji.def",
      "high-throughput-sequencing/taiji/1.1.0/taiji.def",
      "high-throughput-sequencing/flye/2.7/flye.def",
      "high-throughput-sequencing/flye/2.8-1/flye.def",
      "high-throughput-sequencing/mitosuite/1.0.9b/mitosuite.def",
      "high-throughput-sequencing/svtyper/0.7.1/svtyper.def",
      "high-throughput-sequencing/atac_dnase_pipelines/0.3.4-19-gcbd2a00/atac_dnase_pipelines.def",
      "high-throughput-sequencing/abruijn/1.0/abruijn.def",
      "high-throughput-sequencing/pcap-core/4.3.5/pcap-core.def",
      "high-throughput-sequencing/brass/6.1.2/brass.def",
      "high-throughput-sequencing/brass/6.3.4/brass.def",
      "high-throughput-sequencing/seqlinkage/1.0/seqlinkage.def",
      "high-throughput-sequencing/sve/0.1.0/sve.def",
      "high-throughput-sequencing/flappie/2.1.3/flappie.def",
      "high-throughput-sequencing/flappie/1.0.0/flappie.def",
      "high-throughput-sequencing/humann/3.0.0-alpha.3/humann.def",
      "high-throughput-sequencing/biom-format/2.1.10/biom-format.def",
      "high-throughput-sequencing/svtk/0.1/svtk.def",
      "high-throughput-sequencing/mtoolbox/1.1/mtoolbox.def",
      "high-throughput-sequencing/cnvnator/0.4.1/cnvnator.def",
      "high-throughput-sequencing/metabat/2.13/metabat.def",
      "high-throughput-sequencing/multiqc/1.10/multiqc.def",
      "high-throughput-sequencing/multiqc/1.9/multiqc.def",
      "high-throughput-sequencing/idep/0.81/idep.def",
      "high-throughput-sequencing/ricopili/2019_Jun_25.001/ricopili.def",
      "high-throughput-sequencing/atropos/1.1.18/atropos.def",
      "high-throughput-sequencing/stream/20180816/stream.def",
      "high-throughput-sequencing/tetoolkit/2.1.4/tetoolkit.def",
      "high-throughput-sequencing/tetoolkit/2.2.1/tetoolkit.def",
      "high-throughput-sequencing/vagrent/3.3.4/vagrent.def",
      "high-throughput-sequencing/deeptools/3.4.2/deeptools.def",
      "high-throughput-sequencing/deeptools/3.5.0/deeptools.def",
      "high-throughput-sequencing/rsd/1.1.7/rsd.def",
      "high-throughput-sequencing/guppy/3.4.5/guppy.def",
      "high-throughput-sequencing/guppy/4.2.2/guppy.def",
      "high-throughput-sequencing/guppy/4.0.15/guppy.def",
      "high-throughput-sequencing/transvar/2.5.9/transvar.def",
      "high-throughput-sequencing/parliament/0.1.7/parliament.def",
      "high-throughput-sequencing/pepr/1.1.24/pepr.def",
      "high-throughput-sequencing/csvkit/1.0.5/csvkit.def",
      "high-throughput-sequencing/delly/0.8.7/delly.def",
      "high-throughput-sequencing/mageck-vispr/0.5.4/mageck-vispr.def",
      "high-throughput-sequencing/megalodon/2.2.9/megalodon.def",
      "high-throughput-sequencing/cicero/0.3.0/cicero.def",
      "high-throughput-sequencing/repeatmodeler/2.0.1/repeatmodeler.def",
      "high-throughput-sequencing/cnvkit/0.9.6/cnvkit.def",
      "high-throughput-sequencing/cnvkit/0.9.8/cnvkit.def",
      "high-throughput-sequencing/rnapeg/current/rnapeg.def",
      "high-throughput-sequencing/deepsignal/0.1.8/deepsignal.def",
      "high-throughput-sequencing/bigscale2/20191119/bigscale2.def",
      "high-throughput-sequencing/tandemtools/current/tandemtools.def",
      "high-throughput-sequencing/hap.py/0.3.9/hap.py.def",
      "high-throughput-sequencing/epic2/0.0.41/epic2.def",
      "high-throughput-sequencing/neusomatic/0.2.1/neusomatic.def",
      "high-throughput-sequencing/fusioninspector/2.3.0/fusioninspector.def",
      "high-throughput-sequencing/fusioninspector/2.5.0/fusioninspector.def",
      "high-throughput-sequencing/bison/0.4.0/bison.def",
      "high-throughput-sequencing/pychopper/2.4.0/pychopper.def",
      "high-throughput-sequencing/crossmap/0.5.2/crossmap.def",
      "high-throughput-sequencing/htseq/0.11.4/htseq.def",
      "high-throughput-sequencing/lefse/1.0.7/lefse.def",
      "high-throughput-sequencing/lefse/1.0.8/lefse.def",
      "high-throughput-sequencing/rilseq/0.75/rilseq.def",
      "high-throughput-sequencing/umitools/1.1.1/umitools.def",
      "high-throughput-sequencing/freebayes/1.3.5/freebayes.def",
      "high-throughput-sequencing/slamdunk/0.4.3/slamdunk.def",
      "high-throughput-sequencing/bamutil/1.0.15/bamutil.def",
      "high-throughput-sequencing/cancerit-wgs/2.1.0/cancerit-wgs.def",
      "high-throughput-sequencing/vireosnp/0.5.1/vireosnp.def",
      "high-throughput-sequencing/vireosnp/0.3.2/vireosnp.def",
      "high-throughput-sequencing/humann2/2.8.1/humann2.def",
      "high-throughput-sequencing/macs/2.2.7.1/macs.def",
      "high-throughput-sequencing/pvactools/1.5.5/pvactools.def",
      "high-throughput-sequencing/pvactools/2.0.1/pvactools.def",
      "high-throughput-sequencing/ascatngs/4.3.3/ascatngs.def",
      "high-throughput-sequencing/ascatngs/4.3.4/ascatngs.def",
      "high-throughput-sequencing/ascatngs/4.5.0/ascatngs.def",
      "high-throughput-sequencing/tvc/5.10.1/tvc.def",
      "high-throughput-sequencing/cgpbattenberg/3.5.3/cgpbattenberg.def",
      "high-throughput-sequencing/eager/1.92/eager.def",
      "high-throughput-sequencing/vep/103/vep.def",
      "high-throughput-sequencing/vep/101/vep.def",
      "high-throughput-sequencing/vep/97/vep.def",
      "high-throughput-sequencing/bamsurgeon/1111e5d/bamsurgeon.def",
      "high-throughput-sequencing/busco/4.1.3/busco.def",
      "high-throughput-sequencing/busco/5.0.0/busco.def",
      "high-throughput-sequencing/bamreadcount/cram-v0.0.1/bamreadcount.def",
      "high-throughput-sequencing/scramble/1.0.1-32893ef/scramble.def",
      "high-throughput-sequencing/scramble/0.0.20190211.82c78b9/scramble.def",
      "high-throughput-sequencing/dropest/0.8.6/dropest.def",
      "high-throughput-sequencing/rseqc/4.0.0/rseqc.def",
      "high-throughput-sequencing/cellsnp/0.1.7/cellsnp.def",
      "high-throughput-sequencing/cellsnp/0.3.2/cellsnp.def",
      "high-throughput-sequencing/cutadapt/2.10/cutadapt.def",
      "high-throughput-sequencing/cutadapt/3.0/cutadapt.def",
      "high-throughput-sequencing/cutadapt/1.18/cutadapt.def",
      "high-throughput-sequencing/gridss/2.9.4/gridss.def",
      "high-throughput-sequencing/tpmcalculator/0.0.3/tpmcalculator.def",
      "high-throughput-sequencing/tpmcalculator/0.0.4/tpmcalculator.def",
      "high-throughput-sequencing/hicpro/2.11.4/hicpro.def",
      "high-throughput-sequencing/deepvariant/1.1.0/deepvariant.def",
      "high-throughput-sequencing/deepvariant/0.9.0/deepvariant.def",
      "high-throughput-sequencing/deepvariant/0.10.0/deepvariant.def",
      "high-throughput-sequencing/metaphlan/3.0/metaphlan.def",
      "high-throughput-sequencing/metaphlan/3.0.6/metaphlan.def",
      "high-throughput-sequencing/hail/0.2.3/hail.def",
      "high-throughput-sequencing/hail/0.2.61/hail.def",
      "high-throughput-sequencing/hail/0.2.56/hail.def",
      "high-throughput-sequencing/crispresso/2.0.45/crispresso.def",
      "high-throughput-sequencing/crispresso/2.0.40/crispresso.def",
      "high-throughput-sequencing/gossamer/ac492a8/gossamer.def",
      "sequence-analysis/orffinder/0.4.3-sing-install/orffinder.def",
      "sequence-analysis/eukrep/20180308/eukrep.def",
      "sequence-analysis/glu/1.0b3/glu.def",
      "sequence-analysis/augustus/3.3.3/augustus.def",
      "sequence-analysis/chipseq_pipeline/1.2.0/chipseq_pipeline.def",
      "sequence-analysis/htgtsrep/9fe74ff/htgtsrep.def",
      "sequence-analysis/arriba/2.0.0/arriba.def",
      "sequence-analysis/arriba/1.2.0/arriba.def",
      "sequence-analysis/braker/2/braker.def",
      "sequence-analysis/saige/0.44.1/saige.def",
      "sequence-analysis/acfs/20180316/acfs.def",
      "sequence-analysis/cactus/1.2.3/cactus.def",
      "sequence-analysis/roary/3.12.0/roary.def",
      "sequence-analysis/roary/3.13.0/roary.def",
      "sequence-analysis/annogesic/1.0.2/annogesic.def",
      "sequence-analysis/svtools/0.5.1/svtools.def",
      "sequence-analysis/sonicparanoid/1.3.5/sonicparanoid.def",
      "sequence-analysis/sonicparanoid/1.3.2/sonicparanoid.def",
      "sequence-analysis/vcf-kit/0.1.6/vcf-kit.def",
      "sequence-analysis/qtltools/1.3.1/qtltools.def",
      "sequence-analysis/wisexome/20180814/wisexome.def",
      "sequence-analysis/focus/0.6.10/focus.def",
      "sequence-analysis/smoove/0.2.5/smoove.def",
      "sequence-analysis/smoove/0.2.1/smoove.def",
      "sequence-analysis/cicero/1.4.0/cicero.def",
      "sequence-analysis/m-tools/20210208/m-tools.def",
      "sequence-analysis/ldsc/3d0c4464/ldsc.def",
      "sequence-analysis/prokka/1.13/prokka.def",
      "sequence-analysis/prokka/1.14.6/prokka.def",
      "sequence-analysis/deepsea/0.94c/deepsea.def",
      "sequence-analysis/anvio/7/anvio.def",
      "sequence-analysis/mmarge/1.0/mmarge.def",
      "sequence-analysis/xhla/2018-04-04/xhla.def",
      "sequence-analysis/intarna/3.2.0/intarna.def",
      "sequence-analysis/phaser/1.1.1/phaser.def",
      "sequence-analysis/accurity/20180724/accurity.def",
      "sequence-analysis/accurity/20210209/accurity.def",
      "sequence-analysis/asgal/1.0/asgal.def",
      "sequence-analysis/glnexus/1.1.11/glnexus.def",
      "sequence-analysis/glnexus/1.2.7/glnexus.def",
      "sequence-analysis/bamgineer/2-20200624/bamgineer.def",
      "sequence-analysis/netoglyc/3.1d/netoglyc.def",
      "mathematical-statistics/omeclust/1.1.6/omeclust.def",
      "mathematical-statistics/omeclust/1.1.4/omeclust.def",
      "mathematical-statistics/m2clust/1.1.3/m2clust.def",
      "mathematical-statistics/m2clust/0.0.7/m2clust.def",
      "mathematical-statistics/m2clust/0.0.8/m2clust.def",
      "molecular-modeling-graphics/starseqr/0.6.7/starseqr.def",
      "molecular-modeling-graphics/chimerax/1.1/chimerax.def",
      "molecular-modeling-graphics/chimerax/0.93/chimerax.def",
      "molecular-modeling-graphics/blender/2.82/blender.def",
      "image-analysis/resmap/1.95/resmap.def",
      "image-analysis/terastitcher/1.11.10/terastitcher.def",
      "image-analysis/terastitcher/1.10.8/terastitcher.def",
      "image-analysis/minc-toolkit/1.9.18/minc-toolkit.def",
      "image-analysis/minc-toolkit/1.9.16/minc-toolkit.def",
      "image-analysis/civet/2.1.1/civet.def",
      "image-analysis/xcpengine/1.2.1/xcpengine.def",
      "image-analysis/xcpengine/1.0/xcpengine.def",
      "image-analysis/xcpengine/1.2.3/xcpengine.def",
      "image-analysis/baracus/1.1.4/baracus.def",
      "image-analysis/qsiprep/0.8.0/qsiprep.def",
      "image-analysis/deepmedic/0.8.0/deepmedic.def",
      "image-analysis/deepmedic/0.8.2/deepmedic.def",
      "image-analysis/broccoli/1.0.1/broccoli.def",
      "image-analysis/tesseract/4.1.1/tesseract.def",
      "image-analysis/topaz/0.2.5/topaz.def",
      "image-analysis/fitlins/0.8.0/fitlins.def",
      "image-analysis/fitlins/0.7.0/fitlins.def",
      "image-analysis/mriqc/0.15.2/mriqc.def",
      "image-analysis/mriqc/0.15.1/mriqc.def",
      "image-analysis/mriqc/0.16.1/mriqc.def",
      "image-analysis/mriqc/0.15.2-0be03bf/mriqc.def",
      "image-analysis/fmriprep/20.2.0/fmriprep.def",
      "image-analysis/fmriprep/20.2.1/fmriprep.def",
      "image-analysis/fmriprep/20.0.5/fmriprep.def",
      "image-analysis/fmriprep/20.1.1/fmriprep.def",
      "image-analysis/fmriprep/20.1.3/fmriprep.def",
      "image-analysis/mrtrix/3.0.1/mrtrix.def",
      "image-analysis/mrtrix/3.0.2-cuda9.1/mrtrix.def",
      "image-analysis/mrtrix/3.0.0/mrtrix.def",
      "deep-learning/dextr-pytorch/20180710/dextr-pytorch.def",
      "deep-learning/digits/6.0/digits.def",
      "deep-learning/unet/20180704/unet.def",
      "deep-learning/deeplab/20180816/deeplab.def",
      "deep-learning/few-shot-ssl/20180723/few-shot-ssl.def",
      "deep-learning/caffe2/0.8.1/caffe2.def",
      "deep-learning/clairvoyante/1.0/clairvoyante.def",
      "deep-learning/basset/0.1.0/basset.def",
      "deep-learning/tensorrt/18.09/tensorrt.def",
      "deep-learning/polyrnnpp/20180718/polyrnnpp.def"
    ],
    "full_name": "NIH-HPC/singularity-def-files",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nih-hpc-singularity-definition-files\" class=\"anchor\" href=\"#nih-hpc-singularity-definition-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNIH HPC Singularity Definition Files\u003c/h1\u003e\n\u003cp\u003eThese definition files and wrapper scripts are used by the \u003ca href=\"https://hpc.nih.gov/\" rel=\"nofollow\"\u003eNIH HPC (Biowulf)\u003c/a\u003e staff to install containerized applications using \u003ca href=\"https://github.com/sylabs/singularity\"\u003eSingularity\u003c/a\u003e. Each app is installed in a self-contained directory and access to the app is controlled through a module system (\u003ca href=\"https://github.com/TACC/Lmod\"\u003eLmod\u003c/a\u003e). This strategy allows users to transparently access apps that are installed within containers as though they were installed directly on the host system. More details can be found \u003ca href=\"https://hpc.nih.gov/apps/singularity.html#bind-stationary\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTypically, apps are installed under in a directory structure like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ tree appname/ver\nappname/ver\n|-- bin\n|   |-- cmd1 -\u0026gt; ../libexec/wrapper.sh\n|   |-- cmd2 -\u0026gt; ../libexec/wrapper.sh\n|   `-- cmd3 -\u0026gt; ../libexec/wrapper.sh\n`-- libexec\n    |-- app.sif\n    `-- wrapper.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBecause \u003ccode\u003ewrapper.sh\u003c/code\u003e is written to be introspective, any command symlinked to it will be carried through and executed within the associated container. The wrapper script is also sufficiently generic that it can be reused across apps with little or no modification.\u003c/p\u003e\n\u003cp\u003eEach app has its own \u003ccode\u003eREADME.md\u003c/code\u003e that contains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea link to the NIH HPC app page or developer\u0027s documentation\u003c/li\u003e\n\u003cli\u003ea list of symlinks that should be created to the wrapper script to expose executables within the container\u003c/li\u003e\n\u003cli\u003eany app specific installation notes\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFinally, please note that these definition files \u003cstrong\u003eare not guaranteed to reproduce the same container, or even to produce any container at all\u003c/strong\u003e. The internet, upon which these definition files are based, is subject to change without notice. These definition files are therefore intended to be treated as (potentially) helpful suggestions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-computational-chemistry\" class=\"anchor\" href=\"#computational-chemistry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/computational-chemistry\"\u003eComputational Chemistry\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/computational-chemistry/ampl\"\u003eampl\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deep-learning\" class=\"anchor\" href=\"#deep-learning\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/deep-learning\"\u003eDeep Learning\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/caffe2\"\u003eCaffe2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/dextr-pytorch\"\u003eDEXTR-PyTorch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/polyrnnpp\"\u003ePolyRNNpp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/basset\"\u003ebasset\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/clairvoyante\"\u003eclairvoyante\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/deeplab\"\u003edeeplab\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/digits\"\u003edigits\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/few-shot-ssl\"\u003efew-shot-ssl\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/tensorrt\"\u003etensorrt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/deep-learning/unet\"\u003eunet\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-high-throughput-sequencing\" class=\"anchor\" href=\"#high-throughput-sequencing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/high-throughput-sequencing\"\u003eHigh Throughput Sequencing\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/atac_dnase_pipelines\"\u003eATAC-Seq / DNase-Seq Pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/ascatngs\"\u003eAscatNGS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/atropos\"\u003eAtropos\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamsurgeon\"\u003eBAMSurgeon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/brass\"\u003eBRASS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/canvas\"\u003eCanvas\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/maggie\"\u003eMAGGIE\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pcap-core\"\u003ePCAP-core\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pepr\"\u003ePePr\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rsd\"\u003eRSD\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/surpi\"\u003eSURPI\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tpmcalculator\"\u003eTPMCalculator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tvc\"\u003eTVC\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/vagrent\"\u003eVAGrENT\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/vep\"\u003eVEP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/abruijn\"\u003eabruijn\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamliquidator\"\u003ebamliquidator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamreadcount\"\u003ebamreadcount\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bamutil\"\u003ebamutil\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bigscale2\"\u003ebigscale2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/biom-format\"\u003ebiom-format\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/bison\"\u003ebison\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/busco\"\u003ebusco\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cancerit-wgs\"\u003ecancerit-wgs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cellsnp\"\u003ecellsnp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cgpbattenberg\"\u003ecgpBattenberg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cicero\"\u003ecicero\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cnvkit\"\u003ecnvkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cnvnator\"\u003ecnvnator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/crispresso\"\u003ecrispresso\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/crossmap\"\u003ecrossmap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/csvkit\"\u003ecsvkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/cutadapt\"\u003ecutadapt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/deepsignal\"\u003edeepsignal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/deeptools\"\u003edeeptools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/deepvariant\"\u003edeepvariant\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/delly\"\u003edelly\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/dropest\"\u003edropest\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/eager\"\u003eeager\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/epic2\"\u003eepic2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/flappie\"\u003eflappie\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/flye\"\u003eflye\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/freebayes\"\u003efreebayes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/fusioninspector\"\u003efusioninspector\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/gossamer\"\u003egossamer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/gridss\"\u003egridss\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/guppy\"\u003eguppy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hail\"\u003ehail\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hap.py\"\u003ehap.py\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hicexplorer\"\u003ehicexplorer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/hicpro\"\u003ehicpro\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/htseq\"\u003ehtseq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/humann2\"\u003ehumann2\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/idep\"\u003eidep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/lefse\"\u003elefse\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/macs\"\u003emacs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/mageck-vispr\"\u003emageck-vispr\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/medaka\"\u003emedaka\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/megalodon\"\u003emegalodon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/metabat\"\u003emetabat\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/metaphlan\"\u003emetaphlan\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/mitosuite\"\u003emitosuite\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/mtoolbox\"\u003emtoolbox\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/multiqc\"\u003emultiqc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/neusomatic\"\u003eneusomatic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/parliament\"\u003eparliament\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pvactools\"\u003epvactools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/pychopper\"\u003epychopper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/raremetal\"\u003eraremetal\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/repeatmodeler\"\u003erepeatmodeler\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/ricopili\"\u003ericopili\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rilseq\"\u003erilseq\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rmats\"\u003ermats\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rnapeg\"\u003ernapeg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/rseqc\"\u003erseqc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/salmon\"\u003esalmon\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/scramble\"\u003escramble\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/seqlinkage\"\u003eseqlinkage\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/sicer\"\u003esicer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/slamdunk\"\u003eslamdunk\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/stream\"\u003estream\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/sve\"\u003esve\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/svtk\"\u003esvtk\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/svtyper\"\u003esvtyper\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/taiji\"\u003etaiji\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tandemtools\"\u003etandemtools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/tetoolkit\"\u003etetoolkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/transvar\"\u003etransvar\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/umitools\"\u003eumitools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/vireosnp\"\u003evireosnp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/high-throughput-sequencing/xengsort\"\u003exengsort\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-image-analysis\" class=\"anchor\" href=\"#image-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/image-analysis\"\u003eImage Analysis\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/resmap\"\u003eResMap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/terastitcher\"\u003eTeraStitcher\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/baracus\"\u003ebaracus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/broccoli\"\u003ebroccoli\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/civet\"\u003ecivet\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/ctf\"\u003ectf\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/deepmedic\"\u003edeepmedic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/fitlins\"\u003efitlins\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/fmriprep\"\u003efmriprep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/minc-toolkit\"\u003eminc-toolkit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/mriqc\"\u003emriqc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/mrtrix\"\u003emrtrix\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/qsiprep\"\u003eqsiprep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/tesseract\"\u003etesseract\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/topaz\"\u003etopaz\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/image-analysis/xcpengine\"\u003excpengine\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-linkage-phylogenetics\" class=\"anchor\" href=\"#linkage-phylogenetics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/linkage-phylogenetics\"\u003eLinkage Phylogenetics\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/linkage-phylogenetics/bali-phy\"\u003ebali-phy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/linkage-phylogenetics/gubbins\"\u003egubbins\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mass-spectrometry\" class=\"anchor\" href=\"#mass-spectrometry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/mass-spectrometry\"\u003eMass Spectrometry\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/mass-spectrometry/maxquant\"\u003emaxquant\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-mathematicalstatistics\" class=\"anchor\" href=\"#mathematicalstatistics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/mathematical-statistics\"\u003eMathematical/Statistics\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/mathematical-statistics/m2clust\"\u003em2clust\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/mathematical-statistics/omeclust\"\u003eomeClust\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-molecular-modeling-graphics\" class=\"anchor\" href=\"#molecular-modeling-graphics\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/molecular-modeling-graphics\"\u003eMolecular Modeling Graphics\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/molecular-modeling-graphics/chimerax\"\u003eChimeraX\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/molecular-modeling-graphics/blender\"\u003eblender\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/molecular-modeling-graphics/starseqr\"\u003estarseqr\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sequence-analysis\" class=\"anchor\" href=\"#sequence-analysis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/sequence-analysis\"\u003eSequence Analysis\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/acfs\"\u003eACFS\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/annogesic\"\u003eANNOgesic\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/asgal\"\u003eASGAL\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/accurity\"\u003eAccurity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/eukrep\"\u003eEukRep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/glu\"\u003eGLU\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/htgtsrep\"\u003eHTGTSrep\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/orffinder\"\u003eORFfinder\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/saige\"\u003eSAIGE\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/vcf-kit\"\u003eVCF-kit\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/anvio\"\u003eanvio\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/arriba\"\u003earriba\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/augustus\"\u003eaugustus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/bamgineer\"\u003ebamgineer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/braker\"\u003ebraker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/cactus\"\u003ecactus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/chipseq_pipeline\"\u003echipseq_pipeline\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/cicero\"\u003ecicero\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/deepsea\"\u003edeepsea\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/focus\"\u003efocus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/glnexus\"\u003eglnexus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/intarna\"\u003eintarna\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/ldsc\"\u003eldsc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/m-tools\"\u003em-tools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/mmarge\"\u003emmarge\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/netoglyc\"\u003enetOglyc\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/phaser\"\u003ephaser\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/prokka\"\u003eprokka\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/qtltools\"\u003eqtltools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/roary\"\u003eroary\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/smoove\"\u003esmoove\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/sonicparanoid\"\u003esonicparanoid\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/svtools\"\u003esvtools\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/wisexome\"\u003ewisexome\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/sequence-analysis/xhla\"\u003exHLA\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-structural-biology\" class=\"anchor\" href=\"#structural-biology\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/structural-biology\"\u003eStructural Biology\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/structural-biology/parsnip\"\u003eparsnip\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/structural-biology/pymol\"\u003epymol\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/structural-biology/rdock\"\u003erDock\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-systems-biology\" class=\"anchor\" href=\"#systems-biology\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/systems-biology\"\u003eSystems Biology\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/systems-biology/cellphonedb\"\u003ecellphonedb\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-utilities\" class=\"anchor\" href=\"#utilities\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"/utilities\"\u003eUtilities\u003c/a\u003e\n\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"/utilities/xvfb\"\u003eXvfb\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/ariba\"\u003eariba\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/atom\"\u003eatom\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/datalad\"\u003edatalad\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/gdc-client\"\u003egdc-client\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/longshot\"\u003elongshot\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/pdf2svg\"\u003epdf2svg\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/pyega3\"\u003epyega3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/snp-sites\"\u003esnp-sites\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/sysbench\"\u003esysbench\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/uropa\"\u003europa\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/vcf2db\"\u003evcf2db\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/visidata\"\u003evisidata\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"/utilities/whatshap\"\u003ewhatshap\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 9,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1625108554.0
  },
  {
    "data_format": 2,
    "description": "Dockerfile to create an image with OpenFOAM-plus and PyTorch support",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "AndreWeiner/of_pytorch_docker",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-dockersingularity--openfoam--pytorch\" class=\"anchor\" href=\"#dockersingularity--openfoam--pytorch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker/Singularity + OpenFOAM\u00ae + PyTorch\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThe Dockerfile in this repository creates an image with \u003ca href=\"https://openfoam.com/\" rel=\"nofollow\"\u003eOpenFOAM-plus\u003c/a\u003e and \u003ca href=\"https://pytorch.org/\" rel=\"nofollow\"\u003ePyTorch\u003c/a\u003e support. The image is currently based on\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUbuntu 20.04,\u003c/li\u003e\n\u003cli\u003eOpenFOAM-v2106, and\u003c/li\u003e\n\u003cli\u003ePyTorch 1.9.0 (only CPU).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThere are also convenience scripts for creating and running a container based on the image. The \u003cem\u003etest\u003c/em\u003e directory contains two examples demonstrating how to compile applications using \u003cem\u003ecmake\u003c/em\u003e and \u003cem\u003ewmake\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eOpenFOAM is not compiled from scratch but installed via the package manager (\u003ca href=\"https://develop.openfoam.com/Development/openfoam/-/wikis/precompiled/debian\" rel=\"nofollow\"\u003eread more\u003c/a\u003e). Also for PyTorch, only the pre-compiled C++ part of the library, named \u003cem\u003elibtorch\u003c/em\u003e, is contained on the image.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-build-the-images\" class=\"anchor\" href=\"#how-to-build-the-images\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to build the images\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker-image\" class=\"anchor\" href=\"#docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker image\u003c/h3\u003e\n\u003cp\u003eTo build the image yourself, copy this repository and navigate into the top-level folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/AndreWeiner/of_pytorch_docker.git\ncd of_pytorch_docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you want to upload the image to a Docker registry, consider the following naming convention when running the build command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker build -t user_name/of_pytorch:of2106-py1.9.0-cpu -f Dockerfile .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-definition-file\" class=\"anchor\" href=\"#singularity-definition-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity definition file\u003c/h3\u003e\n\u003cp\u003eFor university clusters, \u003ca href=\"https://sylabs.io/guides/3.6/user-guide/introduction.html\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e is often the only supported container tool. In contrast to Docker, the \u003cstrong\u003eexecution\u003c/strong\u003e of Singularity images does not require root-privileges (the image creation does, though). The \u003cem\u003eSingularity.def\u003c/em\u003e file converts the Docker image into a Singularity image named, e.g., \u003cem\u003eof2106-py1.9.0-cpu.sif\u003c/em\u003e. To create the image, run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build of2106-py1.9.0-cpu.sif Singularity.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe image may be used similarly to the Docker image. Convenience scripts like \u003cem\u003ecreate_openfoam_container.sh\u003c/em\u003e or \u003cem\u003estart_openfoam.sh\u003c/em\u003e are not necessary because Singularity performs similar actions by default (e.g., mapping the user and important directories). To start an interactive shell, run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell of2106-py1.9.0-cpu.sif\n# first thing to do inside the container\n. /usr/lib/openfoam/openfoam2106/etc/bashrc\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage-and-examples\" class=\"anchor\" href=\"#usage-and-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage and examples\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-docker-image-1\" class=\"anchor\" href=\"#docker-image-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocker image\u003c/h3\u003e\n\u003cp\u003eCopy this repository and navigate into the top-level folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/AndreWeiner/of_pytorch_docker.git\ncd of_pytorch_docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe script \u003cem\u003ecreate_openfoam_container.sh\u003c/em\u003e creates a container with suitable settings (e.g. mapping the user into the container, mounting the current directory, setting the working directory to \u003cem\u003e./test/\u003c/em\u003e). The script also pulls the Docker image from \u003ca href=\"https://hub.docker.com/repository/docker/andreweiner/of_pytorch\" rel=\"nofollow\"\u003eDockerhub\u003c/a\u003e if it cannot be found locally. The default image and container names can be changed by passing them as command line arguments.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# default settings\n./create_openfoam_container.sh\n\n# use different image, e.g., to use an older version\n./create_openfoam_container.sh \"andreweiner/of_pytorch:of2012-py1.7.1-cpu\" \"of2012-py1.7.1-cpu\" \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003cem\u003estart_openfoam.sh\u003c/em\u003e script starts an interactive shell instance in the running container. If you modified the container name in the previous step, provide the modified name as command line argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# default\n./start_openfoam.sh\n\n# custom container name\n./start_openfoam.sh \"of2012-py1.7.1-cpu\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe container\u0027s entry point is set to the \u003cem\u003etest\u003c/em\u003e directory. There you are presented with two examples:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003etensorCreation\u003c/strong\u003e: PyTorch tensor basics; compiled with \u003cstrong\u003ewmake\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003esimpleMLP\u003c/strong\u003e: implementation of a simple \u003cem\u003emultilayer perceptron\u003c/em\u003e (MLP) class; compiled with \u003cstrong\u003ecmake\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-tensorcreation\" class=\"anchor\" href=\"#tensorcreation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cem\u003etensorCreation\u003c/em\u003e\n\u003c/h3\u003e\n\u003cp\u003eTo compile and run \u003cem\u003etensorCreation\u003c/em\u003e, execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# you must be inside the container for this to work\n# execute ./start_openfoam.sh to launch a shell instance\ncd tensorCreation\nwmake\n./tensorCreation\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-simplemlp\" class=\"anchor\" href=\"#simplemlp\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cem\u003esimpleMLP\u003c/em\u003e\n\u003c/h3\u003e\n\u003cp\u003eTo compile and run \u003cem\u003esimpleMLP\u003c/em\u003e, execute:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# you must be inside the container for this to work\n# execute ./start_openfoam.sh to launch a shell instance\ncd simpleMLP\nmkdir build\ncd build\ncmake ..\nmake\n./simpleMLP\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-singularity-image\" class=\"anchor\" href=\"#singularity-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity image\u003c/h3\u003e\n\u003cp\u003eThe singularity image contains some simple shell logic to execute commands in a given path. This addition simplifies creating batch jobs. The general syntax is:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run of2106-py1.9.0-cpu.sif command [path] [arguments]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAssuming you are in the top-level folder of this repository, you can build and run \u003cem\u003etensorCreation\u003c/em\u003e as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# build\nsingularity run of2106-py1.9.0-cpu.sif wmake test/tensorCreation/\n# run\nsingularity run of2106-py1.9.0-cpu.sif ./tensorCreation test/tensorCreation/\n# clean\nsingularity run of2106-py1.9.0-cpu.sif wclean test/tensorCreation/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAlternatively, one can also define scripts, which are then executed by Singularity. For example, to build and run the second example, \u003cem\u003esimpleMLP\u003c/em\u003e, run the \u003cem\u003ecompileAndRun.sh\u003c/em\u003e script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run of2106-py1.9.0-cpu.sif ./compileAndRun.sh test/simpleMLP/\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo hide the additional complexity from using containers, one can also define shell functions that assemble suitable commands in the background. E.g., in \u003ca href=\"https://github.com/AndreWeiner/naca0012_shock_buffet/blob/main/functions\"\u003ethis\u003c/a\u003e file, the function \u003ccode\u003esingularityRun\u003c/code\u003e works the same way as the frequently used \u003ccode\u003erunApplication\u003c/code\u003e but uses a singularity image in the background.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-get-in-touch\" class=\"anchor\" href=\"#get-in-touch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGet in touch\u003c/h2\u003e\n\u003cp\u003eIf you would like to suggest changes or improvements regarding the\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebuild process,\u003c/li\u003e\n\u003cli\u003epre-installed packages,\u003c/li\u003e\n\u003cli\u003eexamples,\u003c/li\u003e\n\u003cli\u003edocumentation,\u003c/li\u003e\n\u003cli\u003e...\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eplease use the \u003ca href=\"https://github.com/AndreWeiner/of_pytorch_docker/issues\"\u003eissue tracker\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-miscellaneous\" class=\"anchor\" href=\"#miscellaneous\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMiscellaneous\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-issue-with-sourceforge\" class=\"anchor\" href=\"#issue-with-sourceforge\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIssue with Sourceforge\u003c/h3\u003e\n\u003cp\u003eBuilding the Docker image sometimes fails with the error message:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eE: Failed to fetch \u003ca href=\"https://nav.dl.sourceforge.net/project/openfoam/repos/deb/dists/focal/main/pool/2012_1/binary-amd64/openfoam2012_1-2_amd64.deb\" rel=\"nofollow\"\u003ehttps://nav.dl.sourceforge.net/project/openfoam/repos/deb/dists/focal/main/pool/2012_1/binary-amd64/openfoam2012_1-2_amd64.deb\u003c/a\u003e\u003cbr\u003e\nCould not connect to nav.dl.sourceforge.net:443 (5.154.224.27), connection timed out\nE: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe OpenFOAM Debian package is hosted on Sourceforge, and sometimes their servers are not available. Usually waiting a couple of minutes before rebuilding the image helps to solve this issue.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-older-versions-of-the-dockerfile\" class=\"anchor\" href=\"#older-versions-of-the-dockerfile\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOlder versions of the Dockerfile\u003c/h3\u003e\n\u003cp\u003eThere are two more Dockerfiles which were used to build previous versions of the Docker image. They remain part of the repository since they might be helpful to some users.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eDockerfile.abi_source\u003c/em\u003e: OpenFOAM is compiled from scratch; only some third-party packages are installed and, hence, some applications are missing\u003c/li\u003e\n\u003cli\u003e\n\u003cem\u003eDockerfile.no_abi\u003c/em\u003e: OpenFOAM is compiled from scratch with modified compiler flags to be compatible with \u003cem\u003elibtorch\u003c/em\u003e versions prior to version 1.3\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBy default, the OpenFOAM library will be compiled running two jobs in parallel. If you prefer to use more jobs, set the \u003cem\u003eNP\u003c/em\u003e build argument, e.g.:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker build --build-args NP=8 -t user_name/of_pytorch:of1912-py1.5-cpu -f Dockerfile.abi .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eI also recommend to save the Docker output in a log-file:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker build --build-args NP=8 -t user_name/of_pytorch:of1912-py1.5-cpu -f Dockerfile.abi . \u0026amp;\u0026gt; log.docker\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-abi\" class=\"anchor\" href=\"#abi\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eABI\u003c/h3\u003e\n\u003cp\u003eSince version 1.3 of PyTorch, there is a version of libtorch compiled with ABI enabled (\u003ca href=\"https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html\" rel=\"nofollow\"\u003eread more\u003c/a\u003e). The only change in prior versions compared to a regular compilation is the additional flag\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e-D_GLIBCXX_USE_CXX11_ABI=0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhen compiling OpenFOAM. The flag is related to backwards compatibility for standards older than C++11 (\u003ca href=\"https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html\" rel=\"nofollow\"\u003eread more\u003c/a\u003e).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-libtorch\" class=\"anchor\" href=\"#libtorch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLibtorch\u003c/h3\u003e\n\u003cp\u003eThe PyTorch library files are located in \u003cem\u003e/opt/libtorch\u003c/em\u003e. The environment variable \u003cstrong\u003eTORCH_LIBRARIES\u003c/strong\u003e can be used to indicate the location of certain header and library files to the compiler and linker. To compile PyTorch C++ code using \u003cem\u003ewmake\u003c/em\u003e, add\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eEXE_INC = \\\n    -I$(TORCH_LIBRARIES)/include \\\n    -I$(TORCH_LIBRARIES)/include/torch/csrc/api/include \\\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto the include paths, and\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eEXE_LIBS = \\\n    -Wl,-rpath,$(TORCH_LIBRARIES)/lib $(TORCH_LIBRARIES)/lib/libtorch.so $(TORCH_LIBRARIES)/lib/libc10.so \\\n    -Wl,--no-as-needed,$(TORCH_LIBRARIES)/lib/libtorch_cpu.so \\\n    -Wl,--as-needed $(TORCH_LIBRARIES)/lib/libc10.so \\\n    -Wl,--no-as-needed,$(TORCH_LIBRARIES)/lib/libtorch.so\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto the library paths.\u003c/p\u003e\n",
    "stargazers_count": 9,
    "subscribers_count": 0,
    "topics": [],
    "updated_at": 1626257743.0
  },
  {
    "data_format": 2,
    "description": "Scripts for building Singularity images",
    "filenames": [
      "caffe/ubuntu.def",
      "tensorflow/ubuntu.def",
      "circuitscape/ubuntu.def",
      "mxnet/ubuntu.def",
      "caffe2/ubuntu.def",
      "dl/ubuntu.def"
    ],
    "full_name": "clemsonciti/singularity-images",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-image-scripts\" class=\"anchor\" href=\"#singularity-image-scripts\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity image scripts\u003c/h1\u003e\n\u003cp\u003eScripts to generate singularity images\nfor running different software on Palmetto cluster.\u003c/p\u003e\n",
    "stargazers_count": 10,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1597386388.0
  },
  {
    "data_format": 2,
    "description": "Pipeline for processing FASTQ data from an Illumina MiSeq to genotype human RNA viruses like HIV and hepatitis C",
    "filenames": [
      "Singularity"
    ],
    "full_name": "cfe-lab/MiCall",
    "latest_release": "v7.14.2",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-micall\" class=\"anchor\" href=\"#micall\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMiCall\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-processing-fastq-data-from-an-illumina-miseq\" class=\"anchor\" href=\"#processing-fastq-data-from-an-illumina-miseq\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eProcessing FASTQ data from an Illumina MiSeq\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.com/cfe-lab/MiCall\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8cfcb6fc58992b1a5293f99aab18dc7fc4c352993a0aaeee39aca2186b9e02b4/68747470733a2f2f7472617669732d63692e636f6d2f6366652d6c61622f4d6943616c6c2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/cfe-lab/MiCall.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/github/cfe-lab/MiCall?branch=master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/01a5da8acb0a78aabfb093a63c28db15b9df951d3e52aaa03d54180dae171b07/68747470733a2f2f636f6465636f762e696f2f6769746875622f6366652d6c61622f4d6943616c6c2f636f7665726167652e7376673f6272616e63683d6d6173746572\" alt=\"Code Coverage\" data-canonical-src=\"https://codecov.io/github/cfe-lab/MiCall/coverage.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://doi.org/10.5281/zenodo.1289989\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/91063d68c07e035327f80f12cf9c389ffffd45952c4db811e6bf23fc2973b714/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f444f492f31302e353238312f7a656e6f646f2e313238393938392e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.1289989.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMaps all the reads from a sample against a set of reference sequences, then\nstitches all the reads into consensus sequences and coverage maps.\u003c/p\u003e\n\u003cp\u003eA monitoring system regularly checks the file system for unprocessed runs,\ntransfers FASTQ.gz files to the cluster and executes the pipeline.\u003c/p\u003e\n\u003cp\u003eSee the \u003ca href=\"https://cfe-lab.github.io/MiCall/steps\" rel=\"nofollow\"\u003elist of steps and files\u003c/a\u003e for details of what the pipeline does.\nThe \u003ca href=\"https://cfe-lab.github.io/MiCall/admin\" rel=\"nofollow\"\u003eadmin\u003c/a\u003e page describes how to look after the pipeline in Kive, and the\n\u003ca href=\"https://cfe-lab.github.io/MiCall/getting_started\" rel=\"nofollow\"\u003egetting started\u003c/a\u003e page describes how to get the docker version set up and run it\non your own data.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dual-licensing\" class=\"anchor\" href=\"#dual-licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDual Licensing\u003c/h2\u003e\n\u003cp\u003eCopyright (C) 2016, University of British Columbia\u003c/p\u003e\n\u003cp\u003eThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Affero General Public License as published\nby the Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\u003c/p\u003e\n\u003cp\u003eThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Affero General Public License for more details.\u003c/p\u003e\n\u003cp\u003eYou should have received a copy of the GNU Affero General Public License\nalong with this program.  If not, visit \u003ca href=\"https://www.gnu.org/licenses/\" rel=\"nofollow\"\u003egnu.org\u003c/a\u003e. The source code for\nthis program is available from \u003ca href=\"https://github.com/cfe-lab/MiCall\"\u003egithub.com\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe program is also available for a fee under a more permissive license. For\nexample, if you want to run a changed version of the program on a network server\nwithout publishing the changed source code, \u003ca href=\"mailto:micalldev@cfenet.ubc.ca\"\u003econtact us\u003c/a\u003e about\npurchasing a license.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-third-party-components\" class=\"anchor\" href=\"#third-party-components\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThird Party Components\u003c/h2\u003e\n\u003cp\u003eMiCall makes use of several open-source tools. Here is a list of tools with\ntheir licenses.\u003c/p\u003e\n\u003cp\u003eRequests is distributed under the Apache 2.0 license.\u003c/p\u003e\n\u003cp\u003ePython 3 is distributed under the \u003ca href=\"https://docs.python.org/3/license.html\" rel=\"nofollow\"\u003ePython 3 license\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBowtie2, IVA, and Python-Levenshtein are distributed under the GNU General\nPublic License (GPL).\u003c/p\u003e\n\u003cp\u003eMatplotlib is distributed under the \u003ca href=\"https://matplotlib.org/users/license.html\" rel=\"nofollow\"\u003eMatplotlib license\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eReportlab is distributed under the BSD license.\u003c/p\u003e\n\u003cp\u003ePyyaml and Cutadapt are distributed under the MIT license.\u003c/p\u003e\n",
    "stargazers_count": 10,
    "subscribers_count": 13,
    "topics": [
      "bioinformatics",
      "fastq",
      "python",
      "resistance",
      "genotype"
    ],
    "updated_at": 1627316074.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "Singularity"
    ],
    "full_name": "ejolly/IntroToSingularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-getting-setup-with-singularity\" class=\"anchor\" href=\"#getting-setup-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Setup with Singularity\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eThis is a guide to getting started with \u003ca href=\"http://singularity.lbl.gov/\" rel=\"nofollow\"\u003eSingularity containers\u003c/a\u003e in conjunction with Dartmouth College\u0027s \u003ca href=\"http://techdoc.dartmouth.edu/discovery/\" rel=\"nofollow\"\u003eDiscovery HPC\u003c/a\u003e.\u003cbr\u003e\nQuestions can be addressed to \u003ca href=\"mailto:eshin.jolly.gr@dartmouth.edu\"\u003eeshin.jolly.gr@dartmouth.edu\u003c/a\u003e or \u003ca href=\"mailto:mvdoc.gr@dartmouth.edu\"\u003emvdoc.gr@dartmouth.edu\u003c/a\u003e.\u003cbr\u003e\nWe\u0027re not experts but we\u0027re happy to try to help!\u003c/em\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-i-pre-requisites-osx-only\" class=\"anchor\" href=\"#i-pre-requisites-osx-only\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#prereqs\"\u003eI. Pre-requisites (OSX only)\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-ii-creating-a-singularity-container\" class=\"anchor\" href=\"#ii-creating-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#creation\"\u003eII. Creating a Singularity container\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-iii-basic-container-usage\" class=\"anchor\" href=\"#iii-basic-container-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#basicusage\"\u003eIII. Basic container usage\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-iv-using-a-container-on-discovery\" class=\"anchor\" href=\"#iv-using-a-container-on-discovery\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#discovery\"\u003eIV. Using a container on Discovery\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-v-updating-an-existing-container\" class=\"anchor\" href=\"#v-updating-an-existing-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#updating\"\u003eV. Updating an existing container\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-vi-sharing-containers\" class=\"anchor\" href=\"#vi-sharing-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#sharing\"\u003eVI. Sharing containers\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-vii-extra-resources\" class=\"anchor\" href=\"#vii-extra-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"#resources\"\u003eVII. Extra resources\u003c/a\u003e\n\u003c/h4\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content--pre-requisites-osx-only\" class=\"anchor\" href=\"#-pre-requisites-osx-only\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-prereqs\"\u003e\u003c/a\u003e Pre-requisites (OSX only!)\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eBecause singularity runs primarily on linux, we need to create a virtual linux environment on OSX in order to build/manipulate singularity containers. Follow this step first if you\u0027re using OSX.\u003c/em\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-install-homebrew-package-manager\" class=\"anchor\" href=\"#install-homebrew-package-manager\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Homebrew package manager\u003c/h4\u003e\n\u003cp\u003eHomebrew is a package manager for OSX similar to apt-get or yum on linux. It allows you to download and install different software (e.g. wget, or curl) and allows you to build your own packages. Just copy and run the command below:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-use-homebrew-to-install-vagrant\" class=\"anchor\" href=\"#use-homebrew-to-install-vagrant\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse Homebrew to install Vagrant\u003c/h4\u003e\n\u003cp\u003eVagrant is a virtual development environment that can be used to create virtual-machines (kind of similar to Virtualbox, but much more powerful). It can be used to install and run another operating system on your computer that\u0027s completely independent from your host OS. First we\u0027re going to install vagrant via Homebrew.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebrew cask install Virtualbox\nbrew cask install vagrant\nbrew cask install vagrant-manager\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-use-vagrant-to-create-a-virtual-machine\" class=\"anchor\" href=\"#use-vagrant-to-create-a-virtual-machine\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse Vagrant to create a virtual machine\u003c/h4\u003e\n\u003cp\u003eNow that we have vagrant installed, we can use it to make a brand new linux- based virtual machine, \u003cstrong\u003ewithin\u003c/strong\u003e which singularity will be installed. It\u0027s from inside this vm that we\u0027re going to do all future singularity container creation, modification etc.\u003c/p\u003e\n\u003cp\u003eFirst let\u0027s create a folder that our virtual machine will live in.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir singularity-vm\ncd singularity-vm\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow lets download a \u003cem\u003evagrantfile\u003c/em\u003e for a prebuilt Ubuntu system that already has singularity installed.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evagrant init singularityware/singularity-2.4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFinally we can start up virtual machine and move into it.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#If this is the first time you\u0027re building the vm the vagrant up command might take a minute or so to complete\nvagrant up\nvagrant ssh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhenver you\u0027re done using a vagrant vm just use \u003ccode\u003ectrl+c\u003c/code\u003e to exit the machine and type \u003ccode\u003evagrant halt\u003c/code\u003e to shut it down.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-creating-a-singularity-container\" class=\"anchor\" href=\"#creating-a-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-creation\"\u003e\u003c/a\u003eCreating a Singularity container\u003c/h2\u003e\n\u003cp\u003eLet\u0027s begin by creating a new folder within our vm for our brand new container (this isn\u0027t strictly necessary but nice to keep different containers organized):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emkdir miniconda\ncd miniconda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe first thing we need to do in order to create a singularity container is make a singularity \u003cem\u003edefinition\u003c/em\u003e file. This is just an instruction set that singularity will use to create a container. Think of this definition file as a recipe, and the container as the final product. Within this recipe, specify everything you need to in order create your custom analysis environment. Sharing this definition file with others will enable them to identically reproduce the steps it took to create your container.\u003c/p\u003e\n\u003cp\u003eTo get you started here\u0027s an example definition file that we\u0027re going to use for this demo. This is a simple neurodebian flavored container with miniconda installed along with numpy and scipy.\u003cbr\u003e\nLet\u0027s save this to a file called \u003ccode\u003eminiconda.def\u003c/code\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Singularity definition example with miniconda\n# Matteo Visconti di Oleggio Castello; Eshin Jolly\n# mvdoc.gr@dartmouth.edu; eshin.jolly.gr@dartmouth.edu\n# May 2017\n\nbootstrap: docker\nfrom: neurodebian:jessie\n\n# this command assumes at least singularity 2.3\n%environment\n    PATH=\"/usr/local/anaconda/bin:$PATH\"\n%post\n    # install debian packages\n    apt-get update\n    apt-get install -y eatmydata\n    eatmydata apt-get install -y wget bzip2 \\\n      ca-certificates libglib2.0-0 libxext6 libsm6 libxrender1 \\\n      git git-annex-standalone\n    apt-get clean\n\n    # install anaconda\n    if [ ! -d /usr/local/anaconda ]; then\n         wget https://repo.continuum.io/miniconda/Miniconda2-4.3.14-Linux-x86_64.sh \\\n            -O ~/anaconda.sh \u0026amp;\u0026amp; \\\n         bash ~/anaconda.sh -b -p /usr/local/anaconda \u0026amp;\u0026amp; \\\n         rm ~/anaconda.sh\n    fi\n    # set anaconda path\n    export PATH=\"/usr/local/anaconda/bin:$PATH\"\n\n    # install the bare minimum\n    conda install\\\n      numpy scipy\n    conda clean --tarballs\n\n    # make /data and /scripts so we can mount it to access external resources\n    if [ ! -d /data ]; then mkdir /data; fi\n    if [ ! -d /scripts ]; then mkdir /scripts; fi\n\n%runscript\n    echo \"Now inside Singularity container woah...\"\n    exec /bin/bash\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow lets use our vagrant vm and create a blank singularity image allocating 4gb of disk space within our container. You may need to adjust this depending on how much software you plan to install. By default the vagrant vm will share \u003ccode\u003e/vagrant\u003c/code\u003e with your host OS so lets perform our operation in there within the container folder we created earlier.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003evagrant up\nvagrant ssh\ncd /vagrant/miniconda\n# Now let\u0027s build it!\nsudo singularity build miniconda.img miniconda.def\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-basic-container-usage\" class=\"anchor\" href=\"#basic-container-usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-basicusage\"\u003e\u003c/a\u003eBasic container usage\u003c/h2\u003e\n\u003cp\u003eIf all went well we should be able to issue a python command to the python version installed \u003cem\u003ewithin\u003c/em\u003e our container like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec miniconda.img python -c \u0027print \"Hello from Singularity!\"\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can also open up our container and work \u003cem\u003einside\u003c/em\u003e it interactively:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run miniconda.img\nconda list\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePress \u003ccode\u003ectrl+d\u003c/code\u003e to exit the container.\u003c/p\u003e\n\u003cp\u003eMost commonly you\u0027ll use one of three commands with a container:\u003cbr\u003e\n\u003ccode\u003esingularity exec\u003c/code\u003e to run a specific command/file/script using the container\u003cbr\u003e\n\u003ccode\u003esingularity run\u003c/code\u003e to move into a container and use it interactively; what   gets run by this command is dictated by your singularity \u003cem\u003edefinition\u003c/em\u003e file\u003cbr\u003e\n\u003ccode\u003esingularity shell\u003c/code\u003e similar to above, but specifically open up a shell within the container\u003c/p\u003e\n\u003cp\u003eA few other useful flags include:\n\u003ccode\u003e-B\u003c/code\u003e mount an external folder to the container\u003cbr\u003e\n\u003ccode\u003e-c\u003c/code\u003e don\u0027t automatically map /home and /tmp to shared folders with the host OS\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-using-a-container-on-discovery\" class=\"anchor\" href=\"#using-a-container-on-discovery\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-discovery\"\u003e\u003c/a\u003eUsing a container on Discovery\u003c/h2\u003e\n\u003cp\u003eIn order to use a container on Discovery you have to first upload the generated .img file to your home directory. Since containers can be rather large lets compress this and then uncompress on Discovery (starting with Singularity \u0026gt;=2.3.0 this functionality works through \u003ccode\u003eimport\u003c/code\u003e and \u003ccode\u003eexport\u003c/code\u003e commands)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etar -cvzf miniconda.tar.gz miniconda.img\nscp miniconda.tar.gz ejolly@discovery.dartmouth.edu:~\nssh ejolly@discovery.dartmouth.edu\ntar -xvzf miniconda.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow you can utilize the container by loading the singularity module and utilizing any of the singularity commands above. There is \u003cstrong\u003eone catch\u003c/strong\u003e however: by default singularity will try to melt together any environment variables defined in your account on discovery with environment variables defined within the container. The rationale behind this is that singularity offers the ability to \u003cem\u003eseamlessly\u003c/em\u003e blend a custom environment (i.e. your container built with all your goodies) and the functionality of your HPC (i.e. all the goodies that already exist on Discovery). However, often times you want to turn this functionality off and only use environment variables within your container to avoid conflicts (i.e. completely ignore environment variables set on Discovery). Here\u0027s how we do that:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emodule load singularity\nsingularity run -e miniconda.img\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo make our lives easier we can create a simple bash script that executes a command in our container making sure to call it with all the extra flags we want (e.g. mounting some folders, ignoring environment variables). I personally like to create two scripts one for interactively working with a container and one for using it to execute commands for example with job submission. Here are some examples, you\u0027ll need to adapt them to mount the directories you want:\u003cbr\u003e\nLet\u0027s save the following code into a bash file called: exec_miniconda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#!/bin/bash\nsingularity -e  exec \\\n    -B /idata/lchang/Projects:/data \\\n    -B /ihome/ejolly/scripts/:/scripts \\\n    miniconda.img \"$@\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eLet\u0027s save the following code into a bash file called: interact_miniconda\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#!/bin/bash\nsingularity -e run \\\n\t-c \\\n\t-B /idata/lchang/Projects/Pinel:/data \\\n\t-B ~/scripts:/scripts \\\n\tminiconda.img\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow we issue a command to our container (e.g. when submitting a job) like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./exec_miniconda python -c \u0027print \"Hello World!\"\u0027\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe can also use our container interactively with. Here let\u0027s actually serve a jupyter notebook server from the cluster and interact with it using our local web browser. To do so we need to reconnect to Discovery with port-forwarding.  The demo container here isn\u0027t built with a jupyter notebook so this won\u0027t work, but we you can use the same command when building your own container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# You should really connect to something other than the head node here!\nssh ejolly@discovery.dartmouth.edu -N -f -L localhost:3129:localhost:9999\n\n./exec_miniconda jupyter notebook --no-browser --port=9999\n# On local machine navigate to localhost:3129 in a web browser\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-updating-an-existing-container\" class=\"anchor\" href=\"#updating-an-existing-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-updating\"\u003e\u003c/a\u003eUpdating an existing container\u003c/h2\u003e\n\u003cp\u003eThe preferred way to update a container is to modify the definition file and rebuild the image using the steps above. This ensures that any container image is always a product of its definition file and is therefore easy to reproduce.\u003c/p\u003e\n\u003cp\u003eHowever, singularity makes it easy to make changes to an existing container as well using the \u003ccode\u003e--writable\u003c/code\u003e flag with the \u003ccode\u003eexec\u003c/code\u003e, \u003ccode\u003erun\u003c/code\u003e, or \u003ccode\u003eshell\u003c/code\u003e commands, e.g.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec --writable miniconda.img apt-get install curl\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can also increase the size of an existing container with the \u003ccode\u003eexpand\u003c/code\u003e command, e.g.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#Expand a container by 2gb\nsingularity expand --size 2048 miniconda.img\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sharing-containers\" class=\"anchor\" href=\"#sharing-containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-sharing\"\u003e\u003c/a\u003eSharing containers\u003c/h2\u003e\n\u003cp\u003eOne of the nice things about using singularity (and containers in general) is that you can share your analysis environment with others. These are served on \u003ca href=\"https://singularity-hub.org\" rel=\"nofollow\"\u003eSingularity hub\u003c/a\u003e. Many prebuilt containers already exist that you easily download and use.\u003c/p\u003e\n\u003cp\u003eLet\u0027s say we want to use this \u003ca href=\"https://singularity-hub.org/containers/105/\" rel=\"nofollow\"\u003econtainer\u003c/a\u003e prebuilt with tensor flow for GPUs. This is as simple as:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://researchapps/tensorflow:gpu\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen you can setup run and execute scripts like above to use it on Discovery.\u003c/p\u003e\n\u003cp\u003eYou can also easily share you custom container on Singularity hub by committing your singularity definition file to github and flipping the switch for that repository on singularity hub.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-extra-resources\" class=\"anchor\" href=\"#extra-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-resources\"\u003e\u003c/a\u003eExtra resources\u003c/h2\u003e\n\u003cp\u003eMuch of this tutorial is borrowed/integrated from several helpful resources:\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-quick-guides\" class=\"anchor\" href=\"#quick-guides\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick guides\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"http://mvdoc.me/2017/using-singularity-to-make-analyses-reproducible.html\" rel=\"nofollow\"\u003eMatteo Visconti\u0027s blogpost on getting started with singularity\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"http://jinhyuncheong.com/jekyll/update/2016/07/24/How-to-use-the-Discovery-cluster.html\" rel=\"nofollow\"\u003eJin Cheong\u0027s quick guide to using the discovery cluster\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-more-comprehensive-guides\" class=\"anchor\" href=\"#more-comprehensive-guides\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMore comprehensive guides\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"http://singularity.lbl.gov/quickstart\" rel=\"nofollow\"\u003eSingularity Documentation\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"http://techdoc.dartmouth.edu/discovery/\" rel=\"nofollow\"\u003eDiscovery Documentation\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-sharing-is-caring\" class=\"anchor\" href=\"#sharing-is-caring\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSharing is caring\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity hub\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://hub.docker.com/\" rel=\"nofollow\"\u003eDocker hub\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 11,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1624918389.0
  },
  {
    "data_format": 2,
    "description": "Python for Population Genomics",
    "filenames": [
      "Singularity"
    ],
    "full_name": "alexlancaster/pypop",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-python-for-population-genomics-pypop\" class=\"anchor\" href=\"#python-for-population-genomics-pypop\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython for Population Genomics (PyPop)\u003c/h2\u003e\n\u003cp\u003ePyPop is a framework for processing genotype and allele data and running population genetic analyses.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-install-os-specific-development-environment\" class=\"anchor\" href=\"#1-install-os-specific-development-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Install OS-specific development environment\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-macos-x\" class=\"anchor\" href=\"#macos-x\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMacOS X\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003einstall developer command-line tools: \u003ca href=\"https://developer.apple.com/downloads/\" rel=\"nofollow\"\u003ehttps://developer.apple.com/downloads/\u003c/a\u003e  (includes \u003ccode\u003egit\u003c/code\u003e, \u003ccode\u003egcc\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eVisit \u003ca href=\"http://macports.org\" rel=\"nofollow\"\u003ehttp://macports.org\u003c/a\u003e and follow the instructions there to install the latest version of MacPorts for your version of MacOS X.\u003c/li\u003e\n\u003cli\u003eSet environment variables to use macports version of Python and other packages, packages add the following to \u003ccode\u003e~/.bash_profile\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eexport PATH=/opt/local/bin:$PATH\nexport LIBRARY_PATH=/opt/local/lib/:$LIBRARY_PATH\nexport CPATH=/opt/local/include:$CPATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"4\"\u003e\n\u003cli\u003eRerun your bash shell login in order to make these new exports active in your environment.  At the command line type:\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003eexec bash -login\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-clone-the-repository\" class=\"anchor\" href=\"#2-clone-the-repository\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Clone the repository:\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/alexlancaster/pypop.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-install-dependencies\" class=\"anchor\" href=\"#3-install-dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Install dependencies\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-macos\" class=\"anchor\" href=\"#macos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMacOS:\u003c/h4\u003e\n\u003cp\u003eInstall the MacPorts packages\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  sudo port install swig-python gsl py27-numpy py-libxml2 py27-libxslt py-setuptools py27-pip\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSet MacPorts to use the just-installed 2.7 MacPorts version of Python and pip:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  sudo port select --set python python27\n  sudo port select --set pip pip27\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eCheck that the MacPorts version of Python is active by typing: \u003ccode\u003ewhich python\u003c/code\u003e, if it is working correctly you should see the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e/opt/local/bin/python\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-linux-fedoracentosrhel\" class=\"anchor\" href=\"#linux-fedoracentosrhel\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinux (Fedora/Centos/RHEL):\u003c/h4\u003e\n\u003cp\u003eNeed at least Fedora 25 for the appropriate dependencies:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  sudo dnf install swig gsl-devel python2-numpy python-libxml2 libxslt-python python-setuptools python-pip\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSee \u003ca href=\"DEV_NOTES.md\"\u003eDEV_NOTES.md\u003c/a\u003e for instructions on containerizing the install on a Centos/RHEL release.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-linux-ubuntu\" class=\"anchor\" href=\"#linux-ubuntu\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinux (Ubuntu)\u003c/h4\u003e\n\u003cp\u003eInstall the following packages\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  sudo apt install git libgsl-dev python-numpy python-libxml2 python-libxslt1 python-setuptools python-pip\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003eswig\u003c/code\u003e package in recent Ubuntu releases has bugs, you will need to compile the most recent from source, see also \u003ca href=\"DEV_NOTES.md\"\u003eDEV_NOTES.md\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-4-build\" class=\"anchor\" href=\"#4-build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e4. Build\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e./setup.py build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eThese are examples of how to use PyPop. Specify the \u003ccode\u003e--help\u003c/code\u003e option to see an\nexplanation of the options available.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-run-a-minimal-dataset\" class=\"anchor\" href=\"#run-a-minimal-dataset\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun a minimal dataset:\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e./bin/pypop.py -c  tests/data/minimal.ini tests/data/USAFEL-UchiTelle-small.pop\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will generate the following two files, an XML output file and a plain text version:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eUSAFEL-UchiTelle-small-out.xml\nUSAFEL-UchiTelle-small-out.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-test-suite\" class=\"anchor\" href=\"#running-test-suite\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning test suite\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e  ./setup.py test\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you run into errors, file a bug (as per Support, below), include the output of \u003ccode\u003epy.test\u003c/code\u003e run in verbose mode and capturing the output\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  py.test -s -v\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(See DEV_NOTES.md for more details on installing or running \u003ccode\u003epy.test\u003c/code\u003e outside the context of setuptools.)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h2\u003e\n\u003cp\u003ePlease submit bug reports and feature requests\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://github.com/alexlancaster/pypop/issues\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-bug-reporting\" class=\"anchor\" href=\"#bug-reporting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBug reporting\u003c/h3\u003e\n\u003cp\u003eWhen reporting bugs, especially during installation, please run the following and include the output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eecho $CPATH\necho $LIBRARY_PATH\necho $PATH\nwhich python\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you are running on MacOS please also run and include the output of:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eport installed\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003cp\u003eThe code for PyPop is at\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ehttps://github.com/alexlancaster/pypop\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-copyright-and-license\" class=\"anchor\" href=\"#copyright-and-license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCopyright and License\u003c/h2\u003e\n\u003cp\u003ePyPop is Copyright (C) 2003-2015. The Regents of the University of California (Regents)\u003c/p\u003e\n\u003cp\u003ePyPop is distributed under the terms of GPLv2\u003c/p\u003e\n",
    "stargazers_count": 11,
    "subscribers_count": 10,
    "topics": [
      "population-genomics",
      "evolutionary-biology",
      "bioinformatics",
      "open-source",
      "free-software"
    ],
    "updated_at": 1615988636.0
  },
  {
    "data_format": 2,
    "description": "An automated pipeline for integrated preprocessing and quality assurance of diffusion weighted MRI images",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MASILab/PreQual",
    "latest_release": "v1.0.6",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-prequal-dtiqa-v7-multi-user-guide\" class=\"anchor\" href=\"#prequal-dtiqa-v7-multi-user-guide\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePreQual (dtiQA v7 Multi) User Guide\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#overview\"\u003eOverview\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#authors-and-reference\"\u003eAuthors and Reference\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#getting-started\"\u003eGetting Started\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#containerization-of-source-code\"\u003eContainerization of Source Code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#command\"\u003eCommand\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#arguments-and-io\"\u003eArguments and I/O\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#configuration-file\"\u003eConfiguration File\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-bids-data\"\u003eRunning BIDS Data\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#options\"\u003eOptions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pipeline-assumptions\"\u003ePipeline Assumptions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pipeline-processing-steps\"\u003ePipeline Processing Steps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#pipeline-quality-assurance-steps\"\u003ePipeline Quality Assurance Steps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#outputs\"\u003eOutputs\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#note-on-versioning-for-vuiis-xnat-users\"\u003eNote on Versioning for VUIIS XNAT Users\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-overview\" class=\"anchor\" href=\"#overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOverview\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/MASILab/PreQual/blob/master/overview.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/MASILab/PreQual/raw/master/overview.png?raw=true\" alt=\"Pipeline Overview\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSummary:\u003c/strong\u003e Perform integrated preprocessing and quality assurance of diffusion MRI data\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePreprocessing Steps:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMP-PCA denoising (default on)\u003c/li\u003e\n\u003cli\u003eGibbs de-ringing (default off)\u003c/li\u003e\n\u003cli\u003eRician correction (default off)\u003c/li\u003e\n\u003cli\u003eInter-scan normalization (default on)\u003c/li\u003e\n\u003cli\u003eSusceptibility-induced distortion correction, with or without reverse gradient images or field maps\u003c/li\u003e\n\u003cli\u003eEddy current-induced distortion correction\u003c/li\u003e\n\u003cli\u003eInter-volume motion correction\u003c/li\u003e\n\u003cli\u003eSlice-wise signal dropout imputation\u003c/li\u003e\n\u003cli\u003eN4 B1 bias field correction (default off)\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eQuality Assurance Steps:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eVerification of phase encoding schemes\u003c/li\u003e\n\u003cli\u003eAnalysis of gradient directions\u003c/li\u003e\n\u003cli\u003eShell-wise analysis of signal-to-noise and contrast-to-noise ratios\u003c/li\u003e\n\u003cli\u003eVisualization of Gibbs de-ringing changes (if applicable)\u003c/li\u003e\n\u003cli\u003eVisualization of within brain intensity distributions before and after Rician correction (if applicable)\u003c/li\u003e\n\u003cli\u003eCorrection (if applicable) or visualization of inter-scan intensity relationships\u003c/li\u003e\n\u003cli\u003eShell-wise analysis of distortion corrections\u003c/li\u003e\n\u003cli\u003eAnalysis of inter-volume motion and slice-wise signal dropout\u003c/li\u003e\n\u003cli\u003eAnalysis of B1 bias fields (if applicable)\u003c/li\u003e\n\u003cli\u003eVerification of intra-pipeline masking\u003c/li\u003e\n\u003cli\u003eAnalysis of tensor goodness-of-fit\u003c/li\u003e\n\u003cli\u003eVoxel-wise and region-wise quantification of FA\u003c/li\u003e\n\u003cli\u003eVoxel-wise quantification of MD\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors-and-reference\" class=\"anchor\" href=\"#authors-and-reference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors and Reference\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"mailto:leon.y.cai@vanderbilt.edu\"\u003eLeon Y. Cai\u003c/a\u003e, Qi Yang, Colin B. Hansen, Vishwesh Nath, Karthik Ramadass, Graham W. Johnson, Benjamin N. Conrad, Brian D. Boyd, John P. Begnoche, Lori L. Beason-Held, Andrea T. Shafer, Susan M. Resnick, Warren D. Taylor, Gavin R. Price, Victoria L. Morgan, Baxter P. Rogers, Kurt G. Schilling, Bennett A. Landman. \u003cem\u003ePreQual: An automated pipeline for integrated preprocessing and quality assurance of diffusion weighted MRI images\u003c/em\u003e. \u003ca href=\"https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.28678\" rel=\"nofollow\"\u003eMagnetic Resonance in Medicine\u003c/a\u003e, 2021.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://my.vanderbilt.edu/masi\" rel=\"nofollow\"\u003eMedical-image Analysis and Statistical Interpretation (MASI) Lab\u003c/a\u003e, Vanderbilt University, Nashville, TN, USA\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h2\u003e\n\u003cp\u003eThe PreQual software is designed to run inside a \u003ca href=\"#containerization-of-source-code\"\u003eSingularity container\u003c/a\u003e. The container requires an \"\u003ca href=\"#arguments-and-io\"\u003einputs\u003c/a\u003e\" folder that holds all required input diffusion image files (i.e., .nii.gz, .bval, and .bvec files) and a \u003ca href=\"#configuration-file\"\u003econfiguration file\u003c/a\u003e. For those running Synb0-DisCo to correct susceptibility distortions without reverse phase-encoded images, this folder will also contain the \u003ca href=\"#arguments-and-io\"\u003estructural T1 image\u003c/a\u003e. The container also requires an \"\u003ca href=\"#arguments-and-io\"\u003eoutputs\u003c/a\u003e\" folder that will hold all the outputs after the pipeline runs. We also need to know the image \u003cem\u003e\u003ca href=\"#arguments-and-io\"\u003eaxis\u003c/a\u003e\u003c/em\u003e on which phase encoding was performed for all inputs (i.e., \"i\" for the first dimension, \"j\" for the second). To build the configuration file, we need to know the \u003cem\u003e\u003ca href=\"#configuration-file\"\u003edirection\u003c/a\u003e\u003c/em\u003e along said axis in which each image was phase encoded (i.e., \"+\" for positive direction and \"-\" for the negative direction) and the \u003ca href=\"#configuration-file\"\u003ereadout time\u003c/a\u003e for each input image. Once we have this information, we bind the inputs and outputs directories into the container to \u003ca href=\"#command\"\u003erun the pipeline\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eNote: The phase encoding axis, direction, and readout time must be known ahead of time, as this information is not stored in NIFTI headers. Depending on the scanner used, they may be available in JSON sidecars when NIFTIs are converted from DICOMs with \u003ca href=\"#pipeline-assumptions\"\u003edcm2niix\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-containerization-of-source-code\" class=\"anchor\" href=\"#containerization-of-source-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainerization of Source Code\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/MASILab/PreQual.git\ncd /path/to/repo/PreQual\ngit checkout v1.0.6\nsudo singularity build /path/to/prequal.simg Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe use Singularity version 3.4 with root permissions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-command\" class=\"anchor\" href=\"#command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003esingularity run \n-e \n--contain\n-B /path/to/inputs/directory/:/INPUTS\n-B /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n-B /path/to/cuda:/usr/local/cuda\n--nv\n/path/to/prequal.simg\npe_axis\n[options]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eBinding the freesurfer license is optional and only needed for Synb0-DisCo\u003c/li\u003e\n\u003cli\u003eBinding the tmp directory is necessary when running the image with \u003ccode\u003e--contain\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e--nv\u003c/code\u003e and \u003ccode\u003e-B /path/to/cuda:/usr/local/cuda\u003c/code\u003e are optional. See options \u003ccode\u003e--eddy_cuda\u003c/code\u003e and \u003ccode\u003e--eddy_extra_args\u003c/code\u003e. \u003cstrong\u003eGPU support is currently experimental.\u003c/strong\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-arguments-and-io\" class=\"anchor\" href=\"#arguments-and-io\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eArguments and I/O\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInput Directory:\u003c/strong\u003e The dtiQA_config.csv configuration file and at least one diffusion weighted image must be provided.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003edtiQA_config.csv (see \u003ca href=\"#configuration-file\"\u003ebelow\u003c/a\u003e for format, must be named exactly)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;.nii.gz (diffusion weighted image)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;.bval (units of s/mm\u003csup\u003e2\u003c/sup\u003e, in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;.bvec (normalized unit vectors in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;.nii.gz (diffusion weighted image)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;.bval (units of s/mm\u003csup\u003e2\u003c/sup\u003e, in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;.bvec (normalized unit vectors in the \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#Diffusion_data_in_FSL\" rel=\"nofollow\"\u003eFSL format\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003et1.nii.gz (Optional, used for Synb0-DisCo, must be named exactly)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOther files as needed (see \u003ccode\u003e--extra_eddy_args\u003c/code\u003e for more information)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOutput Directory:\u003c/strong\u003e Full outputs listed at the \u003ca href=\"#outputs\"\u003eend\u003c/a\u003e of this document\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThe output preprocessed images are available in the PREPROCESSED subfolder in the output directory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePREPROCESSED/dwmri.nii.gz\u003c/li\u003e\n\u003cli\u003ePREPROCESSED/dwmri.bval\u003c/li\u003e\n\u003cli\u003ePREPROCESSED/dwmri.bvec\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe QA document is available in the PDF subfolder in the output directory:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePDF/dtiQA.pdf\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003epe_axis:\u003c/strong\u003e Phase encoding axis of all the input images. We do NOT support different phase encoding axes between different input images at this time. The options are i and j and correspond to the first and second dimension of the input images, respectively. Note that FSL does not currently support phase encoding in the third dimension (i.e. k, the dimension in which the image slices were acquired, commonly axial for RAS and LAS oriented images). \u003cstrong\u003eThis parameter is direction AGNOSTIC\u003c/strong\u003e. The phase encoding directions of the input images along this axis are specified in the dtiQA_config.csv file. See \u003ca href=\"#configuration-file\"\u003eConfiguration File\u003c/a\u003e and \u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e for more information.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-configuration-file\" class=\"anchor\" href=\"#configuration-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration File\u003c/h2\u003e\n\u003cp\u003eThe format for the lines of the configuration CSV file, dtiQA_config.csv (must be named exactly), are as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026lt;image1\u0026gt;,pe_dir,readout_time\n:\n\u0026lt;imageN\u0026gt;,pe_dir,readout_time\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026lt;image\u0026gt;\u003c/strong\u003e is the shared file PREFIX between the corresponding NIFTI, BVAL, and BVEC files for that particular image in the input directory (i.e., my_dwi.nii.gz/.bval/.bvec -\u0026gt; my_dwi). Do NOT include the path to the input directory.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003epe_dir\u003c/strong\u003e is either + or -, corresponding to the direction along the phase encoding axis (as defined by the parameter \u003ccode\u003epe_axis\u003c/code\u003e) on which the image is phase encoded.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote that a combination of phase encoding axis and direction map to specific anatomical (i.e. APA, APP, etc.) directions based on the orientation of the image. So, for instance in a RAS image, an axis of j and direction of + map to APP. We infer the orientation of the image from the header of the NIFTI using nibabel tools and output the best anatomical phase encoding direction interpretation of the input direction in the PDF for QA.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ereadout_time\u003c/strong\u003e is a non-negative number, the readout_time parameter required by FSL\u2019s eddy. The absolute value of this parameter is used to scale the estimated b0 field. Note a value of 0 indicates that the images are infinite bandwidth (i.e. no susceptibility distortion). See \u003ca href=\"#examples\"\u003eExamples\u003c/a\u003e for more information.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cp\u003eHere are some different example combinations of pe_axis, pe_dir, and readout_time parameters and the corresponding FSL acquisition parameters lines:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003epe_axis\u003c/th\u003e\n\u003cth\u003epe_dir\u003c/th\u003e\n\u003cth\u003ereadout_time\u003c/th\u003e\n\u003cth\u003eacqparams line\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ei\u003c/td\u003e\n\u003ctd\u003e+\u003c/td\u003e\n\u003ctd\u003e0.05\u003c/td\u003e\n\u003ctd\u003e1, 0, 0, 0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003e-\u003c/td\u003e\n\u003ctd\u003e0.1\u003c/td\u003e\n\u003ctd\u003e0, -1, 0, 0.1\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eThese are examples of common use cases. They also all share the same command, as detailed above. The PREPROCESSED output folder will contain the final outputs and the PDF folder will contain the QA report.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003ePhase Encoding\u003cbr\u003eAxis\u003c/th\u003e\n\u003cth\u003eReverse Phase\u003cbr\u003eEncoded (RPE) Image\u003c/th\u003e\n\u003cth\u003eT1\u003cbr\u003eImage\u003c/th\u003e\n\u003cth\u003eContents of\u003cbr\u003eInput Directory\u003c/th\u003e\n\u003cth\u003eContents of\u003cbr\u003edtiQA_config.csv\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eN/A\u003c/td\u003e\n\u003ctd\u003edti1.nii.gz\u003cbr\u003edti1.bval\u003cbr\u003edti1.bvec\u003cbr\u003edti2.nii.gz\u003cbr\u003edti2.bval\u003cbr\u003edti2.bvec\u003cbr\u003erpe.nii.gz\u003cbr\u003erpe.bval\u003cbr\u003erpe.bvec\u003cbr\u003edtiQA_config.csv\u003c/td\u003e\n\u003ctd\u003edti1,+,0.05\u003cbr\u003edti2,+,0.05\u003cbr\u003erpe,-,0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003edti1.nii.gz\u003cbr\u003edti1.bval\u003cbr\u003edti1.bvec\u003cbr\u003edti2.nii.gz\u003cbr\u003edti2.bval\u003cbr\u003edti2.bvec\u003cbr\u003et1.nii.gz\u003cbr\u003edtiQA_config.csv\u003c/td\u003e\n\u003ctd\u003edti1,+,0.05\u003cbr\u003edti2,+,0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ej\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003edti1.nii.gz\u003cbr\u003edti1.bval\u003cbr\u003edti1.bvec\u003cbr\u003edti2.nii.gz\u003cbr\u003edti2.bval\u003cbr\u003edti2.bvec\u003cbr\u003edtiQA_config.csv\u003c/td\u003e\n\u003ctd\u003edti1,+,0.05\u003cbr\u003edti2,+,0.05\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-bids-data\" class=\"anchor\" href=\"#running-bids-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning BIDS Data\u003c/h2\u003e\n\u003cp\u003eWhile not a BIDS pipeline, data in BIDS format can be run with PreQual without moving or copying data. The key is that the input directory structure must be as described relative to \u003cem\u003einside the container\u003c/em\u003e. By creatively binding files/folders into the container, we can achieve the same effect:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e-B /path/to/sub-X/ses-X/dwi/:/INPUTS\n-B /path/to/sub-X/ses-X/anat/sub-X_ses-X_T1w.nii.gz:/INPUTS/t1.nii.gz (optional, Synb0-DisCo only)\n-B /path/to/config/file.csv:/INPUTS/dtiQA_config.csv\n-B /path/to/outputs/directory/:/OUTPUTS\n-B /tmp:/tmp\n-B /path/to/freesurfer/license.txt:/APPS/freesurfer/license.txt\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe outputs directory and configuration file can be created wherever makes the most sense for the user. The contents of the configuration file will look something like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esub-X_ses-X_acq-1_dwi,pe_dir,readout_time\n:\nsub-X_ses-X_acq-N_dwi,pe_dir,readout_time\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-options\" class=\"anchor\" href=\"#options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOptions\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e--bval_threshold N\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA non-negative integer threshold under which to consider a b-value to be zero. Useful when some MRI machines do not allow for more than one b0 volume to be acquired so some users acquire scans with extremely low b-values to be treated like b0 volumes. Setting this value to 0 results in no thresholding. Units = s/mm\u003csup\u003e2\u003c/sup\u003e.\u003c/p\u003e\n\u003cp\u003eDefault = 50\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--nonzero_shells s1,s2,...,sn/auto\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA comma separated list of positive integers (s/mm\u003csup\u003e2\u003c/sup\u003e) indicating nonzero shells for SNR/CNR analysis when there are more unique b-values than shells determined by eddy or automatically determine shells by rounding to nearest 100. Useful when b-values are modulated around a shell value instead of set exactly at that value. Only used when determining shells for SNR/CNR analysis. Original b-values used elsewhere in pipeline.\u003c/p\u003e\n\u003cp\u003eDefault = auto\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--denoise on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eDenoise images prior to preprocessing using Marchenko-Pastur PCA \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwidenoise.html\" rel=\"nofollow\"\u003eimplemented in MRTrix3\u003c/a\u003e. The SNR of the b0s of the final preprocessed images are reported in the PDF output regardless of whether this option is on or off.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--degibbs on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRemove Gibbs ringing artifacts using the local subvoxel-shifts method as \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/mrdegibbs.html\" rel=\"nofollow\"\u003eimplemented in MRTrix3\u003c/a\u003e. We caution against using this feature as it not designed for the partial Fourier schemes with which most echo planar diffusion images are acquired. It is also difficult to quality check, but we include a visualization of averaged residuals across all b = 0 s/mm\u003csup\u003e2\u003c/sup\u003e volumes, looking for larger signals near high contrast (i.e. parenchyma-CSF) interfaces.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--rician on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePerform Rician correction using the method of moments. We normally do not perform this step as we empirically do not find it to affect results drastically. It is also difficult to quality check, but we include a plot of the shell-wise within brain intensity distributions for each input before and after correction, looking for a slight drop in intensity with correction.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--prenormalize on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIntensity normalize images prior to preprocessing by maximizing the intra-mask intensity-histogram intersections between the averaged b0s of the scans. If this option is on, these histograms before and after prenormalization will be reported in the output PDF. This is done to avoid gain differences between different diffusion scans. If this option is off, we assume that the various input images all have the same gain. That being said, we still estimate and report the gain factors and intensity histograms in a gain QA page and report warnings if estimated gains greater than 5% are found.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--synb0 on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003etopup\u003c/code\u003e with a synthetic b0 generated with the Synb0-DisCo deep-learning framework if no reverse phase encoded images are supplied and a T1 image is supplied. Synb0-DisCo requires at least 24GB of RAM.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--topup_first_b0s_only\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003etopup\u003c/code\u003e with only the first b0 from each input image. At the time of writing, \u003cstrong\u003eFSL\u0027s topup cannot be parallelized\u003c/strong\u003e, and the runtime of topup can increase dramatically as more b0 volumes are included. This flag allows for faster processing at the expense of information lost from any interleaved b0s.\u003c/p\u003e\n\u003cp\u003eDefault = use ALL b0s\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--extra_topup_args=string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExtra arguments to pass to FSL\u2019s \u003ccode\u003etopup\u003c/code\u003e. \u003ccode\u003eTopup\u003c/code\u003e will run with the following by default (as listed in the \u003ccode\u003e/SUPPLEMENTAL/topup.cnf\u003c/code\u003e configuration file) but will be overwritten by arguments passed to \u003ccode\u003e--extra_topup_args\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Resolution (knot-spacing) of warps in mm\n--warpres=20,16,14,12,10,6,4,4,4\n# Subsampling level (a value of 2 indicates that a 2x2x2 neighbourhood is collapsed to 1 voxel)\n--subsamp=1,1,1,1,1,1,1,1,1\n# FWHM of gaussian smoothing\n--fwhm=8,6,4,3,3,2,1,0,0\n# Maximum number of iterations\n--miter=10,10,10,10,10,20,20,30,30\n# Relative weight of regularisation\n--lambda=0.00033,0.000067,0.0000067,0.000001,0.00000033,0.000000033,0.0000000033,0.000000000033,0.00000000000067\n# If set to 1 lambda is multiplied by the current average squared difference\n--ssqlambda=1\n# Regularisation model\n--regmod=bending_energy\n# If set to 1 movements are estimated along with the field\n--estmov=1,1,1,1,1,0,0,0,0\n# 0=Levenberg-Marquardt, 1=Scaled Conjugate Gradient\n--minmet=0,0,0,0,0,1,1,1,1\n# Quadratic or cubic splines\n--splineorder=3\n# Precision for calculation and storage of Hessian\n--numprec=double\n# Linear or spline interpolation\n--interp=spline\n# If set to 1 the images are individually scaled to a common mean intensity \n--scale=0\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThese parameters should be formatted as a list separated by +\u0027s with no spaces (i.e., \u003ccode\u003e--extra_topup_args=--scale=1+--regrid=0\u003c/code\u003e). For \u003ccode\u003etopup\u003c/code\u003e options that require additional inputs, place the file in the inputs directory and use the following syntax: \u003ccode\u003e--\u0026lt;myinputoption\u0026gt;=/INPUTS/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. For \u003ccode\u003etopup\u003c/code\u003e options that produce additional outputs, the file will save in the output directory under the \u201cTOPUP\u201d folder by using the following syntax: \u003ccode\u003e--\u0026lt;myoutputoption\u0026gt;=/OUTPUTS/TOPUP/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. Note that in this case \u003ccode\u003e/INPUTS\u003c/code\u003e and \u003ccode\u003e/OUTPUTS\u003c/code\u003e should be named exactly as is and are NOT the path to the input and output directory on your file system.\u003c/p\u003e\n\u003cp\u003eDefault = none\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--eddy_cuda 8.0/9.1/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun FSL\u2019s \u003ccode\u003eeddy\u003c/code\u003e with NVIDIA GPU acceleration. If this parameter is 8.0 or 9.1, either CUDA 8.0 or 9.1 must be installed, properly configured on your system, and bound into the container, respectively. Additionally the \u003ccode\u003e--nv\u003c/code\u003e flag must be run in the singularity command. If this parameter is off, \u003ccode\u003eeddy\u003c/code\u003e is run with OPENMP CPU multithreading. See \u003ccode\u003e--num_threads\u003c/code\u003e for more information. CUDA is required to run \u003ccode\u003eeddy\u003c/code\u003e with \u003ccode\u003e--mporder\u003c/code\u003e (intra-volume slice-wise motion correction). See \u003ccode\u003e--extra_eddy_args\u003c/code\u003e for more information.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--eddy_mask on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003eeddy\u003c/code\u003e with or without a brain mask. If on, FSL\u2019s brain extraction tool (\u003ccode\u003ebet\u003c/code\u003e) is used with a low threshold to create a rough brain mask for \u003ccode\u003eeddy\u003c/code\u003e. This can sometimes produce poor results. If off, no mask is used and produces empirically minor differences in results than when a mask is used. If this option is on, the contour of this mask is drawn in the PDF.\u003c/p\u003e\n\u003cp\u003eDefault = on\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--eddy_bval_scale N/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eRun \u003ccode\u003eeddy\u003c/code\u003e with b-values scaled by the positive number N. All other steps of the pipeline use the original b-values. This can help \u003ccode\u003eeddy\u003c/code\u003e finish distortion correction when extremely low b-values (\u0026lt;200) are involved. If off, no scaling of b-values is used.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--extra_eddy_args=string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eExtra arguments to pass to FSL\u2019s \u003ccode\u003eeddy\u003c/code\u003e. \u003ccode\u003eEddy\u003c/code\u003e will always run with the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e--repol\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that if \u003ccode\u003e--mporder\u003c/code\u003e is passed here, \u003ccode\u003e--eddy_cuda\u003c/code\u003e must be 8.0 or 9.1 and the singularity option \u003ccode\u003e--nv\u003c/code\u003e must be passed into the container, as intra-volume slice-wise motion correction requires GPU acceleration.\u003c/p\u003e\n\u003cp\u003eThese parameters should be formatted as a list separated by +\u0027s with no spaces (i.e., \u003ccode\u003e--extra_eddy_args=--data_is_shelled+--ol_nstd=1\u003c/code\u003e). For \u003ccode\u003eeddy\u003c/code\u003e options that require additional inputs, place the file in the inputs directory and use the following syntax: \u003ccode\u003e--\u0026lt;myinputoption\u0026gt;=/INPUTS/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. For \u003ccode\u003eeddy\u003c/code\u003e options that produce additional outputs, the file will save in the output directory under the \u201cEDDY\u201d folder by using the following syntax: \u003ccode\u003e--\u0026lt;myoutputoption\u0026gt;=/OUTPUTS/EDDY/\u0026lt;file.ext\u0026gt;\u003c/code\u003e. Note that in this case \u003ccode\u003e/INPUTS\u003c/code\u003e and \u003ccode\u003e/OUTPUTS\u003c/code\u003e should be named exactly as is and are NOT the path to the input and output directory on your file system.\u003c/p\u003e\n\u003cp\u003eDefault = none\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--postnormalize on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIntensity normalize images after preprocessing by maximizing the intra-mask intensity-histogram intersections between the averaged b0s of the scans. If this option is on, these histograms before and after postnormalization will be reported in the output PDF.\u003c/p\u003e\n\u003cp\u003eNote: This option was intended for testing and is left for posterity. It is not recommended at this time and will be deprecated.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--correct_bias on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePerform \u003ca href=\"https://manpages.debian.org/testing/ants/N4BiasFieldCorrection.1.en.html\" rel=\"nofollow\"\u003eANTs N4 bias field correction\u003c/a\u003e as \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/reference/commands/dwibiascorrect.html\" rel=\"nofollow\"\u003ecalled in MRTrix3\u003c/a\u003e. If this option is on, the calculated bias field will be visualized in the output PDF.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--improbable_mask on/off\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eCreate an additional mask on the preprocessed data that omits voxels where the minimum b0 signal is smaller than the minimum diffusion weighted signal. This can be helpful for reducing artifacts near the mask border when fitting models.\u003c/p\u003e\n\u003cp\u003eDefault = off\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--glyph_type tensor/vector\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eVisualize either tensors or principal eigenvectors in the QA document.\u003c/p\u003e\n\u003cp\u003eDefault = tensor\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--atlas_reg_type FA/b0\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePerform JHU white matter atlas registration to the subject by either deformably registering the subject\u0027s FA map or average b0 to the MNI FA or T2 template, respectively. Empirically, the FA approach tends to be more accurate for white matter whereas the b0 approach tends to be more accurate globally. The b0 approach is more robust for acquisitions with low shells (i.e., b \u0026lt; 500 s/mm\u003csup\u003e2\u003c/sup\u003e) or poor masking that result in the inclusion of a lot of facial structure.\u003c/p\u003e\n\u003cp\u003eDefault = FA\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--split_outputs\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSplit the fully preprocessed output (a concatenation of the input images) back into their component parts and do NOT keep the concatenated preprocessed output.\u003c/p\u003e\n\u003cp\u003eDefault = Do NOT split and return only the concatenated output\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--keep_intermediates\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eKeep intermediate copies of diffusion data (i.e. denoised, prenormalized, bias-corrected, etc.) used to generate final preprocessed data. Using this flag may result in a large consumption of hard disk space.\u003c/p\u003e\n\u003cp\u003eNote: Due to space concerns, special permission needed to use this option on XNAT.\u003c/p\u003e\n\u003cp\u003eDefault = do NOT keep intermediates\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--num_threads N\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eA positive integer indicating the number of threads to use when running portions of the pipeline that can be multithreaded (i.e. MRTrix3, ANTs, and FSL\u2019s eddy without GPU acceleration). Please note that at the time of writing, \u003cstrong\u003eFSL\u0027s topup cannot be parallelized\u003c/strong\u003e, and that the runtime of topup can increase dramatically as more b0 volumes are included. See \u003ccode\u003e--topup_first_b0s_only\u003c/code\u003e for more information.\u003c/p\u003e\n\u003cp\u003eNote: Due to resource concerns, special permission needed to multi-thread on XNAT.\u003c/p\u003e\n\u003cp\u003eDefault = 1 (do NOT multithread)\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--project string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eString describing project in which the input data belong to label PDF output\u003c/p\u003e\n\u003cp\u003eDefault = proj\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--subject string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eString describing subject from which the input data were acquired to label PDF output\u003c/p\u003e\n\u003cp\u003eDefault = subj\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--session string\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eString describing session in which the input data were acquired to label PDF output\u003c/p\u003e\n\u003cp\u003eDefault = sess\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e--help, -h\u003c/strong\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-assumptions\" class=\"anchor\" href=\"#pipeline-assumptions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Assumptions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eAll NIFTI images are consistent with a conversion from a DICOM using \u003ccode\u003edcm2niix\u003c/code\u003e (\u003ca href=\"https://github.com/rordenlab/dcm2niix/releases/tag/v1.0.20180622\"\u003eat least v1.0.20180622\u003c/a\u003e) by Chris Rorden and are raw NIFTIs without distortion correction. We require this as dcm2niix exports b-value/b-vector files in FSL format and removes ADC or trace images auto-generated in some Philips DICOMs. In addition \u003ccode\u003edcm2niix\u003c/code\u003e correctly moves the gradients from scanner to subject space and does not re-order volumes, both of which can cause spurious results or pipeline failure.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eWe expect raw volumes only, no ADC or trace volumes.\u003c/strong\u003e ADC volumes are sometimes encoded as having a b-value greater than 0 with a corresponding b-vector of (0,0,0) and trace volumes are sometimes encoded as having a b-value of 0 with a corresponding non-unit normalized b-vector, as in the case of some Philips PARREC converters. We check for these cases, remove the affected volumes, and report a warning in the console and in the PDF.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe cannot, unfortunately, account for failure of reorientation of gradients into subject space. Visualization of tensor glyphs or principal eigenvectors can be helpful in distinguishing this. However, this error can be subtle so we suggest proper DICOM to NIFTI conversion with the above release of \u003ccode\u003edcm2niix\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eImages will be processed in the order they are listed in dtiQA_config.csv.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe size of all the volumes across all images must all be the same.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe location of b0 images inside the input images do not matter.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs per the FSL format, we do not support non-unit normalized gradients. We also do not support gradient directions of 0,0,0 when the corresponding b-value is non-zero. Gradients with the latter configurations may cause pipeline failure. We report warnings in the output PDF if we identify these.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe phase encoding axis of all volumes across all images is the same.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe phase encoding direction along the axis is the same for all volumes inside an image and is specified in the dtiQA_config.csv file.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUnless \u003ccode\u003e--prenormalize\u003c/code\u003e is on, we assume all input images have the same gain.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe will preferentially preprocess images with FSL\u2019s topup using available images with complementary phase encoding directions (i.e. + and -, \"reverse phase encodings\"). If none are available and a T1 is available, we will synthesize a susceptibility-corrected b0 from the first image listed in dtiQA_config.csv with Synb0-DisCo for use with topup, unless the user turns the \u003ccode\u003e--synb0\u003c/code\u003e parameter off. The readout time of this synthetic b0 will be zero and the phase encoding direction will be equal to that of the first image in dtiQA_config.csv. Otherwise, we will preprocess without topup and move straight to FSL\u2019s eddy.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe use topup and eddy for preprocessing, both of which at the present moment do NOT officially support DSI acquisitions but only single- and multi-shell. We will force topup and eddy to run on DSI data, but may not produce quality results. Please carefully check the PDF output as we report a warning if eddy detected non-shelled data and thus required the use of the force flag.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNote that eddy may erroneously detect data as non-shelled if there are fewer directions in one of the shells than others. Because we merge the images for preprocessing, a notable example of this is when a reverse-phase encoded image uses a different shell than the forward images and has significantly fewer directions.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor preprocessing, eddy will motion correct to the first b0 of each image.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eMRTrix3 by default preferentially uses the qform for understanding NIFTI orientations. Nibabel uses the sform. We set MRTrix3 to use the sform in our pipeline, and thus we preferentially use the sform when the two don\u2019t match.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNo b0 drift correction is performed.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe use the fit tensor model primarily for QA. If b-values less than 500 s/mm\u003csup\u003e2\u003c/sup\u003e or greater than 1500 s/mm\u003csup\u003e2\u003c/sup\u003e are present, we suggest careful review of the fit prior to use for non-QA purposes.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-processing-steps\" class=\"anchor\" href=\"#pipeline-processing-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Processing Steps\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThreshold all b-values such that values less than the \u003ccode\u003e--bval_threshold\u003c/code\u003e parameter are 0.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck that all b-vectors are unit normalized and all b-values greater than zero have associated non-zero b-vectors. For any volumes where this is not the case, we remove them, flag a warning for the output PDF, and continue the pipeline.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, denoise all diffusion scans with \u003ccode\u003edwidenoise\u003c/code\u003e (Marchenko-Pastur PCA) from MrTrix3 and save the noise profiles (needed for Rician correction later).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, perform Gibbs de-ringing on all diffusion scans with \u003ccode\u003emrdegibbs\u003c/code\u003e from MRTrix3.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, perform Rician correction on all diffusion scans with the method of moments.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf applicable, prenormalize all diffusion scans. To accomplish this, extract all b0 images from each diffusion scan and average them. Then find a rough brain-mask with FSL\u2019s bet and calculate an intensity scale factor such that the histogram intersection between the intra-mask histogram of the different scans\u2019 averaged b0s to that of the first scan is maximized. Apply this scale factor to the entire diffusion weighted scan. This is done to avoid gain differences between different diffusion scans.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIf prenormalization is not indicated, we still run the prenormalization algorithms to calculate rough gain differences and report the gain factors and intensity histograms in a gain QA page. The outputs of the algorithms, however, are NOT propagated through to the rest of the pipeline.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePrepare data for and run preprocessing with topup and eddy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eTopup:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eExtract all b0s from all scans, maintaining their relative order.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e(Optional) If a T1 is supplied and no complementary (i.e. reverse) phase encoded images are provided, use Synb0-DisCo to convert the first b0 of the first scan to a susceptibility-corrected b0.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the acquisition parameters file required by both topup and eddy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFor the number of b0s from each image, add the same phase encoding and readout time line to the acquisition parameters file, as outlined in \"Example Phase Encoding Schemes\".\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExample: In the case where we have a phase encoding axis of j and two images, one with 7 b0s, + direction, and 0.05 readout time and one with 3 b0s, - direction, and 0.02 readout time, this file will have 10 lines. The first 7 lines are identical and equal to [0, 1, 0, 0.05]. The last three lines are also identical and equal to [0, -1, 0, 0.02].\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e(Optional) If Synb0-DisCo is run because no complementary phase encoding directions are supplied and --synb0 is not off, we add an additional line to the end of the file. This line is the same as the first line of the file except that the readout time is 0 instead.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExample: In the case where we have a phase encoding axis of j and two images, one with 7 b0s, + direction, and 0.05 readout time and one with 3 b0s, + direction, and 0.02 readout time, this file will have 11 lines. The first 7 lines are identical and equal to [0, 1, 0, 0.05]. The next three lines are also identical and equal to [0, 1, 0, 0.02]. Finally, the last line is equal to [0, 1, 0, 0].\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then concatenate all the b0s maintaining their order and run topup with the acquisition parameters file if images with complementary phase encoding directions are supplied or if a T1 was supplied. Otherwise, we move on to the next step, eddy.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEddy\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eUsing the acquisition parameters file from the topup step, regardless of whether topup was performed, we build the eddy index file such that each volume in each image corresponds to the line in the acquisition parameters file associated with the first b0 of each scan. This is done to tell eddy that each volume in a given scan has the same underlying phase encoding scheme as the first b0 of that scan.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eExample: In the case where we have two images, one with 7 b0s and 100 total volumes and one with 3 b0s and 10 total volumes, the eddy index file has 100 1\u2019s followed by 10 8\u2019s.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEddy is then run with either a mask generated with bet and the -f 0.25 and -R options or without a mask (aka with a mask of all 1\u2019s), depending on user input (see the --eddy_mask option) and with the output of topup if topup was run. Eddy also runs with the --repol option for outlier slice replacement. We also first run eddy with a check looking for shelled data. If the check fails, eddy is then run with the --data_is_shelled flag to force eddy to run on all scans, DSI included. Note that DSI data is not officially supported by FSL\u2026 yet?\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eIf eddy detects data is not shelled, we report this as a warning\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAs noted in the assumptions section above, eddy may erroneously detect data as non-shelled if there are fewer directions in one of the shells than others. Because we merge the images for preprocessing, a notable example of this is when a reverse-phase encoded image uses a different shell than the forward images and has significantly fewer directions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEddy also performs bvec rotation correction and calculates the voxel-wise signal-to-noise ratios of the b0 images and the voxel-wise contrast-to-noise ratios for the diffusion weighted images. SNR is defined as the mean value divided by the standard deviation. CNR is defined as the standard deviation of the Gaussian Process predictions (GP) divided by the standard deviation of the residuals between the measured data and the GP predictions.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf the user chooses to, we then perform post-normalization in the same fashion as pre-normalization.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf the user chooses to, we then wrap up preprocessing with an N4 bias field correction as implemented in ANTs via MRTrix3\u2019s dwibiascorrect.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe generate a brain mask using FSL\u2019s bet2 with the following options. If applicable, we omit the voxels where the minimum b0 signal is less than the minimum diffusion weighted signal in an additional \"improbable mask\".\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e-f 0.25 -R\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then apply the mask to the preprocessed images while we calculate tensors using MRTrix3\u2019s dwi2tensor function. For visualization we discard tensors that have diagonal elements greater than 3 times the apparent diffusion coefficient of water at 37\u00b0C (~0.01).\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eWe also reconstruct the preprocessed image from the tensor fit for further analysis later. dwi2tensor does this for us.\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then convert the tensor to FA and MD images (and visualize them later too) as well as AD, RD, and V1 eigenvector images for the user. The latter 3 are not visualized.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-pipeline-quality-assurance-steps\" class=\"anchor\" href=\"#pipeline-quality-assurance-steps\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePipeline Quality Assurance Steps\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWe start with the brain mask generated above to generate a mask used for the following quantification of tensor fit using a chi-squared statistic.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eFirst, we calculate the mean image for each unique b-value (0 not included). Then we run FSL\u2019s FAST to isolate the CSF on each meaned image. We then take the average probability of a voxel being CSF across all unique b-values and assign \u0026gt;15% probability to be a positive CSF voxel.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThen we call the final chi-squared mask to be the intersection of the inverted CSF mask and a 1-pixel eroded version of the brain mask.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eOn the voxels inside the chi-squared mask, we perform the following quality assurance:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eWe perform a chi-squared analysis for each slice for each volume in the main image by calculating the ratio between the sum-squared error of the fit and the sum-squared intensities of the slice.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe extract the average FA for a number of white matter ROIs defined by the Hopkins atlas. We do this by non-rigidly registering the atlas to our FA output and extracting the FA values contained in each ROI.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe check the gradients output by eddy (i.e. the preprocessed gradients) with \u003ca href=\"https://mrtrix.readthedocs.io/en/3.0.0/reference/commands/dwigradcheck.html\" rel=\"nofollow\"\u003edwigradcheck from MRTrix3\u003c/a\u003e. This performs tractography and finds the optimal sign and order permutation of the b-vectors such that the average tract length in the brain is most physiological.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eThese optimized gradients are saved in the OPTIMIZED_BVECS output folder, and the gradients output by eddy in the PREPROCESSED folder are NOT overwritten.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe original, preprocessed, and preprocessed + optimized gradients are visualized as outlined below.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize the entire pipeline.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eOn the first page we describe the methods used for that run of the pipeline (what inputs were provided, what sort of preprocessing happened, etc.).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize the raw images with the interpreted phase encoding schemes.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Gibbs de-ringing was run, we visualize central slices of the averaged residuals across b0 volumes before and after Gibbs de-ringing, looking for large residuals near high contrast interfaces (i.e. parenchyma against CSF)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Rician correction was performed, we visualize the within brain intensity distributions of each shell of each image before and after correction, looking for downward shifts after correction.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Synb0-DisCo was run, we then visualize the distorted b0 (first b0 of first scan) and T1 used as inputs as well as the output susceptibility corrected b0 in their native space.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf pre- or post-normalization was performed, we then visualize the intra-mask histograms before and after these steps as well as the calculated scaling factors. If pre-normalization is not performed, we visualize the histograms that would have been generated with pre-normalization ONLY as a check for gain differences.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize the first b0 of the images before and after preprocessing with the contours of the brain and stats masks overlaid as well as the contours of the eddy mask overlaid if it is used. We also report the percent of \"improbable voxels\" in the preprocessed mask, regardless of whether the improbable mask is saved.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe plot the motion and angle correction done by eddy as well as the RMS displacement and median intensity for each volume and the volume\u2019s associated b-value. These values are read in from an eddy output text file and we also compute and save the average of these values. In addition, we plot the outlier slices removed and then imputed by eddy as well as the chi-squared fit, with maximal bounds 0 to 0.2. The median chi-squared values are shown across volumes and slices.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then plot the original raw b-vectors scaled by their b-values, the preprocessed ones output by eddy, and the optimized ones determined by \u003ccode\u003edwigradcheck\u003c/code\u003e applied to the preprocessed ones.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf bias field correction was performed, we then visualize the calculated fields.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize some central slices of the average volumes for all unique b-values, including b = 0 and report the median intra-mask SNR or CNR calculated by eddy as appropriate. If there are more unique b-values than shells deteremined by eddy, we round the b-values to the nearest 100 by default to assign volumes to shells or we choose the nearest shell indicated by the user (see \u003ccode\u003e--nonzero_shells\u003c/code\u003e).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe visualize the tensors (or principal eigenvectors depending on \u003ccode\u003e--glyph_type\u003c/code\u003e) using MRTrix3\u2019s mrview, omitting the tensors with negative eigenvalues or eigenvalues greater than 3 times the ADC of water at 37\u00b0C.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe then visualize some central slices of the FA map clipped from 0 to 1 as well as the average FA for the Hopkins ROIs and the quality of the atlas registration.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLastly, we visualize some central slices of the MD map clipped from 0 to 0.003 (ADC of water at 37\u00b0C).\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-outputs\" class=\"anchor\" href=\"#outputs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOutputs\u003c/h2\u003e\n\u003cp\u003e\u0026lt;imageN_%\u0026gt; denotes the original prefix of imageN with the preceding preprocessing step descriptors tacked on the end. For example, in the case of the PRENORMALIZED directory, the prefix for imageJ may or may not include \"_denoised\" depending on whether the denoising step was run.\u003c/p\u003e\n\u003cp\u003eFolders and files in \u003cstrong\u003ebold\u003c/strong\u003e are always included.\u003c/p\u003e\n\u003cp\u003eFolders and files in \u003cem\u003eitalics\u003c/em\u003e are removed if \u003ccode\u003e--keep_intermediates\u003c/code\u003e is NOT indicated\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTHRESHOLDED_BVALS\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026lt;image1\u0026gt;.bval\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003e\u0026lt;imageN\u0026gt;.bval\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eCHECKED\u003c/em\u003e (these contain the volumes that have passed the bval/bvec checks)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1\u0026gt;_checked.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1\u0026gt;_checked.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1\u0026gt;_checked.bvec\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN\u0026gt;_checked.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN\u0026gt;_checked.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN\u0026gt;_checked.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eDENOISED\u003c/em\u003e (these files are only created if \u003ccode\u003e--denoise\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_denoised.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_noise.nii.gz\u003c/em\u003e (needed for Rician correction)\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_denoised.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_noise.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eDEGIBBS\u003c/em\u003e (these files are only created if \u003ccode\u003e--degibbs\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_degibbs.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_degibbs.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eRICIAN\u003c/em\u003e (these files are only created if \u003ccode\u003e--rician\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_rician.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_rician.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003ePRENORMALIZED\u003c/em\u003e (these files are only created if \u003ccode\u003e--prenormalize\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eGAIN_CHECK\u003c/em\u003e (these files are only created if \u003ccode\u003e--prenormalize\u003c/code\u003e is off)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTOPUP\u003c/strong\u003e (these files are only created if \u003ccode\u003etopup\u003c/code\u003e was run)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eacqparams.txt (same as OUTPUTS/EDDY/acqparams.txt)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_b0_first.nii.gz\u003c/em\u003e (only if Synb0-DisCo is run)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eb0_syn.nii.gz (only if Synb0-DisCo is run)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_b0_all.nii.gz\u003c/em\u003e or \u003cem\u003epreproc_input_b0_all_smooth_with_b0_syn.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_b0_all_topped_up.nii.gz\u003c/em\u003e or \u003cem\u003epreproc_input_b0_all_smooth_with_b0_syn_topped_up.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003epreproc_input_b0_all.topup_log or preproc_input_b0_all_smooth_with_b0_syn.topup_log\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etopup_field.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etopup_results_fieldcoef.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003etopup_results_movpar.txt\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eEDDY\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eacqparams.txt\u003c/strong\u003e (same as OUTPUTS/TOPUP/acqparams.txt)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eindex.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_eddyed.nii.gz\u003c/em\u003e (renamed from \"eddy_results.nii.gz\")\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_eddyed.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003epreproc_input_eddyed.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eeddy_mask.nii.gz (only included if \u003ccode\u003e--eddy_mask\u003c/code\u003e is on)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_command_txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_movement_rms\u003c/strong\u003e (describes volume-wise RMS displacement)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_free_data.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_map\u003c/strong\u003e (describes which slices were deemed outliers)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_n_sqr_stdev_map\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_n_stdev_map\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_outlier_report\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_parameters\u003c/strong\u003e (describes volume-wise rotation and translation)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_post_eddy_shell_alignment_parameters\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_post_eddy_shell_PE_translation_parameters\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_restricted_movement_rms\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_rotated_bvecs (describes properly rotated b-vectors)\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_values_of_all_input_parameters\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_results.eddy_cnr_maps.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003ePOSTNORMALIZED\u003c/em\u003e (these files are only created if \u003ccode\u003e--postnormalize\u003c/code\u003e is on)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;image1_%\u0026gt;_topup_eddy_norm.nii.gz\u003c/em\u003e (\"_topup\" only applies if topup was run)\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003e\u0026lt;imageN_%\u0026gt;_topup_eddy_norm.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003eUNBIASED\u003c/em\u003e (these files are only created if \u003ccode\u003e--correct_bias\u003c/code\u003e is on; this folder is removed if \u003ccode\u003e--correct_bias\u003c/code\u003e is off)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003enormed_unbiased.nii.gz\u003c/em\u003e (if postnormalization is run) or \u003cem\u003epreproc_input_eddyed_unbiased.nii.gz\u003c/em\u003e (if postnormalization is not run)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ebias_field.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePREPROCESSED\u003c/strong\u003e (these represent the final output of the pipeline)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri.nii.gz\u003c/em\u003e (dwmri* files deleted only if \u003ccode\u003e--split_outputs\u003c/code\u003e is also set)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri.bval\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri.bvec\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;_preproc.nii.gz (*_preproc files exist only if \u003ccode\u003e--split_outputs\u003c/code\u003e is set)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;_preproc.bval\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;image1\u0026gt;_preproc.bvec\u003c/p\u003e\n\u003cp\u003e:\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;_preproc.nii.gz\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;_preproc.bval\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u0026lt;imageN\u0026gt;_preproc.bvec\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003emask.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eimprobable_mask.nii.gz (only included if \u003ccode\u003e--improbable_mask\u003c/code\u003e is on)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eTENSOR\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cem\u003edwmri_recon.nii.gz\u003c/em\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSCALARS\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_fa.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_md.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_ad.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_rd.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri_tensor_v1.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSTATS\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eatlas2subj.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eb02template_0GenericAffine.mat\u003c/strong\u003e or \u003cstrong\u003efa2template_0GenericAffine.mat\u003c/strong\u003e depending on \u003ccode\u003e--atlas_reg_type\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eb02template_1Warp.nii.gz\u003c/strong\u003e or \u003cstrong\u003efa2template_1Warp.nii.gz\u003c/strong\u003e depending on \u003ccode\u003e--atlas_reg_type\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eb02template_1InverseWarp.nii.gz\u003c/strong\u003e or \u003cstrong\u003efa2template_1InverseWarp.nii.gz\u003c/strong\u003e depending on \u003ccode\u003e--atlas_reg_type\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003echisq_mask.nii.gz\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003echisq_matrix.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_abs_displacement.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_median_cnr.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_rel_displacement.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_rotations.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eeddy_avg_translations.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eroi_avg_fa.txt\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003estats.csv\u003c/strong\u003e (contains summary of all motion, SNR/CNR, and average FA stats)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOPTIMIZED_BVECS\u003c/strong\u003e (these are sign/axis permuted per \u003ccode\u003edwigradcheck\u003c/code\u003e and are only used for QA purposes)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri.bval\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003edwmri.bvec\u003c/strong\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003ePDF\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003edtiQA.pdf\u003c/strong\u003e (final QA document)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-note-on-versioning-for-vuiis-xnat-users\" class=\"anchor\" href=\"#note-on-versioning-for-vuiis-xnat-users\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNote on Versioning for VUIIS XNAT Users\u003c/h2\u003e\n\u003cp\u003ePreQual was developed at Vanderbilt under the project name \"dtiQA v7 Multi\". PreQual v1.0.0 represents dtiQA v7.2.0. Thus, on XNAT, dtiQA v7.2.x refers to PreQual v1.0.x.\u003c/p\u003e\n",
    "stargazers_count": 12,
    "subscribers_count": 1,
    "topics": [
      "diffusion",
      "mri",
      "preprocessing",
      "quality",
      "assurance"
    ],
    "updated_at": 1627366282.0
  },
  {
    "data_format": 2,
    "description": "\ud83c\udf08",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "funkelab/lsd",
    "latest_release": null,
    "readme": "\u003ch2\u003e\n\u003ca id=\"user-content-local-shape-descriptors-for-neuron-segmentation\" class=\"anchor\" href=\"#local-shape-descriptors-for-neuron-segmentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal Shape Descriptors (for Neuron Segmentation)\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/gifs/lsd_particles.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/gifs/lsd_particles.gif\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository contains code to compute Local Shape Descriptors (LSDs) from an instance segmentation. LSDs can then be used during training as an auxiliary target, which we found to improve boundary prediction and therefore segmentation quality. Read more about it in our \u003ca href=\"https://www.biorxiv.org/content/10.1101/2021.01.18.427039v1\" rel=\"nofollow\"\u003epaper\u003c/a\u003e and/or \u003ca href=\"https://localshapedescriptors.github.io/\" rel=\"nofollow\"\u003eblog post\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"#example\"\u003eQuick 2d Examples\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#nbook\"\u003eNotebooks\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#networks\"\u003eExample networks \u0026amp; pipelines\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"#parallel\"\u003eParallel processing\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eCite:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-bibtex\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003e@article\u003c/span\u003e{\u003cspan class=\"pl-en\"\u003esheridan_local_2021\u003c/span\u003e,\n\t\u003cspan class=\"pl-s\"\u003etitle\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eLocal Shape Descriptors for Neuron Segmentation\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n\t\u003cspan class=\"pl-s\"\u003eurl\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003ehttps://www.biorxiv.org/content/10.1101/2021.01.18.427039v1\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n\t\u003cspan class=\"pl-s\"\u003eurldate\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e2021-01-20\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n\t\u003cspan class=\"pl-s\"\u003ejournal\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003ebioRxiv\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n\t\u003cspan class=\"pl-s\"\u003eauthor\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003eSheridan, Arlo and Nguyen, Tri and Deb, Diptodip and Lee, Wei-Chung Allen and Saalfeld, Stephan and Turaga, Srinivas and Manor, Uri and Funke, Jan\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e,\n\t\u003cspan class=\"pl-s\"\u003eyear\u003c/span\u003e = \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e{\u003c/span\u003e2021\u003cspan class=\"pl-pds\"\u003e}\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003eNotes:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eThis is not production level software and was developed in a pure research environment. Therefore some scripts may not work out of the box. For example, all paper networks were originally written using now deprecated tensorflow/cudnn versions and rely on an outdated singularity container. Because of this, the singularity image will not build from the current recipe - if replicating with the current implementations, please reach out for the singularity container (it is too large to upload here). Alternatively, consider reimplementing networks in pytorch (there are examples below).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePost-proccesing steps were designed for use with a specific cluster and will need to be tweaked for individual use cases. If the need / use increases then we will look into refactoring, packaging and distributing.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCurrently, post-processing scripts (e.g \u003ca href=\"https://github.com/funkelab/lsd/blob/master/lsd/fragments.py\"\u003ewatershed\u003c/a\u003e) are located inside this repo which creates more dependencies than needed for using the lsds. One forseeable issue is that agglomeration requires networkx==2.2 for the MergeTree. These scripts will be migrated to another repository in the future...\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTested on Ubuntu 18.04 with Python 3.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca name=\"user-content-example\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-2d-examples\" class=\"anchor\" href=\"#quick-2d-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick 2d Examples\u003c/h2\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eRequired packages/repos\u003c/summary\u003e\n\u003cul\u003e\n\u003cli\u003erun in conda env or colab notebook with appropriate packages/repos installed\u003c/li\u003e\n\u003cli\u003esince some required post-processing scripts are located in this repo, there are various packages required along with the lsds.\u003c/li\u003e\n\u003cli\u003eif confused, see notebook tutorials for further details\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003epackages (i.e pip install {package})\n\ndaisy\ngunpowder\nmahotas\nmatplotlib\nscikit-image\n\nrepos (i.e pip install git+git://github.com/{repo})\n\nfunkelab/funlib.segment.git\nfunkelab/lsd.git\nfunkey/waterz.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eCoins example\u003c/summary\u003e\n\u003cul\u003e\n\u003cli\u003elogic to create labels borrowed from this \u003ca href=\"https://scikit-image.org/docs/dev/auto_examples/segmentation/plot_expand_labels.html#sphx-glr-auto-examples-segmentation-plot-expand-labels-py\" rel=\"nofollow\"\u003etutorial\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ematplotlib\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003epyplot\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eplt\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enumpy\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eskimage\u003c/span\u003e\n\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elsd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elocal_shape_descriptor\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eskimage\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003efilters\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esobel\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eskimage\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emeasure\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elabel\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eskimage\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003esegmentation\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewatershed\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003e%\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ematplotlib\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003einline\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# get coins dataset\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eskimage\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003ecoins\u003c/span\u003e()\n\n\u003cspan class=\"pl-c\"\u003e# create edges\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eedges\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003esobel\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# generate markers for watershed\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003emarkers\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003ezeros_like\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003eforeground\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ebackground\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003emarkers\u003c/span\u003e[\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e\u0026lt;\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e30.0\u003c/span\u003e] \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ebackground\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003emarkers\u003c/span\u003e[\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e150.0\u003c/span\u003e] \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eforeground\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# get unique labels\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003ews\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003ewatershed\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eedges\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003emarkers\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003elabels\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-en\"\u003elabel\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ews\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e==\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eforeground\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eastype\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003euint64\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# calculate lsds\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003elsds\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elocal_shape_descriptor\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_local_shape_descriptors\u003c/span\u003e(\n              \u003cspan class=\"pl-s1\"\u003esegmentation\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003elabels\u003c/span\u003e,\n              \u003cspan class=\"pl-s1\"\u003esigma\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e15\u003c/span\u003e,)\u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e,\n              \u003cspan class=\"pl-s1\"\u003evoxel_size\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,)\u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# view lsds\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003efig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003eaxes\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eplt\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003esubplots\u003c/span\u003e(\n            \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n            \u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e,\n            \u003cspan class=\"pl-s1\"\u003efigsize\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e25\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e10\u003c/span\u003e),\n            \u003cspan class=\"pl-s1\"\u003esharex\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e,\n            \u003cspan class=\"pl-s1\"\u003esharey\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e,\n            \u003cspan class=\"pl-s1\"\u003esqueeze\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\n\n\u003cspan class=\"pl-k\"\u003edef\u003c/span\u003e \u003cspan class=\"pl-en\"\u003eview_channel\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eax\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e):\n  \u003cspan class=\"pl-s1\"\u003eax\u003c/span\u003e[\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e][\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e].\u003cspan class=\"pl-en\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003esqueeze\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e[\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e:\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e+\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,:,:]), \u003cspan class=\"pl-s1\"\u003ecmap\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027jet\u0027\u003c/span\u003e)\n\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erange\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e):\n  \u003cspan class=\"pl-en\"\u003eview_channel\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eaxes\u003c/span\u003e,\u003cspan class=\"pl-s1\"\u003elsds\u003c/span\u003e,\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e)\n  \n  \u003cspan class=\"pl-c\"\u003e#from left to right: mean offset y, mean offset x, orientation y, orientation x, change (y-x), voxel count\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/coins.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/coins.png\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \u003c/details\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eNeurons example\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eh5py\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eio\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ematplotlib\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003epyplot\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eplt\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enumpy\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003erequests\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elsd\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elocal_shape_descriptor\u003c/span\u003e\n \n\u003cspan class=\"pl-c1\"\u003e%\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ematplotlib\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003einline\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# example data\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eurl\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u0027https://cremi.org/static/data/sample_A_20160501.hdf\u0027\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# convert from binary\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eh5py\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eFile\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eio\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eBytesIO\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003erequests\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eurl\u003c/span\u003e).\u003cspan class=\"pl-s1\"\u003econtent\u003c/span\u003e), \u003cspan class=\"pl-s\"\u003e\u0027r\u0027\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# get corner patch\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003elabels\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003esqueeze\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e[\u003cspan class=\"pl-s\"\u003e\u0027volumes/labels/neuron_ids\u0027\u003c/span\u003e][\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e:\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e:\u003cspan class=\"pl-c1\"\u003e250\u003c/span\u003e,\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e:\u003cspan class=\"pl-c1\"\u003e250\u003c/span\u003e])\n\n\u003cspan class=\"pl-c\"\u003e# calc lsds\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003elsds\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elocal_shape_descriptor\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_local_shape_descriptors\u003c/span\u003e(\n              \u003cspan class=\"pl-s1\"\u003esegmentation\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003elabels\u003c/span\u003e,\n              \u003cspan class=\"pl-s1\"\u003esigma\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e100\u003c/span\u003e,)\u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e,\n              \u003cspan class=\"pl-s1\"\u003evoxel_size\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e[\u003cspan class=\"pl-c1\"\u003e4\u003c/span\u003e,\u003cspan class=\"pl-c1\"\u003e4\u003c/span\u003e])\n\n\u003cspan class=\"pl-c\"\u003e# view\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003efig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003eaxes\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eplt\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003esubplots\u003c/span\u003e(\n            \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n            \u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e,\n            \u003cspan class=\"pl-s1\"\u003efigsize\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e15\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e10\u003c/span\u003e),\n            \u003cspan class=\"pl-s1\"\u003esharex\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e,\n            \u003cspan class=\"pl-s1\"\u003esharey\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eTrue\u003c/span\u003e,\n            \u003cspan class=\"pl-s1\"\u003esqueeze\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003eFalse\u003c/span\u003e)\n\n\u003cspan class=\"pl-k\"\u003edef\u003c/span\u003e \u003cspan class=\"pl-en\"\u003eview_channel\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eax\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e):\n\n  \u003cspan class=\"pl-s1\"\u003eax\u003c/span\u003e[\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e][\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e].\u003cspan class=\"pl-en\"\u003eimshow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enp\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003esqueeze\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003edata\u003c/span\u003e[\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e:\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e+\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,:,:]), \u003cspan class=\"pl-s1\"\u003ecmap\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027jet\u0027\u003c/span\u003e)\n\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-en\"\u003erange\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e):\n  \u003cspan class=\"pl-en\"\u003eview_channel\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eaxes\u003c/span\u003e,\u003cspan class=\"pl-s1\"\u003elsds\u003c/span\u003e,\u003cspan class=\"pl-s1\"\u003echannel\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e)\n  \n\u003cspan class=\"pl-c\"\u003e#from left to right: mean offset y, mean offset x, orientation y, orientation x, change (y-x), voxel count\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/2d_lsds.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/2d_lsds.png\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n \u003c/details\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca name=\"user-content-nbook\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notebooks\" class=\"anchor\" href=\"#notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotebooks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eExamble colab notebooks are located \u003ca href=\"https://github.com/funkelab/lsd/tree/tutorial/lsd/tutorial/notebooks\"\u003ehere\u003c/a\u003e. You can download or run below (control + click open in colab). When running a notebook, you will probably get the message: \"Warning: This notebook was not authored by Google\". This can be ignored, you can run anyway.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe uploaded ~1.7 tb of data (raw/labels/masks/rags etc.) to an s3 bucket. The following tutorial shows some examples for accessing and visualizing the data.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eData download: \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/master/lsd/tutorial/notebooks/lsd_data_download.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf implementing the LSDs in your own training pipeline (i.e pure pytorch/tensorflow), calculate the LSDs on a label array of unique objects and use them as the target for your network (see quick 2d examples above for calculating).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe following tutorials show how to set up 2D training/prediction pipelines using \u003ca href=\"http://funkey.science/gunpowder/\" rel=\"nofollow\"\u003eGunpowder\u003c/a\u003e. It is recommended to follow them in order (skip the basic tutorial if familiar with gunpowder).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBasic Gunpowder tutorial: \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/basic_gp_tutorial.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain Affinities: \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/train_affinities.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain LSDs: \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/train_lsds.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrain MTLSD: \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/train_mtlsd.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInference (using pretrained MTLSD checkpoint): \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/inference.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWatershed, agglomeration, segmentation: \u003ca href=\"https://colab.research.google.com/github/funkelab/lsd/blob/tutorial/lsd/tutorial/notebooks/segment.ipynb\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/84f0493939e0c4de4e6dbe113251b4bfb5353e57134ffd9fcab6b8714514d4d1/68747470733a2f2f636f6c61622e72657365617263682e676f6f676c652e636f6d2f6173736574732f636f6c61622d62616467652e737667\" alt=\"Open In Colab\" data-canonical-src=\"https://colab.research.google.com/assets/colab-badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca name=\"user-content-networks\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-example-networks--pipelines\" class=\"anchor\" href=\"#example-networks--pipelines\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample networks \u0026amp; pipelines\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eThere are some example networks and training/prediction pipelines from the fib25 dataset \u003ca href=\"https://github.com/funkelab/lsd/tree/tutorial/lsd/tutorial/example_nets/fib25\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-training\" class=\"anchor\" href=\"#training\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTraining\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eSince networks in this paper were implemented in Tensorflow, there was a two step process for training. First the networks were created using the \u003ccode\u003emknet.py\u003c/code\u003e files. This saved tensor placeholders and meta data in config files that were then used for both training and prediction. The mknet files used the now deprecated mala repository to create the networks. If reimplementing in Tensorflow, consider migrating to \u003ca href=\"https://github.com/funkelab/funlib.learn.tensorflow\"\u003efunlib.learn.tensorflow\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf using Pytorch, the networks can just be created directly inside the train scripts since placeholders aren\u0027t required. For example, the logic from this \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/mknet.py\"\u003emknet script\u003c/a\u003e and this \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/train.py\"\u003etrain script\u003c/a\u003e can be condensed to \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/train_pytorch.py\"\u003ethis\u003c/a\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor training an autocontext network (e.g \u003ccode\u003eacrlsd\u003c/code\u003e), the current implementation learns the LSDs in a \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/lsd/train.py\"\u003efirst pass\u003c/a\u003e. A saved checkpoint is then used when creating the \u003ca href=\"https://github.com/funkelab/lsd/blob/4397779ea4702eb3d593898d6240819e761fd41a/lsd/tutorial/example_nets/fib25/acrlsd/mknet.py#L122\"\u003esecond pass\u003c/a\u003e in order to \u003ca href=\"https://github.com/funkelab/lsd/blob/4397779ea4702eb3d593898d6240819e761fd41a/lsd/tutorial/example_nets/fib25/acrlsd/train.py#L158\"\u003epredict LSDs\u003c/a\u003e prior to learning the Affinities. One could modify this to use a single setup and remove the need for writing the LSDs to disk.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-inference\" class=\"anchor\" href=\"#inference\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInference\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBy default, the predict scripts (\u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/mtlsd/predict.py\"\u003eexample\u003c/a\u003e) contain the worker logic to be distributed by the scheduler during parallel processing (see below).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you just need to process a relatively small volume, it is sometimes not necessary to use blockwise processing. In this case, it is recommended to use a \u003ca href=\"http://funkey.science/gunpowder/api.html#scan\" rel=\"nofollow\"\u003escan node\u003c/a\u003e, and specify input/output shapes + context. An example can be found in the inference colab notebook above.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSimilar to training, the current autocontext implementations assume the predicted LSDs are written to a zarr/n5 container and then used as input to the second pass to predict affinities. This can also be changed to predict on the fly if needed.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eVisualizations of example training/prediction pipelines\u003c/summary\u003e\n\u003cbr\u003e\u003cbr\u003e\n\u003cdetails\u003e\n  \u003csummary\u003eColor key\u003c/summary\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/a9a97d0a679c3df8d1f607b3d2ae316d26bd7948b9675cdb5252a412ff681153/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6161663265332f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a9a97d0a679c3df8d1f607b3d2ae316d26bd7948b9675cdb5252a412ff681153/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6161663265332f3030303030303f746578743d2b\" alt=\"#aaf2e3\" data-canonical-src=\"https://via.placeholder.com/15/aaf2e3/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#source-nodes\" rel=\"nofollow\"\u003eSource nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/9b7f598e884ee96fac5fea23d7f82d05ae741e128c6e145d32107d6c4fd48074/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666623865372f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9b7f598e884ee96fac5fea23d7f82d05ae741e128c6e145d32107d6c4fd48074/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666623865372f3030303030303f746578743d2b\" alt=\"#ffb8e7\" data-canonical-src=\"https://via.placeholder.com/15/ffb8e7/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#image-processing-nodes\" rel=\"nofollow\"\u003eImage processing nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/d3834c08d34f2ed79d1f788c1d35ce9dcbef13a8c1b813bf3b5df7f2373c9261/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666646561642f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d3834c08d34f2ed79d1f788c1d35ce9dcbef13a8c1b813bf3b5df7f2373c9261/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666646561642f3030303030303f746578743d2b\" alt=\"#ffdead\" data-canonical-src=\"https://via.placeholder.com/15/ffdead/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#location-manipulation-nodes\" rel=\"nofollow\"\u003eLocation manipulation nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/77fc07fd556a8867418042a7732acddec36d245ac5fa8e6fa3982225d7a71488/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6235623362332f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/77fc07fd556a8867418042a7732acddec36d245ac5fa8e6fa3982225d7a71488/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6235623362332f3030303030303f746578743d2b\" alt=\"#b5b3b3\" data-canonical-src=\"https://via.placeholder.com/15/b5b3b3/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#provider-combination-nodes\" rel=\"nofollow\"\u003eProvider combination nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/3f6d952b0f7fce1c2d4f92090f5309333ad278b248ddfd36e34828d88c06e829/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6262662f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3f6d952b0f7fce1c2d4f92090f5309333ad278b248ddfd36e34828d88c06e829/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6262662f3030303030303f746578743d2b\" alt=\"#bbf\" data-canonical-src=\"https://via.placeholder.com/15/bbf/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#augmentation-nodes\" rel=\"nofollow\"\u003eAugmentation nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/f147e3fe453aeaf74231447a042a7a78bd8b94b08cfb80416885830c159eabaa/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666666339312f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f147e3fe453aeaf74231447a042a7a78bd8b94b08cfb80416885830c159eabaa/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666666339312f3030303030303f746578743d2b\" alt=\"#fffc91\" data-canonical-src=\"https://via.placeholder.com/15/fffc91/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#label-manipulation-nodes\" rel=\"nofollow\"\u003eLabel manipulation nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/baf49106c9e6e1fc1f75052028413c99f3a2edf219275a9cdcf116683ca6ced2/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6233653766662f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/baf49106c9e6e1fc1f75052028413c99f3a2edf219275a9cdcf116683ca6ced2/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6233653766662f3030303030303f746578743d2b\" alt=\"#b3e7ff\" data-canonical-src=\"https://via.placeholder.com/15/b3e7ff/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#performance-nodes\" rel=\"nofollow\"\u003ePerformance nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/0af57b13aab71a8b0b0ed524d4b9addd087127e3c1bb76ac794951fad093f83b/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666393136392f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0af57b13aab71a8b0b0ed524d4b9addd087127e3c1bb76ac794951fad093f83b/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6666393136392f3030303030303f746578743d2b\" alt=\"#ff9169\" data-canonical-src=\"https://via.placeholder.com/15/ff9169/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#training-and-prediction-nodes\" rel=\"nofollow\"\u003eTraining and prediction nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/82f697552844f276b2ef97711385a6dc558248eed718c239ce971f11d2aadcbc/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f3732626636392f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/82f697552844f276b2ef97711385a6dc558248eed718c239ce971f11d2aadcbc/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f3732626636392f3030303030303f746578743d2b\" alt=\"#72bf69\" data-canonical-src=\"https://via.placeholder.com/15/72bf69/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#module-gunpowder\" rel=\"nofollow\"\u003eOutput nodes\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/6ee7cadb933bf945d53b440b486621eef6ad7a4ed604f0846c4767ac9efe475d/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6132393166662f3030303030303f746578743d2b\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6ee7cadb933bf945d53b440b486621eef6ad7a4ed604f0846c4767ac9efe475d/68747470733a2f2f7669612e706c616365686f6c6465722e636f6d2f31352f6132393166662f3030303030303f746578743d2b\" alt=\"#a291ff\" data-canonical-src=\"https://via.placeholder.com/15/a291ff/000000?text=+\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"http://funkey.science/gunpowder/api.html#iterative-processing-nodes\" rel=\"nofollow\"\u003eIterative processing nodes\u003c/a\u003e\u003c/p\u003e\n \u003c/details\u003e\n\u003cp\u003eVanilla affinities \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/train.py\"\u003etraining\u003c/a\u003e:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/train_nodes.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/train_nodes.svg\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003cbr\u003e\u003cbr\u003e\nAutocontext \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/lsd/predict.py\"\u003eLSD\u003c/a\u003e and \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/acrlsd/predict.py\"\u003eaffinities\u003c/a\u003e prediction:\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/predict_nodes.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/predict_nodes.svg\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003c/details\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca name=\"user-content-parallel\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parallel-processing\" class=\"anchor\" href=\"#parallel-processing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParallel processing\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIf you are running on small data then this section may be irrelevant. See the \u003ccode\u003eWatershed, agglomeration, segmentation\u003c/code\u003e notebook above if you just want to get a sense of obtaining a segmentation from affinities.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExample processing scripts can be found \u003ca href=\"https://github.com/funkelab/lsd/tree/tutorial/lsd/tutorial/scripts\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWe create segmentations following the approach in \u003ca href=\"https://ieeexplore.ieee.org/document/8364622\" rel=\"nofollow\"\u003ethis paper\u003c/a\u003e. Generally speaking, after training a network there are five steps to obtain a segmentation:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003col\u003e\n\u003cli\u003ePredict boundaries (this can involve the use of LSDs as an auxiliary task)\u003c/li\u003e\n\u003cli\u003eGenerate supervoxels (fragments) using seeded watershed. The fragment centers of mass are stored as region adjacency graph nodes.\u003c/li\u003e\n\u003cli\u003eGenerate edges between nodes using hierarchical agglomeration. The edges are weighted by the underlying affinities. Edges with lower scores are merged earlier.\u003c/li\u003e\n\u003cli\u003eCut the graph at a predefined threshold and relabel connected components. Store the node - component lookup tables.\u003c/li\u003e\n\u003cli\u003eUse the lookup tables to relabel supervoxels and generate a segmentation.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/blob/master/assets/img/pipeline.jpeg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/LocalShapeDescriptors/LocalShapeDescriptors.github.io/raw/master/assets/img/pipeline.jpeg\" alt=\"\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eEverything was done in parallel using \u003ca href=\"https://github.com/funkelab/daisy\"\u003edaisy\u003c/a\u003e, but one could use multiprocessing or dask instead.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFor our experiments we used \u003ca href=\"https://www.mongodb.com/\" rel=\"nofollow\"\u003eMongoDB\u003c/a\u003e for all storage (block checks, rags, scores, etc) due to the size of the data. Depending on use case, it might be better to read/write to file rather than mongo. See watershed for further info.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe following examples were written for use with the Janelia LSF cluster and are just meant to be used as a guide. Users will likely need to customize for their own specs (for example if using a SLURM cluster).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNeed to install \u003ca href=\"https://github.com/funkelab/funlib.segment\"\u003efunlib.segment\u003c/a\u003e and \u003ca href=\"https://github.com/funkelab/funlib.evaluate\"\u003efunlib.evaluate\u003c/a\u003e if using/adapting segmentation/evaluation scripts.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-inference-1\" class=\"anchor\" href=\"#inference-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInference\u003c/h3\u003e\n\u003cp\u003eThe worker logic is located in individual \u003ccode\u003epredict.py\u003c/code\u003e scripts (\u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/example_nets/fib25/vanilla/predict.py\"\u003eexample\u003c/a\u003e). The \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/01_predict_blockwise.py\"\u003emaster script\u003c/a\u003e distributes using \u003ccode\u003edaisy.run_blockwise\u003c/code\u003e. The only need for MongoDb here is for the block check function (to check which blocks have successfully completed). To remove the need for mongo, one could remove the check function (remember to also remove \u003ccode\u003eblock_done_callback\u003c/code\u003e in \u003ccode\u003epredict.py\u003c/code\u003e) or replace with custom function (e.g check chunk completion directly in output container).\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample roi config\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003econtainer\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehemi_roi_1.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eoffset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e140800\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e205120\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e198400\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esize\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e3000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e3000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e3000\u003c/span\u003e]\n}\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample predict config\u003c/summary\u003e\n\u003cpre\u003e\u003ccode\u003e {\n  \"base_dir\": \"/path/to/base/directory\",\n  \"experiment\": \"hemi\",\n  \"setup\": \"setup01\",\n  \"iteration\": 400000,\n  \"raw_file\": \"predict_roi.json\",\n  \"raw_dataset\" : \"volumes/raw\",\n  \"out_base\" : \"output\",\n  \"file_name\": \"foo.zarr\",\n  \"num_workers\": 5,\n  \"db_host\": \"mongodb client\",\n  \"db_name\": \"foo\",\n  \"queue\": \"gpu_rtx\",\n  \"singularity_image\": \"/path/to/singularity/image\"\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/details\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-watershed\" class=\"anchor\" href=\"#watershed\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWatershed\u003c/h3\u003e\n\u003cp\u003eThe worker logic is located in a single \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/workers/extract_fragments_worker.py\"\u003escript\u003c/a\u003e which is then distributed by the \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/02_extract_fragments_blockwise.py\"\u003emaster script\u003c/a\u003e. By default the nodes are stored in mongo using a \u003ca href=\"https://github.com/funkelab/daisy/blob/master/daisy/persistence/mongodb_graph_provider.py\"\u003eMongoDbGraphProvider\u003c/a\u003e. To write to file (i.e compressed numpy arrays), you can use the \u003ca href=\"https://github.com/funkelab/daisy/blob/master/daisy/persistence/file_graph_provider.py\"\u003eFileGraphProvider\u003c/a\u003e instead (inside the worker script).\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample watershed config\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eexperiment\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehemi\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup01\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eiteration\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e400000\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaffs_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaffs_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/affs\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/fragments\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eblock_size\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003econtext\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e248\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e248\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e248\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_host\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emongodb client\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enum_workers\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e6\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_in_xy\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003efalse\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eepsilon_agglomerate\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003equeue\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003elocal\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-agglomerate\" class=\"anchor\" href=\"#agglomerate\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAgglomerate\u003c/h3\u003e\n\u003cp\u003eSame as watershed. \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/workers/agglomerate_worker.py\"\u003eWorker script\u003c/a\u003e, \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/03_agglomerate_blockwise.py\"\u003emaster script\u003c/a\u003e. Change to FileGraphProvider if needed.\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample agglomerate config\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eexperiment\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehemi\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup01\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eiteration\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e400000\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaffs_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eaffs_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/affs\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/fragments\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eblock_size\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003econtext\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e248\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e248\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e248\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_host\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emongodb client\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enum_workers\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003equeue\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003elocal\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emerge_function\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehist_quant_75\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-find-segments\" class=\"anchor\" href=\"#find-segments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFind segments\u003c/h3\u003e\n\u003cp\u003eIn contrast to the above three methods, when \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/04_find_segments.py\"\u003ecreating LUTs\u003c/a\u003e there just needs to be enough RAM to hold the RAG in memory. The only thing done in parallel is reading the graph (\u003ccode\u003egraph_provider.read_blockwise()\u003c/code\u003e). It could be adapted to use multiprocessing/dask for distributing the connected components for each threshold, but if the rag is too large there will be pickling errors when passing the nodes/edges. Daisy doesn\u0027t need to be used for scheduling here since nothing is written to containers.\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample find segments config\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_host\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emongodb client\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_collection\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_hist_quant_75\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethresholds_minmax\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethresholds_step\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.02\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eblock_size\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enum_workers\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e5\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/fragments\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003erun_type\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etest\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-extract-segmentation\" class=\"anchor\" href=\"#extract-segmentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExtract segmentation\u003c/h3\u003e\n\u003cp\u003eThis \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/05_extract_segmentation_from_lut.py\"\u003escript\u003c/a\u003e does use daisy to write the segmentation to file, but doesn\u0027t necessarily require bsub/sbatch to distribute (you can run locally).\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample extract segmentation config\u003c/summary\u003e \n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/fragments\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_collection\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_hist_quant_75\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethreshold\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.4\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eblock_size\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1000\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eout_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eout_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003evolumes/segmentation_40\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enum_workers\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003erun_type\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etest\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-evaluate-volumes\" class=\"anchor\" href=\"#evaluate-volumes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEvaluate volumes\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/05_evaluate_volumes.py\"\u003eEvaluate\u003c/a\u003e Voi scores. Assumes dense voxel ground truth (not skeletons). This also assumes the ground truth (and segmentation) can fit into memory, which was fine for hemi and fib25 volumes assuming ~750 GB of RAM. The script should probably be refactored to run blockwise.\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample evaluate volumes config\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eexperiment\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehemi\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup01\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eiteration\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e400000\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003egt_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehemi_roi_1.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003egt_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003evolumes/labels/neuron_ids\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/fragments\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003edb_host\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emongodb client\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003erag_db_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_collection\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_hist_quant_75\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003escores_db_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003escores\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethresholds_minmax\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e0\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethresholds_step\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.02\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enum_workers\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e4\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emethod\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003evanilla\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003erun_type\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etest\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n\u003c/details\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-evaluate-annotations\" class=\"anchor\" href=\"#evaluate-annotations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEvaluate annotations\u003c/h3\u003e\n\u003cp\u003eFor the zebrafinch, ground truth skeletons were used due to the size of the dataset. These skeletons were cropped, masked, and relabelled for the sub Rois that were tested in the paper. We \u003ca href=\"https://github.com/funkelab/lsd/blob/tutorial/lsd/tutorial/scripts/05_evaluate_annotations.py\"\u003eevaluated\u003c/a\u003e voi, erl, and the mincut metric on the consolidated skeletons. The current implementation could be refactored / made more modular. It also uses \u003ccode\u003enode_collections\u003c/code\u003e which are now deprecated in daisy. To use with the current implementation, you should checkout daisy commit \u003ccode\u003e39723ca\u003c/code\u003e.\u003c/p\u003e\n\u003cdetails\u003e\n \u003csummary\u003eExample evaluate annotations config\u003c/summary\u003e\n\u003cdiv class=\"highlight highlight-source-json\"\u003e\u003cpre\u003e{\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eexperiment\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ezebrafinch\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esetup01\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eiteration\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e400000\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003econfig_slab\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emtlsd\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_file\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo.zarr\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efragments_dataset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/volumes/fragments\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_db_host\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emongodb client\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_db_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_collection\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eedges_hist_quant_75\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003escores_db_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003escores\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eannotations_db_host\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003emongo client\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eannotations_db_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003efoo\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eannotations_skeletons_collection_name\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ezebrafinch\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enode_components\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ezebrafinch_components\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003enode_mask\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ezebrafinch_mask\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eroi_offset\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e50800\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e43200\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e44100\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eroi_shape\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e10800\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e10800\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e10800\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethresholds_minmax\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: [\u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e],\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ethresholds_step\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e,\n  \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003erun_type\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e11_micron_roi_masked\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n}\u003c/pre\u003e\u003c/div\u003e\n \u003c/details\u003e\n",
    "stargazers_count": 12,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1626912710.0
  },
  {
    "data_format": 2,
    "description": "PEMA: a flexible Pipeline for Environmental DNA Metabarcoding Analysis of the 16S/18S rRNA, ITS and COI marker genes",
    "filenames": [
      "Singularity",
      "singularity/Singularity.v.2.0.3",
      "singularity/Singularity.v.1.3.2",
      "singularity/Singularity.latest",
      "singularity/Singularity.v.1.3",
      "singularity/Singularity.v.1.3.1",
      "singularity/Singularity.v.1.1",
      "singularity/Singularity.v.2.1.0",
      "singularity/Singularity.v.2.0.2",
      "singularity/Singularity.v.2.1.3"
    ],
    "full_name": "hariszaf/pema",
    "latest_release": "v.2.1.3",
    "readme": "\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/762c1129f266494bbbb3faff3d673040cf7b1f19d45c6e13f49b08de12f5116a/68747470733a2f2f692e70617374652e706963732f38373031383966616466363638613935386338616163383366333865373939632e706e67\" width=\"300\" align=\"left\" data-canonical-src=\"https://i.paste.pics/870189fadf668a958c8aac83f38e799c.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-pema\" class=\"anchor\" href=\"#pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA:\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\" class=\"anchor\" href=\"#a-flexible-pipeline-for-environmental-dna-metabarcoding-analysis-of-the-16s18s-rrna-its-and-coi-marker-genes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ea flexible Pipeline for Environmental DNA Metabarcoding Analysis of the 16S/18S rRNA, ITS and COI marker genes\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003ePEMA is reposited in\u003c/em\u003e \u003ca href=\"https://hub.docker.com/r/hariszaf/pema\" rel=\"nofollow\"\u003e\u003cem\u003eDocker Hub\u003c/em\u003e\u003c/a\u003e \u003cem\u003eas well as in\u003c/em\u003e \u003ca href=\"https://singularity-hub.org/collections/2295\" rel=\"nofollow\"\u003e\u003cem\u003eSingularity Hub\u003c/em\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-a-pema-tutorial-can-be-found-here\" class=\"anchor\" href=\"#a-pema-tutorial-can-be-found-here\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA PEMA tutorial can be found \u003ca href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\" rel=\"nofollow\"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e.\u003c/h4\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\" class=\"anchor\" href=\"#for-any-troubles-you-may-have-when-running-pema-or-for-any-potential-improvevments-you-would-like-to-suggest-please-share-on-the-pema-gitter-community\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFor any troubles you may have when running PEMA or for any potential improvevments you would like to suggest, please share on the \u003ca href=\"https://gitter.im/pema-helpdesk/community\" rel=\"nofollow\"\u003ePEMA Gitter community\u003c/a\u003e.\u003c/h4\u003e\n\n\u003cp\u003e\u003ca href=\"https://gitter.im/pema-helpdesk/community?utm_source=badge\u0026amp;utm_medium=badge\u0026amp;utm_campaign=pr-badge\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7385c04b449351f12fb57a4bd6f9791ebd68a483493399e50a8f096fadde4246/68747470733a2f2f6261646765732e6769747465722e696d2f70656d612d68656c706465736b2f636f6d6d756e6974792e737667\" alt=\"Gitter\" data-canonical-src=\"https://badges.gitter.im/pema-helpdesk/community.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.gnu.org/licenses/gpl-3.0\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/400c4e52df43f6a0ab8a89b74b1a78d1a64da56a7848b9110c9d2991bb7c3105/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c76332d626c75652e737667\" alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPLv3-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#pema-biodiversity-in-all-its-different-levels\"\u003ePEMA: biodiversity in all its different levels\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#a-container-based-tool\"\u003e A container-based tool\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#how-to-run-pema\"\u003eHow to run PEMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#parameters-file\"\u003eParameters\u0027 file\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#pema-on-hpc\"\u003ePEMA on HPC\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#prerequisites-1\"\u003ePrerequisites\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installing-1\"\u003eInstalling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-pema-1\"\u003eRunning PEMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#example\"\u003eExample\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#pema-on-a-simple-pc\"\u003ePEMA on a simple PC\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#prerequisites\"\u003ePrerequisites\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installing\"\u003eInstalling\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#running-pema\"\u003eRunning PEMA\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#step-1---build-a-docker-container\"\u003eStep 1 - Build a Docker container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#step-2---run-pema\"\u003eStep 2 - Run PEMA\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#the-phyloseq-r-package\"\u003ephyloseq - for a downstream ecological analysis\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#acknowledgments\"\u003eAcknowledgments\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#license\"\u003eLicense\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-diff\"\u003e\u003cpre\u003e\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e convertion of the Illumina raw data is now implemented in the framework of PEMA\u003c/span\u003e\n\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e PEMA now supports 2 extra marker genes, 18S rRNA and ITS. \u003c/span\u003e\n\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e PEMA is now available for macOS!\u003c/span\u003e\n\u003cspan class=\"pl-mi1\"\u003e\u003cspan class=\"pl-mi1\"\u003e+\u003c/span\u003e for anything feel free to contact me at: haris-zaf@hcmr.gr\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-pema-biodiversity-in-all-its-different-levels\" class=\"anchor\" href=\"#pema-biodiversity-in-all-its-different-levels\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA: biodiversity in all its different levels\u003c/h1\u003e\n\u003cp\u003ePEMA supports the metabarcoding analysis of four marker genes, \u003cstrong\u003e16S rRNA\u003c/strong\u003e (Bacteria), \u003cstrong\u003eITS\u003c/strong\u003e (Fungi) as well as \u003cstrong\u003eCOI\u003c/strong\u003e and \u003cstrong\u003e18S rRNA\u003c/strong\u003e (metazoa). As input, PEMA accepts .fastq.gz files as returned by Illumina sequencing platforms.\u003c/p\u003e\n\u003cp\u003ePEMA processes the reads from each sample and \u003cstrong\u003ereturns an OTU- or an ASV-table with the taxonomies\u003c/strong\u003e of the taxa found and their abundances in each sample. It also returns statistics and a FASTQC diagram about the quality of the reads for each sample. Finally, PEMA supports \u003cstrong\u003edownstream ecological analysis\u003c/strong\u003e of the profiles retrieved, facilitated by the \u003ca href=\"http://joey711.github.io/phyloseq/index.html\" rel=\"nofollow\"\u003ephyloseq\u003c/a\u003e R package.\u003c/p\u003e\n\u003cp\u003ePEMA supports both OTU clustering (thanks to VSEARCH and CROP algorithms) and ASV inference (via SWARM) for all four marker genes.\u003c/p\u003e\n\u003cp\u003eFor the case of the 16S rRNA marker gene, PEMA includes two separate approaches for taxonomy assignment: alignment-based and phylogenetic-based. For the latter, a reference tree of 1000 taxa was created using SILVA_132_SSURef, EPA-ng and RaxML-ng.\u003c/p\u003e\n\u003cp\u003ePEMA has been implemented in \u003ca href=\"https://pcingola.github.io/BigDataScript/\" rel=\"nofollow\"\u003eBigDataScript\u003c/a\u003e programming language. BDS\u2019s ad hoc task parallelism and task synchronization, supports heavyweight computation. Thus, PEMA inherits such features and it also supports roll-back checkpoints and on-demand partial pipeline execution. In addition, PEMA takes advantage of all the computational power available on a specific machine; for example, if PEMA is executed on a personal laptop with 4 cores, it is going to use all four of them.\u003c/p\u003e\n\u003cp\u003eFinally, container-based technologies such as Docker and Singularity, make PEMA easy accessible for all operating systems.\nAs you can see in the \u003ca href=\"https://github.com/hariszaf/pema/blob/master/help_files/GitHub%20tutorial.pdf\"\u003ePEMA_tutorial.pdf\u003c/a\u003e, once you have either Docker or Singularity on your computational environment (see below which suits your case better), running PEMA is cakewalk. You can also find the \u003ca href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?usp=sharing\" rel=\"nofollow\"\u003e\u003cstrong\u003ePEMA tutorial\u003c/strong\u003e\u003c/a\u003e as a Google Slides file.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-a-container-based-tool\" class=\"anchor\" href=\"#a-container-based-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eA container-based tool\u003c/h1\u003e\n\u003cp\u003ePEMA can run either on a HPC environment (server, cluster etc) or on a simple PC. However, we definitely suggest to run it on an HPC environment to exploit the full potential of PEMA. Running on a powerful server or a cluster can be time-saving since it would require significantly less computational time than in a common PC. However, for analyses with a small number of samples, a common PC can suffice.\u003c/p\u003e\n\u003cp\u003eThere is one \u003cstrong\u003emajor difference\u003c/strong\u003e between running PEMA on a common PC than running it on a HPC environment. In the first case, PEMA runs through \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003e\u003cstrong\u003eDocker\u003c/strong\u003e\u003c/a\u003e, while in the latter one, it runs through \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOn the following chapters, you can find how to install PEMA both in Docker and Singlularity including examples.\u003c/p\u003e\n\u003cp\u003eRunning PEMA is exactly \u003cstrong\u003ethe same\u003c/strong\u003e procedure in both of those cases.\u003c/p\u003e\n\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run-pema\" class=\"anchor\" href=\"#how-to-run-pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run PEMA\u003c/h2\u003e\n\u003cp\u003eAssuming you have either Docker or Singularity on your system (see below how to get them).\nYou need to create a directory where you will have everything PEMA needs - we will call it \u003cem\u003e\u003cstrong\u003eanalysis directory\u003c/strong\u003e\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn this directory, you need to add the following \u003cstrong\u003emandatory\u003c/strong\u003e files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\u003e\u003cem\u003e\u003cstrong\u003eparameters.tsv\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e file (you can download it from this repository and then \u003cstrong\u003ecomplete it\u003c/strong\u003e according to the needs of your analysis)\u003c/li\u003e\n\u003cli\u003ea subdirectory called \u003cem\u003e\u003cstrong\u003emydata\u003c/strong\u003e\u003c/em\u003e where your .fastq.gz files will be located \u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf your need to perform phyloseq, in the analysis directory you also need to add the following \u003cstrong\u003eoptionally\u003c/strong\u003e files:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ethe \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\u003e\u003cem\u003e\u003cstrong\u003ephyloseq_in_PEMA.R\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e which you can also download from this repository and set it the way you want (that is an R script which we have implemented and has some main features that need to stay always the same in order to be executed as part of PEMA and some parts where the user can set what exactly needs to get from the phyloseq package)\u003c/li\u003e\n\u003cli\u003ethe \u003ca href=\"https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\" rel=\"nofollow\"\u003e\u003cem\u003e\u003cstrong\u003emetadata.csv\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e file which has to be in a \u003cstrong\u003ecomma separated\u003c/strong\u003e format (you can find an example of this file on PEMA\u0027s GitHub repository).\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-attention--\" class=\"anchor\" href=\"#attention--\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003cstrong\u003eAttention!\u003c/strong\u003e  \u003cbr\u003e\n\u003c/h3\u003e\n\u003cp\u003ePEMA will \u003cstrong\u003efail\u003c/strong\u003e unless you name the aforementioned files and directories \u003cstrong\u003eexactly\u003c/strong\u003e as described above.\n\u003cbr\u003e\u003c/p\u003e\n\u003cp\u003eHere is an example of how your \u003cem\u003eanalysis directory\u003c/em\u003e should be in case you do want a phyloseq analysis:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euser@home-PC:~/Desktop/analysis_directory$ ls\nmydata  parameters.tsv  phyloseq_in_PEMA.R  metadata.csv\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand in case you do not:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euser@home-PC:~/Desktop/analysis_directory$ ls\nmydata  parameters.tsv \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/hariszaf/pema/tree/master/analysis_directory\"\u003e\u003cstrong\u003eHere\u003c/strong\u003e\u003c/a\u003e you can find an example of an \u003cem\u003eanalysis directory\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eAfter you have prepared this \u003cem\u003eanalysis directory\u003c/em\u003e you are ready to run PEMA (see below).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAn extended list with PEMA\u0027s ouput can be found \u003ca href=\"https://github.com/hariszaf/pema/blob/master/help_files/PEMA\u0027s%20output%20files.md\"\u003e\u003cstrong\u003ehere\u003c/strong\u003e\u003c/a\u003e.\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-parameters-file\" class=\"anchor\" href=\"#parameters-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParameters\u0027 file\u003c/h1\u003e\n\u003cp\u003eThe most crucial component in running PEMA is the parameters file. This file must be located \u003cstrong\u003ein\u003c/strong\u003e the \u003cem\u003eanalysis directory\u003c/em\u003e and the user needs to fill it \u003cstrong\u003eevery time\u003c/strong\u003e PEMA is about to be called. If you need more than one analyses to run, then you need to make copies of the parameters\u0027 file and have one of those in eah of the analysis directrories you create.\u003c/p\u003e\n\u003cp\u003eSo, here is the \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\u003e\u003cem\u003e\u003cstrong\u003eparameters.tsv\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e file as it looks like, in a study case of our own.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-pema-on-hpc\" class=\"anchor\" href=\"#pema-on-hpc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA on HPC\u003c/h1\u003e\n\u003cp\u003ePEMA is best to run on HPC (server, cluster, cloud). Usually environmental data are quite large and the whole process has huge computational demands. To get PEMA running on your HPC you (actually your system administrator) need to install Singularity as described below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"https://www.sylabs.io/guides/3.0/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/strong\u003e  is a free, cross-platform and open-source computer program that performs operating-system-level virtualization also known as containerization. One of the main uses of Singularity is to bring containers and reproducibility to scientific computing and the high-performance computing (HPC) world.\u003c/p\u003e\n\u003cp\u003eSingularity needs a Linux/Unix system to run.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing\" class=\"anchor\" href=\"#installing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling\u003c/h2\u003e\n\u003cp\u003eAfter you install Singularity in your environment and open it, you need to download PEMA\u0027s image from Singularity Hub, by running the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e singularity pull shub://hariszaf/pema:v.1.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow you have PEMA on your environment. But there is still one really \u003cstrong\u003eimportant\u003c/strong\u003e thing that you need to do! Please \u003cstrong\u003edownload\u003c/strong\u003e the \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/parameters.tsv\"\u003e\u003cem\u003eparameters.tsv\u003c/em\u003e\u003c/a\u003e file and move it or copy it to the same directory with your raw data.\u003c/p\u003e\n\u003cp\u003eNow you are ready to go!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-pema\" class=\"anchor\" href=\"#running-pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning PEMA\u003c/h2\u003e\n\u003cp\u003eSingularity permits the use of a job scheduler that allocates computional resources on clusters and at the same time, works as a queuing system, as \u003cstrong\u003e\u003ca href=\"https://slurm.schedmd.com/overview.html\" rel=\"nofollow\"\u003eSlurm\u003c/a\u003e\u003c/strong\u003e. This way you are able to create a job as you usually do in your system and after editing the parameters file as needed, run PEMA as a job on your cluster.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-example\" class=\"anchor\" href=\"#example\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample\u003c/h3\u003e\n\u003cpre\u003e\u003ccode\u003e#SBATCH --partition=batch\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20\n#SBATCH --mem=\n# Memory per node specification is in MB. It is optional.\n# The default limit is 3000MB per core.\n#SBATCH --job-name=\"testPema\"\n#SBATCH --output=PEMA.output\n#SBATCH --mail-user=haris-zafr@hcmr.gr\n#SBATCH --mail-type=ALL\n#SBATCH --requeue\n\n\nsingularity run -B /\u0026lt;path\u0026gt;/\u0026lt;of\u0026gt;/\u0026lt;input\u0026gt;/\u0026lt;directory\u0026gt;/:/mnt/analysis /\u0026lt;path\u0026gt;/\u0026lt;of\u0026gt;/\u0026lt;PEMA_container\u0026gt;\n\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the above example, we set the cluster \"Zorba\", to run PEMA in 1 node, with 20 cores.\u003c/p\u003e\n\u003cp\u003eFor further information, you can always check \u003ca href=\"https://docs.google.com/presentation/d/1lVH23DPa2NDNBhVvOTRoip8mraw8zfw8VQwbK4vkB1U/edit?fbclid=IwAR14PpWfPtxB8lLBBnoxs7UbG3IJfkArrJBS5f2kRA__kvGDUb8wiJ2Cy_s#slide=id.g57f092f54d_1_21\" rel=\"nofollow\"\u003ePEMA\u0027s tutorial\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-pema-on-a-simple-pc\" class=\"anchor\" href=\"#pema-on-a-simple-pc\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePEMA on a simple PC\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cp\u003eTo run PEMA in a simple PC on your own environment, you first need to install \u003ca href=\"https://docs.docker.com/install/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e, in case you do not already have it.\u003c/p\u003e\n\u003cp\u003eYou should check your software version. A version of Docker is avalable for all Windows, Mac and Linux. If you have Windows 10 Pro or your Mac\u0027s hardware in after 2010, then you can insall Docker straightforward. Otherwise, you need to install the \u003ca href=\"https://docs.docker.com/toolbox/\" rel=\"nofollow\"\u003eDocker toolbox\u003c/a\u003e instead. You can check if your System Requirements are according to the ones mentioned below in order to be sure what you need to do.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eSystem Requirements\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e**__Windows 10 64bit__**:\nPro, Enterprise or Education (1607 Anniversary Update, Build 14393 or later).\nVirtualization is enabled in BIOS. Typically, virtualization is enabled by default.\nThis is different from having Hyper-V enabled. For more detail see Virtualization must be enabled in Troubleshooting.\nCPU SLAT-capable feature.\nAt least 4GB of RAM.\n\n**__Mac__**\nMac hardware must be a 2010 or newer model, with Intel\u2019s hardware support for memory management unit (MMU)\nvirtualization, including Extended Page Tables (EPT) and Unrestricted Mode. You can check to see if your machine\nhas this support by running the following command in a terminal:\nsysctl kern.hv_support macOS El Capitan 10.11 and newer macOS releases are supported.\nWe recommend upgrading to the latest version of macOS.\nAt least 4GB of RAM\nVirtualBox prior to version 4.3.30 must NOT be installed (it is incompatible with Docker for Mac).\nIf you have a newer version of VirtualBox installed, it\u2019s fine.\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installing-1\" class=\"anchor\" href=\"#installing-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstalling\u003c/h2\u003e\n\u003cp\u003eAfter you install Docker in your environment and run it, the only thing you need to do, is to download PEMA\u0027s image, by running the command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull hariszaf/pema\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe PEMA image file is a quite large (~3Gb), so it will take a while until it is downloaded in your computer system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-pema-1\" class=\"anchor\" href=\"#running-pema-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning PEMA\u003c/h2\u003e\n\u003cp\u003eRunning PEMA has two discrete steps.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-1---build-a-docker-container\" class=\"anchor\" href=\"#step-1---build-a-docker-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 1 - Build a Docker container\u003c/h3\u003e\n\u003cp\u003eAt first, you need to let Docker have access in your dataset. To provide access you need to run the following command and specifying the path to where your data is stored, i.e. changing the \u0026lt;path_to_analysis_directory\u0026gt; accordingly:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker run -it -v /\u0026lt;path_to_analysis_directory\u0026gt;/:/mnt/analysis hariszaf/pema\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter you run the command above, you have now built a Docker container, in which you can run PEMA!\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-step-2---run-pema\" class=\"anchor\" href=\"#step-2---run-pema\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStep 2 - Run PEMA\u003c/h3\u003e\n\u003cp\u003eNow, being inside the PEMA container, the only thing remaining to do, is to run PEMA\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e./PEMA_v1.bds\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePEMA is now running. The runtime of PEMA depends on the computational features of your environment, on the size of your data, as well as the parameters you chose.\u003c/p\u003e\n\u003cp\u003ePlease, keep in mind that when you need to copy a whole directory, then you always have to put \"/\" in the end of the path that describes where the directory is located.\u003c/p\u003e\n\u003cp\u003eFinally, you will find the PEMA output in the analysis directory on your computer. \u003cbr\u003e\nAs the output directory is mounted into the built Docker container, you can copy its contents wherever you want. However, in case you want to remove it permanently, you need to do this as a sudo user.\u003c/p\u003e\n\n\u003ch1\u003e\n\u003ca id=\"user-content-the-phyloseq-r-package\" class=\"anchor\" href=\"#the-phyloseq-r-package\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThe \"phyloseq\" R package\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003efor a downstream ecological analysis of OTUs/ASVs retrieved\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003ePEMA performs all the basic functions of the \"phyloseq\" R package. In addition, it performs certain functions of the \u003ca href=\"https://cran.r-project.org/web/packages/vegan/index.html\" rel=\"nofollow\"\u003e\u003cem\u003e\u003cstrong\u003evegan\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e R package.\u003c/p\u003e\n\u003cp\u003eWhen the user asks for a downstream analysis using the \"phyloseq\" R package, then an extra input file called \u003ca href=\"https://github.com/hariszaf/pema/blob/master/analysis_directory/phyloseq_in_PEMA.R\"\u003e\u003cem\u003e\u003cstrong\u003e\"phyloseq_script.R\"\u003c/strong\u003e\u003c/em\u003e\u003c/a\u003e needs to be imported in the \"analysis_directory\". In PEMA\u0027s main repository, you can find a template of this file; this file needs to be as it would run on your own computer, as you would run \u003cem\u003ephyloseq\u003c/em\u003e in any case. PEMA will create the \u003cem\u003e\"phyloseq object\"\u003c/em\u003e automatically and then it will perform the analysis as asked. The output will be placed in an extra subfolder in the main output directory of PEMA called \u003cem\u003ephyloseq_analysis\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eIn addition, the \u003cem\u003e\u003cstrong\u003emetadata.tsv\u003c/strong\u003e\u003c/em\u003e file is also required when the phyloseq option has been selected. An example of this file you can find \u003ca href=\"https://raw.githubusercontent.com/hariszaf/pema/master/analysis_directory/metadata.csv\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-acknowledgments\" class=\"anchor\" href=\"#acknowledgments\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgments\u003c/h1\u003e\n\u003cp\u003ePEMA uses a series of tools, datasets as well as Big Data Script language. We thank all the groups that developed them.\nThe tools \u0026amp; databases that PEMA uses are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBigDataScript programming language - \u003ca href=\"https://pcingola.github.io/BigDataScript/\" rel=\"nofollow\"\u003ehttps://pcingola.github.io/BigDataScript/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eFASTQC - \u003ca href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\" rel=\"nofollow\"\u003ehttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\u03a4rimmomatic - \u003ca href=\"http://www.usadellab.org/cms/?page=trimmomatic\" rel=\"nofollow\"\u003ehttp://www.usadellab.org/cms/?page=trimmomatic\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCutadapt - \u003ca href=\"https://cutadapt.readthedocs.io/en/stable/\" rel=\"nofollow\"\u003ehttps://cutadapt.readthedocs.io/en/stable/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eBayesHammer - included in SPAdes - \u003ca href=\"http://cab.spbu.ru/software/spades/\" rel=\"nofollow\"\u003ehttp://cab.spbu.ru/software/spades/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePANDAseq - \u003ca href=\"https://github.com/neufeld/pandaseq\"\u003ehttps://github.com/neufeld/pandaseq\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eOBITools - \u003ca href=\"https://pythonhosted.org/OBITools/welcome.html\" rel=\"nofollow\"\u003ehttps://pythonhosted.org/OBITools/welcome.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eBLAST Command Line Applications - \u003ca href=\"https://www.ncbi.nlm.nih.gov/books/NBK52640/\" rel=\"nofollow\"\u003ehttps://www.ncbi.nlm.nih.gov/books/NBK52640/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eVSEARCH-2.9.1 - \u003ca href=\"https://github.com/torognes/vsearch/releases/tag/v2.9.1\"\u003ehttps://github.com/torognes/vsearch/releases/tag/v2.9.1\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSWARM - \u003ca href=\"https://github.com/torognes/swarm\"\u003ehttps://github.com/torognes/swarm\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCROP - \u003ca href=\"https://github.com/tingchenlab/CROP\"\u003ehttps://github.com/tingchenlab/CROP\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eCREST - \u003ca href=\"https://github.com/lanzen/CREST\"\u003ehttps://github.com/lanzen/CREST\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRDPClassifier - \u003ca href=\"https://github.com/rdpstaff/classifier\"\u003ehttps://github.com/rdpstaff/classifier\u003c/a\u003e\n(RPDtools are required in order to execute RDPClassifier)\u003c/li\u003e\n\u003cli\u003eSILVA db - \u003ca href=\"https://www.arb-silva.de/no_cache/download/archive/current/Exports/\" rel=\"nofollow\"\u003ehttps://www.arb-silva.de/no_cache/download/archive/current/Exports/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMIDORI db - \u003ca href=\"http://reference-midori.info/index.html\" rel=\"nofollow\"\u003ehttp://reference-midori.info/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\"phat\" algorithm, from the \"gappa\" package - \u003ca href=\"https://github.com/lczech/gappa/wiki/Subcommand:-phat\"\u003ehttps://github.com/lczech/gappa/wiki/Subcommand:-phat\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMAFFT - \u003ca href=\"https://mafft.cbrc.jp/alignment/software/\" rel=\"nofollow\"\u003ehttps://mafft.cbrc.jp/alignment/software/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRAxML -ng - \u003ca href=\"https://github.com/amkozlov/raxml-ng\"\u003ehttps://github.com/amkozlov/raxml-ng\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ePaPaRa - \u003ca href=\"https://cme.h-its.org/exelixis/web/software/papara/index.html\" rel=\"nofollow\"\u003ehttps://cme.h-its.org/exelixis/web/software/papara/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eEPA-ng - \u003ca href=\"https://github.com/Pbdas/epa-ng\"\u003ehttps://github.com/Pbdas/epa-ng\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ephyloseq R package - \u003ca href=\"http://joey711.github.io/phyloseq/index.html\" rel=\"nofollow\"\u003ehttp://joey711.github.io/phyloseq/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003evegan R package - \u003ca href=\"https://cran.r-project.org/web/packages/vegan/index.html\" rel=\"nofollow\"\u003ehttps://cran.r-project.org/web/packages/vegan/index.html\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAnd of course the container-based technologies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocker - \u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003ehttps://www.docker.com/\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSingularity - \u003ca href=\"https://sylabs.io/singularity/\" rel=\"nofollow\"\u003ehttps://sylabs.io/singularity/\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h1\u003e\n\u003cp\u003ePEMA is under the GNU GPLv3 license (for 3rd party components separate licenses apply).\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h1\u003e\n\u003cp\u003eHaris Zafeiropoulos, Ha Quoc Viet, Katerina Vasileiadou, Antonis Potirakis, Christos Arvanitidis, Pantelis Topalis, Christina Pavloudi, Evangelos Pafilis, PEMA: a flexible Pipeline for Environmental DNA Metabarcoding Analysis of the 16S/18S ribosomal RNA, ITS, and COI marker genes, GigaScience, Volume 9, Issue 3, March 2020, giaa022, \u003ca href=\"https://doi.org/10.1093/gigascience/giaa022\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/gigascience/giaa022\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 12,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1623939477.0
  },
  {
    "data_format": 2,
    "description": "An example GitHub Action (CI) to build a Singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "singularityhub/github-ci",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-builder-github-ci\" class=\"anchor\" href=\"#singularity-builder-github-ci\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Builder GitHub CI\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"img/sregistry-github-small.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"img/sregistry-github-small.png\" alt=\"img/sregistry-github-small.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is a simple example of how you can achieve:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eversion control of your recipes\u003c/li\u003e\n\u003cli\u003eversioning to include image hash \u003cem\u003eand\u003c/em\u003e commit id\u003c/li\u003e\n\u003cli\u003ebuild of associated container and\u003c/li\u003e\n\u003cli\u003e(optional) push to a storage endpoint\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003efor a reproducible build workflow.\u003c/p\u003e\n\u003cp\u003eThere are two workflows configured on master that build a container:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003ca href=\".github/workflows/native-install.yml\"\u003enative install\u003c/a\u003e builds Singularity 3.x (with GoLang).\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\".github/workfolws/container.yml\"\u003edocker image\u003c/a\u003e builds in a \u003ca href=\"https://quay.io/repository/singularity/singularity\" rel=\"nofollow\"\u003edocker image\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eWhile the second option is faster to complete and a more simple workflow, it should be noted that docker runs with\n\u003ccode\u003e--privileged\u003c/code\u003e which may lead to issues with the resulting container in a non privileged situation.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should this be managed via Github?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eGithub, by way of easy integration with \u003cstrong\u003enative\u003c/strong\u003e continuous integration, is an easy way\nto have a workflow set up where multiple people can collaborate on a container recipe,\nthe recipe can be tested (with whatever testing you need), discussed in pull requests,\nand tested on merge to master. If you add additional steps in the \u003ca href=\".github/workflows/native-install.yml\"\u003ebuild workflow\u003c/a\u003e\nyou can also use \u003ca href=\"http://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003eSingularity Registry Client\u003c/a\u003e to push your container to a\n\u003ca href=\"https://singularityhub.github.io/sregistry\" rel=\"nofollow\"\u003eSingularity Registry Server\u003c/a\u003e or other\ncloud storage.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWhy should I use this instead of a service?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eYou could use a remote builder, but if you do the build in a continuous integration\nservice you get complete control over it. This means everything from the version of\nSingularity to use, to the tests that you run for your container. You have a lot more\nfreedom in the rate of building, and organization of your repository, because it\u0027s you\nthat writes the configuration.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick Start\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-1-add-your-recipes\" class=\"anchor\" href=\"#1-add-your-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e1. Add Your Recipes\u003c/h3\u003e\n\u003cp\u003eAdd your Singularity recipes to this repository, and edit the \u003ca href=\".github/workflows/native-install.yml\"\u003ebuild workflow\u003c/a\u003e\nsection where the container is built. The default will look for a recipe file called\n\"Singularity\" in the base of the respository, \u003ca href=\"Singularity\"\u003eas we have here\u003c/a\u003e.\nFor example, here is the default:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e    - \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eBuild Container\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003eenv\u003c/span\u003e:\n        \u003cspan class=\"pl-ent\"\u003eSINGULARITY_RECIPE\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eSingularity\u003c/span\u003e\n        \u003cspan class=\"pl-ent\"\u003eOUTPUT_CONTAINER\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003econtainer.sif\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003erun\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e|\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e       ls \u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e       if [ -f \"${SINGULARITY_RECIPE}\" ]; then\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e            sudo -E singularity build ${OUTPUT_CONTAINER} ${SINGULARITY_RECIPE}\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e       else\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e           echo \"${SINGULARITY_RECIPE} is not found.\"\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e           echo \"Present working directory: $PWD\"\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e           ls\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e       fi\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd I could easily change that to build as many recipes as I like, and\neven disregard the environment variable.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e    - \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eBuild Container\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003erun\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e|\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        sudo -E singularity build smokey.sif Singularity.smokey\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        sudo -E singularity build toasty.sif marshmallow/Singularity.toasty\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-2-test-your-container\" class=\"anchor\" href=\"#2-test-your-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e2. Test your Container\u003c/h3\u003e\n\u003cp\u003eImportantly, then you should test your container! Whether that\u0027s running it,\nexec\u0027ing a custom command, or invoking the test command, there is more than\none way to eat a reeses:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e    - \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eTest Container\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003erun\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e|\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        singularity exec smokey.sif python run_tests.py\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        singularity test smokey.sif\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        singularity run toasty.sif\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-3-push-to-a-registry\" class=\"anchor\" href=\"#3-push-to-a-registry\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e3. Push to a registry\u003c/h3\u003e\n\u003cp\u003eYou might be done there. But if not, you can install \u003ca href=\"http://singularityhub.github.io/sregistry-cli\" rel=\"nofollow\"\u003eSingularity Registry Client\u003c/a\u003e and push to your cloud storage of choice! You will want to add python and python-dev to the dependency\ninstall:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e    - \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eInstall Dependencies\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003erun\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e|\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          build-essential \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          libssl-dev \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          uuid-dev \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          libgpgme11-dev \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          squashfs-tools \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          libseccomp-dev \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          pkg-config \\\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e          python-dev python python3-pip\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAnd then install and use sregistry client. Here are many examples:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e    - \u003cspan class=\"pl-ent\"\u003ename\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003eDeploy Container\u003c/span\u003e\n      \u003cspan class=\"pl-ent\"\u003erun\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e|\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        sudo pip3 install sregistry\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        SREGISTRY_CLIENT=google-storage sregistry push --name username/reponame smokey.sif\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        SREGISTRY_CLIENT=s3 sregistry push --name username/reponame smokey.sif\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        SREGISTRY_CLIENT=registry sregistry push --name username/reponame smokey.sif\u003c/span\u003e\n\u003cspan class=\"pl-s\"\u003e        SREGISTRY_CLIENT=dropbox sregistry push --name username/reponame smokey.sif\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSee the \u003ca href=\"https://singularityhub.github.io/sregistry-cli/clients\" rel=\"nofollow\"\u003eclients page\u003c/a\u003e for all the options.\nRemember that the example workflow is intended to run on push to master, so you might want to have\na similar one (without deployment) that runs on pull_request, or other events.\nSee \u003ca href=\"https://help.github.com/en/articles/about-github-actions#core-concepts-for-github-actions\"\u003ehere\u003c/a\u003e\nfor getting started with GitHub actions, and \u003ca href=\"https://www.github.com/singularityhub/github-ci/issues\"\u003eplease open an issue\u003c/a\u003e\nif you need any help.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-other-options\" class=\"anchor\" href=\"#other-options\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOther Options\u003c/h2\u003e\n\u003cp\u003eYou can customize this base recipe in so many ways! For example:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIf you are building a Docker container, you can start with the docker base, build the container, and then pull it down into Singularity and test it. Successful builds can be pushed to Docker Hub, and then you know they will pull okay to a Singularity container.\u003c/li\u003e\n\u003cli\u003eThe action can be configured with a Matrix to run builds on multiple platforms.\u003c/li\u003e\n\u003cli\u003eYou can also do the same, but test multiple versions of Singularity.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHave fun!\u003c/p\u003e\n",
    "stargazers_count": 13,
    "subscribers_count": 2,
    "topics": [
      "singularity-ci",
      "github-ci",
      "singularity",
      "container"
    ],
    "updated_at": 1615502996.0
  },
  {
    "data_format": 2,
    "description": "Turn an existing conda environment into a Docker or Singularity container",
    "filenames": [
      "Singularity"
    ],
    "full_name": "grst/containerize-conda",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containerize-an-existing-conda-environment\" class=\"anchor\" href=\"#containerize-an-existing-conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainerize an existing conda environment\u003c/h1\u003e\n\u003cp\u003eI use conda environments for working on data analysis projects.\nSometimes I need to revert to install using \u003ccode\u003epip\u003c/code\u003e or \u003ccode\u003eR\u003c/code\u003e\u0027s\n\u003ccode\u003einstall.packages\u003c/code\u003e if a package is not on bioconda or conda-forge.\u003c/p\u003e\n\u003cp\u003eThis makes it very hard to reproduce the environment, and therefore,\nthe analysis, on another system. Even pure conda environments stored\nas an \u003ccode\u003eenvironment.yml\u003c/code\u003e file tend to \u003ca href=\"https://github.com/conda/conda/issues/9257\"\u003ebreak after a\nwhile\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eUsing the instructions below allows to package an existing environment\ninto a Docker or Singularity container which should be more portable\nand can also easily be integrated into a \u003ca href=\"https://grst.github.io/bioinformatics/2019/12/23/reportsrender.html\" rel=\"nofollow\"\u003efully reproducible\ndata analysis\nworkflow\u003c/a\u003e\nbased on e.g. \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://conda.github.io/conda-pack/\" rel=\"nofollow\"\u003econda-pack\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eeither Docker, Podman or Singularity\u003c/li\u003e\n\u003cli\u003esource conda environment needs to be on a linux x64 machine.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone this repository (retrieve \u003ccode\u003eDockerfile\u003c/code\u003e/\u003ccode\u003eSingularity\u003c/code\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003egit clone git@github.com:grst/containerize-conda.git\ncd containerize-conda\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003ePack the environment\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003econda-pack -n \u0026lt;MY_ENV\u0026gt; -o packed_environment.tar.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"3\"\u003e\n\u003cli\u003eBuild the container\u003c/li\u003e\n\u003c/ol\u003e\n\u003cpre\u003e\u003ccode\u003e# With singularity\nsingularity build --fakeroot \u0026lt;OUTPUT_CONTAINER.sif\u0026gt; Singularity\n\n# With Docker\ndocker build . -t \u0026lt;TAG\u0026gt;\n\n# With Podman/Buildah\npodman build . -t \u0026lt;TAG\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-it-works\" class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow it works\u003c/h2\u003e\n\u003cp\u003eConda environment can\u0027t be just \"moved\" to another location, as some paths are\nhardcoded into the environment. \u003ccode\u003econda-pack\u003c/code\u003e takes care of replacing these paths\nback to placeholders and creates a \u003ccode\u003e.tar.gz\u003c/code\u003e archive that contains the\nenvironment. This environment can be unpacked to another machine (or, in our\ncase, a container). Running \u003ccode\u003econda-unpack\u003c/code\u003e in the environment replaces the\nplaceholders back to the actual paths matching the new location.\u003c/p\u003e\n",
    "stargazers_count": 13,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1626360667.0
  },
  {
    "data_format": 2,
    "description": "Repository with all my Singularity recipes",
    "filenames": [
      "Singularity.template",
      "nanopore/Singularity.guppygpu",
      "nanopore/Singularity.puntseq",
      "nanopore/Singularity.guppycpu",
      "nanopore/Singularity.nanoporeqc",
      "nanopore/Singularity.taiyaki",
      "assembly/Singularity.racon",
      "assembly/Singularity.canu",
      "assembly/Singularity.polish",
      "assembly/Singularity.haslr",
      "recipes/Singularity.bracken",
      "recipes/Singularity.mccortex",
      "recipes/Singularity.krakenuniq",
      "recipes/Singularity.nanopolish",
      "recipes/Singularity.f5pack",
      "recipes/Singularity.clustalo",
      "recipes/Singularity.porechop",
      "recipes/Singularity.deepbinnergpu",
      "recipes/Singularity.mykrobe",
      "recipes/Singularity.deepbinnercpu",
      "recipes/Singularity.vcftools",
      "recipes/Singularity.samtools",
      "recipes/Singularity.minimap2",
      "recipes/Singularity.kraken2",
      "recipes/Singularity.ngm",
      "recipes/Singularity.pistis",
      "recipes/Singularity.filtlong",
      "recipes/Singularity.mummer",
      "recipes/Singularity.bcftools",
      "recipes/Singularity.centrifuge",
      "recipes/Singularity.spades",
      "recipes/Singularity.pandora",
      "recipes/Singularity.medaka",
      "recipes/Singularity.pilon"
    ],
    "full_name": "mbhall88/Singularity_recipes",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-recipes\" class=\"anchor\" href=\"#singularity-recipes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity Recipes\u003c/h1\u003e\n\u003chr\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE\u003c/strong\u003e: I am in the process of porting these recipes and images to\nsingularity version 3 and \u003ca href=\"https://cloud.sylabs.io/library\" rel=\"nofollow\"\u003eSingularity Library\u003c/a\u003e. Any recipes that\nhave been ported will have their images now hosted \u003ca href=\"https://cloud.sylabs.io/library/mbhall88/default\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/685\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003cbr\u003e\nThis repository is for all of my \u003ca href=\"https://www.sylabs.io/singularity/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e recipes and is linked to Singularity\nHub where they are built and available.\u003c/p\u003e\n\u003cp\u003eYou will find all the programs I have made Singularity recipes for inside the\n\u003ca href=\"https://github.com/mbhall88/Singularity_recipes/tree/master/recipes\"\u003e\u003ccode\u003erecipes\u003c/code\u003e directory\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eI try to keep them up-to-date in terms of versions, but if you notice anything\nout of date, feel free to put in a pull request. Additionally, feel free to\ncontribute recipes for programs I don\u0027t already have in here, or request one and\nif I have time I will try and make one.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-a-pre-built-container\" class=\"anchor\" href=\"#getting-a-pre-built-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting a pre-built container\u003c/h2\u003e\n\u003cp\u003eIf you would like a pre-built version of any of the containers you can pull it\ndown from \u003ca href=\"https://www.singularity-hub.org/collections/685/usage\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e\nlike so\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003etool=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esamtools\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nsingularity pull --name \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$tool\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e.simg shub://mbhall88/Singularity_recipes:\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e$tool\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNote: The \u003ccode\u003e--force\u003c/code\u003e option will override any container that exists in that\nlocation with the same name. If you are trying to pull a container to update a\npre-existing one, then the \u003ccode\u003e--force\u003c/code\u003e flag is necessary.\u003c/p\u003e\n\u003cp\u003eFor a full list of usage examples, check out the \u003ca href=\"https://www.singularity-hub.org/collections/685/usage\" rel=\"nofollow\"\u003eSingularity Hub usage docs\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-template\" class=\"anchor\" href=\"#template\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTemplate\u003c/h2\u003e\n\u003cp\u003eI have also included a template \u003ca href=\"https://github.com/mbhall88/Singularity_recipes/blob/master/Singularity.template\"\u003e\u003ccode\u003eSingularity.template\u003c/code\u003e\u003c/a\u003e\nwhich can be used for building up your own recipes.\u003c/p\u003e\n",
    "stargazers_count": 13,
    "subscribers_count": 1,
    "topics": [
      "singularity-containers",
      "singularity-hub",
      "singularity",
      "nanopore",
      "bioinformatics"
    ],
    "updated_at": 1614220433.0
  },
  {
    "data_format": 2,
    "description": "Genomics datastructures using Apache Arrow",
    "filenames": [
      "Singularity/Singularity"
    ],
    "full_name": "abs-tudelft/ArrowSAM",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-arrowsam\" class=\"anchor\" href=\"#arrowsam\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eArrowSAM\u003c/h1\u003e\n\u003cp\u003eArrowSAM is an in-memory Sequence Alignment/Map (SAM) representation which uses \u003ca href=\"https://arrow.apache.org/\" rel=\"nofollow\"\u003eApache Arrow framework\u003c/a\u003e (A cross-language development platform for in-memory data) and \u003ca href=\"https://arrow.apache.org/blog/2017/08/08/plasma-in-memory-object-store/\" rel=\"nofollow\"\u003ePlasma (Shared-Memory) Object Store\u003c/a\u003e to store and process SAM columnar data in-memory.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-citing-arrowsam\" class=\"anchor\" href=\"#citing-arrowsam\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca name=\"user-content-cite\"\u003e\u003c/a\u003eCiting ArrowSAM\u003c/h3\u003e\n\u003cp\u003eThe following paper describes the ArrowSAM format and its usage to speedup genomics pipelines. If you use ArrowSAM in your work, please cite the following paper.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAhmad et al., (2020). \"ArrowSAM: In-Memory Genomics Data Processing Using Apache Arrow\",\n\u003cem\u003eICCAIS\u003c/em\u003e. \u003ca href=\"https://doi.org/10.1109/ICCAIS48893.2020.9096725\" rel=\"nofollow\"\u003edoi.org/10.1109/ICCAIS48893.2020.9096725\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAhmad et al., \"Optimizing performance of GATK workflows using Apache Arrow In-Memory data framework\",\n\u003cem\u003eBMC Genomics, presented at APBC2020\u003c/em\u003e. \u003ca href=\"https://doi.org/10.1186/s12864-020-07013-y\" rel=\"nofollow\"\u003ehttps://doi.org/10.1186/s12864-020-07013-y\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis repo contains following three components:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eArrowSAM (In-memory SAM data representation) integrated \u003ca href=\"https://github.com/tahashmi/bwa\"\u003eBWA-MEM\u003c/a\u003e, Picard and GATK tools.\u003cbr\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eA Singularity container def file (To create an environment to use all Apache Arrow related tools and libraries for ArrowSAM).\u003cbr\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eScripts to run different GATK best practices recommended workflows (using different in-memory data placement techniques like ArrowSAM, ramDisk and pipes for fast processing) to run complete DNA analysis pipeline efficiently.\u003cbr\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eNote: ArrowSAM and all other workflows are based on single node, multi-core machines.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-run\" class=\"anchor\" href=\"#how-to-run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to run\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall \u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e container\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDownload our Singularity \u003ca href=\"https://github.com/abs-tudelft/arrow-gen/tree/master/Singularity\"\u003escript\u003c/a\u003e and generate singularity image (this image contains all Arrow related packges necessary for building/compiling BWA-MEM, Picard and GATK)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNow enter into generated image using command:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e sudo singularity shell \u0026lt;image_name\u0026gt;.simg\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDownload \u003ca href=\"https://github.com/tahashmi/bwa\"\u003eBWA-MEM\u003c/a\u003e inside image\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git clone https://github.com/tahashmi/bwa.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGo into bwa dir and compile BWA-MEM:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e cd bwa\n make\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNow you can run BWA-MEM.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n",
    "stargazers_count": 15,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1624522487.0
  },
  {
    "data_format": 2,
    "description": "A software framework of conservation-law solvers that use the space-time Conservation Element and Solution Element (CESE) method.",
    "filenames": [
      "contrib/singularity/Singularity",
      "contrib/singularity/Singularity.0.1.0",
      "contrib/singularity/Singularity.1.0.0-0.1.4+"
    ],
    "full_name": "solvcon/solvcon",
    "latest_release": "0.1.4",
    "readme": "\u003ch3\u003e\n\u003ca id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/openhpc/ohpc/blob/master/docs/recipes/install/common/figures/ohpc_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/openhpc/ohpc/raw/master/docs/recipes/install/common/figures/ohpc_logo.png\" width=\"170\" valign=\"middle\" hspace=\"5\" alt=\"OpenHPC\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h3\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-community-building-blocks-for-hpc-systems\" class=\"anchor\" href=\"#community-building-blocks-for-hpc-systems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommunity building blocks for HPC systems\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h4\u003e\n\u003cp\u003eThis stack provides a variety of common, pre-built ingredients required to\ndeploy and manage an HPC Linux cluster including provisioning tools, resource\nmanagement, I/O clients, runtimes, development tools, containers, and a variety of\nscientific libraries.\u003c/p\u003e\n\u003cp\u003eThere are currently two release series:\n\u003ca href=\"https://github.com/openhpc/ohpc/wiki/1.3.X\"\u003e1.3.x\u003c/a\u003e and\n\u003ca href=\"https://github.com/openhpc/ohpc/wiki/2.x\"\u003e2.x\u003c/a\u003e, which target different major\nLinux OS distributions. The 1.3.x series targets CentOS7 and SLES12 while the\n2.x series targets CentOS8 and Leap15.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting started\u003c/h4\u003e\n\u003cp\u003eOpenHPC provides pre-built binaries via repositories for use with standard\nLinux package manager tools (e.g. \u003ccode\u003eyum\u003c/code\u003e or \u003ccode\u003ezypper\u003c/code\u003e). To get started,\nyou can enable an OpenHPC repository locally through installation of an\n\u003ccode\u003eohpc-release\u003c/code\u003e RPM which includes gpg keys for package signing and defines\nthe URL locations for [base] and [update] package repositories. Installation\nguides tailored for each supported provisioning system and resource manager\nwith detailed example instructions for installaing a cluster are also available.\nCopies of the \u003ccode\u003eohpc-release\u003c/code\u003e package and installation guides along with\nmore information is available on the relevant release series pages\n(\u003ca href=\"https://github.com/openhpc/ohpc/wiki/1.3.X\"\u003e1.3.x\u003c/a\u003e or\n\u003ca href=\"https://github.com/openhpc/ohpc/wiki/2.x\"\u003e2.x\u003c/a\u003e).\u003c/p\u003e\n\u003chr\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-questions-comments-or-bug-reports\" class=\"anchor\" href=\"#questions-comments-or-bug-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuestions, Comments, or Bug Reports?\u003c/h4\u003e\n\u003cp\u003eSubscribe to the users email list at \u003ca href=\"https://groups.io/g/openhpc-users\" rel=\"nofollow\"\u003ehttps://groups.io/g/openhpc-users\u003c/a\u003e or see\nthe \u003ca href=\"http://openhpc.community\" rel=\"nofollow\"\u003ehttp://openhpc.community\u003c/a\u003e page for more pointers.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-additional-software-requests\" class=\"anchor\" href=\"#additional-software-requests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdditional Software Requests?\u003c/h4\u003e\n\u003cp\u003ePlease see the component submission page at\n\u003ca href=\"https://github.com/openhpc/submissions\"\u003ehttps://github.com/openhpc/submissions\u003c/a\u003e for more information regarding new\nsoftware inclusion requests.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-register-your-system\" class=\"anchor\" href=\"#register-your-system\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRegister your system\u003c/h4\u003e\n\u003cp\u003eIf you are using elements of OpenHPC, please consider registering your\nsystem(s) using the \u003ca href=\"https://drive.google.com/open?id=1KvFM5DONJigVhOlmDpafNTDDRNTYVdolaYYzfrHkOWI\" rel=\"nofollow\"\u003eSystem Registration\nForm\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 15,
    "subscribers_count": 8,
    "topics": [
      "computational-science"
    ],
    "updated_at": 1622890170.0
  },
  {
    "data_format": 2,
    "description": "Intel HPC Containers using Singularity",
    "filenames": [
      "definitionFiles/namd/namdRun.def",
      "definitionFiles/namd/namdBuild.def",
      "definitionFiles/lammps/lammpsBuild.def",
      "definitionFiles/lammps/lammpsRun.def",
      "definitionFiles/gromacs/gromacsBuild.def",
      "definitionFiles/gromacs/gromacsRun.def",
      "definitionFiles/WRF/wrfBuild.def",
      "definitionFiles/WRF/wrfRun.def",
      "definitionFiles/base/base.def"
    ],
    "full_name": "intel/HPC-containers-from-Intel",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-goal\" class=\"anchor\" href=\"#goal\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGoal:\u003c/h1\u003e\n\u003cp\u003eCreate containers using Singularity definition file for HPC apps and run them on the cloud or bare metal for Single and Cluster runs.\u003c/p\u003e\n\u003cp\u003eThis repo should have definition files only for few HPC applications. Users can utilize them to generate containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-get-help\" class=\"anchor\" href=\"#get-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGet Help\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/intel/HPC-containers-from-Intel/issues\"\u003ePost an issue\u003c/a\u003e if you face any problem building or running a container\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 16,
    "subscribers_count": 9,
    "topics": [
      "hpc",
      "cluster",
      "singularity-containers",
      "cloud"
    ],
    "updated_at": 1619711561.0
  },
  {
    "data_format": 2,
    "description": "fastq quality assessment and filtering tool",
    "filenames": [
      "Singularity",
      "Singularity-Test"
    ],
    "full_name": "jengelmann/FastqPuri",
    "latest_release": "v1.0.6",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-fastqpuri-an-fq-quality-control-and-filter-tool\" class=\"anchor\" href=\"#fastqpuri-an-fq-quality-control-and-filter-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFastqPuri, an fq quality control and filter tool\u003c/h1\u003e\n\u003cp\u003eSoftware and source code of \u003ccode\u003eFastqPuri\u003c/code\u003e. It creates quality reports of\n\u003ccode\u003efastq\u003c/code\u003e files and filters them removing low quality reads, reads\ncontaining too many N\u0027s or contamination reads (unwanted rRNA reads,\nimpurities coming from another organism, ...).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eClone the repository, or download the source. Make sure that\nyour system supplies the following dependencies for FastqPuri.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOS: Linux (clang, gcc), Mac OS (clang, gcc), OpenBSD (clang)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ecmake\u003c/code\u003e (at least version 2.8),\u003c/li\u003e\n\u003cli\u003ea \u003ccode\u003eC\u003c/code\u003e compiler supporting the \u003ccode\u003ec11\u003c/code\u003e standard\n(change the compiler flags otherwise),\u003c/li\u003e\n\u003cli\u003epandoc (optional, see documentation in \u003ccode\u003ePANDOC.md\u003c/code\u003e),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eRscript\u003c/code\u003e (optional),\u003c/li\u003e\n\u003cli\u003eFollowing \u003ccode\u003eR\u003c/code\u003e packages installed (optional):\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003epheatmap\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eknitr\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ermarkdown\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eNOTE:\u003c/strong\u003e  FastqPuri will work without the optional dependencies\nbut will skip creating html reports if they are not available.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ cmake -H. -Bbuild/ [-DRSCRIPT=/path/to/my/R/bin/Rscript] [-DCMAKE_INSTALL_PREFIX=/path/to/my/root] ... \n$ cd build \n$ make \n$ sudo make install  \n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen running \u003ccode\u003ecmake\u003c/code\u003e, there are some variables you can set\nusing the option -D followed by the variable name. These variables are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eCMAKE_C_COMPILER\u003c/code\u003e: \u003ccode\u003eC\u003c/code\u003e compiler (default \u003ccode\u003egcc\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCMAKE_C_FLAGS\u003c/code\u003e: compiler flags (default \u003ccode\u003e-Wall -O3 -march=native -std=c11\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eCMAKE_INSTALL_PREFIX\u003c/code\u003e: root path for \u003ccode\u003emake install\u003c/code\u003e, e.g. to\nredirect to a directory with user access (default /usr/local),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003ePANDOC\u003c/code\u003e: \u003ccode\u003epandoc\u003c/code\u003e executable (default \u003ccode\u003epandoc\u003c/code\u003e),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eRSCRIPT\u003c/code\u003e: \u003ccode\u003eRscript\u003c/code\u003e executable (default \u003ccode\u003eRscript\u003c/code\u003e),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eREAD_MAXLEN\u003c/code\u003e: Maximum Illumina read length\u003c/li\u003e\n\u003cli\u003e(default 400),\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe executables will be created in the folder \u003ccode\u003ebin\u003c/code\u003e and installed in \u003ccode\u003e/usr/local/bin\u003c/code\u003e.\n\u003ccode\u003eR\u003c/code\u003e scripts will be installed in \u003ccode\u003e/usr/local/share/FastqPuri/R\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWARNING:\u003c/strong\u003e do not move the executables that depend on \u003ccode\u003eR\u003c/code\u003e scripts,\nanywhere else, unless you also move the corresponding \u003ccode\u003eR\u003c/code\u003e scripts respecting\nthe local folder structure.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-executables\" class=\"anchor\" href=\"#executables\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExecutables\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eQreport\u003c/code\u003e: creates a quality report in html format (see \u003ccode\u003eREADME_Qreport.md\u003c/code\u003e),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSreport\u003c/code\u003e: creates a summary report in html format on a set of samples,\nregarding either the original files or the filtering process\n(see \u003ccode\u003eREADME_Sreport.md\u003c/code\u003e),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emakeBloom\u003c/code\u003e: creates a  bloom filter from a fasta file of a certain size,\nand stores it in a file (see \u003ccode\u003eREADME_makeBloom.md\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003emakeTree\u003c/code\u003e: creates a tree of a certain depth from a fasta file and stores\nit in a file (see \u003ccode\u003eREADME_makeTree.md\u003c/code\u003e),\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etrimFilter\u003c/code\u003e: performs the filtering process for single-end data\n(see \u003ccode\u003eREADME_trimFilter.md\u003c/code\u003e).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etrimFilterPE\u003c/code\u003e: performs the filtering process for double stranded data\n(see \u003ccode\u003eREADME_trimFilterPE.md\u003c/code\u003e).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn exemplar work flow could be:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eQreport\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSreport\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emakeBloom\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003etrimFilter\u003c/code\u003e or \u003ccode\u003etrimFilterPE\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eQreport\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSreport\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation-of-the-code\" class=\"anchor\" href=\"#documentation-of-the-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation of the code\u003c/h2\u003e\n\u003cp\u003eA Doxygen documentation of the code is available:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003ehtml\u003c/code\u003e version under the folder \u003ccode\u003ehtml\u003c/code\u003e (open \u003ccode\u003eindex.html\u003c/code\u003e with a browser).\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003epdf\u003c/code\u003e version: \u003ccode\u003elatex/refman.pdf\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-use-a-docker-container-to-run-fastqpuri\" class=\"anchor\" href=\"#use-a-docker-container-to-run-fastqpuri\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse a docker container to run FastqPuri\u003c/h2\u003e\n\u003cp\u003eThe file \u0027Dockerfile\u0027 documents the exact linux installation we used\nfor testing. If you have a docker installation ready on your machine,\nyou may want to use a docker container for easy installation and\ncapsulated usage of FastqPuri. After cloning this project from github\nand change to its main directory, you may install a docker container\nas follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ docker build -t fastqpuri .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will create a container based on the debian linux distribution\ncovering all dependencies including R and pandoc.  As soon as such a\ncontainer is installed, you can use it either interactively:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ docker run -v $PWD:/tmp -it fastqpuri\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor by running a pipeline implemented in an executable bash script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ docker run -v $PWD:/tmp fastqpuri ./pipeline.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that this call generates results in the docker container\ndirectory \u003ccode\u003e/tmp\u003c/code\u003e but also keeps them after closing the docker container\nlocally where the container was started.\u003c/p\u003e\n\u003cp\u003eInstead of generating the docker container yourself with \u0027docker\nbuild\u0027, you can also pull a pre-built image from the docker hub as\nfollows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ docker pull clottaz/fastqpuri\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can run such a pre-built image with \u0027docker run\u0027 by indicating the\nimages as \u0027clottaz/fastqpuri\u0027.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-use-a-singularity-container-to-run-fastqpuri\" class=\"anchor\" href=\"#use-a-singularity-container-to-run-fastqpuri\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse a singularity container to run FastqPuri\u003c/h2\u003e\n\u003cp\u003eAlternativly, if you have singularity installed on your machine, you\ncan call our docker container for FastqPuri as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity shell --bind .:/tmp docker://clottaz/fastqpuri\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis call opens a shell within the container.\nWith \u003ccode\u003e--bind\u003c/code\u003e we  mount the current directory also in the container.\nThe syntax is as follows: --bind src:dest; src is the source path on\nthe host and dest is the destination path in the container, i.e. where\nyou would like to make the source path available in your container.\nNote that this destination path in your container should be an existing\ndirectory, the operation will fail if you do not create the directory first.\nHence, when we call \u003ccode\u003esingularity shell\u003c/code\u003e like this, the working directory\nin the container is \u003ccode\u003e/tmp\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eAlternatively, in order to execute a script from the current\ndirectory, call singularity as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity run --bind .:/tmp docker://clottaz/fastqpuri /tmp/pipeline.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that \u003ccode\u003e/tmp/pipeline.sh\u003c/code\u003e relates to the call within the\ncontainer. Thus, \u003ccode\u003epipeline.sh\u003c/code\u003e is located in the directory where singularity\nrun is executed, but will be made available to the container via the \u003ccode\u003e--bind\u003c/code\u003e\nparameter.\u003c/p\u003e\n\u003cp\u003eIf you want to invoke a function of FastqPuri, you can use the \u0027exec\u0027\ncommand like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec docker://clottaz/fastqpuri Qreport -h\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor invoke a script located in your home directory (assuming that\nrun_ex_TREE.sh is located in your home directory):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ singularity exec docker://clottaz/fastqpuri $HOME/run_ex_TREE.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eSingularity documentation can be found here: \u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003ehttps://www.sylabs.io/docs/\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-via-bioconda--under-construction\" class=\"anchor\" href=\"#installation-via-bioconda--under-construction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation via bioconda \u003cstrong\u003e-under construction\u003c/strong\u003e.\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eWe are currently working on a bioconda environment for FastqPuri.\nIf you follow the instructions below, it is quite likely that\nFastqPuri will not yet properly run from the bioconda environment.\nSorry about that and please stay tuned!\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eBioconda is a channel for the conda package manager specializing in\nbioinformatics software. Have a look at the reference:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBjoern Gruening, Ryan Dale, Andreas Sjoedin, Brad A. Chapman, Jillian\nRowe, Christopher H. Tomkins-Tinch, Renan Valieris, the Bioconda\nTeam, and Johannes Koester. 2018. Bioconda: Sustainable and\nComprehensive Software Distribution for the Life Sciences. Nature\nMethods, 2018.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo find out how to use bioconda, see \u003ca href=\"https://bioconda.github.io\" rel=\"nofollow\"\u003ehttps://bioconda.github.io\u003c/a\u003e.\nFor installing FastqPuri in a bioconda environment, you have to install\neither \u003ccode\u003eminiconda\u003c/code\u003e or \u003ccode\u003eanaconda\u003c/code\u003e and register channels as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda config --add channels defaults\n$ conda config --add channels bioconda\n$ conda config --add channels conda-forge\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen you can install \u003ccode\u003efastqpuri\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda install fastqpuri\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eActually, you may also want to use a specific environment for the\nsequencing quality control:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ conda create -n qc fastqpuri\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis call installs \u003ccode\u003eFastqPuri\u003c/code\u003e directly in a separate environment.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors\" class=\"anchor\" href=\"#contributors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors\u003c/h2\u003e\n\u003cp\u003ePaula P\u00e9rez Rubio,\nClaudio Lottaz,\nJulia Engelmann\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eGPL v3 (see LICENSE.txt)\u003c/p\u003e\n",
    "stargazers_count": 16,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1622626216.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "singularity_training/exercise_03_HPL/Singularity_03_HPL_complete",
      "singularity_training/exercise_01_OpenMP/Singularity_01_OpenMP",
      "singularity_training/exercise_04_GPUBurn/Singularity_04",
      "singularity_training/exercise_04_GPUBurn/Singularity_04_multistage",
      "singularity_training/exercise_02_PureMPI/Singularity_02_PureMPI"
    ],
    "full_name": "abdulrahmanazab/docker-training-neic",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-containers-tutorial---prace-training-2021\" class=\"anchor\" href=\"#containers-tutorial---prace-training-2021\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers Tutorial - PRACE training, 2021\u003c/h1\u003e\n\u003cp\u003eThe training infrastructure is offered by\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.cscs.ch/computers/piz-daint/\" rel=\"nofollow\"\u003ePiz Daint\u003c/a\u003e at CSCS, Switserland.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.nrec.no/\" rel=\"nofollow\"\u003eNorwegian Research and Education cloud\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-connect-to-your-vm\" class=\"anchor\" href=\"#connect-to-your-vm\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConnect to your VM\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eYou will get the key file \u003ccode\u003enrec.pem\u003c/code\u003e from your instructor\u003c/li\u003e\n\u003cli\u003eNow use it to connect to your VM:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003echmod 600 nrec.pem \nssh -i nrec.pem debian@\u003cspan class=\"pl-k\"\u003e\u0026lt;\u003c/span\u003eTerminal-IP-Address\u003cspan class=\"pl-k\"\u003e\u0026gt;\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tutorial-contents\" class=\"anchor\" href=\"#tutorial-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTutorial contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abdulrahmanazab/docker-training-neic/blob/prace-training-2021/docker.md\"\u003eDocker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abdulrahmanazab/docker-training-neic/tree/prace-training-2021/singularity_training\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abdulrahmanazab/docker-training-neic/blob/prace-training-2021/sarus.md\"\u003eSarus\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abdulrahmanazab/docker-training-neic/blob/prace-training-2021/Charliecloud/Charliecloud.md\"\u003eCharliecloud\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/abdulrahmanazab/docker-training-neic/blob/prace-training-2021/unikernels.md\"\u003eUnikernels\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 17,
    "subscribers_count": 5,
    "topics": [],
    "updated_at": 1625652278.0
  },
  {
    "data_format": 2,
    "description": "AutoMATES: Automated Model Assembly from Text, Equations, and Software",
    "filenames": [
      "automates/equation_reading/equation_extraction/containers/Singularity.im2markup",
      "automates/equation_reading/equation_extraction/containers/Singularity.pytorch_skimage"
    ],
    "full_name": "ml4ai/automates",
    "latest_release": "v0.1.0",
    "readme": "\u003ch1 align=\"center\"\u003e\n\u003ca id=\"user-content-automated-model-assemblyfrom-text-equations-and-software\" class=\"anchor\" href=\"#automated-model-assemblyfrom-text-equations-and-software\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAutomated Model Assembly\u003cbr\u003efrom Text, Equations, and Software\u003c/h1\u003e\n\u003cp align=\"center\"\u003e\n  \n  \n  \u003ca href=\"https://github.com/ml4ai/automates/actions\"\u003e\n    \u003cimg src=\"https://camo.githubusercontent.com/aca4ff329fcbc8477eb69ee3754d654a8f27d1b798178530242c11421121ca79/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f776f726b666c6f772f7374617475732f6d6c3461692f6175746f6d617465732f436f6e74696e756f7573253230496e746567726174696f6e3f6c6162656c3d7465737473\" alt=\"GH Actions build status\" data-canonical-src=\"https://img.shields.io/github/workflow/status/ml4ai/automates/Continuous%20Integration?label=tests\" style=\"max-width:100%;\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://codecov.io/gh/ml4ai/automates\" rel=\"nofollow\"\u003e\n   \u003cimg src=\"https://camo.githubusercontent.com/f7328d200bac8b4c5c0473f8c3e3d1b870c0811e5802f20ef544cfc68259abbd/68747470733a2f2f636f6465636f762e696f2f67682f6d6c3461692f6175746f6d617465732f6272616e63682f6d61737465722f67726170682f62616467652e737667\" data-canonical-src=\"https://codecov.io/gh/ml4ai/automates/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\n  \u003c/a\u003e\n  \u003ca href=\"https://www.codefactor.io/repository/github/ml4ai/automates\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ce091de26720098ad92a2ee617c9d8975b7bf53366140d5119b22f0025f8e740/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6d6c3461692f6175746f6d617465732f6261646765\" alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/ml4ai/automates/badge\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp\u003eThis repository holds the source code for the AutoMATES documentation\nand several component pipelines.\u003c/p\u003e\n\u003cp\u003eFor documentation: \u003ca href=\"https://ml4ai.github.io/automates\" rel=\"nofollow\"\u003ehttps://ml4ai.github.io/automates\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation-instructions\" class=\"anchor\" href=\"#installation-instructions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation instructions\u003c/h2\u003e\n\u003cp\u003eFor all operating systems, the first step of the installation process is to clone the AutoMATES repository.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-linux-and-macos\" class=\"anchor\" href=\"#linux-and-macos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinux and macOS\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eCreate a new \u003ca href=\"https://docs.python.org/3/library/venv.html\" rel=\"nofollow\"\u003ePython virtualenv\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eActivate your new Python virtualenv\u003c/li\u003e\n\u003cli\u003eInstall Graphviz as defined below\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003epip install -e .\u003c/code\u003e from the root of the AutoMATES directory\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-graphviz-installation\" class=\"anchor\" href=\"#graphviz-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGraphViz installation\u003c/h4\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-debian-flavored-linux\" class=\"anchor\" href=\"#debian-flavored-linux\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDebian flavored linux\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eUse the command: \u003ccode\u003esudo apt-get install graphviz libgraphviz-dev pkg-config\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-macos-with-homebrew\" class=\"anchor\" href=\"#macos-with-homebrew\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003emacOS with Homebrew\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eUse the command: \u003ccode\u003ebrew install graphviz\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eInstall PyGraphviz to your virtualenv with: \u003ccode\u003epip install --install-option=\"--include-path=/usr/local/include/\" --install-option=\"--library-path=/usr/local/lib\" pygraphviz\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-windows\" class=\"anchor\" href=\"#windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWindows\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDownload and install \u003ca href=\"https://www.anaconda.com/products/individual\" rel=\"nofollow\"\u003eAnaconda\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eEdit the \u003ccode\u003ePYTHONPATH\u003c/code\u003e variable in \u003ccode\u003eenvironment.yml\u003c/code\u003e to be your local path to your checkout of the AutoMATES repo\u003c/li\u003e\n\u003cli\u003eRun \u003ccode\u003econda env create --file environment.yml\u003c/code\u003e from the root of the AutoMATES directory\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 17,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1626985595.0
  },
  {
    "data_format": 2,
    "description": "APSIM",
    "filenames": [
      "Release/Containers/Singularity/Singularity.ubuntu"
    ],
    "full_name": "APSIMInitiative/APSIMClassic",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-apsim\" class=\"anchor\" href=\"#apsim\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAPSIM\u003c/h1\u003e\n\u003cp\u003eThe Agricultural Production Systems sIMulator (APSIM) is internationally recognised as a highly advanced simulator of agricultural systems. It contains a suite of modules which enable the simulation of systems that cover a range of plant, animal, soil, climate and management interactions. APSIM is undergoing continual development, with new capability added to regular releases of official versions. Its development and maintenance is underpinned by rigorous science and software engineering standards. The APSIM Initiative has been established to promote the development and use of the science modules and infrastructure software of APSIM.\u003c/p\u003e\n\u003cp\u003eCI builds of this repository can be found \u003ca href=\"https://apsimdev.apsim.info/APSIM.Builds.Portal/Bob.aspx\" rel=\"nofollow\"\u003eHere\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 18,
    "subscribers_count": 16,
    "topics": [],
    "updated_at": 1625734197.0
  },
  {
    "data_format": 2,
    "description": "A snakemake pipeline to assembly, polishing, correction and quality check from Oxford nanopore reads.",
    "filenames": [
      "Containers/Singularity.culebront_tools.def",
      "Containers/Singularity.report.def"
    ],
    "full_name": "SouthGreenPlatform/CulebrONT_pipeline",
    "latest_release": "1.5.2",
    "readme": "\u003cp\u003e\u003ca href=\"./docs/source/_images/culebront_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./docs/source/_images/culebront_logo.png\" alt=\"Culebront Logo\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.python.org/downloads\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e4779c52a0f8acf7c62517ff771deebcf8ab8913544dd508ccdd6cec2f2b400a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707974686f6e2d332e372532422d626c7565\" alt=\"PythonVersions\" data-canonical-src=\"https://img.shields.io/badge/python-3.7%2B-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://snakemake.readthedocs.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/35030e6ddc253302ffcdf599ce8a8e387c27d88eb3de9cfe4e103b3ec6161f96/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736e616b656d616b652d254532253839254135352e31302e302d627269676874677265656e2e7376673f7374796c653d666c6174\" alt=\"SnakemakeVersions\" data-canonical-src=\"https://img.shields.io/badge/snakemake-%E2%89%A55.10.0-brightgreen.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://sylabs.io/docs/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a324f41bf4495d7dc95ac4693962834b38ff77e1a6ed7f5c4dca9c3e3f92a6d3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f73696e67756c61726974792d254532253839254135332e332e302d3745344337342e737667\" alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/singularity-%E2%89%A53.3.0-7E4C74.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://docs.conda.io/projects/conda/en/latest/index.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cacdb0b19bd30d76ae4faaee3355a6d65ecc448b587bac638adbd5eb04339c20/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612d342e382e352532302d677265656e\" alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/conda-4.8.5%20-green\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsing data from long reads obtained by Oxford Nanopore Technologies sequencing makes genome assembly easier, in particular to solve repeats and structural variants, in prokaryotic as well as in eukaryotic genomes, resulting in increased contiguity and accuracy.\u003c/p\u003e\n\u003cp\u003eBunch of softwares and tools are released or updated every week, and a lot of species see their genome assembled using those.\u003c/p\u003e\n\u003cp\u003eThat\u2019s right.\u003c/p\u003e\n\u003cp\u003e\"\u003cem\u003eBut which assembly tool could give the best results for my favorite organism?\u003c/em\u003e\"\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCulebrONT can help you!\u003c/strong\u003e CulebrONT is an open-source, scalable, modulable and traceable snakemake pipeline, able to launch multiple assembly tools in parallel and providing help for choosing the best possible assembly between all possibilities.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHomepage: \u003ca href=\"https://culebront-pipeline.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003ehttps://culebront-pipeline.readthedocs.io/en/latest/\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"user-content-citation\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003e@Authors:\u003c/p\u003e\n\u003cp\u003eJulie Orjuela (IRD), Aurore Comte(IRD), S\u00e9bastien Ravel(CIRAD), Florian Charriat(INRAE), Tram Vi(IRD, AGI), Francois Sabot(IRD) and S\u00e9bastien Cunnac(IRD).\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"user-content-notes\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-useful-notes\" class=\"anchor\" href=\"#useful-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUseful notes\u003c/h2\u003e\n\u003cp\u003eBefore launching CulebrONT, you could base-calling of arbitrarily multiplexed libraries across several Minion runs with sequencing quality control and gather the output files by genome for subsequent steps. For that use \u003ca href=\"https://github.com/vibaotram/baseDmux\"\u003ehttps://github.com/vibaotram/baseDmux\u003c/a\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h4\u003e\n\u003cp\u003eThanks to Ndomassi Tando (i-Trop IRD) by administration support.\u003c/p\u003e\n\u003cp\u003eThe authors acknowledge the IRD i-Trop HPC (South Green Platform) at IRD Montpellier for providing HPC resources that have contributed to this work. \u003ca href=\"https://bioinfo.ird.fr/\" rel=\"nofollow\"\u003ehttps://bioinfo.ird.fr/\u003c/a\u003e - \u003ca href=\"http://www.southgreen.fr\" rel=\"nofollow\"\u003ehttp://www.southgreen.fr\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThanks to Yann Delorme for this beautiful logo \u003ca href=\"https://nimarell.github.io/resume\" rel=\"nofollow\"\u003ehttps://nimarell.github.io/resume\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca name=\"user-content-licence\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eLicencied under CeCill-C (\u003ca href=\"http://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\" rel=\"nofollow\"\u003ehttp://www.cecill.info/licences/Licence_CeCILL-C_V1-en.html\u003c/a\u003e) and GPLv3\nIntellectual property belongs to IRD and authors.\u003c/p\u003e\n",
    "stargazers_count": 19,
    "subscribers_count": 16,
    "topics": [],
    "updated_at": 1626958006.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "containers/Singularity.0.3.3",
      "containers/Singularity.0.3.6",
      "containers/Singularity.0.4.1",
      "containers/Singularity.0.3.5",
      "containers/Singularity.0.4.0"
    ],
    "full_name": "lscsoft/bilby",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-monet\" class=\"anchor\" href=\"#monet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMONET\u003c/h1\u003e\n\u003cp\u003eThis repository holds the source code for \u003cstrong\u003eMONET\u003c/strong\u003e, a Linux/MacOS command-line toolbox to mine molecular and genetic networks, leveraging the top performing methods of the \u003cstrong\u003eDisease Module Identification (DMI) DREAM Challenge\u003c/strong\u003e (see DREAM Challenge paper under section PUBLICATIONS and \u003ca href=\"https://www.synapse.org/modulechallenge\" rel=\"nofollow\"\u003ehttps://www.synapse.org/modulechallenge\u003c/a\u003e)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePREREQUISITES\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eOperating System\u003c/strong\u003e: MONET can be run on \u003cstrong\u003eeither\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinux (it was tested on \u003cem\u003eUbuntu Linux\u003c/em\u003e 20.04, \u003cem\u003eCentOS Linux\u003c/em\u003e 7.5)\u003c/li\u003e\n\u003cli\u003eMacOS (it was tested on \u003cem\u003emacOS Sierra\u003c/em\u003e 10.12)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSoftware\u003c/strong\u003e: MONET requires \u003cstrong\u003eeither\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eDocker\u003c/code\u003e (see \"Install using the repository\" \u003ca href=\"https://docs.docker.com/engine/install/\" rel=\"nofollow\"\u003ehttps://docs.docker.com/engine/install/\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSingularity\u003c/code\u003e (see \u003ca href=\"http://singularity.lbl.gov\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eHardware\u003c/strong\u003e: MONET was tested both on server and on commodity hardware (i.e., regular desktop). For details, please refer to section COMPUTATIONAL RESOURCES below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eINSTALLATION\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eJust like you can \u003ccode\u003els\u003c/code\u003e a folder, after installation will be able to \u003ccode\u003emonet\u003c/code\u003e a network\u003c/strong\u003e from any location on your system.\u003c/p\u003e\n\u003cp\u003eSimply run:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git clone https://github.com/BergmannLab/MONET.git \u0026amp;\u0026amp; cd MONET \u0026amp;\u0026amp; ./install.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eA folder MONET will have been created with the source code: you are free to remove it, if you are not interested. This will not affect MONET, which has now been installed in your system: the command \u003ccode\u003emonet\u003c/code\u003e can be invoked from any location on the system.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-need-some-more-guidance\" class=\"anchor\" href=\"#if-you-need-some-more-guidance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU NEED SOME MORE GUIDANCE\u003c/h4\u003e\n\u003cp\u003eYou can follow this \u003ca href=\"https://form.jotform.com/tomasonimattia/monet-installation\" rel=\"nofollow\"\u003esurvey-tutorial\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit will guide you step by step (assumes no prior knowledge)\u003c/li\u003e\n\u003cli\u003e(optionally) guides you through running some examples (feel free to skip those)\u003c/li\u003e\n\u003cli\u003eit will help us collect information about possible errors on different platforms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-are-on-windows\" class=\"anchor\" href=\"#if-you-are-on-windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU ARE ON WINDOWS\u003c/h4\u003e\n\u003cp\u003eUsers using Windows are encouraged to install a hypervisor (i.e., a software that allows to creates and run virtual machines): for example, install VirtualBox \u003ca href=\"https://www.virtualbox.org/wiki/Downloads\" rel=\"nofollow\"\u003ehttps://www.virtualbox.org/wiki/Downloads\u003c/a\u003e and configure it up to run a virtual Ubuntu Linux inside which to install MONET (using the instructions above).\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-are-a-singularity-user-without-sudo-rights\" class=\"anchor\" href=\"#if-you-are-a-singularity-user-without-sudo-rights\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU ARE A SINGULARITY USER WITHOUT SUDO RIGHTS\u003c/h4\u003e\n\u003cp\u003eSudo rights will be required at installation time for Singularity users: Singularity users will not need sudo rights while running MONET (i.e., Singularity does not require sudo right to run containers), but they will need it at installation time (i.e., at the time the Singularity images are first created).\u003c/p\u003e\n\u003cp\u003eUsers that don\u0027t have sudo rights should follow the regular installation procedure explained above, then refer to MONET/docs/installation_no_sudo.txt where they will find a workaround to complete the installation manually without needing sudo.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing-the-installation\" class=\"anchor\" href=\"#testing-the-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTESTING THE INSTALLATION\u003c/h2\u003e\n\u003cp\u003eAt the end of the install process, you will be asked whether you want to test MONET. This test is completely automatic.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-monet-help-command\" class=\"anchor\" href=\"#monet-help-command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMONET HELP COMMAND\u003c/h2\u003e\n\u003cp\u003eAfter installing MONET, the help command \u003ccode\u003emonet --help\u003c/code\u003e will be available from any location on your system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRUNNING\u003c/h2\u003e\n\u003cp\u003eOnce installed, from any location on your system, you can run the following example command: it will run a method called M1 (see section METHODS for details), on a network contained in your /tmp folder (see section INPUT for details), using docker virtualization (see section PREREQUISITES for details). In the remainder of this document, you will find details about what parameters you can use, what to expect as an output and resource usage (in the PARAMETERS, OUTPUT and COMPUTATIONAL RESOURCES sections respectively).\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ monet --help\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ monet --input=/tmp/input_network.txt \u2014-method=M1 --container=docker\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eINPUT\u003c/h2\u003e\n\u003cp\u003eThe input file is provided to MONET using the \u003ccode\u003e--input\u003c/code\u003e parameter (see section RUNNING and section PARAMETERS).\u003c/p\u003e\n\u003cp\u003eThe format for the input network is the following: a \u003cstrong\u003etab-separated\u003c/strong\u003e file containing one line for each edge.\u003c/p\u003e\n\u003cp\u003eIf an edge is connecting two nodes, gene_a and gene_b, with a certain weight, the file will contain the line:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egene_a \\t gene_b \\t weight \\n\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eDetails:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egene_a and gene_b, the gene ids, can be either \u003cem\u003estring\u003c/em\u003e or \u003cem\u003einteger\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003eweight can be of type \u003cem\u003einteger\u003c/em\u003e or \u003cem\u003efloat\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003e\"\\t\" indicates the tab character and \"\\n\" the newline character\u003c/li\u003e\n\u003cli\u003eno blank spaces should appear, neither as separators nor as part of the gene ids\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor an example, see MONET/.test/system_test/input/zachary_karate_club.txt. The same folder containing the actual inputs to the Disease Module Identification (DMI) DREAM Challenge. Beware that some of the inputs will require high amounts of computational resources and are not suited to be run on a simple laptop or desktop computer; please refer to section COMPUTATIONAL RESOURCES for details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOUTPUT\u003c/h2\u003e\n\u003cp\u003eThe output location is provided to MONET using the \u003ccode\u003e--output\u003c/code\u003e parameter (see section OPTIONAL PARAMETERS).\u003c/p\u003e\n\u003cp\u003eTwo output files will be generated in the directory where you run the command. They are marked with a timestamp, the name of the selected method and the name of your input network. For example, let\u0027s assume if you run M1 on 1st January 2020 at midday on a file called input_network.txt:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea \u003cstrong\u003econsole-output\u003c/strong\u003e file, which will contain the run-time outputs generated by the method you have selected, providing details about the steps that the M1 algorithm took to generate your output. Any errors would also be redirected here. The file would be called: \u003ccode\u003e2020-01-01-120000__M1__console-output__input_network.txt\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ea \u003cstrong\u003eresult-modules\u003c/strong\u003e file, containing the results of your analysis and it will not be generated in case of errors. The file would be called: \u003ccode\u003e2020-01-01-120000__M1__result-modules__input_network.txt\u003c/code\u003e. It will be in tab-separated format, containing one module per line:\n\u003cul\u003e\n\u003cli\u003ethe first value of each line will be a module identifier (in the form of an integer number starting from 1)\u003c/li\u003e\n\u003cli\u003ethe second is a fixed numerical value and can be ignored (curerntly set to \u003ccode\u003e1.0\u003c/code\u003e, it was originally used in the DREAM Challenge to provide module-level confidence scores)\u003c/li\u003e\n\u003cli\u003ethe rest of the values on the line will be the gene ids container in the input (like gene_a and gene_b, see section INPUT)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-methods\" class=\"anchor\" href=\"#methods\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMETHODS\u003c/h2\u003e\n\u003cp\u003eThree methods are available as part of MONET, which emerged as the top-performing methods of the DREAM Challenge.\u003c/p\u003e\n\u003cp\u003eIn order to run one of the three methods, adapt the example command provided in section RUNNING providing the --method option with the name of the chosen method (--method=[K1|M1|R1], for details, see section PARAMETERS).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eK1\u003c/strong\u003e: KERNEL CLUSTERING OPTIMISATION algorithm. K1 is based on the \u201cDiffusion State Distance\u201d (DSD), a novel graph metric which is built on the premise that paths through low-degree nodes are stronger indications of functional similarity than paths that traverse high degree nodes by Cao et al. (2014). The DSD metric is used to define a pairwise distance matrix between all nodes, on which a spectral clustering algorithm is applied. In parallel, dense bipartite sub-graphs are identified using standard graph techniques. Finally, results are merged into a single set of non-overlapping clusters. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7349492/wiki/407359\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7349492/wiki/407359\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eM1\u003c/strong\u003e: MODULARITY OPTIMIZATION algorithm. M1 employs an original technique named Multiresolution introduced by (Arenas et al., 2008) to explore all topological scales at which modules may be found. The novelty of this approach relies on the introduction of a parameter, called resistance, which controls the aversion of nodes to form modules. Modularity (Newman and Girvan, 2004; Arenas et al., 2007) is optimized using an ensemble of algorithms: Extremal optimization (Duch and Arenas, 2005), Spectral optimization (Newman, 2006), Fast algorithm (Newman, 2004), Tabu search (Arenas et al., 2008), and fine-tuning by iterative repositioning of individual nodes in adjacent modules. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7352969/wiki/407384\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7352969/wiki/407384\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eR1\u003c/strong\u003e: RANDOM-WALK-BASED algorithm. R1 is based on a variant of Markov Cluster Algorithm known as balanced Multi-layer Regularized Markov Cluster Algorithm(bMLRMCL) (Satuluriet al., 2010) which scales well to large graphs and minimizes the number of oversized clusters. First, a pre-processing step is applied so that edges with low weights are discarded and all remaining edges are scaled to integer values. Then, bMLRMCL is applied iteratively on modules of size grater than a user-defined threshold. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7286597/wiki/406659\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7286597/wiki/406659\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePARAMETERS\u003c/h2\u003e\n\u003cp\u003ePlease, provide values for the following MANDATORY parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--input\u003c/strong\u003e: path to the network file to be analysed\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--method\u003c/strong\u003e: method to be used to analyse the input: [K1|M1|R1]\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--container\u003c/strong\u003e: virtualisation technology available on the system: [docker|singularity]\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-optional-parameters\" class=\"anchor\" href=\"#optional-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOPTIONAL PARAMETERS\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--output\u003c/strong\u003e: directory in which to output results (default is current directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select K1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--nclusters\u003c/strong\u003e: initial number of output clusters for spectral clustering step; final number may differ (default is 100)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select M1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--smallest\u003c/strong\u003e: min size of output clusters (default is 3)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--largest\u003c/strong\u003e: max size of output clusters (default is 100)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--linksdir\u003c/strong\u003e: directionality of links: [undirected|directed] (default is undirected)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--avgk\u003c/strong\u003e: desired average degree for nodes in output (default is 25)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select R1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--smallest\u003c/strong\u003e: min size of output clusters (default is 3)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--largest\u003c/strong\u003e: max size of output clusters (default is 100)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--c\u003c/strong\u003e: trade-off parameter for computational efficiency; for larger c, the algorithm will run slower, but may provide more accurate results (default is 800)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--i\u003c/strong\u003e: inflation parameter for standard Markov Clustering algorithm on which R1 is based (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--b\u003c/strong\u003e: parameter controlling how balanced the clustering results should be; for b=0, R1 behaves like standard Regularized Markov Cluster (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--threshold\u003c/strong\u003e: remove edges smaller than threshold from the input (default is 4)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--post\u003c/strong\u003e: decide whether to recursively cluster (recluster) or discard too large output clusters: [recluster|discard] (default is discard)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--c2\u003c/strong\u003e: (only used if --post=recluster) sets --c for reclustering round (default is 500)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--i2\u003c/strong\u003e: (only used if --post=recluster) sets --i for reclustering round (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--b2\u003c/strong\u003e: (only used if --post=recluster) sets --b for reclustering round (default is 2)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-computational-resources\" class=\"anchor\" href=\"#computational-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCOMPUTATIONAL RESOURCES\u003c/h2\u003e\n\u003cp\u003eSome of the methods require large amount of resources, depending on your input (please, refer to the MONET paper in the PUBLICATIONS section for details about how resource needs will scale with the size of the input, for the different methods).\u003c/p\u003e\n\u003cp\u003eTo reproduce the results of the DREAM Challenge, you can run MONET/.test/system_test/reproduce_challenge/reproduce_challenge.sh. This might fail on commodity hardware (i.e., a regular laptop or desktop) as about 8GB or RAM need to be available. In that case, you can allocate a larger SWAP partition (on Linux) or run the experiment on more powerful hardware, such as a server. Please browser the rest of the contents of MONET/.test/system_test/reproduce_challenge to view the exact RAM usage (ram_usage.txt) and the challenge outputs produced by MONET (disease_modules_output directory).\u003c/p\u003e\n\u003cp\u003eTo monitor resource usage when running on your own input (and thus determine the amount or RAM / swap needed by your particular input network for a particular method), two simple scripts have been added to MONET/.test/helper_scripts (for Unix and one for MacOS systems): launch them before execution of MONET and redirect their output to file for simple inspection (no other task should be running).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-benchmarking\" class=\"anchor\" href=\"#benchmarking\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBENCHMARKING\u003c/h2\u003e\n\u003cp\u003eFor details about the modularization performance of the MONET methods on a set of artificial benchmarks (Louvain algorithm is shown as a baseline), please refer to the MONET paper in the PUBLICATIONS section; in particular, Fig. 1. MONET/.test/benchmarking for a detailed output of the experiments that have been carried out.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-source-code\" class=\"anchor\" href=\"#source-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSOURCE CODE\u003c/h2\u003e\n\u003cp\u003eThe source code is hosted at: \u003ca href=\"https://github.com/BergmannLab/MONET.git\"\u003ehttps://github.com/BergmannLab/MONET.git\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCONTRIBUTING\u003c/h2\u003e\n\u003cp\u003eIf you are interested in contributing to MONET, we encourage you to get in touch! We will be happy to add you to the list of our developers \u003ca href=\"https://github.com/BergmannLab/MONET/graphs/contributors\"\u003ehttps://github.com/BergmannLab/MONET/graphs/contributors\u003c/a\u003e. \u003cstrong\u003eTHANK YOU!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - CREATING A BRANCH\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFirst, we will create an issue for the specific feature you are willing to contribute; let\u0027s say yours will happen to be issue 999. You will be then asked to create a new git branch where to implement your changes; run the following from the cloned MONET directory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git checkout -b issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git push origin issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAt this point, you are free to make changes to your local code in your laptop. Don\u0027t worry if you mess things up, it\u0027s no problem to add mistakes to a branch.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - TESTING YOUR CHANGES\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce you are done with your changes, you can test them locally by \u003cstrong\u003ereinstalling\u003c/strong\u003e from the modified MONET directory.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - PUBLISHING YOUR CHANGES\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce you have tested your changes, run the following from the cloned MONET directory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git add .\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git commit -m \"adding code for feature # issues_999\"\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git push --set-upstream origin issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git checkout master\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOne of the MONET developers will test the changes in your branch then merge to Master.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-implementing-local-changes-to-monet\" class=\"anchor\" href=\"#implementing-local-changes-to-monet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIMPLEMENTING LOCAL CHANGES TO MONET\u003c/h2\u003e\n\u003cp\u003eIf you wish to implement local changes to MONET, independently from our github repository, you can simply modify the code in your local cloned repository and \u003cstrong\u003ereinstall\u003c/strong\u003e after having made those changes (i.e. run or re-run the \u003ccode\u003einstall.sh\u003c/code\u003e script and confirm if you are asked to reinstall). This procedure can be repeated as many times as you like.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting-common-problems\" class=\"anchor\" href=\"#troubleshooting-common-problems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTROUBLESHOOTING COMMON PROBLEMS\u003c/h2\u003e\n\u003cp\u003eIf a MONET run is suddenly interrupted or if the expected outputs has not been generated, here are few common problems that can occur:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elack of RAM: if the console-output file (see section OUTPUT) contains the word \"Killed\", the MONET processed were stopped by the Operating System, likely due to a lack of RAM. To confirm this, please read section COMPUTATIONAL RESOURCES to learn how to monitor your resource usage while running MONET.\u003c/li\u003e\n\u003cli\u003eoutdated kernel: Singularity users that work on Linux distributions with old kernels (e.g. CentOS 6.1, kernel 2.6) will encounter trouble during the install process; they need to contact their system administrator to inquire whether a kernel upgrade is possible.\u003c/li\u003e\n\u003cli\u003ecan\u0027t implement local changes: please, refer to section IMPLEMENTING LOCAL CHANGES TO MONET.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bug-reports\" class=\"anchor\" href=\"#bug-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBUG-REPORTS\u003c/h2\u003e\n\u003cp\u003ePlease, address your questions and bug reports to Mattia Tomasoni, \u0026lt;mattia.tomasoni AT unil.ch\u0026gt;. An issue will be opened here to address your problem: \u003ca href=\"https://github.com/BergmannLab/MONET/issues\"\u003ehttps://github.com/BergmannLab/MONET/issues\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publications\" class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePUBLICATIONS\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMONET paper\u003c/strong\u003e: Mattia Tomasoni, Sergio G\u00f3mez, Jake Crawford, Weijia Zhang, Sarvenaz Choobdar, Daniel Marbach and Sven Bergmann. MONET: a toolbox integrating top-performing methods for network modularization. Bioinformatics 36 (12), 3920-3921. doi: \u003ca href=\"https://doi.org/10.1093/bioinformatics/btaa236\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/bioinformatics/btaa236\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDREAM Challenge paper\u003c/strong\u003e: Sarvenaz Choobdar, Mehmet Ahsen, Jake Crawford, Mattia Tomasoni, Tao Fang, David Lamparter, Junyuan Lin, Benjamin Hescott, Xiaozhe Hu, Johnathan Mercer, Ted Natoli, Rajiv Narayan, The DREAM Module Identification Challenge Consortium, Aravind Subramanian, Jitao David Zhang, Gustavo Stolovitzky, Zolt\u00e1n Kutalik, Kasper Lage, Donna Slonim, Julio Saez-Rodriguez, Lenore Cowen, Sven Bergmann, Daniel Marbach. Assessment of network module identification across complex diseases. Nature Methods 16 (2019) 843-852. doi: \u003ca href=\"https://doi.org/10.1038/s41592-019-0509-5\" rel=\"nofollow\"\u003ehttps://doi.org/10.1038/s41592-019-0509-5\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 22,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1626969756.0
  },
  {
    "data_format": 2,
    "description": "Splice junction analysis and filtering from BAM files",
    "filenames": [
      "Singularity"
    ],
    "full_name": "maplesond/portcullis",
    "latest_release": "1.2.2",
    "readme": "\u003cp\u003e\u003ca href=\"doc/source/images/portcullis_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"doc/source/images/portcullis_logo.png\" alt=\"alt text\" title=\"Portcullis\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-portcullis\" class=\"anchor\" href=\"#portcullis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePortcullis\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/maplesond/portcullis/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b9c31b04d2671e6317cdfd9e4fdf893512936091302d1b1b56c99cb89ab43df7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f6d61706c65736f6e642f706f727463756c6c69732e737667\" alt=\"Version\" data-canonical-src=\"https://img.shields.io/github/tag/maplesond/portcullis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://jenkins.sdlmapleson.net/job/portcullis/job/develop/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f696e3e0136cfb90f0e05f4f4e0a257ece7cd1e52ff19a0c8963b32df756d3a7/68747470733a2f2f6a656e6b696e732e73646c6d61706c65736f6e2e6e65742f6275696c645374617475732f69636f6e3f6a6f623d706f727463756c6c6973253246646576656c6f70\" alt=\"Build Status\" data-canonical-src=\"https://jenkins.sdlmapleson.net/buildStatus/icon?job=portcullis%2Fdevelop\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/maplesond/portcullis/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ad4d6f3e16da4f0dddcd142fa3b6088042b13242787f5ad939d2db28282d3eb5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d627269676874677265656e2e737667\" alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/maplesond/portcullis/issues\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d3bedf8e24750956939d66108f9ba197e72b83d1de8fc7305708ab2d67c20c17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6d61706c65736f6e642f706f727463756c6c69732e737667\" alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues-raw/maplesond/portcullis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePortcullis stands for PORTable CULLing of Invalid Splice junctions from pre-aligned RNA-seq data.  It is known that RNAseq mapping tools generate many invalid junction predictions, particularly in deep datasets with high coverage over splice sites.  In order to address this, instead for creating a new RNAseq mapper, with a focus on SJ accuracy we created a tool that takes in a BAM file generated by an RNAseq mapper of the user\u0027s own choice (e.g. Tophat2, Gsnap, STAR2 or HISAT2) as input (i.e. it\u0027s portable).  It then, analyses and quantifies all splice junctions in the file before, filtering (culling) those which are unlikely to be genuine.  Portcullis output\u0027s junctions in a variety of formats making it suitable for downstream analysis (such as differential splicing analysis and gene modelling) without additional work.  Portcullis can also filter the original BAM file removing alignments associated with \u003cem\u003ebad\u003c/em\u003e junctions.  Both the filtered junctions and BAM files are cleaner and more usable resources which can more effectively be used to assist in downstream analyses such as gene prediction and genome annotation.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eWe support multiple methods for installing and running portcullis.  Hopefully your favourite container or package manager is supported below.  If not let us know and we\u0027ll try to work to get it integrated there.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDocker\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/maplesond/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/de966674ebe7a3dec2fed423683dd2c64e3630527fab6a691add53421292e384/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d61706c65736f6e642f706f727463756c6c69732e737667\" alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/maplesond/portcullis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Keep in mind you need to mount in any working directories to the container with the `-v` option.\n# Ideally, mount these into the /data directory which is the container\u0027s working directory.\ndocker run --it --rm -v /abspath/to/data/on/host:/data maplesond/portcullis:stable portcullis --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# First download the container:\nsingularity pull --name portcullis.img shub://maplesond/portcullis:master\n\n# Then to execute commands in the container:\nsingularity exec portcullis.img portcullis --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eConda\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/381a7739b713a2bae02343a6ac934de39148a7866dbf4e52b597391b2a07fd4b/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f6c61746573745f72656c656173655f646174652e737667\" alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/latest_release_date.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/77a7c650d2675de3588df907d8e8aec11957abc95bcfd87d3b1b07f78a2bc4ec/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f706c6174666f726d732e737667\" alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/platforms.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/83781f462972e76ba4f2d046533fd48deb7cb72a0512481ff304f79c51bc01e3/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f646f776e6c6f6164732e737667\" alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/downloads.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda install portcullis --channel=bioconda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eBrew\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebrew install brewsci/bio/portcullis\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eFrom source\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/maplesond/portcullis/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3885a69f4777ec0c98cf3d0bee17eb7ca3d3eb69bbf850df2f36895b80168ade/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d61706c65736f6e642f706f727463756c6c69732f746f74616c2e737667\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/github/downloads/maplesond/portcullis/total.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you wish to install from source please first confirm that first you have these dependencies are installed and configured:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eGCC\u003c/strong\u003e V4.8+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eautoconf\u003c/strong\u003e V2.53+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eautomake\u003c/strong\u003e V1.11+\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emake\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003elibtool\u003c/strong\u003e V2.4.2+\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ezlib-dev\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epthreads\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eboost-dev\u003c/strong\u003e V1.52+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003esamtools\u003c/strong\u003e V1.2+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePython3-dev\u003c/strong\u003e V3.5+ (Make sure the following packages are installed: \u003cem\u003epandas\u003c/em\u003e, \u003cem\u003ematplotlib\u003c/em\u003e, \u003cem\u003esetuptools\u003c/em\u003e, \u003cem\u003esphinx\u003c/em\u003e, \u003cem\u003etabulate\u003c/em\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen proceed with the following steps:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Clone the repo\ngit clone git@github.com:maplesond/portcullis.git\n\n# Move into repo directory\ncd portcullis\n\n# Generate configure script\n./autogen.sh\n\n# Confirm dependencies and generate makefiles\n# Adding --prefix \u0026lt;dir\u0026gt; will tell make install to put everything in a \n# particular directory.  Default is /usr/local.\n./configure\n\n# Compile (increasing -j will make it go faster!\nmake -j 2\n\n# Run some unit tests (you can increase -j here too)\nmake -j 2 check\n\n# Install to prefix dir\nmake install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCommon problems\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMany system python installations do not come with the C API immediately available, which prevents Portcullis from embedding python code.  We typically would recommend installing anaconda3 as this would include the latest version of python, all required python packages as well as the C API.  If you are running a debian system and the C libraries are not available by default and you wish to use the system python installation the you can install them using: \u003ccode\u003esudo apt-get install python-dev\u003c/code\u003e.  Also, if you have installed python to a custom location please verify that the \u003cem\u003ebin\u003c/em\u003e directors on the \u003cem\u003ePATH\u003c/em\u003e environment variable, and the lib (or lib64) directory is on the \u003cem\u003eLD_LIBRARY_PATH\u003c/em\u003e or \u003cem\u003eLD_RUN_PATH\u003c/em\u003e as appropriate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Portcullis is failing at the \u003ccode\u003e./autogen.sh\u003c/code\u003e step you will likely need to install autotools.  The following command should do this on MacOS: \u003ccode\u003ebrew install autoconf automake libtool\u003c/code\u003e.  On a debian system this can be done with: \u003ccode\u003esudo apt-get install autoconf automake libtool\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h2\u003e\n\u003cp\u003eAfter portcullis has been installed, the \u003ccode\u003eportcullis\u003c/code\u003e executable should be available.  Typing \u003ccode\u003eportcullis\u003c/code\u003e or \u003ccode\u003eportcullis --help\u003c/code\u003e at the command line will present you with the portcullis help message.\u003c/p\u003e\n\u003cp\u003eThese modes are available:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eprep\u003c/strong\u003e    - Prepares input data so that it is suitable for junction analysis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ejunc\u003c/strong\u003e    - Calculates junction metrics for the prepared data\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efilter\u003c/strong\u003e  - Separates alignments based on whether they are likely to represent genuine splice junctions or not\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebamfilt\u003c/strong\u003e - Filters a BAM to remove any reads associated with invalid junctions\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efull\u003c/strong\u003e    - Runs prep, junc, filter and optionally bamfilt as a complete pipeline\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTyping \u003ccode\u003eportcullis \u0026lt;mode\u0026gt; --help\u003c/code\u003e will bring up help and usage information specific to that mode.\u003c/p\u003e\n\u003cp\u003eIn addition to portcullis, we provide a tool-suite for manipulating junction files called junctools.  Typing \u003ccode\u003ejunctools --help\u003c/code\u003e will provide you with the program options.\u003c/p\u003e\n\u003cp\u003eFor much more information about portcullis\u0027 capabilities and how to configure and run it, an online version of the manual can be found here: \u003ca href=\"https://portcullis.readthedocs.org/en/latest/\" rel=\"nofollow\"\u003ehttps://portcullis.readthedocs.org/en/latest/\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicensing\u003c/h2\u003e\n\u003cp\u003eGNU GPL V3.  See COPYING file for more details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors\" class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDaniel Mapleson\u003c/li\u003e\n\u003cli\u003eLuca Venturini\u003c/li\u003e\n\u003cli\u003eDavid Swarbreck\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee AUTHORS file for more details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgements\u003c/h2\u003e\n\u003cp\u003eAffiliation: The Earlham Institute (EI)\nFunding: The Biotechnology and Biological Sciences Research Council (BBSRC)\u003c/p\u003e\n",
    "stargazers_count": 23,
    "subscribers_count": 3,
    "topics": [
      "portcullis",
      "junction",
      "splice-junctions",
      "bam-files",
      "filter"
    ],
    "updated_at": 1620067152.0
  },
  {
    "data_format": 2,
    "description": "gemBS is a bioinformatics pipeline designed for high throughput analysis of DNA methylation from Whole Genome Bisulfite Sequencing data (WGBS).",
    "filenames": [
      "Singularity",
      "IHEC/Singularity.ihec"
    ],
    "full_name": "heathsc/gemBS",
    "latest_release": "v3.5.1_IHEC",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-news\" class=\"anchor\" href=\"#news\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNews\u003c/h1\u003e\n\u003cp\u003eFirst release of gemBS-rs, a complete rewrite of the gemBS pipeline (apart from the mapper) in Rust bringing increased\nstability while maintaining the high performance of gemBS: \u003ca href=\"https://github.com/heathsc/gemBS-rs.git\"\u003ehttps://github.com/heathsc/gemBS-rs.git\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-gembs\" class=\"anchor\" href=\"#gembs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003egemBS\u003c/h1\u003e\n\u003cp\u003egemBS is a high performance bioinformatic pipeline designed for highthroughput analysis\nof DNA methylation data from whole genome bisulfites sequencing data\n(WGBS). It combines GEM3, a high performance read aligner and\nbs_call, a high performance variant and methyation caller, into a streamlined and efficient pipeline for\nbisulfite sueqnce analysis.\u003c/p\u003e\n\u003cp\u003eThe manuscript describing the pipeline is available \u003ca href=\"https://www.biorxiv.org/content/early/2017/10/11/201988\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicensing\u003c/h2\u003e\n\u003cp\u003egemBS is licensed under GPL.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download\" class=\"anchor\" href=\"#download\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload\u003c/h2\u003e\n\u003cp\u003eUse \u003ccode\u003egit clone --recursive\u003c/code\u003e to retrieve the complete source code including the code from external projects such as \u003ccode\u003ebs_call\u003c/code\u003e and \u003ccode\u003egem3-mapper\u003c/code\u003e.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone --recursive https://github.com/heathsc/gemBS.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eBefore starting the installation of gemBS, you will need to install\nor check the installation of several packages.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ea) gcc with development libraries\nb) python3, pip3, matplotlib, multiprocess\nc) zlib, lzma, openssl, libcurl, libncurses, wget, pigz\u003c/p\u003e\n\u003cp\u003eIf you are working on a clean (fairly recent) Ubuntu installation, you\ncan install everything required with the followiwg commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get update\nsudo apt-get install -y python3 build-essential git python3-pip wget pigz\nsudo apt-get install -y zlib1g-dev libbz2-dev\nsudo apt-get install -y libncurses5-dev liblzma-dev libssl-dev libcurl4-openssl-dev\npip3 install matplotlib multiprocess\n\u003c/code\u003e\u003c/pre\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003e\n\u003cp\u003eDownload the gemBS distribution if you haven\u0027t already done so:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egit clone --recursive https://github.com/heathsc/gemBS.git\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse python install command:\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo install to the standard system location (i.e., so that all users\ncan use gemBS):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e``python3 setup.py install``\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo install to the user\u0027s home directory:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e``python3 setup.py install --user``\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-check-your-installation\" class=\"anchor\" href=\"#check-your-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCheck your installation\u003c/h2\u003e\n\u003cp\u003eFor checking your installation follow this\n\u003ca href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/example.html\" rel=\"nofollow\"\u003eworked example\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eDocumentation can be found at\n\u003ca href=\"http://statgen.cnag.cat/gemBS/v3/UserGuide/_build/html/index.html\" rel=\"nofollow\"\u003egemBS\u003c/a\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-changelog\" class=\"anchor\" href=\"#changelog\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChangelog:\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e3.5.5 Fix logging bug caused by trimming change in 3.5.3\n3.5.4 Fix bug in the output of strand specific cpg txt files (not\n      encode Bed files) where the \u0027C\u0027 entry was not being printed\n3.5.3 Allow for read end specific trimming in bs_call\n3.5.3 Enable range checks and asserts in bs_call all target; add bs_call debug target\n3.5.2 Correct problems with gcc10.  Move to htslib/samtools/bcftools version 1.11\n3.5.1 Check if C compiler requires --std=c99 flag for standards conformant behaviour\n3.5.1 Make sure bgzip is copied correctly during installation\n3.5.0 Make bs_call process contig pools from largest to smallest (this change alters the sqlite db format so\n      if you have a previously started gemBS run you should (a) remove the .gemBS directory, (b) redo the\n      \u0027gemBS prepare\u0027 step to recreate the db file and (3) run \u0027gemBS db-sync\u0027. \n3.5.0 Switch bs_call and snpxtr to use the new dbSNP index format\n3.5.0 Add ability of dbSNP to read the new JSON and VCF  dbSNP format files\n      that are now used for human and non-human species respectively\n3.5.0 Add multithreading to dbSNP_idx\n3.5.0 Change format of dbSNP index to allow (a) efficient loading\n      of SNP data for individual contigs and (b) parallel index creation \n3.5.0 Rewrite mextr and snpxtr as standalone tools rather than\n      bcftools plugins.  Now multithreaded and (relatively) memoryefficient\n3.5.0 Replace bedToBigBed and wigToBigWig to reduce memory usage\n      and improve speed\n3.4.5 Fix crash when using the -k (keep-mismatch) flag, and fix rare hangs at end of processing\n3.4.4 Sort input bcf files to bcftools concat stage to ensure reproducibility.\n3.4.4 Add extra sort keys when generating pools to ensure stability of pool membership in the event of multiple contigs\n      having the same size\n3.4.3 Remove calculation of the goodness of filter (GOF) as this is expensive, non-standard and unreliable.  Removing this\n      removes the dependency on GSL.\n3.4.3 Add autodetection of output format to bs_call (unless explicitly specified on the command line)\n3.4.2 Add CRAM support (via make_cram option in configuration file)\n3.4.1 Add benchmark-mode that does not write date or program version numbers into SAM/BAM or VCF/BCF files\n      Switch to samtools, bcftools and htslib v1.10\n3.4.0 Move to new bs_call version (2.1.0) which is more efficient\n      in memory use and can read BAMs and write BCFs natively.\n      The new bs_call requires a faidx indexed reference, so gemBS\n      no creates this during indexing.\n3.4.0 Add switches to give more control to threads and memory\n      usage in mapping and calling stages\n3.3.3 Remove legacy pathway for config files with no header line (fix issue \u0027error in gemBS index #65)\n3.3.2 Fix error where header line for wig files could be omitted\n3.3.2 Fix generation of non_cpg files\n3.3.1 Fix Attribute error bug due to not checking if conversion is a list\n3.3.0 Make new release for IHEC\n3.3.0 Switch conversion default in IHEC_standard configuration to 0.01,0.05 rather than auto, which can give odd results if conversion controls not present or not working correctly\n3.3.0 Fix bug where conversion parameters could be ignored\n3.2.13 Fix formatting bug in mextr with multiple samples (not triggered in normal gemBS use)\n3.2.12 Ensure that conversion statistics are correctly calculated for non-stranded or reverse conversion protocols\n3.2.11 Introduce reverse_conversion option for mapping where read 1 is G2A converted and read 2 is C2T converted\n3.2.10 Correct regex patch for single end reads\n3.2.9 Update Singularity and Dockerfile recipes to allow kemp utils to be built correctly\n3.2.9 Make setup.py and gemBS/commands.py read the version information from gemBS/version.py, so ensuring consistency\n3.2.9 Fix bug added in last version where options in config file were not being taken into account\n3.2.8 Fix mis specification errors in long options for mextr. \n3.2.8 Fix bug where mextr (methyl extract plugin for bcftools) would crash if cpg output  option was not set.\n3.2.7 Apply patches for bugs in handling single end reads (suggested by I. Moghul)\n3.2.7 Changed regex for filenames to make it more general (suggested by I. Moghul)\n3.2.7 Fixed bug (reported by chhylp123) where zero arguments to some options were being ignored\n3.2.6 Cleaned up compilation and cleaning of gemBS tools\n3.2.6 Fixed python error if either the over conversion reference sequence was not defined\n3.2.6 Added check in bs_call that conversion parameters are valid (between 0 and 1)\n3.2.6 Perform more stringent sanity checking on conversion vaalues when autocomputed by gemBS\n3.2.6 Use --diasble-lzma configuration flag for samtools and bcftools as we don\u0027t need it and it removes an unneccesary dependency\n3.2.6 Add install options --disable-cuda (on by default) and --enable-cuda that affect GEM3 comppilation\n3.2.6 Bug fix with incorrect handling of duplicate reads\n3.2.5 Minor bug fix - correct error with non-paired end non-bisulfite reads\n3.2.4 Modify the bisulfite processing in gem-mapper to be more efficient (in particular for the non-stranded option)\n3.2.4 Modify gemBS to use the new conversion options for gem-mapper\n3.2.4 Switch gem-mapper to use option --underconversion-sequence and --overconversion-sequence rather than --underconversion_sequence to be consistent with other options\n3.2.3 Fixed bug if conversion parameters were not set\n3.2.2 Rework non-stranded mode so that both possible conversions are tried and the results merged\n3.2.2 Fix bug where non-stranded flag was not being passed to mapper in paired end mode\n3.2.1 Move warning message from bscall from stdout to stderr\n3.2.1 Switch Singularity build to use Ubuntu 16.04 rather than 18.04 to allow the image to work in CentOS 6 (Docker build changed as well to keep the two in sync)\n3.2.1 Fix undeclared variable bugs and missing --ignore-deps option in merge-bcfs\n3.2.1 Add default for dbSNP_index if dbSNP_files is set\n3.2.1 Add gsl-path install option\n3.2.0 Make new release\n3.1.0 Make installation process more modular.  Allow for sub-installs\n3.1.0 Add support for reading config from ${index_dir}/gemBS.json if it exists\n3.1.0 Add --reference-bias option to mextr and gemBS extract\n3.1.0 Add support for non-bisulfite mapping of individual datasets\n3.1.0 Allow white space in variable values\n3.1.0 Allow fallback to gzip if pigz not present\n3.1.0 Add --dry-run, --json, --ignore-db and --ignore-dep to extract command\n3.1.0 Add --ignore-dep option to call and merge-bcfs commands\n3.1.0 Add SNP extraction function to extract command\n3.0 Make v3.0 release\n3.0 Merge with master branch.\n3.0 Bump samtools sort memory limit to 2G\n3.0 Add extra_references option for reference generation\n3.0 Allow input files to mapping to be shell commands\n3.0 Add links to documentation\n3.0 Upload new yeast example and add documentation\n3.0 Add --dir option to gemBS\n3.0 Add --ignore-db options for --dry-run / --json\n3.0 Add --json output option for dry runs\n3.0 Update help text to match new functions\n3.0 Introduce standard analysis configurations stored within distribution\n3.0 Switch gem3-mapper distribution to gembs branch on official gem3-mapper repo\n3.0 Removal of incomplete files and roll back of db in the event of pipeline failure\n3.0 Automatic removal of individual BAMs and BCFs after successful merging\n3.0 Prevent pipelines hanging in event of failure\n3.0 Generate ENCODE bed and bigbed files\n3.0 Switch to python 3\n3.0 Switch to mextr for BCF filtering\n3.0 Include fetch and build of samtools / bcftools during build process\n3.0 Add dry-run capability to map and call commands\n3.0 Introduce contig pools to automatically group small contigs\n3.0 Automatic generation of contig.size files from index build\n3.0 Allow use of in memory sqlite3 db as an option\n3.0 Allow multiple instances of gemBS (possible on different hosts) to work \n    simultaneously on the same analysis\n3.0 Reduce and simply commands\n3.0 Add Dockerfile\n3.0 Add multi-threading and multi-processing options for most commands\n3.0 Use sqlite3 to track progress of analyses, file paths etc.\n3.0 Added more flexible configuration options (new csv format + new configuration file)\n3.0 Remove test dataset from distribution (distribute from web site)\n2.1.0 Ensure commands run during pipeline come from installation\n2.1.0 Added Singularity build recipe\n2.1.0 Add new command gemBS direct-mapping\n2.1.0 Fixed Makefile clean in tools\n2.0.2 Fixed bug related with the percentage of High Quality Variant in Variants summary report.\n2.0.2 Check temporary directory existence.\n2.0.2 Fixed QualityNonRefCpg sample name in png image.\n2.0.2 Fixed mapper issues related with aligning performace.\n2.0.2 Fixed arguments for Under/Over Conversion sequence name in gem3-mapper\n2.0.1 On bscall repository, fixed argument -k about discarded reads that do not form proper pairs.\n2.0 Check tmp folder before starting mapping process.\n2.0 Added Left and Right Trimming optional arguments to gemBS bscall.\n2.0 Added GC Coverage correlation value to BS Call Stats Summary.\n2.0 Fixed error when reporting complete path to not found bam files.\n2.0 Fixed iteration over sampleBams dictionary in MergeAll method.\n2.0 Updated: Avoid redo indexing when merging just one file.\n2.0 Changed conversion formula.\n2.0 Added parameter for dbSNP.\n2.0 Added threads to bscall.\n2.0 Removed CpGs reports. Already done from bscall report.\n2.0 Fixed bs_call makefile for the gcc to be used.\n2.0 New bscall version. Generates JSON report.\n2.0 Removed gemBS options snp-stats,cpg-report,cpg-stats.\n2.0 Added summary report from the bs_call json stats\n2.0 New BSCall Report. From bscall son file generates three types of reports:\n    Mapping and Coverage Report\n    Bs-Genotypes Calls Report\n    Methylation Statistics report\n1.7 Added non stranded read conversion parameter\n1.7 Fixed SE crash when estimating overlapped bases.\n1.7 Fixed gem-index (gem3) to follow fastq and SAM specifications. \n    Modified gem3-mapper repository external module.\n    New external module https://github.com/heathsc/gem3-mapper.git\n1.7 Fixed threads parameter to samtools merge\n1.7 Fixed threads parameter to gem-mapper\n1.7 Removed Indels Field on Variants Report.\n1.7 Added Overlapping Bases at Mapping Report\n1.7 Modified Base Counts Overall, removed Base Counts general and Base Counts Overall\n1.7 New Dinucleotide CpGs Report\n    New table dinucleotide stats\n    New plots for Informative Reads and CpGs\n    Methylation levels plots for different types of CpGs\n    Summary Table\n1.7 New Readme file to inform about report test\n1.7 New basic statis table for Variants Report\n1.7 Removed parameter -r (reference length) parameter for mapping reports command (gemBS bsMap).\n1.6 New CpGs Density plot, include box plos, bar plot and fitting curve\n1.6 Change name at CpG report:\n    \"Heterozygous\" for \"Alternative CX\"\n    \"De Novo CpGs Methylation Status\" for \"Non Reference CpGs\"\n    \"CpGs with SNP\" for \"SNPs (CX) at Reference CpGs\"\n1.6 CpGs Report Simplified to Q\u0026gt;20\n1.6 BigWig Default parameters for filtering CpG per a given quality and a total number of supported informative reads   \n1.5 Initial Release  \n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-developers\" class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopers\u003c/h2\u003e\n\u003cp\u003egemBS:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMarcos Fernandez-Callejo - \u003ca href=\"mailto:marcos.fernandez@cnag.crg.eu\"\u003emarcos.fernandez@cnag.crg.eu\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eSimon Heath - \u003ca href=\"mailto:simon.heath@gmail.com\"\u003esimon.heath@gmail.com\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003egem mapper:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSantiago Marco-Sola - \u003ca href=\"mailto:santiagomsola@gmail.com\"\u003esantiagomsola@gmail.com\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ebisulfite caller and filtering:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSimon Heath - \u003ca href=\"mailto:simon.heath@gmail.com\"\u003esimon.heath@gmail.com\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 24,
    "subscribers_count": 4,
    "topics": [],
    "updated_at": 1624379756.0
  },
  {
    "data_format": 2,
    "description": "Phigaro is a scalable command-line tool for predicting phages and prophages",
    "filenames": [
      "Singularity"
    ],
    "full_name": "bobeobibo/phigaro",
    "latest_release": "v2.2.6",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-phigaro-v230\" class=\"anchor\" href=\"#phigaro-v230\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePhigaro v2.3.0\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://badge.fury.io/py/phigaro\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b9bcb6f0446a21e8f918a9a8253f32a15df7cc3df72ced3bcd42d9f23bbc993b/68747470733a2f2f62616467652e667572792e696f2f70792f7068696761726f2e737667\" alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/phigaro.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/bfa41b5d9d74183c62a1e89d4718527f319c054e8090c55ce7837c88f19e5350/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f7068696761726f2f6261646765732f696e7374616c6c65722f636f6e64612e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bfa41b5d9d74183c62a1e89d4718527f319c054e8090c55ce7837c88f19e5350/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f7068696761726f2f6261646765732f696e7374616c6c65722f636f6e64612e737667\" alt=\"Conda installation\" data-canonical-src=\"https://anaconda.org/bioconda/phigaro/badges/installer/conda.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/bobeobibo/phigaro/workflows/Phigaro%20Tests/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/bobeobibo/phigaro/workflows/Phigaro%20Tests/badge.svg\" alt=\"Actions Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/5fab2edf3816ef9fb3ebcaf6e613fa7b40ff7652ec69e5f6e7f695aa24bf5ce6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5fab2edf3816ef9fb3ebcaf6e613fa7b40ff7652ec69e5f6e7f695aa24bf5ce6/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d626c75652e737667\" alt=\"License: MIT\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/psf/black\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d91ed7ac7abbd5a6102cbe988dd8e9ac21bde0a73d97be7603b891ad08ce3479/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f64652532307374796c652d626c61636b2d3030303030302e737667\" alt=\"Code style: black\" data-canonical-src=\"https://img.shields.io/badge/code%20style-black-000000.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePhigaro is a standalone command-line application that is able to detect prophage regions taking raw genome and metagenome assemblies as an input. It also produces dynamic annotated \u201cprophage genome maps\u201d and marks possible transposon insertion spots inside prophages. It is applicable for mining prophage regions from large metagenomic datasets.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-updates-tracker\" class=\"anchor\" href=\"#updates-tracker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUpdates tracker\u003c/h2\u003e\n\u003cp\u003eYou can find the information about updates and releases by \u003ca href=\"https://github.com/bobeobibo/phigaro/blob/master/version_tracker.md\"\u003elink.\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation-installation--usage\" class=\"anchor\" href=\"#documentation-installation--usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation: Installation \u0026amp; Usage\u003c/h2\u003e\n\u003cp\u003ePlease, follow \u003ca href=\"https://phigaro.readthedocs.io/\" rel=\"nofollow\"\u003ethe documentation link\u003c/a\u003e to find installation and usage information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-methods-overview\" class=\"anchor\" href=\"#methods-overview\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMethods overview\u003c/h2\u003e\n\u003cp\u003eOpen-reading frames (i.e. proteins) are predicted from the input FASTA file using Prodigal. Phage genes are annotated with prokaryotic viral orthologous groups (pVOGs) profile Hidden Markov Models (HMMs), which can be downloaded stand-alone from \u003ca href=\"http://dmk-brain.ecn.uiowa.edu/pVOGs/\" rel=\"nofollow\"\u003ehttp://dmk-brain.ecn.uiowa.edu/pVOGs/\u003c/a\u003e. Each contig is represented as a sequence of phage and non-phage genes. A smoothing window algorithm (a triangular window function) determines regions with a high density of phage genes and therefore the prophage regions and boundaries, considering the pVOG annotations and the GC content.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-known-issues\" class=\"anchor\" href=\"#known-issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown issues\u003c/h2\u003e\n\u003cp\u003ePhigaro is tested on Linux systems. For MacOS, you may need to add the following softlink \u003ccode\u003eln -s /usr/libexec/locate.updatedb /usr/local/bin/updated\u003c/code\u003e and run \u003ccode\u003ebrew install wget\u003c/code\u003e. If you encounter any issues while running Phigaro on test data, please report them to us at \u003ca href=\"mailto:estarikova@rcpcm.org\"\u003eestarikova@rcpcm.org\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publication\" class=\"anchor\" href=\"#publication\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublication\u003c/h2\u003e\n\u003cp\u003eElizaveta V. Starikova, Polina O. Tikhonova, Nikita A. Prianichnikov, Chris M. Rands, Evgeny M. Zdobnov, Vadim M. Govorun \u003cbr\u003ePhigaro: high throughput prophage sequence annotation\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBioinformatics, 2020; doi: \u003ca href=\"https://doi.org/10.1093/bioinformatics/btaa250\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/bioinformatics/btaa250\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ebioRxiv, 2019; doi: \u003ca href=\"https://doi.org/10.1101/598243\" rel=\"nofollow\"\u003ehttps://doi.org/10.1101/598243\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(c) E.Starikova, P. Tikhonova, N.Pryanichnikov, 2019\u003c/p\u003e\n",
    "stargazers_count": 25,
    "subscribers_count": 3,
    "topics": [
      "bioinformatics",
      "bioinformatics-tool",
      "bioinformatics-pipeline",
      "bioinformatics-analysis",
      "biological-data-analysis",
      "phage",
      "phages",
      "prophage",
      "genomics",
      "genomic-data-analysis",
      "genomic-regions"
    ],
    "updated_at": 1622668825.0
  },
  {
    "data_format": 2,
    "description": "Geant4 Example Application with Rich features and Small footprints",
    "filenames": [
      "INSTALL/Singularity"
    ],
    "full_name": "jintonic/gears",
    "latest_release": "v1.5.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://codedocs.xyz/jintonic/gears/annotated.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/69e499a9ac348bd92b62e3eac1391967c594141c2855f42783e17f07527553c4/68747470733a2f2f636f6465646f63732e78797a2f6a696e746f6e69632f67656172732e737667\" alt=\"Doxygen\" data-canonical-src=\"https://codedocs.xyz/jintonic/gears.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/78f47a09877ba9d28da1887a93e5c3bc2efb309c1e910eb21135becd2998238a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d4d49542d79656c6c6f772e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/License-MIT-yellow.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"examples\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/11e2b1b7847229fdbb3430cf4a5d908964287c64b59e0119936bce5359e4efb9/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f67656172732d6578616d706c65732d626c75653f7374796c653d666c6174\" alt=\"Examples\" data-canonical-src=\"https://img.shields.io/badge/gears-examples-blue?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"INSTALL\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/04a8d93a22dbf7fc5499c5c388432c900fac24a8e4061f7794fa5c2260199c46/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6765742d737461727465642d6f72616e67653f7374796c653d666c6174\" alt=\"Get Started\" data-canonical-src=\"https://img.shields.io/badge/get-started-orange?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"#how-to-contribute\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b02b9d3708d187a71e102cc95c8a8ad5fcdffb8241ba9b4a102a4272862f1216/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6765742d696e766f6c7665642d6666363962343f7374796c653d666c6174\" alt=\"Get Involved\" data-canonical-src=\"https://img.shields.io/badge/get-involved-ff69b4?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"examples/detector/visualization/gearsX3D.html\"\u003e\u003cimg align=\"right\" width=\"120px\" src=\"examples/detector/visualization/gears.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jintonic/gears\"\u003eGEARS\u003c/a\u003e is a \u003ca href=\"http://geant4.cern.ch\" rel=\"nofollow\"\u003eGeant4\u003c/a\u003e \u003ca href=\"http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Examples/examples.html\" rel=\"nofollow\"\u003eExample\u003c/a\u003e Application with \u003ca href=\"#features\"\u003eRich features\u003c/a\u003e yet Small footprint. The entire C++ coding is minimized down to a single file with about 550 \u003ca href=\"https://en.wikipedia.org/wiki/Source_lines_of_code\" rel=\"nofollow\"\u003eSLOC\u003c/a\u003e. This is achieved mainly by utilizing \u003ca href=\"http://geant4.cern.ch\" rel=\"nofollow\"\u003eGeant4\u003c/a\u003e plain \u003ca href=\"http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Detector/Geometry/geomASCII.html\" rel=\"nofollow\"\u003etext geometry description\u003c/a\u003e, \u003ca href=\"http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Control/commands.html\" rel=\"nofollow\"\u003ebuilt-in UI commands\u003c/a\u003e (macros), and C++ inheritance. It is ideal for student training and fast implementation of small to medium-sized experiments.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"gears.cc\"\u003eSingle small C++ file\u003c/a\u003e, easy to manage, fast to \u003ca href=\"INSTALL#compile-gears\"\u003ecompile\u003c/a\u003e (a few second on a regular PC)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/physics\"\u003eEasy switching between well maintained Geant4 reference physics lists without recompilation\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"examples/physics#physics-processes\"\u003eIndividual processes can be turned on/off without recompilation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"examples/physics#optical-properties-of-materials-and-surfaces\"\u003eFast implementation of optical properties without recompilation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/physics#radioactive-decay\"\u003eOptional radioactive decay simulation\u003c/a\u003e with the possibility to \u003ca href=\"examples/physics#split-decay-chain\"\u003esave the parent and daughter decays into different events if the later happens after a user specified time interval\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/sources#common-sources\"\u003eFrequently used source spectra (AmBe, Am-241, etc.)\u003c/a\u003e in addition to \u003ca href=\"examples/sources\"\u003eGPS\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/output\"\u003eOutput in multiple data format\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"examples/output#root\"\u003eROOT\u003c/a\u003e TTree format (default, no \u003ca href=\"https://root.cern.ch\" rel=\"nofollow\"\u003eROOT\u003c/a\u003e installation is needed)\n\u003cul\u003e\n\u003cli\u003eBuild-in data compression, well suitable for large data processing\u003c/li\u003e\n\u003cli\u003eFast access to independent data members\u003c/li\u003e\n\u003cli\u003eFlat tree (no nested branches or arrays) with short leaf names\n\u003cul\u003e\n\u003cli\u003eEasy to use in TTree::Draw\u003c/li\u003e\n\u003cli\u003eNo need to load extra library to open\u003c/li\u003e\n\u003cli\u003eCan be easily analyzed in \u003ca href=\"https://www.python.org/\" rel=\"nofollow\"\u003ePython\u003c/a\u003e through \u003ca href=\"https://github.com/scikit-hep/uproot4\"\u003eUproot\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.hdfgroup.org/downloads/hdf5/\" rel=\"nofollow\"\u003eHDF5\u003c/a\u003e, universal data format, easy to read by different tools\u003c/li\u003e\n\u003cli\u003eCSV or XML, Human readable ASCII file, capable of dealing with multiple dimensional arrays\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/output#record-information-of-step-0\"\u003eRecord information of step 0\u003c/a\u003e (initStep), which is not available through \u003ca href=\"http://www-geant4.kek.jp/lxr/source/tracking/include/G4UserSteppingAction.hh\" rel=\"nofollow\"\u003eG4UserSteppingAction\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Detector/Geometry/geomASCII.html\" rel=\"nofollow\"\u003esimple text\u003c/a\u003e or \u003ca href=\"https://gdml.web.cern.ch/GDML/\" rel=\"nofollow\"\u003eGDML\u003c/a\u003e geometry I/O\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"examples/detector\"\u003eFast implementation of detector geometry\u003c/a\u003e without C++ programming\u003c/li\u003e\n\u003cli\u003eCreate/Change geometry without re-compilation\u003c/li\u003e\n\u003cli\u003eTurn off data saving in a volume by assigning it a non-positive copy number\u003c/li\u003e\n\u003cli\u003eTurn any volume to a \u003ca href=\"examples/detector#sensitive-volume\"\u003esensitive detector\u003c/a\u003e by adding \"(S)\" in its name\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/detector/optical\"\u003eAssign optical properties in Geant4 plain text geometry description\u003c/a\u003e, which is not available in the official \u003ca href=\"http://geant4.cern.ch\" rel=\"nofollow\"\u003eGeant4\u003c/a\u003e release\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"examples/detector/syntax\"\u003eSyntax highlighting of the simple text geometry description files\u003c/a\u003e in \u003ca href=\"examples/detector/syntax#emacs\"\u003eEmacs\u003c/a\u003e, \u003ca href=\"examples/detector/syntax#vim\"\u003eVim\u003c/a\u003e, \u003ca href=\"examples/detector/syntax#micro\"\u003eMicro\u003c/a\u003e, and \u003ca href=\"examples/detector/syntax#sublime-text\"\u003eSublime Text\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://geant4-userdoc.web.cern.ch/geant4-userdoc/UsersGuides/ForApplicationDeveloper/html/Detector/commandScore.html\" rel=\"nofollow\"\u003eCreating 3D mesh to record and visualize physical variables in it without any change of the C++ code\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://codedocs.xyz/jintonic/gears/\" rel=\"nofollow\"\u003eDoxygen documentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eMany \u003ca href=\"examples\"\u003esample macros\u003c/a\u003e and \u003ca href=\"examples/detector\"\u003egeometry descriptions\u003c/a\u003e for feature demonstration\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-contribute\" class=\"anchor\" href=\"#how-to-contribute\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to contribute\u003c/h2\u003e\n\u003cp\u003ePlease \u003ca href=\"https://help.github.com/en/github/getting-started-with-github/fork-a-repo\"\u003efork GEARS on GitHub\u003c/a\u003e. Run the following to get a local copy of the forked repository and link it to the \u003ca href=\"https://github.com/jintonic/gears\"\u003eoriginal GEARS repository\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git clone git@github.com:yourGitHubAccount/gears.git \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e get forked repository\u003c/span\u003e\n$ git remote add upstream git@github.com:jintonic/gears.git \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e link to original repository\u003c/span\u003e\n$ git remote -v \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e run a check\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eRun the following to keep your local repository updated with the \u003ca href=\"https://github.com/jintonic/gears\"\u003eoriginal GEARS repository\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ git fetch upstream \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e updates are saved in a new branch upstream/master\u003c/span\u003e\n$ git merge upstream/master \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e merge 2 branches: upstream/master and master\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf the merge is successful, run \u003ccode\u003egit push\u003c/code\u003e to update your forked GEARS repository on GitHub.\u003c/p\u003e\n\u003cp\u003eYou can initiate a \u003ca href=\"https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests\"\u003epull request on GitHub\u003c/a\u003e if you\u0027d like to have your update being absorbed in \u003ca href=\"https://github.com/jintonic/gears\"\u003ethe original GEARS repository\u003c/a\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-coding-convention\" class=\"anchor\" href=\"#coding-convention\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCoding convention\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-g4cout-vs-stdcout\" class=\"anchor\" href=\"#g4cout-vs-stdcout\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eG4cout VS std::cout\u003c/h4\u003e\n\u003cp\u003e\u003ccode\u003eG4cout\u003c/code\u003e and \u003ccode\u003eG4endl\u003c/code\u003e is preferred over \u003ccode\u003estd:cout\u003c/code\u003e and \u003ccode\u003estd:endl\u003c/code\u003e because the former handle the output in \u003ca href=\"http://geant4.cern.ch\" rel=\"nofollow\"\u003eGeant4\u003c/a\u003e GUI correctly, while the later can only output to terminal.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-indentation\" class=\"anchor\" href=\"#indentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIndentation\u003c/h4\u003e\n\u003cp\u003eTwo spaces instead of a tab are used to indent a line in \u003ca href=\"gears.cc\"\u003egears.cc\u003c/a\u003e to insure a consistent appearance in different text editors, and to avoid wasting space in front of deeply nested code blocks. The following mode lines are added to the end of \u003ca href=\"gears.cc\"\u003egears.cc\u003c/a\u003e to insure that in \u003ca href=\"https://www.vim.org/\" rel=\"nofollow\"\u003eVim\u003c/a\u003e and \u003ca href=\"https://www.gnu.org/software/emacs/\" rel=\"nofollow\"\u003eEmacs\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-c++\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e//\u003c/span\u003e -*- C++; indent-tabs-mode:nil; tab-width:2 -*-\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e//\u003c/span\u003e vim: ft=cpp:ts=2:sts=2:sw=2:et\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-to-dos\" class=\"anchor\" href=\"#to-dos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTo-do\u0027s\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eexamples\n\u003cul\u003e\n\u003cli\u003eadd an example to show how QE can be implemented\u003c/li\u003e\n\u003cli\u003eadd examples to show how one can distribute source in a volume or surface\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 27,
    "subscribers_count": 11,
    "topics": [
      "geant4",
      "detector",
      "physics",
      "monte-carlo-simulation"
    ],
    "updated_at": 1627212360.0
  },
  {
    "data_format": 2,
    "description": "nanodisco: a toolbox for discovering and exploiting multiple types of DNA methylation from individual bacteria and microbiomes using nanopore sequencing.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "fanglab/nanodisco",
    "latest_release": "v1.0.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nanodisco\" class=\"anchor\" href=\"#nanodisco\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enanodisco\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003enanodisco\u003c/code\u003e is a toolbox for \u003cem\u003ede novo\u003c/em\u003e discovery of all the three types (6mA, 5mC and 4mC) of DNA methylation from individual bacteria and microbiomes using nanopore sequencing. For microbiomes, nanodisco also supports the use of DNA methylation patterns as natural epigenetic barcodes to facilitate high resolution metagenomic binning. Specifically, nanodisco can be used to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cem\u003eDe novo\u003c/em\u003e discover DNA methylation motifs, identify specific type (6mA, 5mC or 4mC, namely \u003cem\u003etyping\u003c/em\u003e) of a methylation motif, and identify which specific position within the motif is methylated (namely \u003cem\u003efine mapping\u003c/em\u003e).\u003c/li\u003e\n\u003cli\u003ePerform metagenomic binning based on microbial DNA methylation pattern by constructing and clustering a methylation profile matrix.\u003c/li\u003e\n\u003cli\u003eIntegrate the two functionalities above together for \u003cem\u003ede novo\u003c/em\u003e methylation motif discovery from microbiomes, and metagenomic analysis.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors-notes\" class=\"anchor\" href=\"#authors-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors\u0027 notes\u003c/h2\u003e\n\u003cp\u003eWe are actively developing \u003ccode\u003enanodisco\u003c/code\u003e to facilitate usage and broaden features. All feedback is more than welcome. You can reach us on twitter (\u003ca href=\"https://twitter.com/iamfanggang\" rel=\"nofollow\"\u003e@iamfanggang\u003c/a\u003e and \u003ca href=\"https://twitter.com/AlanTourancheau\" rel=\"nofollow\"\u003e@AlanTourancheau\u003c/a\u003e) or directly through the \u003ca href=\"https://github.com/fanglab/nanodisco/issues\"\u003eGitHub issues system\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e02/13/21: Updated to \u003ccode\u003ev1.0.2\u003c/code\u003e, including a new \u003ccode\u003enanodisco score\u003c/code\u003e command and the \u003ccode\u003e--split_fasta\u003c/code\u003e option to generate binned fasta files.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-content\" class=\"anchor\" href=\"#content\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContent\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#Installation\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#Tool-showcase\"\u003eTool showcase\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#Prepare-the-container-for-examples\"\u003ePrepare the container for examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#Methylation-typing-and-fine-mapping\"\u003eMethylation typing and fine mapping\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#Methylation-binning-of-metagenomic-contigs\"\u003eMethylation binning of metagenomic contigs\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#Documentation\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#Citation\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003enanodisco\u003c/code\u003e is distributed as a fully functional image bypassing the need to install any dependencies others than the virtualization software. We currently recommend using Singularity (v3.2.1 and above), which can be installed on Linux systems and is often the preferred solution by HPC administrators (\u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html\" rel=\"nofollow\"\u003eQuick Start\u003c/a\u003e). \u003ccode\u003enanodisco\u003c/code\u003e was tested extensively with Singularity v3.2.1 and v3.5.2.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name nanodisco.sif library://fanglab/default/nanodisco \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Download the image from cloud.sylabs.io\u003c/span\u003e\nsingularity build nd_env nanodisco.sif                         \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Create a container named nd_env\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tool-showcase\" class=\"anchor\" href=\"#tool-showcase\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTool showcase\u003c/h2\u003e\n\u003cp\u003eTo showcase the toolbox applications and facilitate an understanding of the methods, we provide examples for the analysis of two datasets presented in our preprint. Those datasets can be download with the following commands from within a \u003ccode\u003enanodisco\u003c/code\u003e container: \u003ccode\u003eget_data_bacteria\u003c/code\u003e and \u003ccode\u003eget_data_microbiome\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prepare-the-container-for-examples\" class=\"anchor\" href=\"#prepare-the-container-for-examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrepare the container for examples\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity build --sandbox nd_example nanodisco.sif \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Create a writable container (directory) named nd_example\u003c/span\u003e\nsingularity run --no-home -w nd_example              \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Start an interactive shell to use nanodisco, type `exit` to leave\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe image retrieved from \u003ca href=\"https://singularity-hub.org/\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with \u003ccode\u003esingularity pull\u003c/code\u003e (nanodisco.sif) is already build and can be reused at will. Containers built with those instructions are writable meaning that results from nanodisco analysis can be retrieved when the container is not running. Outputs for the following commands can be found at \u003ccode\u003e./path/to/nd_example/home/nanodisco/analysis\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-methylation-typing-and-fine-mapping\" class=\"anchor\" href=\"#methylation-typing-and-fine-mapping\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMethylation typing and fine mapping\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Identify the specific type (6mA, 5mC or 4mC, namely \u003cem\u003etyping\u003c/em\u003e) of a methylation motif, and identify which specific position within the motif is methylated (namely \u003cem\u003efine mapping\u003c/em\u003e). The detailed method is described in the preprint.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInputs:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCurrent differences file (pre-computed in the following example, can be generated with \u003ccode\u003enanodisco difference\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eReference genome file (.fasta)\u003c/li\u003e\n\u003cli\u003eMethylation motifs for which one wants to perform typing and fine mapping\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eOutputs:\u003c/strong\u003e For each queried methylation motif, \u003ccode\u003enanodisco\u003c/code\u003e identifies the methylation type and the methylated position summarized in a heatmap (\u003ccode\u003eanalysis/Ecoli_motifs/Motifs_classification_Ecoli_nn_model.pdf\u003c/code\u003e). See Figure 4d in the preprint as an example. In addition, the predicted methylation type and methylated position for each motif is compiled in a text file (\u003ccode\u003eanalysis/Ecoli_motifs/Motifs_classification_Ecoli_nn_model.tsv\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/docs/figures/Motifs_classification_Ecoli_nn_model.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/docs/figures/Motifs_classification_Ecoli_nn_model.png\" alt=\"Output Characterize\" title=\"E. coli methylation motifs classification results\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003csub\u003e\u003cem\u003e1. AACNNNNNNGTGC: highest value (85) is on the 6mA row with offset +1 (relative to the first base), meaning that the second base (A) is 6mA\u003c/em\u003e\u003c/sub\u003e\u003cbr\u003e\n\u003csub\u003e\u003cem\u003e2. CCWGG: highest value (95) is on the 5mC row with offset +1 (relative to the first base), meaning that the second base (C) is 5mC\u003c/em\u003e\u003c/sub\u003e\u003cbr\u003e\n\u003csub\u003e\u003cem\u003e3. GATC: highest value (91) is on the 6mA row with offset +1 (relative to the first base), meaning that the second base (A) is 6mA\u003c/em\u003e\u003c/sub\u003e\u003cbr\u003e\n\u003csub\u003e\u003cem\u003e4. GCACNNNNNNGTT: highest value (84) is on the 6mA row with offset +2 (relative to the first base), meaning that the third base (A) is 6mA\u003c/em\u003e\u003c/sub\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExample commands:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eget_data_bacteria \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Retrieve E. coli current differences and reference genome\u003c/span\u003e\nnanodisco characterize -p 4 -b Ecoli -d dataset/EC_difference.RDS -o analysis/Ecoli_motifs -m GATC,CCWGG,GCACNNNNNNGTT,AACNNNNNNGTGC -t nn -r reference/Ecoli_K12_MG1655_ATCC47076.fasta\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn this example, the current differences file (\u003ccode\u003eEC_difference.RDS\u003c/code\u003e) was generated on a whole \u003cem\u003eE. coli\u003c/em\u003e nanopore sequencing dataset, from the preprint, using \u003ccode\u003enanodisco difference\u003c/code\u003e. \u003cstrong\u003eRuntime is ~1 min with 4 threads\u003c/strong\u003e (~6.5 GB memory used).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-methylation-binning-of-metagenomic-contigs\" class=\"anchor\" href=\"#methylation-binning-of-metagenomic-contigs\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMethylation binning of metagenomic contigs\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eGoal:\u003c/strong\u003e Construct methylation profiles for metagenomic contigs, identify informative features, and perform methylation binning for high-resolution metagenomic analysis.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eInputs:\u003c/strong\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCurrent differences file (pre-computed in the following example)\u003c/li\u003e\n\u003cli\u003eMetagenomic \u003cem\u003ede novo\u003c/em\u003e assembly (.fasta)\u003c/li\u003e\n\u003cli\u003eMetagenomic contigs coverage files\u003c/li\u003e\n\u003cli\u003e\n\u003cem\u003eDe novo\u003c/em\u003e discovered methylation motifs\u003c/li\u003e\n\u003cli\u003e(Optional) Annotation for metagenome contigs (e.g. species of origin) and List of contigs from Mobile Genetic Elements (MGEs)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003eOutputs:\u003c/strong\u003e t-SNE scatter plots that demonstrates the species level clustering of metagenomic contigs (\u003ccode\u003eanalysis/binning/Contigs_methylation_tsne_MGM1_motif.pdf\u003c/code\u003e) as presented in the preprint Figure 5a.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"/docs/figures/Contigs_methylation_tsne_MGM1_motif.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"/docs/figures/Contigs_methylation_tsne_MGM1_motif.png\" alt=\"MGM1 guided metagenomic contigs binning\" width=\"500\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eExample commands:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eget_data_microbiome \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Retrieve current differences, de novo metagenome assembly, etc\u003c/span\u003e\nnanodisco profile -p 4 -r reference/metagenome.fasta -d dataset/metagenome_subset_difference.RDS -w dataset/metagenome_WGA.cov -n dataset/metagenome_NAT.cov -b MGM1_motif -o analysis/binning --motifs_file dataset/list_de_novo_discovered_motifs.txt\nnanodisco binning -r reference/metagenome.fasta -s dataset/methylation_profile_MGM1_motif.RDS -b MGM1_motif -o analysis/binning\nnanodisco plot_binning -r reference/metagenome.fasta -u analysis/binning/methylation_binning_MGM1_motif.RDS -b MGM1_motif -o analysis/binning -a reference/motif_binning_annotation.RDS --MGEs_file dataset/list_MGE_contigs.txt\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn this example, the current differences file (\u003ccode\u003emetagenome_subset_difference.RDS\u003c/code\u003e) was generated on a mouse gut microbiome nanopore sequencing dataset, MGM1 from the preprint, using \u003ccode\u003enanodisco difference\u003c/code\u003e. This examples correspond to the procedure refered to as guided methylation binning where methylation motifs were already \u003cem\u003ede novo\u003c/em\u003e discovered. \u003cstrong\u003eRuntime is ~10 min with 4 threads\u003c/strong\u003e and ~4 GB of memory used.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003eFor a comprehensive description of \u003ccode\u003enanodisco\u003c/code\u003e including installation guide, and a detailed tutorial, please consult the \u003ca href=\"https://nanodisco.readthedocs.io/en/latest/overview.html\" rel=\"nofollow\"\u003ecomplete documentation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003eTourancheau, A., Mead, E.A., Zhang, XS. \u0026amp; Fang, G. Discovering multiple types of DNA methylation from bacteria and microbiome using nanopore sequencing. \u003cem\u003eNat Methods\u003c/em\u003e (2021). doi:\u003ca href=\"https://doi.org/10.1038/s41592-021-01109-3\" rel=\"nofollow\"\u003e10.1038/s41592-021-01109-3\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 28,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1626321091.0
  },
  {
    "data_format": 2,
    "description": "Tomography Reconstructon Pipeline",
    "filenames": [
      "install/savu_singularity/conda-recipes/Singularity",
      "install/savu_singularity/singularity-recipes/Singularity.SavuAstra",
      "install/savu_singularity/singularity-recipes/Singularity.SavuDeps",
      "install/savu_singularity/singularity-recipes/Singularity.SavuCore"
    ],
    "full_name": "DiamondLightSource/Savu",
    "latest_release": "v4.0",
    "readme": "\u003cp\u003e\u003ca href=\"doc/source/images/portcullis_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"doc/source/images/portcullis_logo.png\" alt=\"alt text\" title=\"Portcullis\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-portcullis\" class=\"anchor\" href=\"#portcullis\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePortcullis\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/maplesond/portcullis/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b9c31b04d2671e6317cdfd9e4fdf893512936091302d1b1b56c99cb89ab43df7/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f7461672f6d61706c65736f6e642f706f727463756c6c69732e737667\" alt=\"Version\" data-canonical-src=\"https://img.shields.io/github/tag/maplesond/portcullis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://jenkins.sdlmapleson.net/job/portcullis/job/develop/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f696e3e0136cfb90f0e05f4f4e0a257ece7cd1e52ff19a0c8963b32df756d3a7/68747470733a2f2f6a656e6b696e732e73646c6d61706c65736f6e2e6e65742f6275696c645374617475732f69636f6e3f6a6f623d706f727463756c6c6973253246646576656c6f70\" alt=\"Build Status\" data-canonical-src=\"https://jenkins.sdlmapleson.net/buildStatus/icon?job=portcullis%2Fdevelop\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/maplesond/portcullis/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ad4d6f3e16da4f0dddcd142fa3b6088042b13242787f5ad939d2db28282d3eb5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4c6963656e73652d47504c25323076332d627269676874677265656e2e737667\" alt=\"License: GPL v3\" data-canonical-src=\"https://img.shields.io/badge/License-GPL%20v3-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/maplesond/portcullis/issues\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d3bedf8e24750956939d66108f9ba197e72b83d1de8fc7305708ab2d67c20c17/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f6d61706c65736f6e642f706f727463756c6c69732e737667\" alt=\"Issues\" data-canonical-src=\"https://img.shields.io/github/issues-raw/maplesond/portcullis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003ePortcullis stands for PORTable CULLing of Invalid Splice junctions from pre-aligned RNA-seq data.  It is known that RNAseq mapping tools generate many invalid junction predictions, particularly in deep datasets with high coverage over splice sites.  In order to address this, instead for creating a new RNAseq mapper, with a focus on SJ accuracy we created a tool that takes in a BAM file generated by an RNAseq mapper of the user\u0027s own choice (e.g. Tophat2, Gsnap, STAR2 or HISAT2) as input (i.e. it\u0027s portable).  It then, analyses and quantifies all splice junctions in the file before, filtering (culling) those which are unlikely to be genuine.  Portcullis output\u0027s junctions in a variety of formats making it suitable for downstream analysis (such as differential splicing analysis and gene modelling) without additional work.  Portcullis can also filter the original BAM file removing alignments associated with \u003cem\u003ebad\u003c/em\u003e junctions.  Both the filtered junctions and BAM files are cleaner and more usable resources which can more effectively be used to assist in downstream analyses such as gene prediction and genome annotation.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eWe support multiple methods for installing and running portcullis.  Hopefully your favourite container or package manager is supported below.  If not let us know and we\u0027ll try to work to get it integrated there.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eDocker\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://hub.docker.com/r/maplesond/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/de966674ebe7a3dec2fed423683dd2c64e3630527fab6a691add53421292e384/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6d61706c65736f6e642f706f727463756c6c69732e737667\" alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/maplesond/portcullis.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Keep in mind you need to mount in any working directories to the container with the `-v` option.\n# Ideally, mount these into the /data directory which is the container\u0027s working directory.\ndocker run --it --rm -v /abspath/to/data/on/host:/data maplesond/portcullis:stable portcullis --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# First download the container:\nsingularity pull --name portcullis.img shub://maplesond/portcullis:master\n\n# Then to execute commands in the container:\nsingularity exec portcullis.img portcullis --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eConda\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/381a7739b713a2bae02343a6ac934de39148a7866dbf4e52b597391b2a07fd4b/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f6c61746573745f72656c656173655f646174652e737667\" alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/latest_release_date.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/77a7c650d2675de3588df907d8e8aec11957abc95bcfd87d3b1b07f78a2bc4ec/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f706c6174666f726d732e737667\" alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/platforms.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/bioconda/portcullis\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/83781f462972e76ba4f2d046533fd48deb7cb72a0512481ff304f79c51bc01e3/68747470733a2f2f616e61636f6e64612e6f72672f62696f636f6e64612f706f727463756c6c69732f6261646765732f646f776e6c6f6164732e737667\" alt=\"Anaconda-Server Badge\" data-canonical-src=\"https://anaconda.org/bioconda/portcullis/badges/downloads.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda install portcullis --channel=bioconda\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eBrew\u003c/strong\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ebrew install brewsci/bio/portcullis\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eFrom source\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/maplesond/portcullis/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3885a69f4777ec0c98cf3d0bee17eb7ca3d3eb69bbf850df2f36895b80168ade/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f646f776e6c6f6164732f6d61706c65736f6e642f706f727463756c6c69732f746f74616c2e737667\" alt=\"Downloads\" data-canonical-src=\"https://img.shields.io/github/downloads/maplesond/portcullis/total.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you wish to install from source please first confirm that first you have these dependencies are installed and configured:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eGCC\u003c/strong\u003e V4.8+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eautoconf\u003c/strong\u003e V2.53+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eautomake\u003c/strong\u003e V1.11+\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emake\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003elibtool\u003c/strong\u003e V2.4.2+\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ezlib-dev\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003epthreads\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eboost-dev\u003c/strong\u003e V1.52+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003esamtools\u003c/strong\u003e V1.2+\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ePython3-dev\u003c/strong\u003e V3.5+ (Make sure the following packages are installed: \u003cem\u003epandas\u003c/em\u003e, \u003cem\u003ematplotlib\u003c/em\u003e, \u003cem\u003esetuptools\u003c/em\u003e, \u003cem\u003esphinx\u003c/em\u003e, \u003cem\u003etabulate\u003c/em\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen proceed with the following steps:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# Clone the repo\ngit clone git@github.com:maplesond/portcullis.git\n\n# Move into repo directory\ncd portcullis\n\n# Generate configure script\n./autogen.sh\n\n# Confirm dependencies and generate makefiles\n# Adding --prefix \u0026lt;dir\u0026gt; will tell make install to put everything in a \n# particular directory.  Default is /usr/local.\n./configure\n\n# Compile (increasing -j will make it go faster!\nmake -j 2\n\n# Run some unit tests (you can increase -j here too)\nmake -j 2 check\n\n# Install to prefix dir\nmake install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eCommon problems\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMany system python installations do not come with the C API immediately available, which prevents Portcullis from embedding python code.  We typically would recommend installing anaconda3 as this would include the latest version of python, all required python packages as well as the C API.  If you are running a debian system and the C libraries are not available by default and you wish to use the system python installation the you can install them using: \u003ccode\u003esudo apt-get install python-dev\u003c/code\u003e.  Also, if you have installed python to a custom location please verify that the \u003cem\u003ebin\u003c/em\u003e directors on the \u003cem\u003ePATH\u003c/em\u003e environment variable, and the lib (or lib64) directory is on the \u003cem\u003eLD_LIBRARY_PATH\u003c/em\u003e or \u003cem\u003eLD_RUN_PATH\u003c/em\u003e as appropriate.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf Portcullis is failing at the \u003ccode\u003e./autogen.sh\u003c/code\u003e step you will likely need to install autotools.  The following command should do this on MacOS: \u003ccode\u003ebrew install autoconf automake libtool\u003c/code\u003e.  On a debian system this can be done with: \u003ccode\u003esudo apt-get install autoconf automake libtool\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quickstart\" class=\"anchor\" href=\"#quickstart\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart\u003c/h2\u003e\n\u003cp\u003eAfter portcullis has been installed, the \u003ccode\u003eportcullis\u003c/code\u003e executable should be available.  Typing \u003ccode\u003eportcullis\u003c/code\u003e or \u003ccode\u003eportcullis --help\u003c/code\u003e at the command line will present you with the portcullis help message.\u003c/p\u003e\n\u003cp\u003eThese modes are available:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eprep\u003c/strong\u003e    - Prepares input data so that it is suitable for junction analysis\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ejunc\u003c/strong\u003e    - Calculates junction metrics for the prepared data\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efilter\u003c/strong\u003e  - Separates alignments based on whether they are likely to represent genuine splice junctions or not\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003ebamfilt\u003c/strong\u003e - Filters a BAM to remove any reads associated with invalid junctions\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003efull\u003c/strong\u003e    - Runs prep, junc, filter and optionally bamfilt as a complete pipeline\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTyping \u003ccode\u003eportcullis \u0026lt;mode\u0026gt; --help\u003c/code\u003e will bring up help and usage information specific to that mode.\u003c/p\u003e\n\u003cp\u003eIn addition to portcullis, we provide a tool-suite for manipulating junction files called junctools.  Typing \u003ccode\u003ejunctools --help\u003c/code\u003e will provide you with the program options.\u003c/p\u003e\n\u003cp\u003eFor much more information about portcullis\u0027 capabilities and how to configure and run it, an online version of the manual can be found here: \u003ca href=\"https://portcullis.readthedocs.org/en/latest/\" rel=\"nofollow\"\u003ehttps://portcullis.readthedocs.org/en/latest/\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-licensing\" class=\"anchor\" href=\"#licensing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicensing\u003c/h2\u003e\n\u003cp\u003eGNU GPL V3.  See COPYING file for more details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-authors\" class=\"anchor\" href=\"#authors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthors\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eDaniel Mapleson\u003c/li\u003e\n\u003cli\u003eLuca Venturini\u003c/li\u003e\n\u003cli\u003eDavid Swarbreck\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSee AUTHORS file for more details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgements\u003c/h2\u003e\n\u003cp\u003eAffiliation: The Earlham Institute (EI)\nFunding: The Biotechnology and Biological Sciences Research Council (BBSRC)\u003c/p\u003e\n",
    "stargazers_count": 28,
    "subscribers_count": 14,
    "topics": [],
    "updated_at": 1626814904.0
  },
  {
    "data_format": 2,
    "description": "Unified kit with all the scripts required for maintaining the repository.",
    "filenames": [
      "guest/Singularity"
    ],
    "full_name": "chaotic-aur/toolbox",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-chaotic-aur\" class=\"anchor\" href=\"#chaotic-aur\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChaotic AUR\u003c/h1\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDisclaimer\u003c/h2\u003e\n\u003cp\u003eI receive questions like \"Why didn\u0027t you write it in language X? Why didn\u0027t you use sudoers instead of \u003ccode\u003esetuid\u003c/code\u003e? Why don\u0027t you guarantee reproducible builds? Why don\u0027t you use submodules and review every PKGBUILD change?\"\u003c/p\u003e\n\u003cp\u003eBecause I can\u0027t, I am only one person, taking care of all the steps required for updating 3800 packages non-stop, treating all the differences between those PKGBUILDs and their sources, in an infra that runs in donated VMs which are not any similar.\u003c/p\u003e\n\u003cp\u003eIf at some point you see something that could be better, then please open a PR. And keep it simple, the more complex the codebase becomes, the more complicated will be in the future for one man alone to maintain it.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cli\" class=\"anchor\" href=\"#cli\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCLI\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic pr{,epare} ${INPUTDIR} $@\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIt generates a building script to be later run in a containerized environment.\n\u003ccode\u003e$INPUTDIR\u003c/code\u003e is the name of directory in \"$PWD\" which contains a PKGBUILD.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {lw,lowerstrap}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eIt generates a lowerdir for later chrooting.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {mk,makepkg} ${INPUTDIR} $@\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eBuilds the package in a container using systed-nspawn.\n\u003ccode\u003e$INPUTDIR\u003c/code\u003e is the result of a \u003ccode\u003eprepare\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {mkd,makepwd} [${PACKAGES[@]}]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePrepare and build all packages in the current directory.\nIf \u003ccode\u003ePACKAGES\u003c/code\u003e are not provided then it will try to build all sub-directories.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {si,iterfere-sync}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSync packages\u0027 interference repo.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {sp,package-lists-sync}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSync package list repo.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {dp,deploy} ${INPUTDIR}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSign the package and send to primary node.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {dbb,db-bump}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAdd recently deployed packages to the database, while moving replaced packages to archive.\nUses \u003ccode\u003erepoctl\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {rm,remove} ${PACKAGES[@]}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eRemove and archive all listed packages.\nUses \u003ccode\u003erepoctl\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {get,aur-download} [-r] ${PACKAGES[@]}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eDownload listed packages\u0027 sources from AUR.\nUses \u003ccode\u003erepoctl\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic cl{,eanup} ${INPUTDIR}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eSafely deletes old package sources.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic help {syncthing,rsync}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eInstructions to the mirroring services.\nRSync is one-way (primary-\u0026gt;cluster) only, and Syncthing both ways.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic routine {hourly,morning,afternoon,nightly,midnight}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eRun the specified routine.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic routine clean-archive\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eWhen on a primary node, clean up the archive folder.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {clg,clean-logs}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAfter a \u003ccode\u003echaotic makepwd\u003c/code\u003e, remove successfull and \"already built\" logs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003echaotic {cls,clean-srccache} ${PACKAGE}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eRemoves cached sources from a specific package.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-involved-directories\" class=\"anchor\" href=\"#involved-directories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInvolved directories\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e/var/cache/chaotic/sources/${PACKAGETAG}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePer-package \u003ccode\u003eSRCDEST\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e/var/cache/chaotic/lower/{latest,$DATESTAMP}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eLowerdirs.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e/var/cache/chaotic/cc/{PACKAGETAG}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePer-package \u003ccode\u003e~/.ccache\u003c/code\u003e.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e/var/cache/chaotic/packages\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eContainer-shared pacman\u0027s cache.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003e/var/lib/chaotic/interfere\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eCloned version of \u003ca href=\"https://github.com/chaotic-aur/interfere\"\u003einterfere repository\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003epacman -S --needed base-devel git arch-install-scripts repoctl fuse-overlayfs rsync python-telegram-send\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOne needs an active mirror or a setting (in /etc/chaotic.conf) like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_URL=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ehttps://builds.garudalinux.org/repos/chaotic-aur/x86_64\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e REPOCTL_CONFIG=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e/etc/chaotic/repoctl.conf\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_REPOCTL_DB_URL=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${CAUR_URL}\u003c/span\u003e/chaotic-aur.db.tar.zst\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_REPOCTL_DB_FILE=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/tmp/chaotic/db.tar.zst\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eTo create a gpg key for the root user refer to this \u003ca href=\"https://wiki.archlinux.org/index.php/GnuPG#Create_a_key_pair\" rel=\"nofollow\"\u003eArchWiki article\u003c/a\u003e for more information. If you find problems when using \"sudo\", read the \"\u003ca href=\"https://wiki.archlinux.org/index.php/GnuPG#su\" rel=\"nofollow\"\u003esu\u003c/a\u003e\" subsection.\nThen generate a ssh keypair for the root user.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo ssh-keygen\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe ssh public key (cat /root/.ssh/id_rsa.pub) then needs to be added to the primary servers root authorized keys (/root/.ssh/authorized_keys). After that follow these \u003ca href=\"https://wiki.archlinux.org/index.php/GnuPG#Export_your_public_key\" rel=\"nofollow\"\u003einstructions\u003c/a\u003e to export the gpg public key. This key will have to be \u003ca href=\"https://wiki.archlinux.org/index.php/GnuPG#Sending_keys\" rel=\"nofollow\"\u003euploaded\u003c/a\u003e to \u003ca href=\"keyserver.ubuntu.com\"\u003ekeyserver.ubuntu.com\u003c/a\u003e in order for the key to be verified.\nThen, configure it as follows in \u003ccode\u003e/etc/chaotic.conf\u003c/code\u003e, like this:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_DEPLOY_PKGS=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e/var/www/chaotic-aur/x86_64\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_URL=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003ehttp://localhost:8080/chaotic-aur/x86_64\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_SIGN_KEY=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e8A9E14A07010F7E3\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CAUR_TYPE=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ecluster\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e REPOCTL_CONFIG=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e/etc/chaotic/repoctl.toml\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou\u0027ll find more options in \u003ccode\u003esrc/chaotic\u003c/code\u003e first lines.\u003c/p\u003e\n\u003cp\u003eSupported \u003ccode\u003etype\u003c/code\u003e values are: \u003ccode\u003eprimary\u003c/code\u003e, \u003ccode\u003ecluster\u003c/code\u003e, and \u003ccode\u003edev\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eTo have clean logs \u0026amp; less bandwidth usage \u003ccode\u003e/etc/pacman.conf\u003c/code\u003e settings need to be adjusted:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eEnable \u003ccode\u003eNoProgressBar\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eUse \u003ccode\u003eServer = file:///path-to-local-repo\u003c/code\u003e as repo link if a local mirror is available\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDon\u0027t use \u003ccode\u003eILoveCandy\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTo deploy faster replace \u003ccode\u003eopenssh\u003c/code\u003e with \u003ccode\u003eopenssh-hpn\u003c/code\u003e on all nodes (adds peformance related \u003ca href=\"https://www.psc.edu/research/networking/hpn-ssh/\" rel=\"nofollow\"\u003epatches\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eInstall dependencies, then:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo groupadd chaotic_op\nsudo usermod -aG chaotic_op $(whoami)\n\nmake build \u0026amp;\u0026amp; sudo make install\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-lint\" class=\"anchor\" href=\"#lint\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLint\u003c/h2\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epacman -S --needed yarn shellcheck\nyarn install\nyarn run lint\u003c/pre\u003e\u003c/div\u003e\n",
    "stargazers_count": 29,
    "subscribers_count": 11,
    "topics": [
      "archlinux",
      "repository-management",
      "automation",
      "continuous-deployment",
      "pkgbuild",
      "aur"
    ],
    "updated_at": 1627305961.0
  },
  {
    "data_format": 2,
    "description": "MONET : MOdularising NEtwork Toolbox - https://doi.org/10.1093/bioinformatics/btaa236",
    "filenames": [
      ".containers/R1/singularity/Singularity",
      ".containers/M1/singularity/Singularity",
      ".containers/K1/singularity/Singularity"
    ],
    "full_name": "BergmannLab/MONET",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-monet\" class=\"anchor\" href=\"#monet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMONET\u003c/h1\u003e\n\u003cp\u003eThis repository holds the source code for \u003cstrong\u003eMONET\u003c/strong\u003e, a Linux/MacOS command-line toolbox to mine molecular and genetic networks, leveraging the top performing methods of the \u003cstrong\u003eDisease Module Identification (DMI) DREAM Challenge\u003c/strong\u003e (see DREAM Challenge paper under section PUBLICATIONS and \u003ca href=\"https://www.synapse.org/modulechallenge\" rel=\"nofollow\"\u003ehttps://www.synapse.org/modulechallenge\u003c/a\u003e)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePREREQUISITES\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eOperating System\u003c/strong\u003e: MONET can be run on \u003cstrong\u003eeither\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinux (it was tested on \u003cem\u003eUbuntu Linux\u003c/em\u003e 20.04, \u003cem\u003eCentOS Linux\u003c/em\u003e 7.5)\u003c/li\u003e\n\u003cli\u003eMacOS (it was tested on \u003cem\u003emacOS Sierra\u003c/em\u003e 10.12)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eSoftware\u003c/strong\u003e: MONET requires \u003cstrong\u003eeither\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eDocker\u003c/code\u003e (see \"Install using the repository\" \u003ca href=\"https://docs.docker.com/engine/install/\" rel=\"nofollow\"\u003ehttps://docs.docker.com/engine/install/\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eSingularity\u003c/code\u003e (see \u003ca href=\"http://singularity.lbl.gov\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eHardware\u003c/strong\u003e: MONET was tested both on server and on commodity hardware (i.e., regular desktop). For details, please refer to section COMPUTATIONAL RESOURCES below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eINSTALLATION\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eJust like you can \u003ccode\u003els\u003c/code\u003e a folder, after installation will be able to \u003ccode\u003emonet\u003c/code\u003e a network\u003c/strong\u003e from any location on your system.\u003c/p\u003e\n\u003cp\u003eSimply run:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git clone https://github.com/BergmannLab/MONET.git \u0026amp;\u0026amp; cd MONET \u0026amp;\u0026amp; ./install.sh\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eA folder MONET will have been created with the source code: you are free to remove it, if you are not interested. This will not affect MONET, which has now been installed in your system: the command \u003ccode\u003emonet\u003c/code\u003e can be invoked from any location on the system.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-need-some-more-guidance\" class=\"anchor\" href=\"#if-you-need-some-more-guidance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU NEED SOME MORE GUIDANCE\u003c/h4\u003e\n\u003cp\u003eYou can follow this \u003ca href=\"https://form.jotform.com/tomasonimattia/monet-installation\" rel=\"nofollow\"\u003esurvey-tutorial\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eit will guide you step by step (assumes no prior knowledge)\u003c/li\u003e\n\u003cli\u003e(optionally) guides you through running some examples (feel free to skip those)\u003c/li\u003e\n\u003cli\u003eit will help us collect information about possible errors on different platforms\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-are-on-windows\" class=\"anchor\" href=\"#if-you-are-on-windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU ARE ON WINDOWS\u003c/h4\u003e\n\u003cp\u003eUsers using Windows are encouraged to install a hypervisor (i.e., a software that allows to creates and run virtual machines): for example, install VirtualBox \u003ca href=\"https://www.virtualbox.org/wiki/Downloads\" rel=\"nofollow\"\u003ehttps://www.virtualbox.org/wiki/Downloads\u003c/a\u003e and configure it up to run a virtual Ubuntu Linux inside which to install MONET (using the instructions above).\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-if-you-are-a-singularity-user-without-sudo-rights\" class=\"anchor\" href=\"#if-you-are-a-singularity-user-without-sudo-rights\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIF YOU ARE A SINGULARITY USER WITHOUT SUDO RIGHTS\u003c/h4\u003e\n\u003cp\u003eSudo rights will be required at installation time for Singularity users: Singularity users will not need sudo rights while running MONET (i.e., Singularity does not require sudo right to run containers), but they will need it at installation time (i.e., at the time the Singularity images are first created).\u003c/p\u003e\n\u003cp\u003eUsers that don\u0027t have sudo rights should follow the regular installation procedure explained above, then refer to MONET/docs/installation_no_sudo.txt where they will find a workaround to complete the installation manually without needing sudo.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-testing-the-installation\" class=\"anchor\" href=\"#testing-the-installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTESTING THE INSTALLATION\u003c/h2\u003e\n\u003cp\u003eAt the end of the install process, you will be asked whether you want to test MONET. This test is completely automatic.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-monet-help-command\" class=\"anchor\" href=\"#monet-help-command\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMONET HELP COMMAND\u003c/h2\u003e\n\u003cp\u003eAfter installing MONET, the help command \u003ccode\u003emonet --help\u003c/code\u003e will be available from any location on your system.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running\" class=\"anchor\" href=\"#running\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRUNNING\u003c/h2\u003e\n\u003cp\u003eOnce installed, from any location on your system, you can run the following example command: it will run a method called M1 (see section METHODS for details), on a network contained in your /tmp folder (see section INPUT for details), using docker virtualization (see section PREREQUISITES for details). In the remainder of this document, you will find details about what parameters you can use, what to expect as an output and resource usage (in the PARAMETERS, OUTPUT and COMPUTATIONAL RESOURCES sections respectively).\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ monet --help\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ monet --input=/tmp/input_network.txt \u2014-method=M1 --container=docker\u003c/code\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-input\" class=\"anchor\" href=\"#input\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eINPUT\u003c/h2\u003e\n\u003cp\u003eThe input file is provided to MONET using the \u003ccode\u003e--input\u003c/code\u003e parameter (see section RUNNING and section PARAMETERS).\u003c/p\u003e\n\u003cp\u003eThe format for the input network is the following: a \u003cstrong\u003etab-separated\u003c/strong\u003e file containing one line for each edge.\u003c/p\u003e\n\u003cp\u003eIf an edge is connecting two nodes, gene_a and gene_b, with a certain weight, the file will contain the line:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egene_a \\t gene_b \\t weight \\n\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eDetails:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003egene_a and gene_b, the gene ids, can be either \u003cem\u003estring\u003c/em\u003e or \u003cem\u003einteger\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003eweight can be of type \u003cem\u003einteger\u003c/em\u003e or \u003cem\u003efloat\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003e\"\\t\" indicates the tab character and \"\\n\" the newline character\u003c/li\u003e\n\u003cli\u003eno blank spaces should appear, neither as separators nor as part of the gene ids\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor an example, see MONET/.test/system_test/input/zachary_karate_club.txt. The same folder containing the actual inputs to the Disease Module Identification (DMI) DREAM Challenge. Beware that some of the inputs will require high amounts of computational resources and are not suited to be run on a simple laptop or desktop computer; please refer to section COMPUTATIONAL RESOURCES for details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-output\" class=\"anchor\" href=\"#output\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOUTPUT\u003c/h2\u003e\n\u003cp\u003eThe output location is provided to MONET using the \u003ccode\u003e--output\u003c/code\u003e parameter (see section OPTIONAL PARAMETERS).\u003c/p\u003e\n\u003cp\u003eTwo output files will be generated in the directory where you run the command. They are marked with a timestamp, the name of the selected method and the name of your input network. For example, let\u0027s assume if you run M1 on 1st January 2020 at midday on a file called input_network.txt:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea \u003cstrong\u003econsole-output\u003c/strong\u003e file, which will contain the run-time outputs generated by the method you have selected, providing details about the steps that the M1 algorithm took to generate your output. Any errors would also be redirected here. The file would be called: \u003ccode\u003e2020-01-01-120000__M1__console-output__input_network.txt\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003ea \u003cstrong\u003eresult-modules\u003c/strong\u003e file, containing the results of your analysis and it will not be generated in case of errors. The file would be called: \u003ccode\u003e2020-01-01-120000__M1__result-modules__input_network.txt\u003c/code\u003e. It will be in tab-separated format, containing one module per line:\n\u003cul\u003e\n\u003cli\u003ethe first value of each line will be a module identifier (in the form of an integer number starting from 1)\u003c/li\u003e\n\u003cli\u003ethe second is a fixed numerical value and can be ignored (curerntly set to \u003ccode\u003e1.0\u003c/code\u003e, it was originally used in the DREAM Challenge to provide module-level confidence scores)\u003c/li\u003e\n\u003cli\u003ethe rest of the values on the line will be the gene ids container in the input (like gene_a and gene_b, see section INPUT)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-methods\" class=\"anchor\" href=\"#methods\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMETHODS\u003c/h2\u003e\n\u003cp\u003eThree methods are available as part of MONET, which emerged as the top-performing methods of the DREAM Challenge.\u003c/p\u003e\n\u003cp\u003eIn order to run one of the three methods, adapt the example command provided in section RUNNING providing the --method option with the name of the chosen method (--method=[K1|M1|R1], for details, see section PARAMETERS).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eK1\u003c/strong\u003e: KERNEL CLUSTERING OPTIMISATION algorithm. K1 is based on the \u201cDiffusion State Distance\u201d (DSD), a novel graph metric which is built on the premise that paths through low-degree nodes are stronger indications of functional similarity than paths that traverse high degree nodes by Cao et al. (2014). The DSD metric is used to define a pairwise distance matrix between all nodes, on which a spectral clustering algorithm is applied. In parallel, dense bipartite sub-graphs are identified using standard graph techniques. Finally, results are merged into a single set of non-overlapping clusters. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7349492/wiki/407359\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7349492/wiki/407359\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eM1\u003c/strong\u003e: MODULARITY OPTIMIZATION algorithm. M1 employs an original technique named Multiresolution introduced by (Arenas et al., 2008) to explore all topological scales at which modules may be found. The novelty of this approach relies on the introduction of a parameter, called resistance, which controls the aversion of nodes to form modules. Modularity (Newman and Girvan, 2004; Arenas et al., 2007) is optimized using an ensemble of algorithms: Extremal optimization (Duch and Arenas, 2005), Spectral optimization (Newman, 2006), Fast algorithm (Newman, 2004), Tabu search (Arenas et al., 2008), and fine-tuning by iterative repositioning of individual nodes in adjacent modules. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7352969/wiki/407384\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7352969/wiki/407384\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003eR1\u003c/strong\u003e: RANDOM-WALK-BASED algorithm. R1 is based on a variant of Markov Cluster Algorithm known as balanced Multi-layer Regularized Markov Cluster Algorithm(bMLRMCL) (Satuluriet al., 2010) which scales well to large graphs and minimizes the number of oversized clusters. First, a pre-processing step is applied so that edges with low weights are discarded and all remaining edges are scaled to integer values. Then, bMLRMCL is applied iteratively on modules of size grater than a user-defined threshold. For further details, please see: \u003ca href=\"https://www.synapse.org/#!Synapse:syn7286597/wiki/406659\" rel=\"nofollow\"\u003ehttps://www.synapse.org/#!Synapse:syn7286597/wiki/406659\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-parameters\" class=\"anchor\" href=\"#parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePARAMETERS\u003c/h2\u003e\n\u003cp\u003ePlease, provide values for the following MANDATORY parameters:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--input\u003c/strong\u003e: path to the network file to be analysed\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--method\u003c/strong\u003e: method to be used to analyse the input: [K1|M1|R1]\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--container\u003c/strong\u003e: virtualisation technology available on the system: [docker|singularity]\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-optional-parameters\" class=\"anchor\" href=\"#optional-parameters\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOPTIONAL PARAMETERS\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--output\u003c/strong\u003e: directory in which to output results (default is current directory)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select K1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--nclusters\u003c/strong\u003e: initial number of output clusters for spectral clustering step; final number may differ (default is 100)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select M1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--smallest\u003c/strong\u003e: min size of output clusters (default is 3)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--largest\u003c/strong\u003e: max size of output clusters (default is 100)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--linksdir\u003c/strong\u003e: directionality of links: [undirected|directed] (default is undirected)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--avgk\u003c/strong\u003e: desired average degree for nodes in output (default is 25)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eif you select R1\u003c/strong\u003e as a method, you may additionally provide the following:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003e--smallest\u003c/strong\u003e: min size of output clusters (default is 3)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--largest\u003c/strong\u003e: max size of output clusters (default is 100)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--c\u003c/strong\u003e: trade-off parameter for computational efficiency; for larger c, the algorithm will run slower, but may provide more accurate results (default is 800)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--i\u003c/strong\u003e: inflation parameter for standard Markov Clustering algorithm on which R1 is based (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--b\u003c/strong\u003e: parameter controlling how balanced the clustering results should be; for b=0, R1 behaves like standard Regularized Markov Cluster (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--threshold\u003c/strong\u003e: remove edges smaller than threshold from the input (default is 4)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--post\u003c/strong\u003e: decide whether to recursively cluster (recluster) or discard too large output clusters: [recluster|discard] (default is discard)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--c2\u003c/strong\u003e: (only used if --post=recluster) sets --c for reclustering round (default is 500)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--i2\u003c/strong\u003e: (only used if --post=recluster) sets --i for reclustering round (default is 2)\u003c/li\u003e\n\u003cli\u003e\n\u003cstrong\u003e--b2\u003c/strong\u003e: (only used if --post=recluster) sets --b for reclustering round (default is 2)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-computational-resources\" class=\"anchor\" href=\"#computational-resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCOMPUTATIONAL RESOURCES\u003c/h2\u003e\n\u003cp\u003eSome of the methods require large amount of resources, depending on your input (please, refer to the MONET paper in the PUBLICATIONS section for details about how resource needs will scale with the size of the input, for the different methods).\u003c/p\u003e\n\u003cp\u003eTo reproduce the results of the DREAM Challenge, you can run MONET/.test/system_test/reproduce_challenge/reproduce_challenge.sh. This might fail on commodity hardware (i.e., a regular laptop or desktop) as about 8GB or RAM need to be available. In that case, you can allocate a larger SWAP partition (on Linux) or run the experiment on more powerful hardware, such as a server. Please browser the rest of the contents of MONET/.test/system_test/reproduce_challenge to view the exact RAM usage (ram_usage.txt) and the challenge outputs produced by MONET (disease_modules_output directory).\u003c/p\u003e\n\u003cp\u003eTo monitor resource usage when running on your own input (and thus determine the amount or RAM / swap needed by your particular input network for a particular method), two simple scripts have been added to MONET/.test/helper_scripts (for Unix and one for MacOS systems): launch them before execution of MONET and redirect their output to file for simple inspection (no other task should be running).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-benchmarking\" class=\"anchor\" href=\"#benchmarking\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBENCHMARKING\u003c/h2\u003e\n\u003cp\u003eFor details about the modularization performance of the MONET methods on a set of artificial benchmarks (Louvain algorithm is shown as a baseline), please refer to the MONET paper in the PUBLICATIONS section; in particular, Fig. 1. MONET/.test/benchmarking for a detailed output of the experiments that have been carried out.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-source-code\" class=\"anchor\" href=\"#source-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSOURCE CODE\u003c/h2\u003e\n\u003cp\u003eThe source code is hosted at: \u003ca href=\"https://github.com/BergmannLab/MONET.git\"\u003ehttps://github.com/BergmannLab/MONET.git\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCONTRIBUTING\u003c/h2\u003e\n\u003cp\u003eIf you are interested in contributing to MONET, we encourage you to get in touch! We will be happy to add you to the list of our developers \u003ca href=\"https://github.com/BergmannLab/MONET/graphs/contributors\"\u003ehttps://github.com/BergmannLab/MONET/graphs/contributors\u003c/a\u003e. \u003cstrong\u003eTHANK YOU!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - CREATING A BRANCH\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eFirst, we will create an issue for the specific feature you are willing to contribute; let\u0027s say yours will happen to be issue 999. You will be then asked to create a new git branch where to implement your changes; run the following from the cloned MONET directory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git checkout -b issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git push origin issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAt this point, you are free to make changes to your local code in your laptop. Don\u0027t worry if you mess things up, it\u0027s no problem to add mistakes to a branch.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - TESTING YOUR CHANGES\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce you are done with your changes, you can test them locally by \u003cstrong\u003ereinstalling\u003c/strong\u003e from the modified MONET directory.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eCONTRIBUTING - PUBLISHING YOUR CHANGES\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOnce you have tested your changes, run the following from the cloned MONET directory:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git add .\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git commit -m \"adding code for feature # issues_999\"\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git push --set-upstream origin issues_999\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$ git checkout master\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOne of the MONET developers will test the changes in your branch then merge to Master.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-implementing-local-changes-to-monet\" class=\"anchor\" href=\"#implementing-local-changes-to-monet\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIMPLEMENTING LOCAL CHANGES TO MONET\u003c/h2\u003e\n\u003cp\u003eIf you wish to implement local changes to MONET, independently from our github repository, you can simply modify the code in your local cloned repository and \u003cstrong\u003ereinstall\u003c/strong\u003e after having made those changes (i.e. run or re-run the \u003ccode\u003einstall.sh\u003c/code\u003e script and confirm if you are asked to reinstall). This procedure can be repeated as many times as you like.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-troubleshooting-common-problems\" class=\"anchor\" href=\"#troubleshooting-common-problems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTROUBLESHOOTING COMMON PROBLEMS\u003c/h2\u003e\n\u003cp\u003eIf a MONET run is suddenly interrupted or if the expected outputs has not been generated, here are few common problems that can occur:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003elack of RAM: if the console-output file (see section OUTPUT) contains the word \"Killed\", the MONET processed were stopped by the Operating System, likely due to a lack of RAM. To confirm this, please read section COMPUTATIONAL RESOURCES to learn how to monitor your resource usage while running MONET.\u003c/li\u003e\n\u003cli\u003eoutdated kernel: Singularity users that work on Linux distributions with old kernels (e.g. CentOS 6.1, kernel 2.6) will encounter trouble during the install process; they need to contact their system administrator to inquire whether a kernel upgrade is possible.\u003c/li\u003e\n\u003cli\u003ecan\u0027t implement local changes: please, refer to section IMPLEMENTING LOCAL CHANGES TO MONET.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-bug-reports\" class=\"anchor\" href=\"#bug-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBUG-REPORTS\u003c/h2\u003e\n\u003cp\u003ePlease, address your questions and bug reports to Mattia Tomasoni, \u0026lt;mattia.tomasoni AT unil.ch\u0026gt;. An issue will be opened here to address your problem: \u003ca href=\"https://github.com/BergmannLab/MONET/issues\"\u003ehttps://github.com/BergmannLab/MONET/issues\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-publications\" class=\"anchor\" href=\"#publications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePUBLICATIONS\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eMONET paper\u003c/strong\u003e: Mattia Tomasoni, Sergio G\u00f3mez, Jake Crawford, Weijia Zhang, Sarvenaz Choobdar, Daniel Marbach and Sven Bergmann. MONET: a toolbox integrating top-performing methods for network modularization. Bioinformatics 36 (12), 3920-3921. doi: \u003ca href=\"https://doi.org/10.1093/bioinformatics/btaa236\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/bioinformatics/btaa236\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eDREAM Challenge paper\u003c/strong\u003e: Sarvenaz Choobdar, Mehmet Ahsen, Jake Crawford, Mattia Tomasoni, Tao Fang, David Lamparter, Junyuan Lin, Benjamin Hescott, Xiaozhe Hu, Johnathan Mercer, Ted Natoli, Rajiv Narayan, The DREAM Module Identification Challenge Consortium, Aravind Subramanian, Jitao David Zhang, Gustavo Stolovitzky, Zolt\u00e1n Kutalik, Kasper Lage, Donna Slonim, Julio Saez-Rodriguez, Lenore Cowen, Sven Bergmann, Daniel Marbach. Assessment of network module identification across complex diseases. Nature Methods 16 (2019) 843-852. doi: \u003ca href=\"https://doi.org/10.1038/s41592-019-0509-5\" rel=\"nofollow\"\u003ehttps://doi.org/10.1038/s41592-019-0509-5\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 29,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1626079655.0
  },
  {
    "data_format": 2,
    "description": "Bioinformatics tool outputs converter to JSON or YAML",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "bow/crimson",
    "latest_release": "v0.5.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-crimson\" class=\"anchor\" href=\"#crimson\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ccode\u003ecrimson\u003c/code\u003e\n\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://pypi.org/project/crimson\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b88d8e72d91a58b5e0d8651dfb03de679b9d91a059cb88b8f4809639801721f3/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6372696d736f6e\" alt=\"pypi\" data-canonical-src=\"https://img.shields.io/pypi/v/crimson\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://builds.sr.ht/~bow/crimson?\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/21063ea62384043fb1ba2e562d346bea30f475aa5b3a714dbce1776ad5f0da70/68747470733a2f2f6275696c64732e73722e68742f7e626f772f6372696d736f6e2e737667\" alt=\"sourcehut\" data-canonical-src=\"https://builds.sr.ht/~bow/crimson.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ecrimson\u003c/code\u003e converts non-standard bioinformatics tool outputs to JSON or YAML.\u003c/p\u003e\n\u003cp\u003eCurrently it can convert outputs of the following tools:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.bioinformatics.babraham.ac.uk/projects/fastqc/%3E\" rel=\"nofollow\"\u003eFastQC\u003c/a\u003e (\u003ccode\u003efastqc\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/ndaniel/fusioncatcher\"\u003eFusionCatcher\u003c/a\u003e (\u003ccode\u003efusioncatcher\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.htslib.org/doc/samtools.html\" rel=\"nofollow\"\u003esamtools\u003c/a\u003e flagstat (\u003ccode\u003eflagstat\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://broadinstitute.github.io/picard/\" rel=\"nofollow\"\u003ePicard\u003c/a\u003e metrics tools (\u003ccode\u003epicard\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/alexdobin/STAR\"\u003eSTAR\u003c/a\u003e log file (\u003ccode\u003estar\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/STAR-Fusion/STAR-Fusion\"\u003eSTAR-Fusion\u003c/a\u003e hits table (\u003ccode\u003estar-fusion\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"http://www.ensembl.org/info/docs/tools/vep/index.html\" rel=\"nofollow\"\u003eVariant Effect Predictor\u003c/a\u003e\nplain text output (\u003ccode\u003evep\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe conversion can be done using the command line interface or by calling the\ntool-specificparser functions in your Python script.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003ecrimson\u003c/code\u003e is available on the \u003ca href=\"https://pypi.org/project/crimson/\" rel=\"nofollow\"\u003ePython Package Index\u003c/a\u003e\nand you can install it via \u003ccode\u003epip\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ pip install crimson\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIt is also available on\n\u003ca href=\"https://bioconda.github.io/recipes/crimson/README.html\" rel=\"nofollow\"\u003eBioConda\u003c/a\u003e, both through the\n\u003ccode\u003econda\u003c/code\u003e package manager or as a\n\u003ca href=\"https://quay.io/repository/biocontainers/crimson?tab=tags\" rel=\"nofollow\"\u003eDocker container\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-as-a-command-line-tool\" class=\"anchor\" href=\"#as-a-command-line-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAs a command line tool\u003c/h3\u003e\n\u003cp\u003eThe general command is \u003ccode\u003ecrimson {program_name}\u003c/code\u003e and by default the output is written to\n\u003ccode\u003estdout\u003c/code\u003e. For example, to use the \u003ccode\u003epicard\u003c/code\u003e parser, you would execute:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ crimson picard /path/to/a/picard.metrics\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can also specify a file name directly to write to a file. The following command will\nwrite the output to a file named \u003ccode\u003econverted.json\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ crimson picard /path/to/a/picard.metrics converted.json\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eSome parsers may also accept additional input format. The FastQC parser, for example, also\nworks if you specify a path to a FastQC output directory:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ crimson fastqc /path/to/a/fastqc/dir\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor path to a zipped result:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ crimson fastqc /path/to/a/fastqc_result.zip\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhen in doubt, use the \u003ccode\u003e--help\u003c/code\u003e flag:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e$ crimson --help            \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e for the general help\u003c/span\u003e\n$ crimson fastqc --help     \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e for parser-specific (FastQC) help\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-as-a-python-library-function\" class=\"anchor\" href=\"#as-a-python-library-function\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAs a Python library function\u003c/h3\u003e\n\u003cp\u003eGenerally, the function to import is located at \u003ccode\u003ecrimson.{program_name}.parser\u003c/code\u003e. For\nexample, to use the \u003ccode\u003epicard\u003c/code\u003e parser in your script, you can do:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ecrimson\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003epicard\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# You can specify the input file name as a string ...\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eparsed\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003epicard\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eparse\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"/path/to/a/picard.metrics\"\u003c/span\u003e)\n\n\u003cspan class=\"pl-c\"\u003e# ... or a file handle\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003ewith\u003c/span\u003e \u003cspan class=\"pl-en\"\u003eopen\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"/path/to/a/picard.metrics\"\u003c/span\u003e) \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003esrc\u003c/span\u003e:\n    \u003cspan class=\"pl-s1\"\u003eparsed\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003epicard\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eparse\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003esrc\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-why\" class=\"anchor\" href=\"#why\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhy?\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eNot enough tools use standard output formats.\u003c/li\u003e\n\u003cli\u003eWriting and re-writing the same parsers across different scripts is not a productive\nway to spend the day.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-local-development\" class=\"anchor\" href=\"#local-development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLocal Development\u003c/h2\u003e\n\u003cp\u003eSetting up a local development requires that you set up all of the supported Python\nversions. We recommend using \u003ca href=\"https://github.com/pyenv/pyenv\"\u003epyenv\u003c/a\u003e for this.\u003c/p\u003e\n\u003cp\u003eThe following steps can be your guide for your local development setup:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Clone the repository and cd into it.\u003c/span\u003e\n$ git clone https://git.sr.ht/~bow/crimson\n$ \u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e crimson\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Create your virtualenv.\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e If you already have pyenv installed, you may use the Makefile rule below.\u003c/span\u003e\n$ make dev-pyenv\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Install the package along with its development dependencies.\u003c/span\u003e\n$ make dev\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Run the test and linter suite to verify the setup.\u003c/span\u003e\n$ make lint \u003cspan class=\"pl-c1\"\u003etest\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eIf you are interested, \u003ccode\u003ecrimson\u003c/code\u003e accepts the following types contribution:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDocumentation additions (if anything seems unclear, feel free to open an issue)\u003c/li\u003e\n\u003cli\u003eBug reports\u003c/li\u003e\n\u003cli\u003eSupport for tools\u0027 outputs which can be converted to JSON or YAML.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor any of these, feel free to open an issue in the \u003ca href=\"https://github.com/bow/crimson/issues%3E\"\u003eissue\ntracker\u003c/a\u003e or submit a pull request.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003ecrimson\u003c/code\u003e is BSD-licensed. Refer to the \u003ccode\u003eLICENSE\u003c/code\u003e file for the full license.\u003c/p\u003e\n",
    "stargazers_count": 30,
    "subscribers_count": 3,
    "topics": [
      "bioinformatics",
      "json",
      "yaml",
      "converter"
    ],
    "updated_at": 1626187656.0
  },
  {
    "data_format": 2,
    "description": " A Simple Way of Creating Job Workflows in Go running in Processes, Containers, Tasks, Pods, or Jobs ",
    "filenames": [
      "examples/singularity/SingularityRecipe"
    ],
    "full_name": "dgruber/wfl",
    "latest_release": "v1.2.6",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-wfl---a-simple-and-pluggable-workflow-language-for-go\" class=\"anchor\" href=\"#wfl---a-simple-and-pluggable-workflow-language-for-go\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ewfl - A Simple and Pluggable Workflow Language for Go\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003eDon\u0027t mix wfl with \u003ca href=\"https://en.wikipedia.org/wiki/Work_Flow_Language\" rel=\"nofollow\"\u003eWFL\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/dgruber/wfl/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2e68f5d3e6715dff94d43886169409bb991226c5562ede8aa6996cb6cd43277d/68747470733a2f2f636972636c6563692e636f6d2f67682f646772756265722f77666c2f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/dgruber/wfl/tree/master.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/dgruber/wfl\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/c0ba5989abe58a671d275f14915277b3c1ab0782fd00555895dfc667863f9c93/68747470733a2f2f636f6465636f762e696f2f67682f646772756265722f77666c2f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/dgruber/wfl/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003eUpdate\u003c/em\u003e: In order to reflect the underlying drmaa2os changes which separates\ndifferent backends more clearly some context creation functions are moved\nto pkg/context. That avoids having to deal with dependencies from bigger libraries\nlike Kubernetes or Docker when not using them.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eCreating process, container, pod, task, or job workflows based on raw interfaces of\noperating systems, Docker, Singularity, Kubernetes, Cloud Foundry, and HPC job schedulers can be\na tedios. Lots of repeating code is required. All workload management systems have a\ndifferent API.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003ewfl\u003c/em\u003e abstracts away from the underlying details of the processes, containers, and\nworkload management systems. \u003cem\u003ewfl\u003c/em\u003e provides a simple, unified interface which allows\nto quickly define and execute a job workflow and change between different execution\nbackends without changing the workflow itself.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003ewfl\u003c/em\u003e does not come with many features but is simple to use and enough to define and\nrun jobs and job workflows with inter-job dependencies.\u003c/p\u003e\n\u003cp\u003eIn its simplest form a process can be started and waited for:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"convert\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"image.jpg\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"image.png\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf the output of the command needs to be displayed on the terminal you can set the out path in the\ndefault \u003cem\u003eJobTemplate\u003c/em\u003e (see below) configuration:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e\t\u003cspan class=\"pl-s1\"\u003etemplate\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e drmaa2interface.\u003cspan class=\"pl-smi\"\u003eJobTemplate\u003c/span\u003e{\n\t\t\u003cspan class=\"pl-c1\"\u003eErrorPath\u003c/span\u003e:  \u003cspan class=\"pl-s\"\u003e\"/dev/stderr\"\u003c/span\u003e,\n\t\t\u003cspan class=\"pl-c1\"\u003eOutputPath\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"/dev/stdout\"\u003c/span\u003e,\n\t}\n\t\u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContextByCfg\u003c/span\u003e(wfl.\u003cspan class=\"pl-smi\"\u003eProcessConfig\u003c/span\u003e{\n\t\t\u003cspan class=\"pl-c1\"\u003eDefaultTemplate\u003c/span\u003e: \u003cspan class=\"pl-s1\"\u003etemplate\u003c/span\u003e,\n\t}))\n\t\u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"hello\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eRunning a job as a Docker container requires a different context (and the image\nalready pulled before).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e (\n\t\u003cspan class=\"pl-s\"\u003e\"github.com/dgruber/drmaa2interface\"\u003c/span\u003e\n\t\u003cspan class=\"pl-s\"\u003e\"github.com/dgruber/wfl\"\u003c/span\u003e\n\t\u003cspan class=\"pl-s\"\u003e\"github.com/dgruber/wfl/pkg/context/docker\"\u003c/span\u003e\n    )\n    \n    \u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003edocker\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewDockerContextByCfg\u003c/span\u003e(docker.\u003cspan class=\"pl-smi\"\u003eConfig\u003c/span\u003e{\u003cspan class=\"pl-c1\"\u003eDefaultDockerImage\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"golang:latest\"\u003c/span\u003e})\n    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"60\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eStarting a Docker container without a \u003cem\u003erun command\u003c/em\u003e which exposes ports requires more\nconfiguration which can be provided by using a \u003cem\u003eJobTemplate\u003c/em\u003e together with the \u003cem\u003eRunT()\u003c/em\u003e\nmethod.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ejt\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e drmaa2interface.\u003cspan class=\"pl-smi\"\u003eJobTemplate\u003c/span\u003e{\n        \u003cspan class=\"pl-c1\"\u003eJobCategory\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"swaggerapi/swagger-editor\"\u003c/span\u003e,\n    }\n    \u003cspan class=\"pl-s1\"\u003ejt\u003c/span\u003e.\u003cspan class=\"pl-c1\"\u003eExtensionList\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-k\"\u003emap\u003c/span\u003e[\u003cspan class=\"pl-smi\"\u003estring\u003c/span\u003e]\u003cspan class=\"pl-smi\"\u003estring\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"exposedPorts\"\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"80:8080/tcp\"\u003c/span\u003e}\n    \n    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewJob\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003edocker\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewDockerContext\u003c/span\u003e())).\u003cspan class=\"pl-en\"\u003eRunT\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ejt\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eStarting a Kubernetes batch job and waiting for its end is not much different.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ekubernetes\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewKubernetesContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"60\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003ewfl\u003c/em\u003e also supports submitting jobs into HPC schedulers like SLURM, Grid Engine and so on.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003elibdrmaa\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewLibDRMAAContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"60\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003ewfl\u003c/em\u003e aims to work for any kind of workload. It works on a Mac and Raspberry Pi the same way\nas on a high-performance compute cluster. Things missing: On small scale you probably miss data\nmanagement - moving results from one job to another. That\u0027s deliberately not implemented.\nOn large scale you are missing checkpoint and restart functionality or HA of the workflow\nprocess itself.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003ewfl\u003c/em\u003e works with simple primitives: \u003cem\u003econtext\u003c/em\u003e, \u003cem\u003eworkflow\u003c/em\u003e, \u003cem\u003ejob\u003c/em\u003e, and \u003cem\u003ejobtemplate\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eExperimental: Jobs can also be processed in \u003ca href=\"https://github.com/dgruber/wfl/blob/master/examples/stream/stream.go\"\u003ejob control streams\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFirst support for logging is also available. Log levels can be controlled by environment variables\n(\u003cem\u003eexport WFL_LOGLEVEL=DEBUG\u003c/em\u003e or \u003cem\u003eINFO\u003c/em\u003e/\u003cem\u003eWARNING\u003c/em\u003e/\u003cem\u003eERROR\u003c/em\u003e/\u003cem\u003eNONE\u003c/em\u003e). Applications can use the same\nlogging facility by getting the logger from the workflow (\u003cem\u003eworkflow.Logger()\u003c/em\u003e) or registering\nyour own logger in a workflow \u003cem\u003e(workflow.SetLogger(Logger interface)\u003c/em\u003e). Default is set to ERROR.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started\u003c/h3\u003e\n\u003cp\u003eDependencies of \u003cem\u003ewfl\u003c/em\u003e (like drmaa2) are vendored in. The only external package required to be installed\nmanually is the \u003cem\u003edrmaa2interface\u003c/em\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-k\"\u003ego\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eget\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egithub\u003c/span\u003e.\u003cspan class=\"pl-c1\"\u003ecom\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edgruber\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e/\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003edrmaa2interface\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-context\" class=\"anchor\" href=\"#context\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContext\u003c/h2\u003e\n\u003cp\u003eA context defines the execution backend for the workflow. Contexts can be easily created\nwith the \u003cem\u003eNew\u003c/em\u003e functions which are defined in the \u003cem\u003econtext.go\u003c/em\u003e file or in the separate\npackages found in \u003cem\u003epkg/context\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eFor creating a context which executes the jobs of a workflow in operating system processses use:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf the workflow needs to be executed in containers the \u003cem\u003eDockerContext\u003c/em\u003e can be used:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003edocker\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewDockerContext\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf the Docker context needs to be configured with a default Docker image\n(when Run() is used or RunT() without a configured \u003cem\u003eJobCategory\u003c/em\u003e (which \u003cem\u003eis\u003c/em\u003e the Docker image))\nthen the \u003cem\u003eContextByCfg()\u003c/em\u003e can be called.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003edocker\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewDockerContextByCfg\u003c/span\u003e(docker.\u003cspan class=\"pl-smi\"\u003eConfig\u003c/span\u003e{\u003cspan class=\"pl-c1\"\u003eDefaultDockerImage\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"golang:latest\"\u003c/span\u003e})\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWhen you want to run the workflow as Cloud Foundry tasks the \u003cem\u003eCloudFoundryContext\u003c/em\u003e can be used:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ecloudfoundry\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewCloudFoundryContext\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eWithout a config it uses following environment variables to access the Cloud Foundry cloud controller API:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCF_API (like \u003ca href=\"https://api.run.pivotal.io\" rel=\"nofollow\"\u003ehttps://api.run.pivotal.io\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eCF_USER\u003c/li\u003e\n\u003cli\u003eCF_PASSWORD\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor submitting Kubernetes batch jobs a Kubernetes context exists.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e   \u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ekubernetes\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewKubernetesContext\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNote that each job requires a container image specified which can be done by using\nthe JobTemplate\u0027s JobCategory. When the same container image is used within the whole\njob workflow it makes sense to use the Kubernetes config.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e   \u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ekubernetes\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewKubernetesContextByCfg\u003c/span\u003e(kubernetes.\u003cspan class=\"pl-smi\"\u003eConfig\u003c/span\u003e{\u003cspan class=\"pl-c1\"\u003eDefaultImage\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"busybox:latest\"\u003c/span\u003e})\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Singularity_(software)\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e containers can be executed\nwithin the Singularity context. When setting the \u003cem\u003eDefaultImage\u003c/em\u003e (like in the Kubernetes Context)\nthen then \u003cem\u003eRun()\u003c/em\u003e methods can be used otherwise the Container image must be specified in the\nJobTemplate\u0027s \u003cem\u003eJobCategory\u003c/em\u003e field separately for each job. The \u003cem\u003eDefaultImage\u003c/em\u003e\ncan always be overridden by the \u003cem\u003eJobCategory\u003c/em\u003e. Note that each task / job\nexecutes a separate Singularity container process.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e   \u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e wfl.\u003cspan class=\"pl-smi\"\u003eNewSingularityContextByCfg\u003c/span\u003e(wfl.\u003cspan class=\"pl-smi\"\u003eSingularityConfig\u003c/span\u003e{\u003cspan class=\"pl-c1\"\u003eDefaultImage\u003c/span\u003e: \u003cspan class=\"pl-s\"\u003e\"\"\u003c/span\u003e}))\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor working with HPC schedulers the libdrmaa context can be used. This context requires\n\u003cem\u003elibdrmaa.so\u003c/em\u003e available in the library path at runtime. Grid Engine ships \u003cem\u003elibdrmaa.so\u003c/em\u003e\nbut the \u003cem\u003eLD_LIBRARY_PATH\u003c/em\u003e needs to be typically set. For SLURM \u003cem\u003elibdrmaa.so\u003c/em\u003e often needs\nto be \u003ca href=\"https://github.com/natefoo/slurm-drmaa\"\u003ebuild\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSince C go is used under the hood (drmaa2os which uses go drmaa) some compiler flags needs\nto be set during build time. Those flags depend on the workload manager used. Best check\nout the go drmaa project for finding the right flags.\u003c/p\u003e\n\u003cp\u003eFor building SLURM requires:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eexport CGO_LDFLAGS=\"-L$SLURM_DRMAA_ROOT/lib\"\nexport CGO_CFLAGS=\"-DSLURM -I$SLURM_DRMAA_ROOT/include\"\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf all set a libdrmaa context can be created by importing:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e   \u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003elibdrmaa\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewLibDRMAAContext\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe JobCategory is whatever the workloadmanager associates with it. Typically it is a\nset of submission parameters. A basic example is \u003ca href=\"https://github.com/dgruber/wfl/blob/master/examples/libdrmaa/libdrmaa.go\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-workflow\" class=\"anchor\" href=\"#workflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWorkflow\u003c/h2\u003e\n\u003cp\u003eA workflow encapsulates a set of jobs using the same backend (context). Depending on the execution\nbackend it can be seen as a namespace.\u003c/p\u003e\n\u003cp\u003eIt can be created by using:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewf\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eErrors during creation can be catched with\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewf\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eOnError\u003c/span\u003e(\u003cspan class=\"pl-k\"\u003efunc\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ee\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eerror\u003c/span\u003e) {\u003cspan class=\"pl-en\"\u003epanic\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ee\u003c/span\u003e)})\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor with\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewf\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eHasError\u003c/span\u003e() {\n        \u003cspan class=\"pl-en\"\u003epanic\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewf\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eError\u003c/span\u003e())\n    }\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-job\" class=\"anchor\" href=\"#job\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJob\u003c/h2\u003e\n\u003cp\u003eJobs are the main objects in \u003cem\u003ewfl\u003c/em\u003e. A job defines helper methods. Many of them return the job object itself to allow chaining calls in an easy way. A job can also be seen as a container and control unit for tasks. Tasks are often mapped to jobs of the underlying\nworkload manager (like in Kubernetes, HPC schedulers etc.).\u003c/p\u003e\n\u003cp\u003eIn some systems it is required to delete job related resources after the job is finished\nand no more information needs to be queried about its execution. This functionality is\nimplemented in the DRMAA2 Reap() method which can be executed by ReapAll() for each\ntask in the job object. Afterwards the job object should not be used anymore as some\ninformation might not be available anymore.\u003c/p\u003e\n\u003cp\u003eMethods can be classified in blocking, non-blocking, job template based, function based, and error handlers.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-job-submission\" class=\"anchor\" href=\"#job-submission\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJob Submission\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFunction Name\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003cth\u003eBlocking\u003c/th\u003e\n\u003cth\u003eExamples\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eRun()\u003c/td\u003e\n\u003ctd\u003eStarts a process, container, or submits a task and comes back immediately\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRunT()\u003c/td\u003e\n\u003ctd\u003eLike above but with a JobTemplate as parameter\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRunArray()\u003c/td\u003e\n\u003ctd\u003eSubmits a bulk job which runs many iterations of the same command\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eResubmit()\u003c/td\u003e\n\u003ctd\u003eSubmits a job \u003cem\u003en\u003c/em\u003e-times (Run().Run().Run()...)\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRunEvery()\u003c/td\u003e\n\u003ctd\u003eSubmits a task every d \u003cem\u003etime.Duration\u003c/em\u003e\n\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRunEveryT()\u003c/td\u003e\n\u003ctd\u003eLike \u003cem\u003eRunEvery()\u003c/em\u003e but with JobTemplate as param\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-job-control\" class=\"anchor\" href=\"#job-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJob Control\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFunction Name\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003cth\u003eBlocking\u003c/th\u003e\n\u003cth\u003eExamples\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSuspend()\u003c/td\u003e\n\u003ctd\u003eStops a task from execution (e.g. sending SIGTSTP to the process group)...\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eResume()\u003c/td\u003e\n\u003ctd\u003eContinues a task (e.g. sending SIGCONT)...\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eKill()\u003c/td\u003e\n\u003ctd\u003eStops process (SIGKILL), container, task, job immediately.\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-function-execution\" class=\"anchor\" href=\"#function-execution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFunction Execution\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFunction Name\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003cth\u003eBlocking\u003c/th\u003e\n\u003cth\u003eExamples\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eDo()\u003c/td\u003e\n\u003ctd\u003eExecutes a Go function\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eThen()\u003c/td\u003e\n\u003ctd\u003eWaits for end of process and executes a Go function\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnSuccess()\u003c/td\u003e\n\u003ctd\u003eExecutes a function if the task run successfully (exit code 0)\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnFailure()\u003c/td\u003e\n\u003ctd\u003eExecutes a function if the task failed (exit code != 0)\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnError()\u003c/td\u003e\n\u003ctd\u003eExecutes a function if the task could not be created\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-blocker\" class=\"anchor\" href=\"#blocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBlocker\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFunction Name\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003cth\u003eBlocking\u003c/th\u003e\n\u003cth\u003eExamples\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eAfter()\u003c/td\u003e\n\u003ctd\u003eBlocks a specific amount of time and continues\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWait()\u003c/td\u003e\n\u003ctd\u003eWaits until the task submitted latest finished\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSynchronize()\u003c/td\u003e\n\u003ctd\u003eWaits until all submitted tasks finished\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-job-flow-control\" class=\"anchor\" href=\"#job-flow-control\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJob Flow Control\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFunction Name\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003cth\u003eBlocking\u003c/th\u003e\n\u003cth\u003eExamples\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eThenRun()\u003c/td\u003e\n\u003ctd\u003eWait() (last task finished) followed by an async Run()\u003c/td\u003e\n\u003ctd\u003epartially\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eThenRunT()\u003c/td\u003e\n\u003ctd\u003eThenRun() with template\u003c/td\u003e\n\u003ctd\u003epartially\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnSuccessRun()\u003c/td\u003e\n\u003ctd\u003eWait() if Success() then Run()\u003c/td\u003e\n\u003ctd\u003epartially\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnSuccessRunT()\u003c/td\u003e\n\u003ctd\u003eOnSuccessRun() but with template as param\u003c/td\u003e\n\u003ctd\u003epartially\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnFailureRun()\u003c/td\u003e\n\u003ctd\u003eWait() if Failed() then Run()\u003c/td\u003e\n\u003ctd\u003epartially\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eOnFailureRunT()\u003c/td\u003e\n\u003ctd\u003eOnFailureRun() but with template as param\u003c/td\u003e\n\u003ctd\u003epartially\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRetry()\u003c/td\u003e\n\u003ctd\u003ewait() + !success() + resubmit() + wait() + !success()\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eAnyFailed()\u003c/td\u003e\n\u003ctd\u003eCchecks if one of the tasks in the job failed\u003c/td\u003e\n\u003ctd\u003eyes\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-job-status-and-general-checks\" class=\"anchor\" href=\"#job-status-and-general-checks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJob Status and General Checks\u003c/h3\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eFunction Name\u003c/th\u003e\n\u003cth\u003ePurpose\u003c/th\u003e\n\u003cth\u003eBlocking\u003c/th\u003e\n\u003cth\u003eExamples\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eJobID()\u003c/td\u003e\n\u003ctd\u003eReturns the ID of the submitted job\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eJobInfo()\u003c/td\u003e\n\u003ctd\u003eReturns the DRMAA2 JobInfo of the job\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eTemplate()\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eState()\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eLastError()\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eFailed()\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSuccess()\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eExitStatus()\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eReapAll()\u003c/td\u003e\n\u003ctd\u003eCleans up all job related resources from the workload manager. Do not\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003euse the job object afterwards. Calls DRMAA2 Reap() on all tasks.\u003c/td\u003e\n\u003ctd\u003eno\u003c/td\u003e\n\u003ctd\u003e\u00a0\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-jobtemplate\" class=\"anchor\" href=\"#jobtemplate\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eJobTemplate\u003c/h2\u003e\n\u003cp\u003eJobTemplates are specifying the details about a job. In the simplest case the job is specified by the application name and its arguments like it is typically done in the OS shell. In that case the \u003cem\u003eRun()\u003c/em\u003e methods (\u003cem\u003eThenRun()\u003c/em\u003e, \u003cem\u003eOnSuccessRun()\u003c/em\u003e, \u003cem\u003eOnFailureRun()\u003c/em\u003e) can be used. Job template based methods (like \u003cem\u003eRunT()\u003c/em\u003e) can be completely avoided by providing a\ndefault template when creating the context (\u003cem\u003e...ByConfig()\u003c/em\u003e). Then each \u003cem\u003eRun()\u003c/em\u003e inherits the settings (like \u003cem\u003eJobCategory\u003c/em\u003e for the container image name and \u003cem\u003eOutputPath\u003c/em\u003e for redirecting output to \u003cem\u003estdout\u003c/em\u003e). If more details for specifying the jobs are required the \u003cem\u003eRunT()\u003c/em\u003e methods needs to be used.\nI\u0027m using currently the \u003ca href=\"https://github.com/dgruber/drmaa2interface/blob/master/jobtemplate.go\"\u003eDRMAA2 Go JobTemplate\u003c/a\u003e. In most cases only \u003cem\u003eRemoteCommand\u003c/em\u003e, \u003cem\u003eArgs\u003c/em\u003e, \u003cem\u003eWorkingDirectory\u003c/em\u003e, \u003cem\u003eJobCategory\u003c/em\u003e, \u003cem\u003eJobEnvironment\u003c/em\u003e,  \u003cem\u003eStageInFiles\u003c/em\u003e are evaluated. Functionality and semantic is up to the underlying \u003ca href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker\"\u003edrmaa2os job tracker\u003c/a\u003e.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker/simpletracker\"\u003eFor the process mapping see here\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/libdrmaa\"\u003eFor the mapping to a drmaa1 implementation (libdrmaa.so) for SLURM, Grid Engine, PBS, ...\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dgruber/drmaa2os/tree/master/pkg/jobtracker/dockertracker\"\u003eFor the Docker mapping here\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/cftracker\"\u003eFor the Cloud Foundry Task mapping here\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/kubernetestracker\"\u003eFor the Kubernetes batch job mapping here\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/dgruber/drmaa2os/blob/master/pkg/jobtracker/singularity\"\u003eSingularity support\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe \u003ca href=\"https://github.com/dgruber/wfl/blob/master/template.go\"\u003e\u003cem\u003eTemplate\u003c/em\u003e\u003c/a\u003e object provides helper functions for job templates and required as generators of job \u003ca href=\"https://github.com/dgruber/wfl/blob/master/examples/stream/stream.go\"\u003estreams\u003c/a\u003e. For an example see \u003ca href=\"https://github.com/dgruber/wfl/tree/master/examples/template/template.go\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h1\u003e\n\u003cp\u003eFor examples please have a look into the examples directory. \u003ca href=\"https://github.com/dgruber/wfl/tree/master/examples/template/template.go\"\u003etemplate\u003c/a\u003e is a canonical example of a pre-processing job, followed by parallel execution, followed by a post-processing job.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dgruber/wfl/blob/master/test/test.go\"\u003etest\u003c/a\u003e is an use case for testing. It compiles\nall examples with the local go compiler and then within a Docker container using the \u003cem\u003egolang:latest\u003c/em\u003e image\nand reports errors.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dgruber/wfl/blob/master/examples/cloudfoundry/cloudfoundry.go\"\u003ecloudfoundry\u003c/a\u003e demonstrates how a Cloud Foundry taks can be created.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/dgruber/wfl/blob/master/examples/singularity/singularity.go\"\u003eSingularity containers\u003c/a\u003e can also be created which is helpful when managing a simple Singularity \u003cem\u003ewfl\u003c/em\u003e container workflow within a single HPC job either to fully exploit all resources and reduce the amount of HPC jobs.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-creating-a-workflow-which-is-executed-as-os-processes\" class=\"anchor\" href=\"#creating-a-workflow-which-is-executed-as-os-processes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCreating a Workflow which is Executed as OS Processes\u003c/h2\u003e\n\u003cp\u003eThe allocated context defines which workload management system / job execution backend is used.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eDifferent contexts can be used within a single program. That way multi-clustering potentially\nover different cloud solutions is supported.\u003c/p\u003e\n\u003cp\u003eUsing a context a workflow can be established.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e())\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eHandling an error during workflow generation can be done by specifying a function which\nis only called in the case of an error.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eOnError\u003c/span\u003e(\u003cspan class=\"pl-k\"\u003efunc\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ee\u003c/span\u003e \u003cspan class=\"pl-smi\"\u003eerror\u003c/span\u003e) {\n\t\t\u003cspan class=\"pl-en\"\u003epanic\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ee\u003c/span\u003e)\n\t})\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe workflow is used in order to instantiate the first job using the \u003cem\u003eRun()\u003c/em\u003e method.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"123\"\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eBut you can also create an initial job like that:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewJob\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()))\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor more detailed settings (like resource limits) the DRMAA2 job template can be used as parameter for \u003cem\u003eRunT()\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eJobs allow the execution of workload as well as expressing dependencies.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"2\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eThenRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe line above executes two OS processes sequentially and waits until the last job in chain is finished.\u003c/p\u003e\n\u003cp\u003eIn the following example the two sleep processes are executed in parallel. \u003cem\u003eWait()\u003c/em\u003e only waitf for the sleep 1 job. Hence sleep 2 still runs after the wait call comes back.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"2\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eRunning two jobs in parallel and waiting until all jobs finished can be done \u003cem\u003eSynchronize()\u003c/em\u003e.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewProcessContext\u003c/span\u003e()).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"2\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eSynchronize\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eJobs can also be suspended (stopped) and resumed (continued) - if supported by the execution backend (like OS, Docker).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewf\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eAfter\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etime\u003c/span\u003e.\u003cspan class=\"pl-c1\"\u003eMillisecond\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e100\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eSuspend\u003c/span\u003e().\u003cspan class=\"pl-en\"\u003eAfter\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etime\u003c/span\u003e.\u003cspan class=\"pl-c1\"\u003eMillisecond\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e100\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eResume\u003c/span\u003e().\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe exit status is available as well. \u003cem\u003eExitStatus()\u003c/em\u003e blocks until the previously submitted job is finished.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"hello\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eExitStatus\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIn order to run jobs depending on the exit status the \u003cem\u003eOnFailure\u003c/em\u003e and \u003cem\u003eOnSuccess\u003c/em\u003e methods can be used:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ewf\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"false\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eOnFailureRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"true\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eOnSuccessRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"false\"\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor executing a function on a submission error \u003cem\u003eOnError()\u003c/em\u003e can be used.\u003c/p\u003e\n\u003cp\u003eMore methods can be found in the sources.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-basic-workflow-patterns\" class=\"anchor\" href=\"#basic-workflow-patterns\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBasic Workflow Patterns\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-sequence\" class=\"anchor\" href=\"#sequence\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSequence\u003c/h3\u003e\n\u003cp\u003eThe successor task runs after the completion of the pre-decessor task.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eThenRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"second task\"\u003c/span\u003e)\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"second task\"\u003c/span\u003e)\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-parallel-split\" class=\"anchor\" href=\"#parallel-split\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eParallel Split\u003c/h3\u003e\n\u003cp\u003eAfter completion of a task run multiple branches of tasks.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e\n    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\n\n    \u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewNotifier\u003c/span\u003e()\n\n    \u003cspan class=\"pl-k\"\u003ego\u003c/span\u003e \u003cspan class=\"pl-k\"\u003efunc\u003c/span\u003e() {\n        \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewJob\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)).\n            \u003cspan class=\"pl-en\"\u003eTagWith\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"BranchA\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eThenRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"3\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eSynchronize\u003c/span\u003e().\n            \u003cspan class=\"pl-en\"\u003eNotify\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e)\n    }\n\n    \u003cspan class=\"pl-k\"\u003ego\u003c/span\u003e \u003cspan class=\"pl-k\"\u003efunc\u003c/span\u003e() {\n        \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewJob\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)).\n            \u003cspan class=\"pl-en\"\u003eTagWith\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"BranchB\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eThenRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"3\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eSynchronize\u003c/span\u003e().\n            \u003cspan class=\"pl-en\"\u003eNotify\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e)\n    }\n\n    \u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eReceiveJob\u003c/span\u003e()\n    \u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eReceiveJob\u003c/span\u003e()\n\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-synchronization-of-tasks\" class=\"anchor\" href=\"#synchronization-of-tasks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSynchronization of Tasks\u003c/h3\u003e\n\u003cp\u003eWait until all tasks of a job which are running in parallel are finished.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e).\n        \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"second task\"\u003c/span\u003e).\n        \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"third task\"\u003c/span\u003e).\n        \u003cspan class=\"pl-en\"\u003eSynchronize\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-synchronization-of-branches\" class=\"anchor\" href=\"#synchronization-of-branches\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSynchronization of Branches\u003c/h3\u003e\n\u003cp\u003eWait until all branches of a workflow are finished.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e\n    \u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewNotifier\u003c/span\u003e()\n\n    \u003cspan class=\"pl-k\"\u003ego\u003c/span\u003e \u003cspan class=\"pl-k\"\u003efunc\u003c/span\u003e() {\n        \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewJob\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)).\n            \u003cspan class=\"pl-en\"\u003eTagWith\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"BranchA\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e().\n\t\t\t\u003cspan class=\"pl-en\"\u003eNotify\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e)\n    }\n\n    \u003cspan class=\"pl-k\"\u003ego\u003c/span\u003e \u003cspan class=\"pl-k\"\u003efunc\u003c/span\u003e() {\n        \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewJob\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)).\n            \u003cspan class=\"pl-en\"\u003eTagWith\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"BranchB\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"sleep\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"1\"\u003c/span\u003e).\n            \u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e().\n\t\t\t\u003cspan class=\"pl-en\"\u003eNotify\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e)\n    }\n\n    \u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eReceiveJob\u003c/span\u003e()\n    \u003cspan class=\"pl-s1\"\u003enotifier\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eReceiveJob\u003c/span\u003e()\n\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-exclusive-choice\" class=\"anchor\" href=\"#exclusive-choice\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExclusive Choice\u003c/h3\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\n\n    \u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eSuccess\u003c/span\u003e() {\n        \u003cspan class=\"pl-c\"\u003e// do something\u003c/span\u003e\n    } \u003cspan class=\"pl-k\"\u003eelse\u003c/span\u003e {\n        \u003cspan class=\"pl-c\"\u003e// do something else\u003c/span\u003e\n    }\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-fork-pattern\" class=\"anchor\" href=\"#fork-pattern\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFork Pattern\u003c/h3\u003e\n\u003cp\u003eWhen a task is finished \u003cem\u003en\u003c/em\u003e tasks needs to be started in parallel.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e).\n        \u003cspan class=\"pl-en\"\u003eThenRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"parallel task 1\"\u003c/span\u003e).\n        \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"parallel task 2\"\u003c/span\u003e).\n        \u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"parallel task 3\"\u003c/span\u003e)\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-go\"\u003e\u003cpre\u003e    \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ewfl\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eNewWorkflow\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ectx\u003c/span\u003e)\n    \n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eflow\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"first task\"\u003c/span\u003e)\n    \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eWait\u003c/span\u003e()\n    \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e:=\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e; \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e\u0026lt;=\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e3\u003c/span\u003e; \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e++\u003c/span\u003e {\n        \u003cspan class=\"pl-s1\"\u003ejob\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eRun\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"echo\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efmt\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eSprintf\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"parallel task %d\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e))\n    }\n    \u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor missing functionality or bugs please open an issue on github. Contributions welcome!\u003c/p\u003e\n",
    "stargazers_count": 33,
    "subscribers_count": 4,
    "topics": [
      "docker",
      "processes",
      "cloud-foundry",
      "k8s",
      "workflow",
      "hpc",
      "macos",
      "linux",
      "high-throughput",
      "singularity"
    ],
    "updated_at": 1622034364.0
  },
  {
    "data_format": 2,
    "description": "A repository of definition files for bootstrapping Singularity containers around the software applications, frameworks, and libraries you need to run on high-performance computing systems.",
    "filenames": [
      "definition-files/ciml/Singularity.tape-0.4",
      "definition-files/ciml/Singularity.r-3.6.1",
      "definition-files/ciml/Singularity.pyspark-3.1.2",
      "definition-files/ciml/Singularity.esm-0.3.1",
      "definition-files/ciml/Singularity.sparkr-2.3.1",
      "definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5",
      "definition-files/paraview/Singularity.paraview-5.8.1-ubuntu-18.04-openmpi-3.1.4-osmesa-20.1.5",
      "definition-files/paraview/Singularity.paraview-5.9.0-ubuntu-18.04-openmpi-3.1.6-osmesa-20.1.5",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.6",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2-openmpi-4.0.5",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.2-openmpi-3.1.6",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-11.2",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-mvapich-2.3.2",
      "definition-files/ubuntu/Singularity.ubuntu-18.04",
      "definition-files/ubuntu/Singularity.ubuntu-20.04",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-4.0.5",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-openmpi-3.1.4",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168",
      "definition-files/ubuntu/Singularity.ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/tensorflow/Singularity.tensorflow-2.5.0-ubuntu-18.04-cuda-11.2-openmpi-4.0.5",
      "definition-files/tensorflow/Singularity.tensorflow-2.3.0-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/beast/Singularity.beast-2.6.1-ubuntu-18.04-cuda-10.1.168",
      "definition-files/beast/Singularity.beast-1.10.4-ubuntu-18.04-cuda-10.1.168",
      "definition-files/xcrysden/Singularity.xcrysden-1.6.2-ubuntu-18.04",
      "definition-files/miniconda/Singularity.miniconda3-py37-4.9.2-ubuntu-18.04",
      "definition-files/miniconda/Singularity.miniconda3-py38-4.9.2-ubuntu-18.04",
      "definition-files/miniconda/Singularity.miniconda2-py27-4.8.3-ubuntu-18.04",
      "definition-files/miniconda/Singularity.miniconda3-py39-4.9.2-ubuntu-18.04",
      "definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-4.0.5-openblas-0.3.14",
      "definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.6-openblas-0.3.10",
      "definition-files/hpl/Singularity.hpl-2.3-ubuntu-18.04-openmpi-3.1.4-openblas-0.3.10",
      "definition-files/singularity/Singularity.singularity-3.5.3",
      "definition-files/centos/Singularity.centos-7.9.2009-cuda-10.1.168",
      "definition-files/centos/Singularity.centos-7.9.2009-mvapich-2.3.2",
      "definition-files/centos/Singularity.centos-7.7.1908-openmpi-3.1.6",
      "definition-files/centos/Singularity.centos-7.9.2009-openmpi-3.1.4",
      "definition-files/centos/Singularity.centos-7.7.1908",
      "definition-files/centos/Singularity.centos-7.9.2009",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.4",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-openmpi-3.1.6",
      "definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-cuda-11.2-openmpi-4.0.5",
      "definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-openmpi-3.1.4",
      "definition-files/omb/Singularity.omb-5.6.3-centos-7.9.2009-mvapich-2.3.2",
      "definition-files/omb/Singularity.omb-5.7-centos-7.7.1908-openmpi-3.1.6",
      "definition-files/omb/Singularity.omb-5.6.3-ubuntu-18.04-mvapich-2.3.2",
      "definition-files/omb/Singularity.omb-5.7-ubuntu-18.04-openmpi-4.0.5",
      "definition-files/fenics/Singularity.fenics-2019.1.0-ubuntu-18.04-openmpi-3.1.4",
      "definition-files/stream/Singularity.stream-5.10-ubuntu-18.04",
      "definition-files/spark/Singularity.spark-3.1.2-hadoop-3.2-ubuntu-18.04",
      "definition-files/spark/Singularity.spark-2.3.1-hadoop-2.7-ubuntu-18.04",
      "definition-files/mxnet/Singularity.mxnet-1.7.0-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/gromacs/Singularity.gromacs-2020.3-ubuntu-18.04-cuda-10.1.168-tmpi-avx-512-cuda-70",
      "definition-files/anaconda/Singularity.anaconda2-py27-2019.10-ubuntu-18.04",
      "definition-files/anaconda/Singularity.anaconda3-py38-2020.11-ubuntu-18.04",
      "definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.4",
      "definition-files/ior/Singularity.ior-3.3.0-ubuntu-18.04-openmpi-4.0.5",
      "definition-files/ior/Singularity.ior-3.3.0rc1-ubuntu-18.04-openmpi-3.1.6",
      "definition-files/visit/Singularity.visit-3.1.4-ubuntu-18.04-openmpi-3.1.6",
      "definition-files/pytorch/Singularity.pytorch-1.5.0-ubuntu-18.04-cuda-10.1.168-openmpi-3.1.4",
      "definition-files/pytorch/Singularity.pytorch-1.8.1-ubuntu-18.04-cuda-11.2-openmpi-4.0.5"
    ],
    "full_name": "mkandes/naked-singularity",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-naked-singularity\" class=\"anchor\" href=\"#naked-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enaked-singularity\u003c/h1\u003e\n\u003cp\u003eA repository of definition files for building\n\u003ca href=\"https://sylabs.io/guides/latest/user-guide\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e containers\naround the software applications, frameworks, and libraries you need to\nrun on high-performance computing systems.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-singularity\" class=\"anchor\" href=\"#install-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Singularity\u003c/h2\u003e\n\u003cp\u003eInstall Singularity on your Linux desktop, laptop, or virtual machine.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo ./naked-singularity.sh install\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build-a-singularity-container-from-a-definition-file\" class=\"anchor\" href=\"#build-a-singularity-container-from-a-definition-file\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild a Singularity container from a definition file\u003c/h2\u003e\n\u003cp\u003eBuild an Ubuntu Singularity container from one of the definition files\navailable in this repository.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build ubuntu.sif definition-files/ubuntu/Singularity.ubuntu-18.04\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-download-an-existing-singularity-container\" class=\"anchor\" href=\"#download-an-existing-singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDownload an existing Singularity container\u003c/h2\u003e\n\u003cp\u003eA number of pre-built containers from this repository are also now\nhosted at Singularity Hub.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull shub://mkandes/naked-singularity:ubuntu-18.04\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003ca href=\"https://vsoch.github.io/2021/singularity-hub-archive\" rel=\"nofollow\"\u003eSingularity Hub has been archived\u003c/a\u003e. At least for the time being, naked-singularity definition\nfiles that rely on containers that were built and hosted on Singularity\nHub prior to it being archived will continue to pull in these container\ndependencies and build properly. Alternative container build and hosting\noptions for all future work are still under consideration.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-status\" class=\"anchor\" href=\"#status\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStatus\u003c/h2\u003e\n\u003cp\u003eA work in progress.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contribute\" class=\"anchor\" href=\"#contribute\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContribute\u003c/h2\u003e\n\u003cp\u003eIf you would like to contribute one of your own Singularity container\ndefinition files for a specific application OR request a modification to\nan existing container definition, then please submit a pull request.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-author\" class=\"anchor\" href=\"#author\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthor\u003c/h2\u003e\n\u003cp\u003eMarty Kandes, Ph.D.\u003cbr\u003e\nComputational \u0026amp; Data Science Research Specialist\u003cbr\u003e\nHigh-Performance Computing User Services Group\u003cbr\u003e\nSan Diego Supercomputer Center\u003cbr\u003e\nUniversity of California, San Diego\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-version\" class=\"anchor\" href=\"#version\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eVersion\u003c/h2\u003e\n\u003cp\u003e1.7.6\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-last-updated\" class=\"anchor\" href=\"#last-updated\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLast Updated\u003c/h2\u003e\n\u003cp\u003eFriday, July 23rd, 2021\u003c/p\u003e\n",
    "stargazers_count": 34,
    "subscribers_count": 2,
    "topics": [],
    "updated_at": 1627313852.0
  },
  {
    "data_format": 2,
    "description": "Collection of hyperparameter optimization benchmark problems",
    "filenames": [
      "hpobench/container/recipes/Singularity.template",
      "hpobench/container/recipes/rl/Singularity.learnaBenchmark",
      "hpobench/container/recipes/rl/Singularity.Cartpole",
      "hpobench/container/recipes/nas/Singularity.nasbench_201",
      "hpobench/container/recipes/nas/Singularity.TabularBenchmarks",
      "hpobench/container/recipes/nas/Singularity.nasbench_101",
      "hpobench/container/recipes/nas/Singularity.nasbench_1shot1",
      "hpobench/container/recipes/surrogates/Singularity.SupportVectorMachine",
      "hpobench/container/recipes/surrogates/Singularity.ParamnetBenchmark",
      "hpobench/container/recipes/ml/Singularity.PyBNN",
      "hpobench/container/recipes/ml/Singularity.SupportVectorMachine",
      "hpobench/container/recipes/ml/Singularity.XGBoostBenchmark"
    ],
    "full_name": "automl/HPOBench",
    "latest_release": "0.0.8",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hpobench\" class=\"anchor\" href=\"#hpobench\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPOBench\u003c/h1\u003e\n\u003cp\u003eHPOBench is a library for hyperparameter optimization and black-box optimization benchmark with a focus on reproducibility.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e HPOBench is under active construction. Stay tuned for more benchmarks. Information on how to contribute a new benchmark will follow shortly.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e If you are looking for a different or older version of our benchmarking library, you might be looking for\n\u003ca href=\"https://github.com/automl/HPOlib1.5\"\u003eHPOlib1.5\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-in-4-lines-of-code\" class=\"anchor\" href=\"#in-4-lines-of-code\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIn 4 lines of code\u003c/h2\u003e\n\u003cp\u003eRun a random configuration within a singularity container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003econtainer\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003econtainer_source\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027library://phmueller/automl\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"dataset_fraction\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e}, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAll benchmarks can also be queried with fewer or no fidelities:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003econtainer\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003econtainer_source\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027library://phmueller/automl\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e,}, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eContainerized benchmarks do not rely on external dependencies and thus do not change. To do so, we rely on \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/\" rel=\"nofollow\"\u003eSingularity (version 3.5)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFurther requirements are: \u003ca href=\"https://github.com/automl/ConfigSpace\"\u003eConfigSpace\u003c/a\u003e, \u003cem\u003escipy\u003c/em\u003e and \u003cem\u003enumpy\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e Each benchmark can also be run locally, but the dependencies must be installed manually and might conflict with other benchmarks.\nThis can be arbitrarily complex and further information can be found in the docstring of the benchmark.\u003c/p\u003e\n\u003cp\u003eA simple example is the XGBoost benchmark which can be installed with \u003ccode\u003epip install .[xgboost]\u003c/code\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e)\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfiguration\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"dataset_fraction\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e}, \u003cspan class=\"pl-s1\"\u003erng\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eBefore we start, we recommend using a virtual environment. To run any benchmark using its singularity container,\nrun the following:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/automl/HPOBench.git\ncd HPOBench \npip install .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e This does not install \u003cem\u003esingularity (version 3.5)\u003c/em\u003e. Please follow the steps described here: \u003ca href=\"https://sylabs.io/guides/3.5/user-guide/quick_start.html#quick-installation-steps\" rel=\"nofollow\"\u003euser-guide\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-available-containerized-benchmarks\" class=\"anchor\" href=\"#available-containerized-benchmarks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAvailable Containerized Benchmarks\u003c/h2\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth align=\"left\"\u003eBenchmark Name\u003c/th\u003e\n\u003cth\u003eContainer Name\u003c/th\u003e\n\u003cth\u003eAdditional Info\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eBNNOn*\u003c/td\u003e\n\u003ctd\u003epybnn\u003c/td\u003e\n\u003ctd\u003eThere are 4 benchmark in total (ToyFunction, BostonHousing, ProteinStructure, YearPrediction)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eCartpoleFull\u003c/td\u003e\n\u003ctd\u003ecartpole\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eCartpoleReduced\u003c/td\u003e\n\u003ctd\u003ecartpole\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eSliceLocalizationBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eProteinStructureBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eNavalPropulsionBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eParkinsonsTelemonitoringBenchmark\u003c/td\u003e\n\u003ctd\u003etabular_benchmarks\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eNASCifar10*Benchmark\u003c/td\u003e\n\u003ctd\u003enasbench_101\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes. There are 3 benchmark in total (A, B, C)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003e*NasBench201Benchmark\u003c/td\u003e\n\u003ctd\u003enasbench_201\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes. There are 3 benchmarks in total (Cifar10Valid, Cifar100, ImageNet)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eNASBench1shot1SearchSpace*Benchmark\u003c/td\u003e\n\u003ctd\u003enasbench_1shot1\u003c/td\u003e\n\u003ctd\u003eLoading may take several minutes. There are 3 benchmarks in total (1,2,3)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eParamNet*OnStepsBenchmark\u003c/td\u003e\n\u003ctd\u003eparamnet\u003c/td\u003e\n\u003ctd\u003eThere are 6 benchmarks in total (Adult, Higgs, Letter, Mnist, Optdigits, Poker)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eParamNet*OnTimeBenchmark\u003c/td\u003e\n\u003ctd\u003eparamnet\u003c/td\u003e\n\u003ctd\u003eThere are 6 benchmarks in total (Adult, Higgs, Letter, Mnist, Optdigits, Poker)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eSurrogateSVMBenchmark\u003c/td\u003e\n\u003ctd\u003esurrogate_svm\u003c/td\u003e\n\u003ctd\u003eRandom Forest Surrogate of a SVM on MNIST\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eLearna\u207a\u003c/td\u003e\n\u003ctd\u003elearna_benchmark\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eMetaLearna\u207a\u003c/td\u003e\n\u003ctd\u003elearna_benchmark\u003c/td\u003e\n\u003ctd\u003eNot deterministic.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eXGBoostBenchmark\u207a\u003c/td\u003e\n\u003ctd\u003exgboost_benchmark\u003c/td\u003e\n\u003ctd\u003eWorks with OpenML task ids.\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eXGBoostExtendedBenchmark\u207a\u003c/td\u003e\n\u003ctd\u003exgboost_benchmark\u003c/td\u003e\n\u003ctd\u003eWorks with OpenML task ids + Contains Additional Parameter `Booster\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd align=\"left\"\u003eSupportVectorMachine\u207a\u003c/td\u003e\n\u003ctd\u003esvm_benchmark\u003c/td\u003e\n\u003ctd\u003eWorks with OpenML task ids.\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u207a these benchmarks are not yet final and might change\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e All containers are uploaded \u003ca href=\"https://gitlab.tf.uni-freiburg.de/muelleph/hpobench-registry/container_registry\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-further-notes\" class=\"anchor\" href=\"#further-notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFurther Notes\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-configure-the-hpobench\" class=\"anchor\" href=\"#configure-the-hpobench\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfigure the HPOBench\u003c/h3\u003e\n\u003cp\u003eAll of HPOBench\u0027s settings are stored in a file, the \u003ccode\u003ehpobenchrc\u003c/code\u003e-file.\nIt is a yaml file, which is automatically generated at the first use of HPOBench.\nBy default, it is placed in \u003ccode\u003e$XDG_CONFIG_HOME\u003c/code\u003e. If \u003ccode\u003e$XDG_CONFIG_HOME\u003c/code\u003e is not set, then the\n\u003ccode\u003ehpobenchrc\u003c/code\u003e-file is saved to \u003ccode\u003e\u0027~/.config/hpobench\u0027\u003c/code\u003e. When using the containerized benchmarks, the Unix socket is\ndefined via \u003ccode\u003e$TEMP_DIR\u003c/code\u003e. This is by default \u003ccode\u003e\\tmp\u003c/code\u003e. Make sure to have write permissions in those directories.\u003c/p\u003e\n\u003cp\u003eIn the \u003ccode\u003ehpobenchrc\u003c/code\u003e, you can specify for example the directory, in that the benchmark containers are\ndownloaded. We encourage you to take a look into the \u003ccode\u003ehpobenchrc\u003c/code\u003e, to find out more about all\npossible settings.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-build-a-container-locally\" class=\"anchor\" href=\"#how-to-build-a-container-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to build a container locally\u003c/h3\u003e\n\u003cp\u003eWith singularity installed run the following to built the xgboost container\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e hpobench/container/recipes/ml\nsudo singularity build xgboost_benchmark Singularity.XGBoostBenchmark\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eYou can use this local image with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003efrom\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ehpobench\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003econtainer\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003ebenchmarks\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eml\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003exgboost_benchmark\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eXGBoostBenchmark\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003etask_id\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e167149\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003econtainer_name\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\"xgboost_benchmark\"\u003c/span\u003e, \n                     \u003cspan class=\"pl-s1\"\u003econtainer_source\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027./\u0027\u003c/span\u003e) \u003cspan class=\"pl-c\"\u003e# path to hpobench/container/recipes/ml\u003c/span\u003e\n\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eget_configuration_space\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eseed\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e1\u003c/span\u003e).\u003cspan class=\"pl-en\"\u003esample_configuration\u003c/span\u003e()\n\u003cspan class=\"pl-s1\"\u003eresult_dict\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eb\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eobjective_function\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003econfig\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003efidelity\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e{\u003cspan class=\"pl-s\"\u003e\"n_estimators\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e128\u003c/span\u003e, \u003cspan class=\"pl-s\"\u003e\"dataset_fraction\"\u003c/span\u003e: \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e})\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-remove-all-data-containers-and-caches\" class=\"anchor\" href=\"#remove-all-data-containers-and-caches\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRemove all data, containers, and caches\u003c/h3\u003e\n\u003cp\u003eUpdate: In version 0.0.8, we have added the script \u003ccode\u003ehpobench/util/clean_up_script.py\u003c/code\u003e. It allows to easily remove all\ndata, downloaded containers, and caches. To get more information, you can use the following command.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003epython ./hpobench/util/clean_up_script.py --help\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf you like to delete only specific parts, i.e. a single container,\nyou can find the benchmark\u0027s data, container, and caches in the following directories:\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-hpobench-data\" class=\"anchor\" href=\"#hpobench-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHPOBench data\u003c/h4\u003e\n\u003cp\u003eHPOBench stores downloaded containers and datasets at the following locations:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-smi\"\u003e$XDG_CONFIG_HOME\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ~/.config/hpobench\u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003e$XDG_CACHE_HOME\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ~/.cache/hpobench\u003c/span\u003e\n\u003cspan class=\"pl-smi\"\u003e$XDG_DATA_HOME\u003c/span\u003e \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e ~/.local/share/hpobench\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor crashes or when not properly shutting down containers, there might be socket files left under \u003ccode\u003e/tmp/hpobench_socket\u003c/code\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-openml-data\" class=\"anchor\" href=\"#openml-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOpenML data\u003c/h4\u003e\n\u003cp\u003eOpenML data additionally maintains its cache which is located at \u003ccode\u003e~/.openml/\u003c/code\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container\u003c/h4\u003e\n\u003cp\u003eSingularity additionally maintains its cache which can be removed with \u003ccode\u003esingularity cache clean\u003c/code\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-use-hpobench-benchmarks-in-research-projects\" class=\"anchor\" href=\"#use-hpobench-benchmarks-in-research-projects\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUse HPOBench benchmarks in research projects\u003c/h3\u003e\n\u003cp\u003eIf you use a benchmark in your experiments, please specify the version number of the HPOBench as well as the version of\nthe used container. When starting an experiment, HPOBench writes automatically the 2 version numbers to the log.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-troubleshooting\" class=\"anchor\" href=\"#troubleshooting\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTroubleshooting\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSingularity throws an \u0027Invalid Image format\u0027 exception\u003c/strong\u003e\nUse a singularity version \u0026gt; 3. For users of the Meta-Cluster in Freiburg, you have to set the following path:\n\u003ccode\u003eexport PATH=/usr/local/kislurm/singularity-3.5/bin/:$PATH\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eA Benchmark fails with \u003ccode\u003eSystemError: Could not start an instance of the benchmark. Retried 5 times\u003c/code\u003e but the container\ncan be started locally with \u003ccode\u003esingularity instance start \u0026lt;pathtocontainer\u0026gt; test\u003c/code\u003e\u003c/strong\u003e\nSee whether in \u003ccode\u003e~/.singularity/instances/sing/$HOSTNAME/*/\u003c/code\u003e there is a file that does not end with \u0027}\u0027. If yes delete this file and retry.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-status\" class=\"anchor\" href=\"#status\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStatus\u003c/h2\u003e\n\u003cp\u003eStatus for Master Branch:\n\u003ca href=\"https://https://github.com/automl/HPOBench/actions\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/automl/HPOBench/workflows/Test%20Pull%20Requests/badge.svg?branch=master\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/automl/HPOBench\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a034097751613923b89642227271f5932b385694eb079b833b66cc6bce2b924a/68747470733a2f2f636f6465636f762e696f2f67682f6175746f6d6c2f48504f42656e63682f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/automl/HPOBench/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eStatus for Development Branch:\n\u003ca href=\"https://https://github.com/automl/HPOBench/actions\" rel=\"nofollow\"\u003e\u003cimg src=\"https://github.com/automl/HPOBench/workflows/Test%20Pull%20Requests/badge.svg?branch=development\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/automl/HPOBench\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/86b984292042d233294c09940c1b7dade7c877c1220bc419f2e2227eb0f775f0/68747470733a2f2f636f6465636f762e696f2f67682f6175746f6d6c2f48504f42656e63682f6272616e63682f646576656c6f706d656e742f67726170682f62616467652e737667\" alt=\"codecov\" data-canonical-src=\"https://codecov.io/gh/automl/HPOBench/branch/development/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 36,
    "subscribers_count": 7,
    "topics": [
      "hyperparameter-optimization",
      "benchmarking",
      "bayesian-optimization",
      "benchmark",
      "containerized-benchmarks",
      "hpolib"
    ],
    "updated_at": 1627288050.0
  },
  {
    "data_format": 2,
    "description": "RStudio Server in a Singularity container",
    "filenames": [
      "Singularity.3.6.2",
      "Singularity"
    ],
    "full_name": "nickjer/singularity-rstudio",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity-rstudio-server\" class=\"anchor\" href=\"#singularity-rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity RStudio Server\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/nickjer/singularity-rstudio\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/291de9d065fa77b739def518b0430f977c5793f78b1b4ce88d235e61c42332ee/68747470733a2f2f7472617669732d63692e6f72672f6e69636b6a65722f73696e67756c61726974792d7273747564696f2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/nickjer/singularity-rstudio.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/463\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"Singularity Hub\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/45b4ffbd594af47fe09a3432f9f8e122c6518aa6352b4ce453a1a2563da2905c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4d49542d677265656e2e737667\" alt=\"GitHub License\" data-canonical-src=\"https://img.shields.io/badge/license-MIT-green.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSingularity image for \u003ca href=\"https://www.rstudio.com/products/rstudio/\" rel=\"nofollow\"\u003eRStudio Server\u003c/a\u003e. It was built on top of the base\nSingularity image \u003ca href=\"https://github.com/nickjer/singularity-r\"\u003enickjer/singularity-r\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis is still a work in progress.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-build\" class=\"anchor\" href=\"#build\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuild\u003c/h2\u003e\n\u003cp\u003eYou can build a local Singularity image named \u003ccode\u003esingularity-rstudio.simg\u003c/code\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esudo singularity build singularity-rstudio.simg Singularity\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy\" class=\"anchor\" href=\"#deploy\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy\u003c/h2\u003e\n\u003cp\u003eInstead of building it yourself you can download the pre-built image from\n\u003ca href=\"https://www.singularity-hub.org\" rel=\"nofollow\"\u003eSingularity Hub\u003c/a\u003e with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity pull --name singularity-rstudio.simg shub://nickjer/singularity-rstudio\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run\" class=\"anchor\" href=\"#run\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-rstudio-server\" class=\"anchor\" href=\"#rstudio-server\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRStudio Server\u003c/h3\u003e\n\u003cp\u003eThe \u003ccode\u003erserver\u003c/code\u003e command is launched using the default run command:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run singularity-rstudio.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eor as an explicit app:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003esingularity run --app rserver singularity-rstudio.simg\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eExample:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-text-shell-session\"\u003e\u003cpre\u003e$ \u003cspan class=\"pl-s1\"\u003esingularity run --app rserver singularity-rstudio.simg --help\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecommand-line options:\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003everify:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --verify-installation arg (=0)        verify the current installation\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003eserver:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-working-dir arg (=/)         program working directory\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-user arg (=rstudio-server)   program user\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-daemonize arg (=0)           run program as daemon\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-app-armor-enabled arg (=1)   is app armor enabled for this session\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003e  --server-set-umask arg (=1)           set the umask to 022 on startup\u003c/span\u003e\n\n\u003cspan class=\"pl-c1\"\u003e...\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-simple-password-authentication\" class=\"anchor\" href=\"#simple-password-authentication\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSimple Password Authentication\u003c/h4\u003e\n\u003cp\u003eTo secure the RStudio Server you will need to:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eLaunch the container with the environment variable \u003ccode\u003eRSTUDIO_PASSWORD\u003c/code\u003e set to\na password of your choosing.\u003c/li\u003e\n\u003cli\u003eLaunch the \u003ccode\u003erserver\u003c/code\u003e command with the PAM helper script \u003ccode\u003erstudio_auth\u003c/code\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn example is given as:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003eRSTUDIO_PASSWORD=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003epassword\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e singularity run singularity-rstudio.simg \\\n  --auth-none 0 \\\n  --auth-pam-helper rstudio_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNow when you attempt to access the RStudio Server you will be presented with a\nlog in form. You can log in with your current user name and password you set in\n\u003ccode\u003eRSTUDIO_PASSWORD\u003c/code\u003e.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-ldap-authentication\" class=\"anchor\" href=\"#ldap-authentication\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLDAP Authentication\u003c/h4\u003e\n\u003cp\u003eAnother option is using an LDAP (or Active Directory) server for\nauthentication. Configuration of the LDAP authentication script \u003ccode\u003eldap_auth\u003c/code\u003e is\nhandled through the following environment variables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003eLDAP_HOST\u003c/code\u003e - the host name of the LDAP server\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eLDAP_USER_DN\u003c/code\u003e - the formatted string (where \u003ccode\u003e%s\u003c/code\u003e is replaced with the\nusername supplied during log in) of the bind DN used for LDAP authentication\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003eLDAP_CERT_FILE\u003c/code\u003e - the file containing the CA certificates used by\nthe LDAP server (default: use system CA certificates)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAn example for an LDAP server with signed SSL certificate from a trusted CA:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_HOST=ldap.example.com\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_USER_DN=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ecn=%s,dc=example,dc=com\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\nsingularity run singularity-rstudio.simg \\\n  --auth-none 0 \\\n  --auth-pam-helper-path ldap_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn example for an LDAP server with a self-signed SSL certificate:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_HOST=ldap.example.com\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_USER_DN=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003ecn=%s,dc=example,dc=com\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e LDAP_CERT_FILE=/ca-certs.pem\nsingularity run \\\n  --bind /path/to/ca-certs.pem:/ca-certs.pem \\\n  singularity-rstudio.simg \\\n    --auth-none 0 \\\n    --auth-pam-helper-path ldap_auth\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eNote that we had to bind mount the CA certificates file from the host machine\ninto the container and specify the container\u0027s path in \u003ccode\u003eLDAP_CERT_FILE\u003c/code\u003e (not\nthe host\u0027s path).\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-r-and-rscript\" class=\"anchor\" href=\"#r-and-rscript\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eR and Rscript\u003c/h3\u003e\n\u003cp\u003eSee \u003ca href=\"https://github.com/nickjer/singularity-r\"\u003enickjer/singularity-r\u003c/a\u003e for more information on how to run \u003ccode\u003eR\u003c/code\u003e and\n\u003ccode\u003eRscript\u003c/code\u003e from within this Singularity image.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eBug reports and pull requests are welcome on GitHub at\n\u003ca href=\"https://github.com/nickjer/singularity-rstudio\"\u003ehttps://github.com/nickjer/singularity-rstudio\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003eThe code is available as open source under the terms of the \u003ca href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\"\u003eMIT License\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 40,
    "subscribers_count": 5,
    "topics": [
      "rstudio-server",
      "singularity-image"
    ],
    "updated_at": 1626890325.0
  },
  {
    "data_format": 2,
    "description": "Antibiotic resistance prediction in minutes",
    "filenames": [
      "Singularity.def"
    ],
    "full_name": "Mykrobe-tools/mykrobe",
    "latest_release": "v0.10.0",
    "readme": "\u003cp\u003e\u003ca href=\"https://travis-ci.com/Mykrobe-tools/mykrobe\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/da2ddf6fcf06b53f15b9d00f38e707b0d71ba8fb2a5ffefb316ef464440308d0/68747470733a2f2f7472617669732d63692e636f6d2f4d796b726f62652d746f6f6c732f6d796b726f62652e7376673f6272616e63683d6d6173746572\" alt=\"Travis build Status\" data-canonical-src=\"https://travis-ci.com/Mykrobe-tools/mykrobe.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-mykrobe\" class=\"anchor\" href=\"#mykrobe\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMykrobe\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"http://www.mykrobe.com\" rel=\"nofollow\"\u003ehttp://www.mykrobe.com\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-documentation\" class=\"anchor\" href=\"#documentation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDocumentation\u003c/h2\u003e\n\u003cp\u003ePlease see the \u003ca href=\"https://github.com/Mykrobe-tools/mykrobe/wiki\"\u003emykrobe wiki\u003c/a\u003e for documentation.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eInstall\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ebioconda - \u003ccode\u003econda install -c bioconda mykrobe\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003efrom source - \u003ccode\u003epip3 install . \u0026amp;\u0026amp; mykrobe panels update_metadata \u0026amp;\u0026amp; mykrobe panels update_species all\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eor using singularity or docker (see wiki for details)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eRun\u003c/strong\u003e on Mtb, making a JSON file of results:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emykrobe predict --sample my_sample_name \\\n  --species tb \\\n  --output out.json \\\n  --format json \\\n  --seq reads.fq.gz\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTest reads can be obtained by running:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget -O reads.fq.gz https://ndownloader.figshare.com/files/21059229\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 45,
    "subscribers_count": 6,
    "topics": [],
    "updated_at": 1625665948.0
  },
  {
    "data_format": 2,
    "description": "Fast visualization tool for large-scale and high dimensional single-cell data",
    "filenames": [
      "Singularity"
    ],
    "full_name": "aertslab/SCope",
    "latest_release": "untagged-64f13ec6d922418df08e",
    "readme": "\u003cp\u003e\u003ca href=\"https://www.codefactor.io/repository/github/aertslab/scope\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2a72055500b7462275ffcdff1c3045b8ec007caf55f0aabab6980bf1816e7e60/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f61657274736c61622f73636f70652f6261646765\" alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/aertslab/scope/badge\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-scope-v182-visualization-of-large-scale-and-high-dimensional-single-cell-data\" class=\"anchor\" href=\"#scope-v182-visualization-of-large-scale-and-high-dimensional-single-cell-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSCope v1.8.2: Visualization of large-scale and high dimensional single cell data\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"images/SCope_Logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"images/SCope_Logo.png\" width=\"640\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eSCope is a fast visualization tool for large-scale and high dimensional scRNA-seq datasets.\nCurrently the data format supported by SCope is \u003ccode\u003e.loom\u003c/code\u003e. This file format for very large omics datasets is maintained by the Linnarsson Lab through the \u003ccode\u003eloompy\u003c/code\u003e Python package (\u003ca href=\"https://github.com/linnarsson-lab/loompy\"\u003ehttps://github.com/linnarsson-lab/loompy\u003c/a\u003e).\u003c/p\u003e\n\u003cp\u003eView the \u003ca href=\"CHANGELOG.md\"\u003echange log here\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-demo\" class=\"anchor\" href=\"#demo\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDemo\u003c/h2\u003e\n\u003cp\u003eVisit \u003ca href=\"https://scope.aertslab.org\" rel=\"nofollow\"\u003ehttps://scope.aertslab.org\u003c/a\u003e to test out SCope on several published datasets! Personal loom file files can be uploaded but will only be kept for 5 days.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-loom-file-generation\" class=\"anchor\" href=\"#loom-file-generation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLoom File Generation\u003c/h2\u003e\n\u003cp\u003eCurrently there are two packages to generate extended loom files compatible with SCope.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eR: \u003ca href=\"https://github.com/aertslab/SCopeLoomR\"\u003eSCopeLoomR\u003c/a\u003e - Dedicated R package\u003c/li\u003e\n\u003cli\u003ePython: \u003ca href=\"https://github.com/aertslab/pySCENIC\"\u003epySCENIC\u003c/a\u003e - Single function for generation from SCENIC results\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eEventually the functionality from pySCENIC will be expanded and put in its own python package.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-scope\" class=\"anchor\" href=\"#run-scope\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun SCope\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-standalone-app\" class=\"anchor\" href=\"#standalone-app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eStandalone App\u003c/h3\u003e\n\u003cp\u003eStandalone apps for \u003cstrong\u003emacOS\u003c/strong\u003e and \u003cstrong\u003eLinux\u003c/strong\u003e can be downloaded from \u003ca href=\"https://github.com/aertslab/SCope/releases\"\u003ethe releases page.\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cg-emoji class=\"g-emoji\" alias=\"exclamation\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2757.png\"\u003e\u2757\u003c/g-emoji\u003e SCope standalone app requires Node.js (\u0026gt; v9). To install it, go to \u003ca href=\"https://nodejs.org/en/download/\" rel=\"nofollow\"\u003ehttps://nodejs.org/en/download/\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eA \u003cstrong\u003eWindows\u003c/strong\u003e app is under development, but currently has no ETA.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-command-line\" class=\"anchor\" href=\"#command-line\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommand Line\u003c/h3\u003e\n\u003cp\u003eYou will need access to at least Python 3.7 do run this.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eClone the GitHub repository and install,\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Define where you want to clone the SCope repository.\u003c/span\u003e\nLOCAL_SCOPE_REPO=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${HOME}\u003c/span\u003e/repos/SCope\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Clone SCope git repository.\u003c/span\u003e\ngit clone https://github.com/aertslab/SCope \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${LOCAL_SCOPE_REPO}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Go to your local cloned SCope repository.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${LOCAL_SCOPE_REPO}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Install SCope.\u003c/span\u003e\nnpm install\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eRun,\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Go to your local cloned SCope repository.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${LOCAL_SCOPE_REPO}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\nSCOPE_CONFIG=config.json npm run scope\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-deploy-a-cloud-based-instance\" class=\"anchor\" href=\"#deploy-a-cloud-based-instance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploy a Cloud-based Instance\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-amazon-web-services\" class=\"anchor\" href=\"#amazon-web-services\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAmazon Web Services\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-public-ami\" class=\"anchor\" href=\"#public-ami\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePublic AMI\u003c/h4\u003e\n\u003cp\u003eNo ETA.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-source\" class=\"anchor\" href=\"#source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSource\u003c/h4\u003e\n\u003cp\u003eTo create a SCope AWS instance from scratch please read the tutorial \u003ca href=\"https://github.com/aertslab/SCope/tree/master/tutorials/aws-deployment-source\"\u003eaws-deployment-source\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-enabling-orcid-functionality\" class=\"anchor\" href=\"#enabling-orcid-functionality\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eEnabling ORCID Functionality\u003c/h3\u003e\n\u003cp\u003eTo enable colaborative annotations and login via ORCID ID, API credentials (\u003ccode\u003eorcidAPIClientID\u003c/code\u003e, \u003ccode\u003eorcidAPIClientSecret\u003c/code\u003e and \u003ccode\u003eorcidAPIRedirectURI\u003c/code\u003e) must be added to the config file provided.\nThese can be generated at the \u003ca href=\"https://orcid.org/developer-tools\" rel=\"nofollow\"\u003eorcid developer tools page\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003edataHashSecret\u003c/code\u003e entry in the config file should be filled in with a randomly generated string for example from the python \u003ca href=\"https://docs.python.org/3/library/secrets.html\" rel=\"nofollow\"\u003esecrets package\u003c/a\u003e.\nThis string will be used to salt all annotation data, allowing validation of data generated on the instance of SCope. Any changes in this string will invalidate all pre-existing annotations.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-development\" class=\"anchor\" href=\"#development\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopment\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003eClone the GitHub repository and install,\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Define where you want to clone the SCope repository.\u003c/span\u003e\nLOCAL_SCOPE_REPO=\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${HOME}\u003c/span\u003e/repos/SCope\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Clone SCope git repository.\u003c/span\u003e\ngit clone https://github.com/aertslab/SCope \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${LOCAL_SCOPE_REPO}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Go to your local cloned SCope repository.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${LOCAL_SCOPE_REPO}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Install SCope.\u003c/span\u003e\nnpm install\u003c/pre\u003e\u003c/div\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eRun,\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Go to your local cloned SCope repository.\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-smi\"\u003e${LOCAL_SCOPE_REPO}\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Start SCope Server (terminal 1).\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e opt\npoetry run hypercorn main:scope_api --reload\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Start SCope Client (terminal 2).\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e ..\nnpm run dev\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-configuration-file-configjson\" class=\"anchor\" href=\"#configuration-file-configjson\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eConfiguration file (\u003ccode\u003econfig.json\u003c/code\u003e)\u003c/h3\u003e\n\u003cp\u003eKeys:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ccode\u003edata\u003c/code\u003e: This is a directory containing data files (e.g. the \u003ccode\u003emotd.txt\u003c/code\u003e message of the day).\nCan be an absolute path or a relative path from where you start SCope. By default it is\n\u003ccode\u003e./data/\u003c/code\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-deploying-scope-with-docker\" class=\"anchor\" href=\"#deploying-scope-with-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeploying SCope with Docker\u003c/h3\u003e\n\u003cp\u003e\u003ccode\u003edocker-compose.yml\u003c/code\u003e is configured to spin up 2 containers: One to run the SCope backend and another to run an Apache\nreverse proxy server.\u003c/p\u003e\n\u003cp\u003eThe SCope application will be available on port \u003ccode\u003e80\u003c/code\u003e by default. You can specify a port by using env variable: \u003ccode\u003eSCOPE_PORT\u003c/code\u003e\nbefore running the docker-compose command. Apache will proxy requests through to the appropriate port inside the container.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003edocker-compose.yml\u003c/code\u003e will serve the assets from inside the scope container, and the \u003ccode\u003edocker-compose.host.yml\u003c/code\u003e will serve them from the host.\nThis supports as many use cases as possible, because you can either build the assets on the host yourself using whatever configuration you need,\nor serve them from the container if your environment doesn\u0027t allow for that (e.g. you don\u0027t have npm installed on the host).\u003c/p\u003e\n\u003cp\u003eBefore running the compose build, you can specify a SCOPE_PORT with: \u003ccode\u003edocker-compose build --build-arg SCOPE_PORT=8080\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe scope webpack assets will have to be built with the config: \u003ccode\u003e\"reverseProxyOn\": true\u003c/code\u003e.\nYou can use environment variable: \u003ccode\u003eSCOPE_CONFIG=path to your config\u003c/code\u003e to specify a config file instead of changing the main one.\u003c/p\u003e\n\u003cp\u003eYou can configure where the dockerised SCope data directories should be located\non the host machine by using the env var \u003ccode\u003eSCOPE_DATA_DIR\u003c/code\u003e before launching the docker-compose.\nThe default location is \u003ccode\u003e./scope_data\u003c/code\u003e which will be created if you do not specify one.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e: in this config, you do not need to specify the port in \u003ccode\u003epublicHostAddress\u003c/code\u003e. The env var \u003ccode\u003eSCOPE_PORT\u003c/code\u003e gets appended for you.\u003c/p\u003e\n\u003cp\u003eIf deploying the container on a specific port with another external apache reverse-proxy server,\nyou may have to add a config to the external apache site config to allow http and websocket reverse-proxying.\nHere is an example:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e    ProxyPass / http://0.0.0.0:8080/\n    RewriteEngine on\n    RewriteCond %{HTTP:Upgrade} websocket [NC]\n    RewriteCond %{HTTP:Connection} upgrade [NC]\n    RewriteRule ^/?(.*) \"ws://0.0.0.0:8080/$1\" [P,L]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-example-serve-from-container\" class=\"anchor\" href=\"#example-serve-from-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExample serve from container\u003c/h5\u003e\n\u003col\u003e\n\u003cli\u003eCopy \u003ccode\u003econfig.json\u003c/code\u003e to a new file and modify with \u003ccode\u003e\"reverseProxyOn\": true,\u003c/code\u003e and \u003ccode\u003epublicHostAddress\u003c/code\u003e set to your domain\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003edocker-compose build --build-arg SCOPE_PORT=8080\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSCOPE_DATA_DIR=$HOME/scope_data SCOPE_PORT=8080 docker-compose up -d\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch5\u003e\n\u003ca id=\"user-content-or-serve-from-host\" class=\"anchor\" href=\"#or-serve-from-host\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOR Serve from host\u003c/h5\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003enpm run build\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eSCOPE_DATA_DIR=$HOME/scope_data SCOPE_PORT=8080 docker-compose -f docker-compose.host.yml up -d\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou should be able to visit \u003ccode\u003ehttp://localhost:8080\u003c/code\u003e and see the app!\u003c/p\u003e\n",
    "stargazers_count": 49,
    "subscribers_count": 8,
    "topics": [
      "single-cell",
      "large-scale-data-visualization",
      "gene-expression",
      "gene-regulatory-network",
      "aws",
      "cloud",
      "loom",
      "reactjs",
      "grpc"
    ],
    "updated_at": 1624266405.0
  },
  {
    "data_format": 2,
    "description": "Python package and CLI for whole-genome duplication related analyses",
    "filenames": [
      "Singularity"
    ],
    "full_name": "arzwa/wgd",
    "latest_release": "v1.1.1",
    "readme": "\u003cp\u003e\u003ca href=\"http://wgd.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/482fadaf050db308e22a1c37fb38a8d39be0f5a0d3b645ed48b114c86e55497e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7767642f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/wgd/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/2097\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/9b0b8670bab3cab652cf5c31fdae614cf89b2ceb2e013cd2d7dd570e9f8530f2/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f686f737465642d73696e67756c61726974792d2d6875622d626c75652e737667\" alt=\"Hosted\" data-canonical-src=\"https://img.shields.io/badge/hosted-singularity--hub-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eCopyright (C) 2018 Arthur Zwaenepoel\u003c/p\u003e\n\u003cp\u003eVIB/UGent center for plant systems biology -\nBioinformatics \u0026amp; evolutionary genomics group \u003ca href=\"https://www.vandepeerlab.org/\" rel=\"nofollow\"\u003ehttps://www.vandepeerlab.org/\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-wgd---simple-command-line-tools-for-the-analysis-of-ancient-whole-genome-duplications\" class=\"anchor\" href=\"#wgd---simple-command-line-tools-for-the-analysis-of-ancient-whole-genome-duplications\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ewgd - simple command line tools for the analysis of ancient whole-genome duplications\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e If you are interested in the methods implemented in \u003ccode\u003ewgd\u003c/code\u003e, you may also want to\nconsider the \u003ca href=\"https://github.com/VIB-PSB/ksrates\"\u003e\u003ccode\u003eksrates\u003c/code\u003e\u003c/a\u003e tool by Sensalari \u003cem\u003eet al.\u003c/em\u003e\nwhich can be used to carefully compare multiple Ks distributions and model them (\u003ccode\u003eksrates\u003c/code\u003e\nuses \u003ccode\u003ewgd\u003c/code\u003e under the hood).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003ePython package and command line interface (CLI) for the analysis of\nwhole-genome duplications (WGDs). Tested with Python3 on Linux. If you don\u0027t have\npython or pip installed a simple \u003ccode\u003esudo apt-get install python3-pip\u003c/code\u003e should do.\u003c/p\u003e\n\u003cp\u003eTo install, simply run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/arzwa/wgd.git\ncd wgd\npip install --user .\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that depending on your python installation and whether you\u0027re in a\nvirtualenv, \u003ccode\u003epip\u003c/code\u003e may default either to \u003ccode\u003epip2\u003c/code\u003e or \u003ccode\u003epip3\u003c/code\u003e. If the\nabove installation step fails, please try to use \u003ccode\u003epip3\u003c/code\u003e instead of\n\u003ccode\u003epip\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor the command line interface, upon installation run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ wgd\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto get a list of the available commands. To get usage instructions for\na command (e.g. \u003ccode\u003eksd\u003c/code\u003e) run\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ wgd ksd --help\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor \u003cstrong\u003eexternal software\u003c/strong\u003e requirements: please consult the relevant section\nin the \u003ca href=\"https://wgd.readthedocs.io/en/latest/index.html#external-software\" rel=\"nofollow\"\u003edocs\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote:\u003c/strong\u003e if you encounter issues, do verify you have the latest\n\u003ca href=\"http://abacus.gene.ucl.ac.uk/software/#phylogenetic-analysis-by-maximum-likelihood-paml\" rel=\"nofollow\"\u003ePAML\u003c/a\u003e version.\nTo install the latest version, you best not rely on \u003ccode\u003eapt-get\u003c/code\u003e or any other\npackage manager but install from source. Something like this should work\n(from within the directory where you want to install paml)\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewget http://abacus.gene.ucl.ac.uk/software/paml4.9j.tgz\ntar -xzf paml4.9j.tgz\npushd paml4.9j/src \u0026amp;\u0026amp; make -f Makefile \u0026amp;\u0026amp; popd \nexport PATH=$PATH:$PWD/paml4.9j/src/\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cp\u003eThe main aim of \u003ccode\u003ewgd\u003c/code\u003e is computing whole-paranome and one-vs.-one ortholog Ks\ndistributions. For a whole-paranome distribution of a CDS sequence fasta file,\nthe minimal commands are:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ wgd dmd ath.cds.fasta\n$ wgd ksd wgd_dmd/ath.cds.fasta.mcl ath.cds.fasta\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor one-vs.one orthologs the minimal commands are\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ wgd dmd ath.cds.fasta vvi.cds.fasta\n$ wgd ksd wgd_dmd/ath1000.fasta_vvi1000.fasta.rbh ath.cds.fasta vvi.cds.fasta\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more information and these methods and other tools implemented in \u003ccode\u003ewgd\u003c/code\u003e,\nplease consult the \u003ca href=\"https://wgd.readthedocs.io/en/latest/\" rel=\"nofollow\"\u003edocs\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-singularity-container\" class=\"anchor\" href=\"#singularity-container\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity container\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eNote\u003c/strong\u003e this hasn\u0027t been updated in a while, it may or may not work.\u003c/p\u003e\n\u003cp\u003eA singularity container is available for \u003ccode\u003ewgd\u003c/code\u003e, allowing all to use\nall tools in \u003ccode\u003ewgd\u003c/code\u003e except \u003ccode\u003ewgd syn\u003c/code\u003e, without having to install all\nrequired software on your system. To install Singularity follow\nthe instructions \u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you have singulaity installed (and you\u0027re in the virtual machine when\nrunning on Windows or Mac), you can run the following to get the container\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull --name wgd.simg shub://arzwa/wgd\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThen you can use \u003ccode\u003ewgd\u003c/code\u003e as follows\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec wgd.simg wgd \u0026lt;command\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-notes\" class=\"anchor\" href=\"#notes\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNotes\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003eBug tracking:\u003c/strong\u003e If the program crashes, exits unexpectedly or some\nunexpected results are obtained, please run it again with the\n\u003ccode\u003e--verbosity debug\u003c/code\u003e flag \u003cem\u003ebefore\u003c/em\u003e the subcommand of interest (\u003cem\u003ee.g.\u003c/em\u003e\n\u003ccode\u003ewgd --verbosity debug ksd gf.mcl cds.fasta\u003c/code\u003e). If the anomaly persists,\nplease open an issue on this GitHub site.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote on input data:\u003c/strong\u003e while the input data is rather straightforward\n(a CDS fasta file will do for most analyses) it may be of interest that\nthe wgd suite was extensively tested with data from the PLAZA platform,\nso for examples of the right input data formats (in particular CDS fasta\nfiles for sequence data and GFF files for structural annotation), please\nhave a look \u003ca href=\"https://bioinformatics.psb.ugent.be/plaza/versions/plaza_v4_dicots/download/\" rel=\"nofollow\"\u003ethere\u003c/a\u003e.\nIt is generally advised not to include pipe characters (\u003ccode\u003e|\u003c/code\u003e) in your gene\nIDs, since these can have special meanings in certain parts of \u003ccode\u003ewgd\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote on virtualenv:\u003c/strong\u003e you can install wgd in a \u003cem\u003evirtual environment\u003c/em\u003e\n(using \u003ca href=\"https://virtualenv.pypa.io/en/stable/\" rel=\"nofollow\"\u003e\u003ccode\u003evirtualenv\u003c/code\u003e\u003c/a\u003e). If you\nwould however encounter problems with running the executable directly\n(e.g. \u003ccode\u003ewgd --help\u003c/code\u003e doesn\u0027t work) you can circumvent this by directly\ncalling the CLI, using \u003ccode\u003epython3 ./wgd_cli.py --help\u003c/code\u003e (assuming you are\ncurrently in the directory where you cloned wgd).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003ePlease cite us at \u003ca href=\"https://doi.org/10.1093/bioinformatics/bty915\" rel=\"nofollow\"\u003ehttps://doi.org/10.1093/bioinformatics/bty915\u003c/a\u003e\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eZwaenepoel, A., and Van de Peer, Y. \nwgd - simple command line tools for the analysis of ancient whole genome duplications. \nBioinformatics., bty915, https://doi.org/10.1093/bioinformatics/bty915\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor citation of the tools used in wgd, please consult the documentation at\n\u003ca href=\"https://wgd.readthedocs.io/en/latest/index.html#citation\" rel=\"nofollow\"\u003ehttps://wgd.readthedocs.io/en/latest/index.html#citation\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 57,
    "subscribers_count": 4,
    "topics": [
      "wgd",
      "duplication",
      "polyploidy",
      "bioinformatics",
      "genomics",
      "evolution"
    ],
    "updated_at": 1626469613.0
  },
  {
    "data_format": 2,
    "description": "Example Nextflow pipelines and programming techniques",
    "filenames": [
      "Singularity/Singularity"
    ],
    "full_name": "stevekm/nextflow-demos",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-nextflow-demos\" class=\"anchor\" href=\"#nextflow-demos\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003enextflow-demos\u003c/h1\u003e\n\u003cp\u003eDemonstrations of various programming techniques for use inside \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003eNextflow\u003c/a\u003e pipelines. This repository is meant to be a supplement to the \u003ca href=\"https://www.nextflow.io/docs/latest/getstarted.html\" rel=\"nofollow\"\u003eofficial Nextflow documentation\u003c/a\u003e (links below).\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003ean overview presentation about Nextflow can be found \u003ca href=\"https://github.com/stevekm/nextflow-demos/blob/docs/docs/Nextflow_presentation.pdf\"\u003ehere\u003c/a\u003e (view \u003ca href=\"https://docs.google.com/viewer?url=https://raw.githubusercontent.com/stevekm/nextflow-demos/docs/docs/Nextflow_presentation.pdf\" rel=\"nofollow\"\u003ehere\u003c/a\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow HTML report examples can be found here:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://htmlpreview.github.io/?https://github.com/stevekm/nextflow-demos/blob/report-output/reporting/nextflow-report.html\" rel=\"nofollow\"\u003epipeline report\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ca href=\"https://htmlpreview.github.io/?https://github.com/stevekm/nextflow-demos/blob/report-output/reporting/timeline-report.html\" rel=\"nofollow\"\u003etimeline report\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cem\u003eNOTE\u003c/em\u003e: Some of the techniques demonstrated here may be deprecated by the new \u003ca href=\"https://www.nextflow.io/docs/latest/dsl2.html\" rel=\"nofollow\"\u003eDSL2\u003c/a\u003e syntax offered by Nextflow. Be sure to check that out as well.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h1\u003e\n\u003cp\u003eClone this repo:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:stevekm/nextflow-demos.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e nextflow-demos\u003c/pre\u003e\u003c/div\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contents\" class=\"anchor\" href=\"#contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContents\u003c/h1\u003e\n\u003cp\u003eEach subdirectory contains files to run sample Nextflow pipelines.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-files\" class=\"anchor\" href=\"#files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFiles\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eMakefile\u003c/code\u003e: shortcut to commands to install and clean up Nextflow and its pipeline output\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003emain.nf\u003c/code\u003e: Nextflow pipeline file\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003enextflow.config\u003c/code\u003e: config file for Nextflow pipeline (optional)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sample-pipeline-directories\" class=\"anchor\" href=\"#sample-pipeline-directories\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSample Pipeline Directories\u003c/h2\u003e\n\u003cp\u003e(listed in recommended order for new users)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eprint-samples\u003c/code\u003e: Prints samples from a list to the terminal\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003emake-files\u003c/code\u003e: Creates files based on sample ID inputs\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eoutput-files\u003c/code\u003e: Same as \u003ccode\u003emake-files\u003c/code\u003e but includes custom file output options\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003easync\u003c/code\u003e: demonstration of asynchronous process execution\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ecustom-email-output\u003c/code\u003e: Creates files from sample ID\u0027s then sends the user an email with a pipeline summary and files attached\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eoutput-variable-name\u003c/code\u003e: Same as \u003ccode\u003eoutput-files\u003c/code\u003e but includes inline variable definition of output file names\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eR-Python\u003c/code\u003e: methods for using other scripting languages inside the Nextflow pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ejoin-pairs\u003c/code\u003e: joining pairs of samples based on ID across input channels\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eparse-samplesheet\u003c/code\u003e: parsing of a samplesheet as input for Nextflow pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003ereporting\u003c/code\u003e: execution of Nextflow pipeline with reporting and config features enabled.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eprofiles-Docker-module\u003c/code\u003e: usage of \u0027profiles\u0027 to change process execution behavior to use Docker or environment modules\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003eGroovy-code\u003c/code\u003e: example of using inline Groovy code inside the Nextflow pipeline\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h1\u003e\n\u003cp\u003eYou can use the following commands inside the provided demo subdirs to run the demo pipelines.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install-nextflow\" class=\"anchor\" href=\"#install-nextflow\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall Nextflow\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# in a subdir in this repo\nmake\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-run-pipeline\" class=\"anchor\" href=\"#run-pipeline\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRun pipeline\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e./nextflow run main.nf\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003emake run\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cleanup\" class=\"anchor\" href=\"#cleanup\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCleanup\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003emake clean\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-resources\" class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResources\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow Homepage: \u003ca href=\"https://www.nextflow.io/\" rel=\"nofollow\"\u003ehttps://www.nextflow.io/\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow Docs: \u003ca href=\"https://www.nextflow.io/docs/latest/getstarted.html\" rel=\"nofollow\"\u003ehttps://www.nextflow.io/docs/latest/getstarted.html\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow Patterns: \u003ca href=\"https://nextflow-io.github.io/patterns/index.html\" rel=\"nofollow\"\u003ehttps://nextflow-io.github.io/patterns/index.html\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow GitHub: \u003ca href=\"https://github.com/nextflow-io/nextflow\"\u003ehttps://github.com/nextflow-io/nextflow\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow Google Group: \u003ca href=\"https://groups.google.com/forum/#!forum/nextflow\" rel=\"nofollow\"\u003ehttps://groups.google.com/forum/#!forum/nextflow\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-examples\" class=\"anchor\" href=\"#examples\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eExamples\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow tutorial: \u003ca href=\"https://github.com/nextflow-io/hack17-tutorial\"\u003ehttps://github.com/nextflow-io/hack17-tutorial\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNextflow examples: \u003ca href=\"https://github.com/nextflow-io/examples\"\u003ehttps://github.com/nextflow-io/examples\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePipeline examples: \u003ca href=\"https://github.com/nextflow-io/awesome-nextflow\"\u003ehttps://github.com/nextflow-io/awesome-nextflow\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBoilerplate example for writing pipelines: \u003ca href=\"https://github.com/stevekm/nextflow-boilerplate\"\u003ehttps://github.com/stevekm/nextflow-boilerplate\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eNYU pipelines:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eexome sequencing: \u003ca href=\"https://github.com/NYU-Molecular-Pathology/NGS580-nf\"\u003ehttps://github.com/NYU-Molecular-Pathology/NGS580-nf\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003edemultiplexing: \u003ca href=\"https://github.com/NYU-Molecular-Pathology/demux-nf\"\u003ehttps://github.com/NYU-Molecular-Pathology/demux-nf\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 59,
    "subscribers_count": 5,
    "topics": [
      "nextflow"
    ],
    "updated_at": 1625030182.0
  },
  {
    "data_format": 2,
    "description": "SingularityCE is the Community Edition of Singularity, an open source container platform designed to be simple, fast, and secure.",
    "filenames": [
      "e2e/testdata/Docker_registry.def",
      "e2e/testdata/sshfs.def",
      "e2e/testdata/inspecter_container.def",
      "e2e/testdata/regressions/issue_5399.def",
      "e2e/testdata/regressions/issue_5315.def",
      "e2e/testdata/regressions/issue_4967.def",
      "e2e/testdata/regressions/issue_4969.def",
      "e2e/testdata/regressions/issue_4203.def",
      "e2e/testdata/regressions/issue_4583.def",
      "e2e/testdata/regressions/issue_5250.def",
      "e2e/testdata/regressions/issue_4820.def",
      "examples/legacy/2.3/contrib/raspbian.def",
      "examples/legacy/2.2/arch.def",
      "examples/legacy/2.2/ubuntu.def",
      "examples/legacy/2.2/busybox.def",
      "examples/legacy/2.2/docker.def",
      "examples/legacy/2.2/debian.def",
      "examples/legacy/2.2/scientific.def",
      "examples/legacy/2.2/centos.def",
      "examples/legacy/2.2/contrib/ubuntu-root.def",
      "examples/legacy/2.2/contrib/debian85-tensorflow-0.10.def",
      "examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1-gpu.def",
      "examples/legacy/2.2/contrib/linuxbrew_and_non-root_software_example.def",
      "examples/legacy/2.2/contrib/ubuntu-openfoam.def",
      "examples/legacy/2.2/contrib/centos7-ompi_master.def",
      "examples/legacy/2.2/contrib/ubuntu-bio.def",
      "examples/legacy/2.2/contrib/centos-minimal.def",
      "examples/legacy/2.2/contrib/centos7-ompi_cuda.def",
      "examples/legacy/2.2/contrib/ubuntu16-tensorflow-0.12.1.def",
      "examples/legacy/2.2/contrib/r_python_julia.def",
      "examples/legacy/2.2/contrib/fedora.def",
      "examples/build-singularity/build-singularity.def"
    ],
    "full_name": "sylabs/singularity",
    "latest_release": "v3.8.1",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularityce\" class=\"anchor\" href=\"#singularityce\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularityCE\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://circleci.com/gh/sylabs/singularity/tree/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ff56e7dd170e08e53c09fda12031315bb91f5b4220f2d3cfaf46044700f32fa1/68747470733a2f2f636972636c6563692e636f6d2f67682f73796c6162732f73696e67756c61726974792f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/sylabs/singularity/tree/master.svg?style=svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"CONTRIBUTING.md\"\u003eGuidelines for Contributing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\".github/PULL_REQUEST_TEMPLATE.md\"\u003ePull Request Template\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"LICENSE.md\"\u003eProject License\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.sylabs.io/docs/\" rel=\"nofollow\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/sylabs/singularityce-community\"\u003eCommunity Meetings / Minutes / Roadmap\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#support\"\u003eSupport\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citing-singularity\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"CODE_OF_CONDUCT.md\"\u003eCode of Conduct\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSingularityCE is the Community Edition of Singularity, an open source container\nplatform designed to be simple, fast, and secure. Singularity is optimized for\ncompute focused enterprise and HPC workloads, allowing untrusted users to run\nuntrusted containers in a trusted way.\u003c/p\u003e\n\u003cp\u003eCheck out \u003ca href=\"https://www.sylabs.io/videos\" rel=\"nofollow\"\u003etalks about Singularity\u003c/a\u003e and some\n\u003ca href=\"https://sylabs.io/case-studies\" rel=\"nofollow\"\u003euse cases of Singularity\u003c/a\u003e on our website.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started-with-singularityce\" class=\"anchor\" href=\"#getting-started-with-singularityce\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started with SingularityCE\u003c/h2\u003e\n\u003cp\u003eTo install SingularityCE from source, see the\n\u003ca href=\"INSTALL.md\"\u003einstallation instructions\u003c/a\u003e. For other installation options, see\n\u003ca href=\"https://www.sylabs.io/guides/latest/admin-guide/\" rel=\"nofollow\"\u003eour guide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSystem administrators can learn how to configure SingularityCE, and get an\noverview of its architecture and security features in the\n\u003ca href=\"https://www.sylabs.io/guides/latest/admin-guide/\" rel=\"nofollow\"\u003eadministrator guide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor users, see the \u003ca href=\"https://www.sylabs.io/guides/latest/user-guide/\" rel=\"nofollow\"\u003euser guide\u003c/a\u003e\nfor details on how to use and build Singularity containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing-to-singularityce\" class=\"anchor\" href=\"#contributing-to-singularityce\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing to SingularityCE\u003c/h2\u003e\n\u003cp\u003eCommunity contributions are always greatly appreciated. To start developing\nSingularityCE, check out the \u003ca href=\"CONTRIBUTING.md\"\u003eguidelines for contributing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePlease note we have a \u003ca href=\"CODE_OF_CONDUCT.md\"\u003ecode of conduct\u003c/a\u003e. Please follow it in\nall your interactions with the project members and users.\u003c/p\u003e\n\u003cp\u003eOur roadmap, other documents, and user/developer meeting information can be\nfound in the\n\u003ca href=\"https://github.com/sylabs/singularityce-community\"\u003esingularityce-community repository\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe also welcome contributions to our\n\u003ca href=\"https://github.com/sylabs/singularity-userdocs\"\u003euser guide\u003c/a\u003e and\n\u003ca href=\"https://github.com/sylabs/singularity-admindocs\"\u003eadmin guide\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h2\u003e\n\u003cp\u003eTo get help with SingularityCE, check out the community spaces detailed at our\n\u003ca href=\"https://www.sylabs.io/singularity/community/\" rel=\"nofollow\"\u003eCommunity Portal\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSee also our \u003ca href=\"SUPPORT.md\"\u003eSupport Guidelines\u003c/a\u003e for further information about the\nbest place, and how, to raise different kinds of issues and questions.\u003c/p\u003e\n\u003cp\u003eFor additional support, \u003ca href=\"https://www.sylabs.io/contact/\" rel=\"nofollow\"\u003econtact us\u003c/a\u003e to receive\nmore information.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-singularity\" class=\"anchor\" href=\"#citing-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting Singularity\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eKurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for\nmobility of compute. PLoS ONE 12(5): e0177459.\n\u003ca href=\"https://doi.org/10.1371/journal.pone.0177459\" rel=\"nofollow\"\u003ehttps://doi.org/10.1371/journal.pone.0177459\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe also have a Zenodo citation:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eKurtzer, Gregory M. et. al. Singularity - Linux application and environment\ncontainers for science. 10.5281/zenodo.1310023\n\u003ca href=\"https://doi.org/10.5281/zenodo.1310023\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.1310023\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThis is an \u0027all versions\u0027 DOI. Follow the link to Zenodo to obtain a DOI\nspecific to a particular version of Singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eUnless otherwise noted, this project is licensed under a 3-clause BSD license\nfound in the \u003ca href=\"LICENSE.md\"\u003elicense file\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 61,
    "subscribers_count": 10,
    "topics": [
      "containers",
      "hpc",
      "linux"
    ],
    "updated_at": 1627337082.0
  },
  {
    "data_format": 2,
    "description": ":floppy_disk: C++ \u0026 Python API for Scientific I/O",
    "filenames": [
      "Singularity"
    ],
    "full_name": "openPMD/openPMD-api",
    "latest_release": "0.13.4",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-c--python-api-for-scientific-io-with-openpmd\" class=\"anchor\" href=\"#c--python-api-for-scientific-io-with-openpmd\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eC++ \u0026amp; Python API for Scientific I/O with openPMD\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/openPMD/openPMD-standard/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5957dc11cb68f6705ef9ef6683152c6c4fcc057a24dff340e1e8bada1bee0d01/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6f70656e504d442d312e302e302d2d312e312e302d626c7565\" alt=\"Supported openPMD Standard\" data-canonical-src=\"https://img.shields.io/badge/openPMD-1.0.0--1.1.0-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.openpmd.org/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2d54fa5adcf7ca50d2cdb45823be5064379b613d526fa06f6e193ba2ce616318/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c7565\" alt=\"Doxygen\" data-canonical-src=\"https://img.shields.io/badge/API-Doxygen-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://gitter.im/openPMD/API\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f551141eea2176c34aa163092549d6fdde9a220d605d6efe9128b024c6e5932b/68747470733a2f2f696d672e736869656c64732e696f2f6769747465722f726f6f6d2f6f70656e504d442f415049\" alt=\"Gitter chat\" data-canonical-src=\"https://img.shields.io/gitter/room/openPMD/API\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\" alt=\"Supported Platforms\" title=\"Supported Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.gnu.org/licenses/lgpl-3.0.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4fc8091716dbd967fa2a82eef3d29227110e5081f3128aaa38663e547c24f812/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c7565\" alt=\"License\" data-canonical-src=\"https://img.shields.io/badge/license-LGPLv3-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://doi.org/10.14278/rodare.27\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/104f6901ae04bff8385acc8273bb276ab84656a927a418108b940d09fd6e5d7c/68747470733a2f2f726f646172652e687a64722e64652f62616467652f444f492f31302e31343237382f726f646172652e32372e737667\" alt=\"DOI\" data-canonical-src=\"https://rodare.hzdr.de/badge/DOI/10.14278/rodare.27.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0d568047fbbd300af56b766d9a4235a20534485f5827194e859d4f5c20e58334/68747470733a2f2f7777772e636f6465666163746f722e696f2f7265706f7369746f72792f6769746875622f6f70656e706d642f6f70656e706d642d6170692f6261646765\" alt=\"CodeFactor\" data-canonical-src=\"https://www.codefactor.io/repository/github/openpmd/openpmd-api/badge\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:cpp\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/63a7f9e783999e3afc03ef38ee82e2048017e4e6d279ff4120ad8b8718480ccd/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f6370702f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\" alt=\"LGTM: C/C++\" data-canonical-src=\"https://img.shields.io/lgtm/grade/cpp/g/openPMD/openPMD-api?logo=lgtm\u0026amp;logoWidth=18\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/context:python\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5046bf66a4612476a030d38de817c23fa03990183d2d74fa92c5f1379feb5d09/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f67726164652f707974686f6e2f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\" alt=\"LGTM: Python\" data-canonical-src=\"https://img.shields.io/lgtm/grade/python/g/openPMD/openPMD-api?logo=lgtm\u0026amp;logoWidth=18\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://lgtm.com/projects/g/openPMD/openPMD-api/alerts/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/85e32deb8face392eea9bfa2be4da4c11ca7c0f834fa069223fbc63758b68c4f/68747470733a2f2f696d672e736869656c64732e696f2f6c67746d2f616c657274732f672f6f70656e504d442f6f70656e504d442d6170693f6c6f676f3d6c67746d266c6f676f57696474683d3138\" alt=\"LGTM: Total alerts\" data-canonical-src=\"https://img.shields.io/lgtm/alerts/g/openPMD/openPMD-api?logo=lgtm\u0026amp;logoWidth=18\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://coveralls.io/github/openPMD/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f0487a8fc14d269210ae8ce80b556d4afcd93684f02e61386d2d02daa4423cbd/68747470733a2f2f636f766572616c6c732e696f2f7265706f732f6769746875622f6f70656e504d442f6f70656e504d442d6170692f6261646765\" alt=\"Coverage Status\" data-canonical-src=\"https://coveralls.io/repos/github/openPMD/openPMD-api/badge\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://openpmd-api.readthedocs.io/en/latest/?badge=latest\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8f438ca4eaa308f982d0a28c48f05bf63ca1a86e52c3d2817f6d7559d1dee68e/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f6f70656e706d642d6170692f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/openpmd-api/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.com/openPMD/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8278d89e3634e25a818a2d673fe997c2aa5a09fd5995f716aeffa743388030ee/68747470733a2f2f7472617669732d63692e636f6d2f6f70656e504d442f6f70656e504d442d6170692e7376673f6272616e63683d646576\" alt=\"Linux/OSX Build Status dev\" data-canonical-src=\"https://travis-ci.com/openPMD/openPMD-api.svg?branch=dev\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://ci.appveyor.com/project/ax3l/openpmd-api/branch/dev\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/30679331f3c6a5ae54970eda85f36a30347dee8a9111448d7aa48587fd524a1d/68747470733a2f2f63692e6170707665796f722e636f6d2f6170692f70726f6a656374732f7374617475732f78393571346e36323070716b306530742f6272616e63682f6465763f7376673d74727565\" alt=\"Windows Build Status dev\" data-canonical-src=\"https://ci.appveyor.com/api/projects/status/x95q4n620pqk0e0t/branch/dev?svg=true\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/openPMD/openPMD-api/actions?query=workflow%3Awheels\"\u003e\u003cimg src=\"https://github.com/openPMD/openPMD-api/workflows/wheels/badge.svg?branch=wheels\u0026amp;event=push\" alt=\"PyPI Wheel Release\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://dev.azure.com/axelhuebl/openPMD-api/_build/latest?definitionId=1\u0026amp;branchName=azure_install\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1fc9c860bb1fc8e7e145ea9dd6723b9ac6ac2743c195e5e899f6cfa3d38a5a69/68747470733a2f2f6465762e617a7572652e636f6d2f6178656c687565626c2f6f70656e504d442d6170692f5f617069732f6275696c642f7374617475732f6f70656e504d442e6f70656e504d442d6170693f6272616e63684e616d653d617a7572655f696e7374616c6c266c6162656c3d6e696768746c792532307061636b61676573\" alt=\"Nightly Packages Status\" data-canonical-src=\"https://dev.azure.com/axelhuebl/openPMD-api/_apis/build/status/openPMD.openPMD-api?branchName=azure_install\u0026amp;label=nightly%20packages\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://scan.coverity.com/projects/openpmd-openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0b068d7b5718aafccb46e2ee35f8249938ef189a36ba353c15222ed5e6f65e49/68747470733a2f2f7363616e2e636f7665726974792e636f6d2f70726f6a656374732f31373630322f62616467652e737667\" alt=\"Coverity Scan Build Status\" data-canonical-src=\"https://scan.coverity.com/projects/17602/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eopenPMD is an open meta-data schema that provides meaning and self-description for data sets in science and engineering.\nSee \u003ca href=\"https://github.com/openPMD/openPMD-standard\"\u003ethe openPMD standard\u003c/a\u003e for details of this schema.\u003c/p\u003e\n\u003cp\u003eThis library provides a reference API for openPMD data handling.\nSince openPMD is a schema (or markup) on top of portable, hierarchical file formats, this library implements various backends such as HDF5, ADIOS1, ADIOS2 and JSON.\nWriting \u0026amp; reading through those backends and their associated files is supported for serial and \u003ca href=\"https://www.mpi-forum.org/docs/\" rel=\"nofollow\"\u003eMPI-parallel\u003c/a\u003e workflows.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-c\" class=\"anchor\" href=\"#c\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eC++\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://isocpp.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/042b5af19c304a2d4f876865d00baa90a284f2d35056ed9728c944befbb07733/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231342d79656c6c6f77677265656e\" alt=\"C++14\" title=\"C++14 API\" data-canonical-src=\"https://img.shields.io/badge/language-C%2B%2B14-yellowgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\" alt=\"C++14 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-c++\"\u003e\u003cpre\u003e#\u003cspan class=\"pl-k\"\u003einclude\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0026lt;\u003c/span\u003eopenPMD/openPMD.hpp\u003cspan class=\"pl-pds\"\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\n#\u003cspan class=\"pl-k\"\u003einclude\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0026lt;\u003c/span\u003eiostream\u003cspan class=\"pl-pds\"\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e//\u003c/span\u003e ...\u003c/span\u003e\n\n\u003cspan class=\"pl-k\"\u003eauto\u003c/span\u003e s = openPMD::Series(\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003esamples/git-sample/data%T.h5\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e, openPMD::Access::READ_ONLY);\n\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e( \u003cspan class=\"pl-k\"\u003eauto\u003c/span\u003e \u003cspan class=\"pl-k\"\u003econst\u003c/span\u003e\u0026amp; i : s.iterations ) {\n    std::cout \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003eIteration: \u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u0026lt;\u0026lt; i.\u003cspan class=\"pl-smi\"\u003efirst\u003c/span\u003e \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-cce\"\u003e\\n\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e;\n\n    \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e( \u003cspan class=\"pl-k\"\u003eauto\u003c/span\u003e \u003cspan class=\"pl-k\"\u003econst\u003c/span\u003e\u0026amp; m : i.\u003cspan class=\"pl-smi\"\u003esecond\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003emeshes\u003c/span\u003e ) {\n        std::cout \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e  Mesh \u0027\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u0026lt;\u0026lt; m.\u003cspan class=\"pl-smi\"\u003efirst\u003c/span\u003e \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u0027 attributes:\u003cspan class=\"pl-cce\"\u003e\\n\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e;\n        \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e( \u003cspan class=\"pl-k\"\u003eauto\u003c/span\u003e \u003cspan class=\"pl-k\"\u003econst\u003c/span\u003e\u0026amp; val : m.\u003cspan class=\"pl-smi\"\u003esecond\u003c/span\u003e.\u003cspan class=\"pl-c1\"\u003eattributes\u003c/span\u003e() )\n            std::cout \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e    \u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u0026lt;\u0026lt; val \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003cspan class=\"pl-cce\"\u003e\\n\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e;\n    }\n\n    \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e( \u003cspan class=\"pl-k\"\u003eauto\u003c/span\u003e \u003cspan class=\"pl-k\"\u003econst\u003c/span\u003e\u0026amp; p : i.\u003cspan class=\"pl-smi\"\u003esecond\u003c/span\u003e.\u003cspan class=\"pl-smi\"\u003eparticles\u003c/span\u003e ) {\n        std::cout \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e  Particle species \u0027\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u0026lt;\u0026lt; p.\u003cspan class=\"pl-smi\"\u003efirst\u003c/span\u003e \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u0027 attributes:\u003cspan class=\"pl-cce\"\u003e\\n\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e;\n        \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e( \u003cspan class=\"pl-k\"\u003eauto\u003c/span\u003e \u003cspan class=\"pl-k\"\u003econst\u003c/span\u003e\u0026amp; val : p.\u003cspan class=\"pl-smi\"\u003esecond\u003c/span\u003e.\u003cspan class=\"pl-c1\"\u003eattributes\u003c/span\u003e() )\n            std::cout \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e    \u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u0026lt;\u0026lt; val \u0026lt;\u0026lt; \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003cspan class=\"pl-cce\"\u003e\\n\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\u0027\u003c/span\u003e\u003c/span\u003e;\n    }\n}\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-python\" class=\"anchor\" href=\"#python\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePython\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://www.python.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/acd285afab5d7ddd4942e5215ade53e84551c9d7d635642ba92c19fde7d4345b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d507974686f6e332d79656c6c6f77677265656e\" alt=\"Python3\" title=\"Python3 API\" data-canonical-src=\"https://img.shields.io/badge/language-Python3-yellowgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e \u003ca href=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7ec85c013128e804b5696f4fdf0c910a9f5456c4295b16217b194e8691f026b5/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f70686173652d626574612d79656c6c6f77677265656e\" alt=\"Python3 API: Beta\" title=\"Status: Beta\" data-canonical-src=\"https://img.shields.io/badge/phase-beta-yellowgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eimport\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eopenpmd_api\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eas\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eio\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e# ...\u003c/span\u003e\n\n\u003cspan class=\"pl-s1\"\u003eseries\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eio\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eSeries\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"samples/git-sample/data%T.h5\"\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003eio\u003c/span\u003e.\u003cspan class=\"pl-v\"\u003eAccess\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eread_only\u003c/span\u003e)\n\n\u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ek_i\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eseries\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eiterations\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eitems\u003c/span\u003e():\n    \u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"Iteration: {0}\"\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eformat\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ek_i\u003c/span\u003e))\n\n    \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ek_m\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003em\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003emeshes\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eitems\u003c/span\u003e():\n        \u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"  Mesh \u0027{0}\u0027 attributes:\"\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eformat\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ek_m\u003c/span\u003e))\n        \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ea\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003em\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eattributes\u003c/span\u003e:\n            \u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"    {0}\"\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eformat\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ea\u003c/span\u003e))\n\n    \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ek_p\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ep\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ei\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eparticles\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eitems\u003c/span\u003e():\n        \u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"  Particle species \u0027{0}\u0027 attributes:\"\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eformat\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ek_p\u003c/span\u003e))\n        \u003cspan class=\"pl-k\"\u003efor\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ea\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003ein\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ep\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eattributes\u003c/span\u003e:\n            \u003cspan class=\"pl-en\"\u003eprint\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"    {0}\"\u003c/span\u003e.\u003cspan class=\"pl-en\"\u003eformat\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ea\u003c/span\u003e))\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-more\" class=\"anchor\" href=\"#more\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMore!\u003c/h3\u003e\n\u003cp\u003eCurious?\nOur manual shows full \u003ca href=\"https://openpmd-api.readthedocs.io/en/latest/usage/firstwrite.html\" rel=\"nofollow\"\u003eread \u0026amp; write examples\u003c/a\u003e, both serial and MPI-parallel!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-dependencies\" class=\"anchor\" href=\"#dependencies\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDependencies\u003c/h2\u003e\n\u003cp\u003eRequired:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCMake 3.15.0+\u003c/li\u003e\n\u003cli\u003eC++14 capable compiler, e.g. g++ 5.0+, clang 5.0+, VS 2017+\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eShipped internally in \u003ccode\u003eshare/openPMD/thirdParty/\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/mpark/variant\"\u003eMPark.Variant\u003c/a\u003e 1.4.0+ (\u003ca href=\"https://github.com/mpark/variant/blob/master/LICENSE.md\"\u003eBSL-1.0\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/catchorg/Catch2\"\u003eCatch2\u003c/a\u003e 2.13.4+ (\u003ca href=\"https://github.com/catchorg/Catch2/blob/master/LICENSE.txt\"\u003eBSL-1.0\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/pybind/pybind11\"\u003epybind11\u003c/a\u003e 2.6.2+ (\u003ca href=\"https://github.com/pybind/pybind11/blob/master/LICENSE\"\u003enew BSD\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/nlohmann/json\"\u003eNLohmann-JSON\u003c/a\u003e 3.9.1+ (\u003ca href=\"https://github.com/nlohmann/json/blob/develop/LICENSE.MIT\"\u003eMIT\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI/O backends:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/JSON\" rel=\"nofollow\"\u003eJSON\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://support.hdfgroup.org/HDF5\" rel=\"nofollow\"\u003eHDF5\u003c/a\u003e 1.8.13+ (optional)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.olcf.ornl.gov/center-projects/adios\" rel=\"nofollow\"\u003eADIOS1\u003c/a\u003e 1.13.1+ (optional)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/ornladios/ADIOS2\"\u003eADIOS2\u003c/a\u003e 2.7.0+ (optional)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ewhile those can be built either with or without:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMPI 2.1+, e.g. OpenMPI 1.6.5+ or MPICH2\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eOptional language bindings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePython:\n\u003cul\u003e\n\u003cli\u003ePython 3.6 - 3.9\u003c/li\u003e\n\u003cli\u003epybind11 2.6.2+\u003c/li\u003e\n\u003cli\u003enumpy 1.15+\u003c/li\u003e\n\u003cli\u003empi4py 2.1+ (optional, for MPI)\u003c/li\u003e\n\u003cli\u003epandas 1.0+ (optional, for dataframes)\u003c/li\u003e\n\u003cli\u003edask 2021+ (optional, for dask dataframes)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/580cdb6331254f66680bd967326d9630f5124291efd5ace18549fbb93cbae6db/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f737061636b2e696f2d6f70656e706d642d2d6170692d627269676874677265656e\" alt=\"Spack Package\" data-canonical-src=\"https://img.shields.io/badge/spack.io-openpmd--api-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/conda-forge/openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3c3728308a9e416589fa8b3b8168967287c515037bfbd4b40f1b14f266de56fb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636f6e64612e696f2d6f70656e706d642d2d6170692d627269676874677265656e\" alt=\"Conda Package\" data-canonical-src=\"https://img.shields.io/badge/conda.io-openpmd--api-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/openPMD/homebrew-openPMD\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/92b7b7bb69b2b17d1981ae5fdcdb32f971ee57f54a98ea4804e93fd0a2eb58ef/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772e73682d6f70656e706d642d2d6170692d627269676874677265656e\" alt=\"Brew Package\" data-canonical-src=\"https://img.shields.io/badge/brew.sh-openpmd--api-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/4eeb2c48c0e554ea8dbe8e90f73a6f2d217e775e0bd5e5cb90ddf4619ef3a6a0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707970692e6f72672d6f70656e706d642d2d6170692d627269676874677265656e\" alt=\"PyPI Package\" data-canonical-src=\"https://img.shields.io/badge/pypi.org-openpmd--api-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://cmake.org\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2babf79954ea524c24f2f9b1cfa05728fdaccbe0a80c2da6a7a7595cc131894c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f66726f6d5f736f757263652d434d616b652d627269676874677265656e\" alt=\"From Source\" data-canonical-src=\"https://img.shields.io/badge/from_source-CMake-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOur community loves to help each other.\nPlease \u003ca href=\"https://github.com/openPMD/openPMD-api/issues/new?labels=install\u0026amp;template=install_problem.md\"\u003ereport installation problems\u003c/a\u003e in case you should get stuck.\u003c/p\u003e\n\u003cp\u003eChoose \u003cem\u003eone\u003c/em\u003e of the install methods below to get started:\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-spack\" class=\"anchor\" href=\"#spack\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://spack.io\" rel=\"nofollow\"\u003eSpack\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/becffbecb6a50502286f02ec86c4e62f239f3671148d11341152e0dc695a2fc9/68747470733a2f2f696d672e736869656c64732e696f2f737061636b2f762f6f70656e706d642d617069\" alt=\"Spack Version\" data-canonical-src=\"https://img.shields.io/spack/v/openpmd-api\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://spack.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\" alt=\"Spack Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://spack.readthedocs.io/en/latest/package_list.html#openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b29fc54abf92a2a6d1d2c70159eb276df53b67a1294ae3f82b1b0bf22a3c586b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392c5f646576656c6f706d656e742c5f4850432d627269676874677265656e\" alt=\"Spack Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29,_development,_HPC-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:               +python +adios1 -adios2 -hdf5 -mpi\u003c/span\u003e\nspack install openpmd-api\nspack load -r openpmd-api\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-conda\" class=\"anchor\" href=\"#conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://conda.io\" rel=\"nofollow\"\u003eConda\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://anaconda.org/conda-forge/openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3ad2b345bb3589aa2d733ca20df72938ed54a48342b69f7cbe9eacab43d84549/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f766e2f636f6e64612d666f7267652f6f70656e706d642d617069\" alt=\"Conda Version\" data-canonical-src=\"https://img.shields.io/conda/vn/conda-forge/openpmd-api\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/conda-forge/openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\" alt=\"Conda Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/conda-forge/openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\" alt=\"Conda Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://anaconda.org/conda-forge/openpmd-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a51d3cd2bfa3c6b01bd0f2e36abc206fb9db5700105fac2acd2c2105fced2ec4/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f636f6e64612d666f7267652f6f70656e706d642d617069\" alt=\"Conda Downloads\" data-canonical-src=\"https://img.shields.io/conda/dn/conda-forge/openpmd-api\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                      OpenMPI support  =*=mpi_openmpi*\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                        MPICH support  =*=mpi_mpich*\u003c/span\u003e\nconda create -n openpmd -c conda-forge openpmd-api\nconda activate openpmd\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-brew\" class=\"anchor\" href=\"#brew\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://brew.sh\" rel=\"nofollow\"\u003eBrew\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/openPMD/homebrew-openPMD\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d873c8c473fece8abf1c34a8299a50b7ee0da9772eb50cce8649e7ca32468a6c/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f627265772d6c61746573745f76657273696f6e2d6f72616e6765\" alt=\"Brew Version\" data-canonical-src=\"https://img.shields.io/badge/brew-latest_version-orange\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://docs.brew.sh/Homebrew-on-Linux\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/ceb548d628bb13bf7eed61d8bb9292368f02fb952a549f53c7f34c8afad3ed11/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f73782532302d626c7565\" alt=\"Brew Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://brew.sh\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7b767b67facc26db7d850ae5c3155fa949048991dde7a0f5471c1e6862f0a586/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f253238432532422532422c5f70792532392d627269676874677265656e\" alt=\"Brew Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28C%2B%2B,_py%29-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ebrew tap openpmd/openpmd\nbrew install openpmd-api\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-pypi\" class=\"anchor\" href=\"#pypi\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://pypi.org\" rel=\"nofollow\"\u003ePyPI\u003c/a\u003e\n\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://pypi.org/project/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fc28bd368523d0b8adab5f4b414aebb304743130eef42bca203f4e9eed428ae7/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f6f70656e504d442d617069\" alt=\"PyPI Version\" data-canonical-src=\"https://img.shields.io/pypi/v/openPMD-api\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/openPMD-api/#files\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/114e64f6c29b3e409c6de5b19ee4074ec3053396d43319fe4876231f1480e0d1/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d6c696e75782532302537432532306f737825323025374325323077696e2d626c7565\" alt=\"PyPI Platforms\" data-canonical-src=\"https://img.shields.io/badge/platforms-linux%20%7C%20osx%20%7C%20win-blue\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a422d06dfefa14bef9b5ad8fbd0df126763371e6f988172ee60ff850303a4fa4/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d6465736b746f705f25323870792532392d627269676874677265656e\" alt=\"PyPI Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-desktop_%28py%29-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/7cef25e887c028f65d5d7e20f3e7abe394ab328ecb499e34e47460afd2c78558/68747470733a2f2f696d672e736869656c64732e696f2f707970692f666f726d61742f6f70656e504d442d617069\" alt=\"PyPI Format\" data-canonical-src=\"https://img.shields.io/pypi/format/openPMD-api\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://pypi.org/project/openPMD-api\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/82acdd95515851a5ba4a3006871151f76cfe4d6583f6c24e80bd0fd29d391845/68747470733a2f2f696d672e736869656c64732e696f2f707970692f646d2f6f70656e504d442d617069\" alt=\"PyPI Downloads\" data-canonical-src=\"https://img.shields.io/pypi/dm/openPMD-api\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eOn very old macOS versions (\u0026lt;10.9) or on exotic processor architectures, this install method \u003cem\u003ecompiles from source\u003c/em\u003e against the found installations of HDF5, ADIOS1, ADIOS2, and/or MPI (in system paths, from other package managers, or loaded via a module system, ...).\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e we need pip 19 or newer\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                   --user\u003c/span\u003e\npython3 -m pip install -U pip\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                        --user\u003c/span\u003e\npython3 -m pip install openpmd-api\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eIf MPI-support shall be enabled, we always have to recompile:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                                    --user\u003c/span\u003e\npython3 -m pip install -U pip setuptools wheel\npython3 -m pip install -U cmake\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                                                                   --user\u003c/span\u003e\nopenPMD_USE_MPI=ON python3 -m pip install openpmd-api --no-binary openpmd-api\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor some exotic architectures and compilers, you might need to disable a compiler feature called \u003ca href=\"https://en.wikipedia.org/wiki/Interprocedural_optimization\" rel=\"nofollow\"\u003elink-time/interprocedural optimization\u003c/a\u003e if you encounter linking problems:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CMAKE_INTERPROCEDURAL_OPTIMIZATION=OFF\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional:                                                --user\u003c/span\u003e\npython3 -m pip install openpmd-api --no-binary openpmd-api\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-from-source\" class=\"anchor\" href=\"#from-source\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFrom Source\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://cmake.org\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/0a40953107090abff3b564ec3436668756b873cd4d9e021738a046781a5a0e6b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f7573655f636173652d646576656c6f706d656e742d627269676874677265656e\" alt=\"Source Use Case\" data-canonical-src=\"https://img.shields.io/badge/use_case-development-brightgreen\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eopenPMD-api can also be built and installed from source using \u003ca href=\"https://cmake.org/\" rel=\"nofollow\"\u003eCMake\u003c/a\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone https://github.com/openPMD/openPMD-api.git\n\nmkdir openPMD-api-build\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e openPMD-api-build\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional: for full tests, with unzip\u003c/span\u003e\n../openPMD-api/share/openPMD/download_samples.sh\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e for own install prefix append:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e   -DCMAKE_INSTALL_PREFIX=$HOME/somepath\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e for options append:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e   -DopenPMD_USE_...=...\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e e.g. for python support add:\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e   -DopenPMD_USE_PYTHON=ON\u003c/span\u003e\ncmake ../openPMD-api\n\ncmake --build \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional\u003c/span\u003e\nctest\n\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e sudo might be required for system paths\u003c/span\u003e\ncmake --build \u003cspan class=\"pl-c1\"\u003e.\u003c/span\u003e --target install\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThe following options can be added to the \u003ccode\u003ecmake\u003c/code\u003e call to control features.\nCMake controls options with prefixed \u003ccode\u003e-D\u003c/code\u003e, e.g. \u003ccode\u003e-DopenPMD_USE_MPI=OFF\u003c/code\u003e:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCMake Option\u003c/th\u003e\n\u003cth\u003eValues\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_MPI\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eAUTO\u003c/strong\u003e/ON/OFF\u003c/td\u003e\n\u003ctd\u003eParallel, Multi-Node I/O for clusters\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_HDF5\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eAUTO\u003c/strong\u003e/ON/OFF\u003c/td\u003e\n\u003ctd\u003eHDF5 backend (\u003ccode\u003e.h5\u003c/code\u003e files)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_ADIOS1\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eAUTO\u003c/strong\u003e/ON/OFF\u003c/td\u003e\n\u003ctd\u003eADIOS1 backend (\u003ccode\u003e.bp\u003c/code\u003e files up to version BP3)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_ADIOS2\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eAUTO\u003c/strong\u003e/ON/OFF\u003c/td\u003e\n\u003ctd\u003eADIOS2 backend (\u003ccode\u003e.bp\u003c/code\u003e files in BP3, BP4 or higher)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_PYTHON\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eAUTO\u003c/strong\u003e/ON/OFF\u003c/td\u003e\n\u003ctd\u003eEnable Python bindings\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_INVASIVE_TESTS\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003eON/\u003cstrong\u003eOFF\u003c/strong\u003e\n\u003c/td\u003e\n\u003ctd\u003eEnable unit tests that modify source code \u003csup\u003e1\u003c/sup\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_VERIFY\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eEnable internal VERIFY (assert) macro independent of build type \u003csup\u003e2\u003c/sup\u003e\n\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_INSTALL\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eAdd installation targets\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003ePython_EXECUTABLE\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e(newest found)\u003c/td\u003e\n\u003ctd\u003ePath to Python executable\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e\u003csup\u003e1\u003c/sup\u003e \u003cem\u003ee.g. changes C++ visibility keywords, breaks MSVC\u003c/em\u003e\n\u003csup\u003e2\u003c/sup\u003e \u003cem\u003ethis includes most pre-/post-condition checks, disabling without specific cause is highly discouraged\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eAdditionally, the following libraries are shipped internally.\nThe following options allow to switch to external installs:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCMake Option\u003c/th\u003e\n\u003cth\u003eValues\u003c/th\u003e\n\u003cth\u003eLibrary\u003c/th\u003e\n\u003cth\u003eVersion\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_INTERNAL_VARIANT\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eMPark.Variant\u003c/td\u003e\n\u003ctd\u003e1.4.0+\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_INTERNAL_CATCH\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eCatch2\u003c/td\u003e\n\u003ctd\u003e2.13.4+\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_INTERNAL_PYBIND11\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003epybind11\u003c/td\u003e\n\u003ctd\u003e2.6.2+\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_USE_INTERNAL_JSON\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eNLohmann-JSON\u003c/td\u003e\n\u003ctd\u003e3.9.1+\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eBy default, this will build as a shared library (\u003ccode\u003elibopenPMD.[so|dylib|dll]\u003c/code\u003e) and installs also its headers.\nIn order to build a static library, append \u003ccode\u003e-DBUILD_SHARED_LIBS=OFF\u003c/code\u003e to the \u003ccode\u003ecmake\u003c/code\u003e command.\nYou can only build a static or a shared library at a time.\u003c/p\u003e\n\u003cp\u003eBy default, the \u003ccode\u003eRelease\u003c/code\u003e version is built.\nIn order to build with debug symbols, pass \u003ccode\u003e-DCMAKE_BUILD_TYPE=Debug\u003c/code\u003e to your \u003ccode\u003ecmake\u003c/code\u003e command.\u003c/p\u003e\n\u003cp\u003eBy default, tests, examples and command line tools are built.\nIn order to skip building those, pass \u003ccode\u003eOFF\u003c/code\u003e to these \u003ccode\u003ecmake\u003c/code\u003e options:\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eCMake Option\u003c/th\u003e\n\u003cth\u003eValues\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_BUILD_TESTING\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eBuild tests\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_BUILD_EXAMPLES\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eBuild examples\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eopenPMD_BUILD_CLI_TOOLS\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\n\u003cstrong\u003eON\u003c/strong\u003e/OFF\u003c/td\u003e\n\u003ctd\u003eBuild command-line tools\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-linking-to-your-project\" class=\"anchor\" href=\"#linking-to-your-project\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLinking to your project\u003c/h2\u003e\n\u003cp\u003eThe install will contain header files and libraries in the path set with \u003ccode\u003e-DCMAKE_INSTALL_PREFIX\u003c/code\u003e.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-cmake\" class=\"anchor\" href=\"#cmake\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCMake\u003c/h3\u003e\n\u003cp\u003eIf your project is using CMake for its build, one can conveniently use our provided \u003ccode\u003eopenPMDConfig.cmake\u003c/code\u003e package which is installed alongside the library.\u003c/p\u003e\n\u003cp\u003eFirst set the following environment hint if openPMD-api was \u003cem\u003enot\u003c/em\u003e installed in a system path:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional: only needed if installed outside of system paths\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e CMAKE_PREFIX_PATH=\u003cspan class=\"pl-smi\"\u003e$HOME\u003c/span\u003e/somepath:\u003cspan class=\"pl-smi\"\u003e$CMAKE_PREFIX_PATH\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eUse the following lines in your project\u0027s \u003ccode\u003eCMakeLists.txt\u003c/code\u003e:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-cmake\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e supports:                       COMPONENTS MPI NOMPI HDF5 ADIOS1 ADIOS2\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003efind_package\u003c/span\u003e(openPMD 0.9.0 \u003cspan class=\"pl-k\"\u003eCONFIG\u003c/span\u003e)\n\n\u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e(openPMD_FOUND)\n    \u003cspan class=\"pl-c1\"\u003etarget_link_libraries\u003c/span\u003e(YourTarget \u003cspan class=\"pl-k\"\u003ePRIVATE\u003c/span\u003e openPMD::openPMD)\n\u003cspan class=\"pl-k\"\u003eendif\u003c/span\u003e()\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003e\u003cem\u003eAlternatively\u003c/em\u003e, add the openPMD-api repository source directly to your project and use it via:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-cmake\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003eadd_subdirectory\u003c/span\u003e(\u003cspan class=\"pl-s\"\u003e\"path/to/source/of/openPMD-api\"\u003c/span\u003e)\n\n\u003cspan class=\"pl-c1\"\u003etarget_link_libraries\u003c/span\u003e(YourTarget \u003cspan class=\"pl-k\"\u003ePRIVATE\u003c/span\u003e openPMD::openPMD)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eFor development workflows, you can even automatically download and build openPMD-api from within a depending CMake project.\nJust replace the \u003ccode\u003eadd_subdirectory\u003c/code\u003e call with:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-cmake\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003einclude\u003c/span\u003e(FetchContent)\n\u003cspan class=\"pl-c1\"\u003eset\u003c/span\u003e(CMAKE_POLICY_DEFAULT_CMP0077 \u003cspan class=\"pl-k\"\u003eNEW\u003c/span\u003e)\n\u003cspan class=\"pl-c1\"\u003eset\u003c/span\u003e(openPMD_BUILD_CLI_TOOLS \u003cspan class=\"pl-k\"\u003eOFF\u003c/span\u003e)\n\u003cspan class=\"pl-c1\"\u003eset\u003c/span\u003e(openPMD_BUILD_EXAMPLES \u003cspan class=\"pl-k\"\u003eOFF\u003c/span\u003e)\n\u003cspan class=\"pl-c1\"\u003eset\u003c/span\u003e(openPMD_BUILD_TESTING \u003cspan class=\"pl-k\"\u003eOFF\u003c/span\u003e)\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e set(openPMD_BUILD_SHARED_LIBS OFF)  # precedence over BUILD_SHARED_LIBS if needed; or:\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eset\u003c/span\u003e(openPMD_INSTALL \u003cspan class=\"pl-smi\"\u003e${BUILD_SHARED_LIBS}\u003c/span\u003e)  \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e only install if used as shared a library\u003c/span\u003e\n\u003cspan class=\"pl-c1\"\u003eset\u003c/span\u003e(openPMD_USE_PYTHON \u003cspan class=\"pl-k\"\u003eOFF\u003c/span\u003e)\nFetchContent_Declare(openPMD\n  GIT_REPOSITORY \u003cspan class=\"pl-s\"\u003e\"https://github.com/openPMD/openPMD-api.git\"\u003c/span\u003e\n  GIT_TAG        \u003cspan class=\"pl-s\"\u003e\"dev\"\u003c/span\u003e)\nFetchContent_MakeAvailable(openPMD)\u003c/pre\u003e\u003c/div\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-manually\" class=\"anchor\" href=\"#manually\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eManually\u003c/h3\u003e\n\u003cp\u003eIf your (Linux/OSX) project is build by calling the compiler directly or uses a manually written \u003ccode\u003eMakefile\u003c/code\u003e, consider using our \u003ccode\u003eopenPMD.pc\u003c/code\u003e helper file for \u003ccode\u003epkg-config\u003c/code\u003e which are installed alongside the library.\u003c/p\u003e\n\u003cp\u003eFirst set the following environment hint if openPMD-api was \u003cem\u003enot\u003c/em\u003e installed in a system path:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e optional: only needed if installed outside of system paths\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eexport\u003c/span\u003e PKG_CONFIG_PATH=\u003cspan class=\"pl-smi\"\u003e$HOME\u003c/span\u003e/somepath/lib/pkgconfig:\u003cspan class=\"pl-smi\"\u003e$PKG_CONFIG_PATH\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAdditional linker and compiler flags for your project are available via:\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e switch to check if openPMD-api was build as static library\u003c/span\u003e\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e (via BUILD_SHARED_LIBS=OFF) or as shared library (default)\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eif\u003c/span\u003e [ \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e$(\u003c/span\u003epkg-config --variable=static openPMD\u003cspan class=\"pl-pds\"\u003e)\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e \u003cspan class=\"pl-k\"\u003e==\u003c/span\u003e \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003etrue\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e ]\n\u003cspan class=\"pl-k\"\u003ethen\u003c/span\u003e\n    pkg-config --libs --static openPMD\n    \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e -L/usr/local/lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lopenPMD -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/libmpi.so /usr/lib/x86_64-linux-gnu/hdf5/openmpi/libhdf5.so /usr/lib/x86_64-linux-gnu/libsz.so /usr/lib/x86_64-linux-gnu/libz.so /usr/lib/x86_64-linux-gnu/libdl.so /usr/lib/x86_64-linux-gnu/libm.so -pthread /usr/lib/libmpi.so -pthread /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so /usr/lib/libmpi.so\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003eelse\u003c/span\u003e\n    pkg-config --libs openPMD\n    \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e -L${HOME}/somepath/lib -lopenPMD\u003c/span\u003e\n\u003cspan class=\"pl-k\"\u003efi\u003c/span\u003e\n\npkg-config --cflags openPMD\n\u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e -I${HOME}/somepath/include\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-author-contributions\" class=\"anchor\" href=\"#author-contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAuthor Contributions\u003c/h2\u003e\n\u003cp\u003eopenPMD-api is developed by many people.\nIt was initially started by the \u003ca href=\"https://hzdr.de/crp\" rel=\"nofollow\"\u003eComputational Radiation Physics Group\u003c/a\u003e at \u003ca href=\"https://www.hzdr.de/\" rel=\"nofollow\"\u003eHZDR\u003c/a\u003e as successor to \u003ca href=\"https://github.com/ComputationalRadiationPhysics/libSplash/\"\u003elibSplash\u003c/a\u003e, generalizing the \u003ca href=\"https://arxiv.org/abs/1706.00522\" rel=\"nofollow\"\u003esuccessful HDF5 \u0026amp; ADIOS1 implementations\u003c/a\u003e in \u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu\"\u003ePIConGPU\u003c/a\u003e.\nThe following people and institutions \u003ca href=\"https://github.com/openPMD/openPMD-api/graphs/contributors\"\u003econtributed\u003c/a\u003e to openPMD-api:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/ax3l\"\u003eAxel Huebl (HZDR, now LBNL)\u003c/a\u003e:\nproject lead, releases, documentation, automated CI/CD, Python bindings, Dask, installation \u0026amp; packaging, prior reference implementations\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/franzpoeschel\"\u003eFranz Poeschel (CASUS)\u003c/a\u003e:\nJSON \u0026amp; ADIOS2 backend, data staging/streaming, reworked class design\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/C0nsultant\"\u003eFabian Koller (HZDR)\u003c/a\u003e:\ninitial library design and implementation with HDF5 \u0026amp; ADIOS1 backend\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/guj\"\u003eJunmin Gu (LBNL)\u003c/a\u003e:\nnon-collective parallel I/O fixes, ADIOS improvements, benchmarks\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFurther thanks go to improvements and contributions from:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/CFGrote\"\u003eCarsten Fortmann-Grote (EU XFEL GmbH, now MPI-EvolBio)\u003c/a\u003e:\ndraft of our Python unit tests\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/StanczakDominik\"\u003eDominik Sta\u0144czak (Warsaw University of Technology)\u003c/a\u003e:\ndocumentation improvements\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/mingwandroid\"\u003eRay Donnelly (Anaconda, Inc.)\u003c/a\u003e:\nsupport on conda packaging and libc++ quirks\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/amundson\"\u003eJames Amundson (FNAL)\u003c/a\u003e:\ncompile fix for newer compilers\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/psychocoderHPC\"\u003eRen\u00e9 Widera (HZDR)\u003c/a\u003e:\ndesign improvements for initial API design\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/erikzenker\"\u003eErik Zenker (HZDR)\u003c/a\u003e:\ndesign improvements for initial API design\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/sbastrakov\"\u003eSergei Bastrakov (HZDR)\u003c/a\u003e:\ndocumentation improvements (windows)\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/RemiLehe\"\u003eR\u00e9mi Lehe (LBNL)\u003c/a\u003e:\npackage integration testing on macOS and Linux\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/LDAmorim\"\u003eL\u00edgia Diana Amorim (LBNL)\u003c/a\u003e:\npackage integration testing on macOS\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/KseniaBastrakova\"\u003eKseniia Bastrakova (HZDR)\u003c/a\u003e:\ncompatibility testing\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/PrometheusPi\"\u003eRichard Pausch (HZDR)\u003c/a\u003e:\ncompatibility testing, documentation improvements\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/pordyna\"\u003ePawe\u0142 Ordyna (HZDR)\u003c/a\u003e:\nreport on NVCC warnings\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/dmitry-ganyushin\"\u003eDmitry Ganyushin (ORNL)\u003c/a\u003e:\nDask dataframe support\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/jakirkham\"\u003eJohn Kirkham (NVIDIA)\u003c/a\u003e:\nDask guidance \u0026amp; reviews\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-grants\" class=\"anchor\" href=\"#grants\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGrants\u003c/h3\u003e\n\u003cp\u003eThe openPMD-api authors acknowledge support via the following programs.\nThis project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 654220.\nSupported by the Consortium for Advanced Modeling of Particles Accelerators (CAMPA), funded by the U.S. DOE Office of Science under Contract No. DE-AC02-05CH11231.\nSupported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy organizations (Office of Science and the National Nuclear Security Administration).\nThis work was partially funded by the Center of Advanced Systems Understanding (CASUS), which is financed by Germany\u0027s Federal Ministry of Education and Research (BMBF) and by the Saxon Ministry for Science, Culture and Tourism (SMWK) with tax funds on the basis of the budget approved by the Saxon State Parliament.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-transitive-contributions\" class=\"anchor\" href=\"#transitive-contributions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTransitive Contributions\u003c/h3\u003e\n\u003cp\u003eopenPMD-api stands on the shoulders of giants and we are grateful for the following projects included as direct dependencies:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/ornladios/ADIOS\"\u003eADIOS1\u003c/a\u003e and \u003ca href=\"https://github.com/ornladios/ADIOS2\"\u003eADIOS2\u003c/a\u003e by \u003ca href=\"https://csmd.ornl.gov/adios\" rel=\"nofollow\"\u003eS. Klasky (ORNL), team, collaborators\u003c/a\u003e and \u003ca href=\"https://github.com/ornladios/ADIOS2/graphs/contributors\"\u003econtributors\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/catchorg/Catch2\"\u003eCatch2\u003c/a\u003e by \u003ca href=\"https://github.com/philsquared\"\u003ePhil Nash\u003c/a\u003e, \u003ca href=\"https://github.com/horenmar\"\u003eMartin Ho\u0159e\u0148ovsk\u00fd\u003c/a\u003e and \u003ca href=\"https://github.com/catchorg/Catch2/graphs/contributors\"\u003econtributors\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eHDF5 by \u003ca href=\"https://www.hdfgroup.org\" rel=\"nofollow\"\u003ethe HDF group\u003c/a\u003e and community\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/nlohmann/json\"\u003ejson\u003c/a\u003e by \u003ca href=\"https://github.com/nlohmann\"\u003eNiels Lohmann\u003c/a\u003e and \u003ca href=\"https://github.com/nlohmann/json/graphs/contributors\"\u003econtributors\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/pybind/pybind11\"\u003epybind11\u003c/a\u003e by \u003ca href=\"https://github.com/wjakob\"\u003eWenzel Jakob (EPFL)\u003c/a\u003e and \u003ca href=\"https://github.com/pybind/pybind11/graphs/contributors\"\u003econtributors\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eall contributors to the evolution of modern C++ and early library preview developers, e.g. \u003ca href=\"https://github.com/mpark\"\u003eMichael Park (Facebook)\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003ethe \u003ca href=\"https://cmake.org\" rel=\"nofollow\"\u003eCMake build system\u003c/a\u003e and \u003ca href=\"https://github.com/Kitware/CMake/blob/master/Copyright.txt\"\u003econtributors\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003epackaging support by the \u003ca href=\"https://conda-forge.org\" rel=\"nofollow\"\u003econda-forge\u003c/a\u003e, \u003ca href=\"https://pypi.org\" rel=\"nofollow\"\u003ePyPI\u003c/a\u003e and \u003ca href=\"https://spack.io\" rel=\"nofollow\"\u003eSpack\u003c/a\u003e communities, among others\u003c/li\u003e\n\u003cli\u003ethe \u003ca href=\"https://github.com/openPMD/openPMD-standard\"\u003eopenPMD-standard\u003c/a\u003e by \u003ca href=\"https://github.com/ax3l\"\u003eAxel Huebl (HZDR, now LBNL)\u003c/a\u003e and \u003ca href=\"https://github.com/openPMD/openPMD-standard/blob/latest/AUTHORS.md\"\u003econtributors\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 71,
    "subscribers_count": 9,
    "topics": [
      "openpmd",
      "openscience",
      "hdf5",
      "adios",
      "mpi",
      "hpc",
      "research",
      "file-handling",
      "python3",
      "meta-data",
      "opendata",
      "cpp14"
    ],
    "updated_at": 1626978256.0
  },
  {
    "data_format": 2,
    "description": "This repository consists for gpu bootcamp material for HPC and AI",
    "filenames": [
      "ai/RAPIDS/Singularity",
      "ai/DeepStream_Perf_Lab/Singularity",
      "ai/DeepStream/Singularity",
      "hpc_ai/ai_science_cfd/Singularity",
      "hpc_ai/ai_science_climate/Singularity",
      "misc/jupyter_lab_template/appName/Singularity",
      "hpc/openacc/Singularity",
      "hpc/miniprofiler/Singularity",
      "hpc/nways/Singularity"
    ],
    "full_name": "gpuhackathons-org/gpubootcamp",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-gpubootcamp-official-training-materials\" class=\"anchor\" href=\"#gpubootcamp-official-training-materials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGPUBootcamp Official Training Materials\u003c/h1\u003e\n\u003cp\u003eGPU Bootcamps are designed to help build confidence in Accelerated Computing and eventually prepare developers to enroll for \u003ca href=\"http://gpuhackathons.org/\" rel=\"nofollow\"\u003eHackathons\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis repository consists of GPU bootcamp material for HPC, AI and convergence of both:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc\"\u003eHPC\u003c/a\u003e ::\nThe bootcamp content focuses on how to follow the Analyze, Parallelize and Optimize Cycle to write parallel codes using different parallel programming models accelerating HPC simulations.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLab\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc/nways\"\u003eN-Ways\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eThis lab will cover multiple GPU programming models and choose the one that best fits your needs. The material supports different programmin glangauges including C ( CUDA C, OpenACC C, OpenMP C, C++ stdpar ),  Fortran ( CUDA Fortran, OpenACC Fortran, OpenMP Fortran, ISO DO CONCURRENT\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc/openacc\"\u003eOpenACC\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eThe lab will cover how to write portable parallel program that can run on multicore CPUs and accelerators like GPUs and how to apply incremental parallelization strategies using OpenACC\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai\"\u003eConvergence of HPC and AI\u003c/a\u003e ::\nThe bootcamp content focuses on how AI can accelerate HPC simulations by introducing concepts of Deep Neural Networks, including data pre-processing, techniques on how to build, compare and improve accuracy of deep learning models.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLab\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai/ai_science_climate\"\u003eWeather Pattern Recognition\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eThis Bootcamp will introduce developers to fundamentals of AI and how data driven approach can be applied to Climate/Weather domain\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/hpc_ai/ai_science_cfd\"\u003eCFD Flow Prediction\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eThis Bootcamp will introduce developers to fundamentals of AI and how they can be applied to CFD (Computational Fluid Dynamics)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai\"\u003eAI\u003c/a\u003e::\nThe bootcamp content focuses on using popular accelerated AI frameworks and using optimization techniques to get max performance from accelerators like GPU.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eLab\u003c/th\u003e\n\u003cth\u003eDescription\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai/DeepStream\"\u003eAccelerated Intelligent Video Analytics\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eLearn how Nvidia DeepStream SDK can be used to create optimized Intelligent Video Analytics (IVA) pipeline. Participants will be exposed to the building blocks for creating IVA pipeline followed by profiling exercise to identify hotspots in the pipeline and methods to optimize and get higher throughput\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/tree/master/ai/RAPIDS\"\u003eAccelerated Data Science\u003c/a\u003e\u003c/td\u003e\n\u003ctd\u003eLearn how RAPIDS suite of open source software libraries gives you the freedom to execute end-to-end data science and analytics pipelines entirely on GPUs. Participants will be exposed to using libraries that can be easily integrated with the daily data science pipeline and accelerate computations for faster execution\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-system-requirements\" class=\"anchor\" href=\"#system-requirements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSystem Requirements\u003c/h1\u003e\n\u003cp\u003eEach lab contains docker and singularity definition files. Follow the readme files inside each on how to build the container and run the labs inside it.\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-contribution\" class=\"anchor\" href=\"#contribution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContribution\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eThe repository uses Apache 2.0 license. For more details on folder structure developers may refer to CONTRIBUTING.md file.\u003c/li\u003e\n\u003cli\u003eA project template for reference is located at \u003ca href=\"https://github.com/bharatk-parallel/gpubootcamp-1/tree/nways_md_fortran/misc/jupyter_lab_template/appName\"\u003eTemplate\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-feature-request-or-filing-issues\" class=\"anchor\" href=\"#feature-request-or-filing-issues\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeature Request or filing issues\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eBootcamp users may request for newer training material or file a bug by filing a github issues\u003c/li\u003e\n\u003cli\u003ePlease do go through the existing list of issues to get more details of upcoming features and bugs currently being fixed \u003ca href=\"https://github.com/gpuhackathons-org/gpubootcamp/issues\"\u003eIssues\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-questions\" class=\"anchor\" href=\"#questions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuestions?\u003c/h2\u003e\n\u003cp\u003ePlease join \u003ca href=\"https://openacclang.slack.com/messages/openaccusergroup\" rel=\"nofollow\"\u003eOpenACC Slack Channel\u003c/a\u003e for questions.\u003c/p\u003e\n",
    "stargazers_count": 81,
    "subscribers_count": 7,
    "topics": [],
    "updated_at": 1627359697.0
  },
  {
    "data_format": 2,
    "description": "Run Rstudio Server in a conda environment",
    "filenames": [
      "singularity/Singularity"
    ],
    "full_name": "grst/rstudio-server-conda",
    "latest_release": "v0.3.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-running-rstudio-server-in-a-conda-environment\" class=\"anchor\" href=\"#running-rstudio-server-in-a-conda-environment\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server in a Conda Environment\u003c/h1\u003e\n\u003cp\u003eI usually rely on the \u003ca href=\"https://docs.conda.io/en/latest/\" rel=\"nofollow\"\u003econda package manager\u003c/a\u003e to manage my environments during development. Thanks to \u003ca href=\"https://conda-forge.org/\" rel=\"nofollow\"\u003econda-forge\u003c/a\u003e and \u003ca href=\"https://bioconda.github.io/\" rel=\"nofollow\"\u003ebioconda\u003c/a\u003e most R packages are now also available through conda. For production,\nI \u003ca href=\"https://github.com/grst/containerize-conda\"\u003econvert them to containers\u003c/a\u003e as these are easier to share.\u003c/p\u003e\n\u003cp\u003eUnfortunately, there seems to be \u003ca href=\"https://community.rstudio.com/t/start-rstudio-server-session-in-conda-environment/12516/15\" rel=\"nofollow\"\u003eno straightforward way\u003c/a\u003e to use conda envs in Rstudio server. This repository provides three approaches to make rstudio server work with conda envs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#running-rstudio-server-with-singularity\"\u003eRunning Rstudio Server in a Singularity Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-rstudio-server-with-podmandocker\"\u003eRunning Rstudio Server in a Docker/Podman Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-locally\"\u003eRunning Rstudio Server locally\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-rstudio-server-with-singularity\" class=\"anchor\" href=\"#running-rstudio-server-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server with Singularity\u003c/h2\u003e\n\u003cp\u003eWith this approach Rstudio Server runs in a Singularity container (based on \u003ca href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\"\u003erocker/rstudio\u003c/a\u003e).\u003cbr\u003e\nThe conda environment gets mounted into the container - like that there\u0027s no need to rebuild the container to add a package and\n\u003ccode\u003einstall.packages\u003c/code\u003e can be used without issues. The container-based approach has the following benefits:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAuthentication works (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003e#3\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSeveral separate instances of Rstudio server can run in parallel, even without the \u003cem\u003ePro\u003c/em\u003e version.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites\" class=\"anchor\" href=\"#prerequisites\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://sylabs.io/guides/3.0/user-guide/quick_start.html\" rel=\"nofollow\"\u003eSingularity\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage\" class=\"anchor\" href=\"#usage\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:grst/rstudio-server-conda.git\n\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e rstudio-server-conda/singularity\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eActivate the target conda env or set the environment variable \u003ccode\u003eCONDA_PREFIX\u003c/code\u003e\nto point to the location of the conda env.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the \u003ccode\u003erun_singularity.sh\u003c/code\u003e script. In particular, you may need to add additional bind mounts\n(e.g. a global data directory).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eExecute the \u003ccode\u003erun_singularity.sh\u003c/code\u003e script. It will automatically build the container if it is not available.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003ePORT=8787 PASSWORD=notsafe ./run_singularity.sh\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLog into Rstudio\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eopen rstudio server at \u003ccode\u003ehttp://localhost:8787\u003c/code\u003e (or whatever port you specified)\u003c/li\u003e\n\u003cli\u003elogin with your default username and the password you specified via the \u003ccode\u003ePASSWORD\u003c/code\u003e environment variable.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-rstudio-server-with-podmandocker\" class=\"anchor\" href=\"#running-rstudio-server-with-podmandocker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Rstudio Server with Podman/Docker\u003c/h2\u003e\n\u003cp\u003eThis approach is similar to \u003ca href=\"#running-rstudio-server-with-singularity\"\u003eSingularity\u003c/a\u003e, but uses\nDocker or Podman and a \u003ccode\u003edocker-compose.yml\u003c/code\u003e file instead.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-known-limitations\" class=\"anchor\" href=\"#known-limitations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNo access to shared group directories (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/14\"\u003e#14\u003c/a\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites-1\" class=\"anchor\" href=\"#prerequisites-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.docker.com/\" rel=\"nofollow\"\u003eDocker\u003c/a\u003e or \u003ca href=\"https://podman.io/\" rel=\"nofollow\"\u003ePodman\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/docker/compose\"\u003edocker-compose\u003c/a\u003e or \u003ca href=\"https://github.com/containers/podman-compose\"\u003epodman-compose\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-1\" class=\"anchor\" href=\"#usage-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repository\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003egit clone git@github.com:grst/rstudio-server-conda.git\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBuild the rstudio container (fetches the latest version of \u003ca href=\"https://hub.docker.com/r/rocker/rstudio\" rel=\"nofollow\"\u003erocker/rstudio\u003c/a\u003e and adds some custom scripts)\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003ecd\u003c/span\u003e rstudio-server-conda/docker\ndocker-compose build     \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e or podman-compose\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCopy the docker-compose.yml file into your project directory and adjust the paths.\u003c/p\u003e\n\u003cp\u003eYou may want to add additional volumes with your data.\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-yaml\"\u003e\u003cpre\u003e\u003cspan class=\"pl-s\"\u003e[...]\u003c/span\u003e\n   \u003cspan class=\"pl-ent\"\u003eports\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e port on the host : port in the container (the latter is always 8787)\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e8889:8787\u003cspan class=\"pl-pds\"\u003e\"\u003c/span\u003e\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003evolumes\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e mount conda env into exactely the same path as on the host system - some paths are hardcoded in the env.\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/anaconda3/envs/R400:/home/sturm/anaconda3/envs/R400\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e Share settings between rstudio instances\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/.local/share/rstudio/monitored/user-settings:/root/.local/share/rstudio/monitored/user-settings\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e mount the working directory containing your R project.\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003e/home/sturm/projects:/projects\u003c/span\u003e\n    \u003cspan class=\"pl-ent\"\u003eenvironment\u003c/span\u003e:\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e password used for authentication\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003ePASSWORD=notsafe\u003c/span\u003e\n      \u003cspan class=\"pl-c\"\u003e\u003cspan class=\"pl-c\"\u003e#\u003c/span\u003e repeat the path of the conda environment (must be identical to the path in \"volumes\")\u003c/span\u003e\n      - \u003cspan class=\"pl-s\"\u003eCONDAENV=/home/sturm/anaconda3/envs/R400\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun your project-specific instance of Rstudio-server\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-shell\"\u003e\u003cpre\u003edocker-compose up \u003c/pre\u003e\u003c/div\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLog into Rstudio\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eOpen your server at \u003ccode\u003ehttp://localhost:8889\u003c/code\u003e (or whatever port you specified)\u003c/li\u003e\n\u003cli\u003eLogin with the user \u003ccode\u003erstudio\u003c/code\u003e (when using Docker) or \u003ccode\u003eroot\u003c/code\u003e (when using Podman) and the password you specified\nin the \u003ccode\u003edocker-compose.yml\u003c/code\u003e. If you are using Podman and login with \u003ccode\u003erstudio\u003c/code\u003e you won\u0027t have permissions to\naccess the mounted volumes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-locally\" class=\"anchor\" href=\"#running-locally\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning Locally\u003c/h2\u003e\n\u003cp\u003eWith this approach a locally installed Rstudio server is ran such that it uses the conda env.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-known-limitations-1\" class=\"anchor\" href=\"#known-limitations-1\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKnown limitations\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eno authentication (\u003ca href=\"https://github.com/grst/rstudio-server-conda/issues/3\"\u003e#3\u003c/a\u003e). Use this approach only in a secure network!\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-prerequisites-2\" class=\"anchor\" href=\"#prerequisites-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePrerequisites\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.rstudio.com/products/rstudio/download-server/\" rel=\"nofollow\"\u003erstudio server\u003c/a\u003e installed locally\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003econda\u003c/a\u003e or \u003ca href=\"https://github.com/conda-forge/miniforge#mambaforge\"\u003emamba\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-usage-2\" class=\"anchor\" href=\"#usage-2\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsage\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eClone this repo\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit clone https://github.com/grst/rstudio-server-conda.git\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun rstudio server in the conda env\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd rstudio-server-conda/local\nconda activate my_project\n./start_rstudio_server.sh 8787  # use any free port number here. \n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eConnect to Rstudio\u003c/p\u003e\n\u003cp\u003eYou should now be able to connect to rstudio server on the port you specify.\n\u003cstrong\u003eIf an R Session has previously been running, you\u0027ll need to rstart the Rsession now\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eObviously, if your env does not have a version of \u003ccode\u003eR\u003c/code\u003e installed, this will either not\nwork at all, or fall back to the system-wide R installation.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-it-works\" class=\"anchor\" href=\"#how-it-works\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow it works\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRstudio server, can be started in non-daemonized mode by each user individually on a custom port (similar to a jupyter notebook). This instance can then run in a conda environment:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e\u0026gt; conda activate my_project\n\u0026gt; /usr/lib/rstudio-server/bin/rserver \\\n   --server-daemonize=0 \\\n   --www-port 8787 \\\n   --rsession-which-r=$(which R) \\\n   --rsession-ld-library-path=$CONDA_PREFIX/lib\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTo avoid additional problems with library paths, also \u003ccode\u003ersession\u003c/code\u003e needs to run within the conda environment. This is achieved by wrapping \u003ccode\u003ersession\u003c/code\u003e into the \u003ca href=\"https://github.com/grst/rstudio-server-conda/blob/master/local/rsession.sh\"\u003ersession.sh\u003c/a\u003e script. The path to the wrapped \u003ccode\u003ersession\u003c/code\u003e executable can be passed to \u003ccode\u003erserver\u003c/code\u003e as command line argument.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003erserver # ...\n    --rsession-path=rsession.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eWhen using multiple users a unique \u003ccode\u003esecret-cookie-key\u003c/code\u003e has to be generated for each user. The path to the secret cookie key can be passed to \u003ccode\u003erserver\u003c/code\u003e as a command line parameter.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003euuid \u0026gt; /tmp/rstudio-server/${USER}_secure-cookie-key\nrserver # ...\n  --secure-cookie-key-file /tmp/rstudio-server/${USER}_secure-cookie-key\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 81,
    "subscribers_count": 3,
    "topics": [],
    "updated_at": 1626788622.0
  },
  {
    "data_format": 2,
    "description": null,
    "filenames": [
      "image/Singularity.def",
      "image/Singularity.bionic"
    ],
    "full_name": "kalibera/rchk",
    "latest_release": null,
    "readme": "\u003cp\u003eThis project consists of several bug-finding tools that look for memory\nprotection errors in C source code using R API, that is in the source code\nof \u003ca href=\"http://www.r-project.org/\" rel=\"nofollow\"\u003eR\u003c/a\u003e itself and packages.  The tools perform\nwhole-program static analysis on LLVM bitcode and run on Linux.  About\n200-300 memory protection bugs have been found using rchk and fixed in R.\nrchk is now regularly used to check \u003ca href=\"https://github.com/kalibera/cran-checks/tree/master/rchk\"\u003eCRAN\npackages\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eTo use the tool, one needs to build R from source using a special compiler\nwrapper, which builds LLVM bitcode in addition to native code (both shared\nlibraries and executables). R packages are then installed using this version\nof R, providing LLVM bitcode for their shared libraries as well. The core of\nrchk is implemented in C++ and analyzes the LLVM bitcode of R packages and R\nitself. Several installation options are provided, including containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThe tool is available in pre-built containers, Docker and Singularity, for\n\u003cem\u003enon-interactive\u003c/em\u003e use. The container is invoked as a command to check a\nparticular package:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull kalibera/rchk:latest\ndocker run kalibera/rchk:latest audio\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003esingularity pull shub://kalibera/rchk:def\nsingularity run kalibera-rchk-master-def.simg jpeg\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eFor more details, see \u003ca href=\"doc/DOCKER.md\"\u003eDocker rchk container\u003c/a\u003e and\n\u003ca href=\"doc/SINGULARITY.md\"\u003eSingularity rchk container\u003c/a\u003e. This setup is good for\noccasional checking of a single package. Docker clients are\navailable for Linux, macOS and Windows. Singularity only for Linux.\u003c/p\u003e\n\u003cp\u003eThe tool can also be used interactively in a virtual machine running Ubuntu,\nwhich can be automatically installed using Vagrant scripts. This setup is\ngood for Linux, Windows and macOS users and makes it faster to repeatedly\ncheck the same package and easier to customize the process. See\n\u003ca href=\"doc/INSTALLATION.md\"\u003eAutomated installation (Docker/Virtualbox) for interactive use\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFinally, the tool can be installed natively on Linux, compiled from source.\nThis setup is good for interactive use and reduces disk space overhead. The\nsetup is not automated, but only requires several steps described for recent\nLinux distributions. See \u003ca href=\"doc/INSTALLATION.md\"\u003eNative installation on Linux for interactive use\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAn alternative docker image is also available from third parties on R-hub\n(\u003ccode\u003erhub/ubuntu-rchk\u003c/code\u003e,\n\u003ca href=\"https://github.com/r-hub/rhub-linux-builders/tree/master/ubuntu-rchk\"\u003esource\u003c/a\u003e).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-checking-the-first-package-interactive-use\" class=\"anchor\" href=\"#checking-the-first-package-interactive-use\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eChecking the first package (interactive use)\u003c/h2\u003e\n\u003cp\u003eThis part applies to interactive installation of rchk (natively or automated\ninstall in Docker/Virtualbox).  For this that one also needs to install\n\u003ccode\u003esubversion\u003c/code\u003e, \u003ccode\u003ersync\u003c/code\u003e (\u003ccode\u003eapt-get install subversion rsync\u003c/code\u003e, but already\navailable in the automated install).  More importantly, one also needs any\ndependencies needed by that package.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eBuild R producing also LLVM bitcode\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esvn checkout https://svn.r-project.org/R/trunk\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecd trunk\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e. ../scripts/config.inc\u003c/code\u003e (\u003cem\u003ein automated install\u003c/em\u003e, \u003ccode\u003e. /opt/rchk/scripts/config.inc\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e. ../scripts/cmpconfig.inc\u003c/code\u003e (\u003cem\u003ein automated install\u003c/em\u003e, \u003ccode\u003e. /opt/rchk/scripts/cmpconfig.inc\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e../scripts/build_r.sh\u003c/code\u003e (\u003cem\u003ein automated install\u003c/em\u003e, \u003ccode\u003e/opt/rchk/scripts/build_r.sh\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eInstall and check the package\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003eecho \u0027install.packages(\"jpeg\",repos=\"http://cloud.r-project.org\")\u0027 |  ./bin/R --no-echo\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ccode\u003e../scripts/check_package.sh jpeg\u003c/code\u003e (in VM install, \u003ccode\u003e/opt/rchk/scripts/check_package.sh jpeg\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe output of the checking is in files\n\u003ccode\u003epackages/lib/jpeg/libs/jpeg.so.*check\u003c/code\u003e. For version 0.1-8 of the package,\n\u003ccode\u003ejpeg.so.maacheck\u003c/code\u003e includes\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eWARNING Suspicious call (two or more unprotected arguments) to Rf_setAttrib at read_jpeg /rchk/trunk/packages/build/IsnsJjDm/jpeg/src/read.c:131\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ewhich is a true error. \u003ccode\u003ebcheck\u003c/code\u003e does not find any errors, \u003ccode\u003ejpeg.so.bcheck\u003c/code\u003e\nonly contains something like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eAnalyzed 15 functions, traversed 1938 states.\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo check the next package, just follow the same steps, installing it into\nthis customized version of R.  When checking a tarball, one would typically\nfirst install the CRAN/BIOC version of the package to get all dependencies\nin, and then use \u003ccode\u003eR CMD INSTALL\u003c/code\u003e to install the newest version to check from\nthe tarball.\u003c/p\u003e\n\u003cp\u003eOne can reduce the number of required R package dependencies by only\ninstalling LinkingTo dependencies of the package and then installing the\npackage with \u003ccode\u003e--libs-only\u003c/code\u003e option (only shared libraries are built and\ninstalled). This is enough to build shared libraries of most but not all\npackages. Docker and singularity rchk containers for non-interactive use do\nthis, see \u003ccode\u003escripts/utils.r\u003c/code\u003e and definitions of the containers for more\ndetails.\u003c/p\u003e\n\u003cp\u003eFurther information:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"doc/INSTALLATION.md\"\u003eInstallation\u003c/a\u003e - installation instructions.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"doc/USAGE.md\"\u003eUser documentation\u003c/a\u003e - how to use the tools and what they check.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"doc/INTERNALS.md\"\u003eInternals\u003c/a\u003e - how the tools work internally.\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"doc/BUILDING.md\"\u003eBuilding\u003c/a\u003e - how to get the necessary bitcode files for R/packages; this is now encapsulated in scripts, but the background is here\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/2534\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 96,
    "subscribers_count": 10,
    "topics": [],
    "updated_at": 1624654368.0
  },
  {
    "data_format": 2,
    "description": "A deep learning python package for neuroimaging data. Made by:",
    "filenames": [
      "deepneuro/pipelines/Skull_Stripping/Singularity.deepneuro_skullstripping",
      "deepneuro/pipelines/Segment_Brain_Mets/Singularity.deepneuro_segment_mets",
      "deepneuro/pipelines/Ischemic_Stroke/Singularity.deepneuro_segment_ischemic_stroke",
      "deepneuro/pipelines/Segment_GBM/Singularity.deepneuro_segment_gbm"
    ],
    "full_name": "QTIM-Lab/DeepNeuro",
    "latest_release": null,
    "readme": "\u003cp\u003e\u003ca href=\"./package_resources/logos/DeepNeuro_alt.PNG?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"./package_resources/logos/DeepNeuro_alt.PNG?raw=true\" alt=\"Alt text\" title=\"DeepNeuro\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/QTIM-Lab/DeepNeuro\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/645f122a503a7934dcfcfc971aff595f877adc6da0142112c94b4df371fdd88d/68747470733a2f2f7472617669732d63692e6f72672f5154494d2d4c61622f446565704e6575726f2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/QTIM-Lab/DeepNeuro.svg?branch=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-deepneuro\" class=\"anchor\" href=\"#deepneuro\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDeepNeuro\u003c/h1\u003e\n\u003cp\u003eA deep learning python package for neuroimaging data. Focused on validated command-line tools you can use today. Created by the Quantitative Tumor Imaging Lab at the Martinos Center (Harvard-MIT Program in Health, Sciences, and Technology / Massachusetts General Hospital).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#about\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"question\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2753.png\"\u003e\u2753\u003c/g-emoji\u003e About\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"floppy_disk\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4be.png\"\u003e\ud83d\udcbe\u003c/g-emoji\u003e Installation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#tutorials\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"mortar_board\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f393.png\"\u003e\ud83c\udf93\u003c/g-emoji\u003e Tutorials\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#modules\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"gift\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f381.png\"\u003e\ud83c\udf81\u003c/g-emoji\u003e Modules\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#contact\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"speech_balloon\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4ac.png\"\u003e\ud83d\udcac\u003c/g-emoji\u003e Contact\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"mega\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4e3.png\"\u003e\ud83d\udce3\u003c/g-emoji\u003e Citation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#acknowledgements\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"yellow_heart\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f49b.png\"\u003e\ud83d\udc9b\u003c/g-emoji\u003e Acknowledgements\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about\" class=\"anchor\" href=\"#about\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout\u003c/h2\u003e\n\u003cp\u003eDeepNeuro is an open-source toolset of deep learning applications for neuroimaging. We have several goals for this package:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eProvide easy-to-use command line tools for neuroimaging using deep learning.\u003c/li\u003e\n\u003cli\u003eCreate Docker containers for each tool and all out-of-package pre-processing steps, so they can each can be run without having install prerequisite libraries.\u003c/li\u003e\n\u003cli\u003eProvide freely available deep learning models trained on a wealth of neuroimaging data.\u003c/li\u003e\n\u003cli\u003eProvide training scripts and links to publically-available data to replicate the results of DeepNeuro\u0027s models.\u003c/li\u003e\n\u003cli\u003eProvide implementations of popular models for medical imaging data, and pre-processed datasets for educational purposes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThis package is under active development, but we encourage users to both try the modules with pre-trained modules highlighted below, and try their hand at making their own DeepNeuro modules using the tutorials below.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall Docker from Docker\u0027s website here: \u003ca href=\"https://www.docker.com/get-started\" rel=\"nofollow\"\u003ehttps://www.docker.com/get-started\u003c/a\u003e. Follow instructions on that link to get Docker set up properly on your workstation.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall the Docker Engine Utility for NVIDIA GPUs, AKA nvidia-docker. You can find installation instructions at their Github page, here: \u003ca href=\"https://github.com/NVIDIA/nvidia-docker\"\u003ehttps://github.com/NVIDIA/nvidia-docker\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePull the DeepNeuro Docker container from \u003ca href=\"https://hub.docker.com/r/qtimlab/deepneuro_segment_gbm/\" rel=\"nofollow\"\u003ehttps://hub.docker.com/r/qtimlab/deepneuro_segment_gbm/\u003c/a\u003e. Use the command \"docker pull qtimlab/deepneuro\"\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf you want to run DeepNeuro outside of a Docker container, you can install the DeepNeuro Python package locally using the pip package manager. On the command line, run \u003ccode\u003epip install deepneuro\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-tutorials\" class=\"anchor\" href=\"#tutorials\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTutorials\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Preprocess_and_Augment.ipynb\" rel=\"nofollow\"\u003e\n\u003cimg src=\"./notebooks/resources/train_preprocess_icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Train_Model.ipynb\" rel=\"nofollow\"\u003e\n\u003cimg src=\"./notebooks/resources/train_model_icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://colab.research.google.com/github/QTIM-Lab/DeepNeuro/blob/master/notebooks/Run_Inference.ipynb\" rel=\"nofollow\"\u003e\n\u003cimg src=\"./notebooks/resources/model_inference_icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-modules\" class=\"anchor\" href=\"#modules\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eModules\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Segment_GBM\"\u003e\n\u003cimg src=\"./deepneuro/pipelines/Segment_GBM/resources/icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Skull_Stripping\"\u003e\n\u003cimg src=\"./deepneuro/pipelines/Skull_Stripping/resources/icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Segment_Brain_Mets\"\u003e\n\u003cimg src=\"./deepneuro/pipelines/Segment_Brain_Mets/resources/icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003ca href=\"https://github.com/QTIM-Lab/DeepNeuro/tree/master/deepneuro/pipelines/Ischemic_Stroke\"\u003e\n\u003cimg src=\"./deepneuro/pipelines/Ischemic_Stroke/resources/icon.png?raw=true\" width=\"684\" alt=\"\" style=\"max-width:100%;\"\u003e\n\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citation\" class=\"anchor\" href=\"#citation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCitation\u003c/h2\u003e\n\u003cp\u003eIf you use DeepNeuro in your published work, please cite:\u003c/p\u003e\n\u003cp\u003eBeers, A., Brown, J., Chang, K., Hoebel, K., Patel, J., Ly, K. Ina, Tolaney, S.M., Brastianos, P., Rosen, B., Gerstner, E., and Kalpathy-Cramer, J. (2020). \u003ca href=\"https://link.springer.com/article/10.1007/s12021-020-09477-5\" rel=\"nofollow\"\u003eDeepNeuro: an open-source deep learning toolbox for neuroimaging\u003c/a\u003e. Neuroinformatics. DOI: 10.1007/s12021-020-09477-5. PMID: 32578020\u003c/p\u003e\n\u003cp\u003eIf you use the MRI skull-stripping or glioblastoma segmentation modules, please cite:\u003c/p\u003e\n\u003cp\u003eChang, K., Beers, A.L., Bai, H.X., Brown, J.M., Ly, K.I., Li, X., Senders, J.T., Kavouridis, V.K., Boaro, A., Su, C., Bi, W.L., Rapalino, O., Liao, W., Shen, Q., Zhou, H., Xiao, B., Wang, Y., Zhang, P.J., Pinho, M.C., Wen, P.Y., Batchelor, T.T., Boxerman, J.L., Arnaout, O., Rosen, B.R., Gerstner, E.R., Yang, L., Huang, R.Y., and Kalpathy-Cramer, J., 2019. \u003ca href=\"https://academic.oup.com/neuro-oncology/advance-article/doi/10.1093/neuonc/noz106/5514498?searchresult=1\" rel=\"nofollow\"\u003eAutomatic assessment of glioma burden: A deep learning algorithm for fully automated volumetric and bi-dimensional measurement\u003c/a\u003e. Neuro-Oncology. DOI: 10.1093/neuonc/noz106. PMID: 31190077\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contact\" class=\"anchor\" href=\"#contact\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContact\u003c/h2\u003e\n\u003cp\u003eDeepNeuro is under active development, and you may run into errors or want additional features. Send any questions or requests for methods to \u003ca href=\"mailto:qtimlab@gmail.com\"\u003eqtimlab@gmail.com\u003c/a\u003e. You can also submit a Github issue if you run into a bug.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-acknowledgements\" class=\"anchor\" href=\"#acknowledgements\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAcknowledgements\u003c/h2\u003e\n\u003cp\u003eThe Center for Clinical Data Science at Massachusetts General Hospital and the Brigham and Woman\u0027s Hospital provided technical and hardware support for the development of DeepNeuro, including access to graphics processing units. The DeepNeuro project is also indebted to the following \u003ca href=\"https://github.com/ellisdg/3DUnetCNN\"\u003eGithub repository\u003c/a\u003e for the 3D UNet by user ellisdg, which formed the original kernel for much of its code in early stages. Long live open source deep learning :)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-disclaimer\" class=\"anchor\" href=\"#disclaimer\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDisclaimer\u003c/h2\u003e\n\u003cp\u003eThis software package and the deep learning models within are intended for research purposes only and have not yet been validated for clinical use.\u003c/p\u003e\n",
    "stargazers_count": 104,
    "subscribers_count": 15,
    "topics": [],
    "updated_at": 1622068798.0
  },
  {
    "data_format": 2,
    "description": "pySCENIC is a lightning-fast python implementation of the SCENIC pipeline (Single-Cell rEgulatory Network Inference and Clustering) which enables biologists to infer transcription factors, gene regulatory networks and cell types from single-cell RNA-seq data.",
    "filenames": [
      "Singularity.0.9.18"
    ],
    "full_name": "aertslab/pySCENIC",
    "latest_release": "0.11.2",
    "readme": "\u003cp\u003e\u003ca href=\"https://singularity-hub.org/collections/702\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-start\" class=\"anchor\" href=\"#quick-start\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick start\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003e# Download a (versioned) container\nsingularity pull shub://MPIB/singularity-fsl:6.0.4\n\n# Run it\nsingularity exec singularity-fsl_6.0.4.sif fslmaths\nsingularity exec --nv singularity-fsl_6.0.4.sif eddy_cuda9.1\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-fsl\" class=\"anchor\" href=\"#fsl\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFSL\u003c/h2\u003e\n\u003cp\u003eProject Home: \u003ca href=\"https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/\" rel=\"nofollow\"\u003ehttps://fsl.fmrib.ox.ac.uk/fsl/fslwiki/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThese are containers primarily used at the MPI for Human Development.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-cuda\" class=\"anchor\" href=\"#cuda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCuda\u003c/h2\u003e\n\u003cp\u003eStarting with Singularity 6.0.2 we include Nvidia CUDA through Debian backports repositories.\nMake sure your Nvidia driver on the host \u003ca href=\"https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility\" rel=\"nofollow\"\u003esupports it\u003c/a\u003e and add the \u003ccode\u003e--nv\u003c/code\u003e flag with singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-note\" class=\"anchor\" href=\"#note\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eNote\u003c/h2\u003e\n\u003cp\u003ePlease be aware of FSL\u0027s strict license regarding non-commercial use.\u003c/p\u003e\n",
    "stargazers_count": 157,
    "subscribers_count": 13,
    "topics": [
      "single-cell",
      "transcriptomics",
      "gene-regulatory-network",
      "transcription-factors"
    ],
    "updated_at": 1626741013.0
  },
  {
    "data_format": 2,
    "description": "Eukaryotic Genome Annotation Pipeline",
    "filenames": [
      "Singularity"
    ],
    "full_name": "nextgenusfs/funannotate",
    "latest_release": "v1.8.7",
    "readme": "\u003cp\u003e\u003ca href=\"https://github.com/nextgenusfs/funannotate/releases/latest\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/77ba51f3f201259675cc5dd53ada0a361559748add45bf62437f21ab6ba102a6/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6e65787467656e757366732f66756e616e6e6f746174652e737667\" alt=\"Latest Github release\" data-canonical-src=\"https://img.shields.io/github/release/nextgenusfs/funannotate.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://zenodo.org/badge/latestdoi/48254740\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/60682e17fa8054e0d7609e1eccfc1226867f83f55d9f19a2d9ddf6aadb09ba70/68747470733a2f2f7a656e6f646f2e6f72672f62616467652f34383235343734302e737667\" alt=\"DOI\" data-canonical-src=\"https://zenodo.org/badge/48254740.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/6dfb6879b08d70d126676bad7d9ba75f1600e35f7ffbe85772b833121a2d0897/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f62696f636f6e64612f66756e616e6e6f74617465\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6dfb6879b08d70d126676bad7d9ba75f1600e35f7ffbe85772b833121a2d0897/68747470733a2f2f696d672e736869656c64732e696f2f636f6e64612f646e2f62696f636f6e64612f66756e616e6e6f74617465\" alt=\"Conda\" data-canonical-src=\"https://img.shields.io/conda/dn/bioconda/funannotate\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/025770f53027c0679a57f4dcce8795b4fdc998bc94e79ea907db5b3e44cc321c/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6e65787467656e757366732f66756e616e6e6f746174652f6c6174657374\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/025770f53027c0679a57f4dcce8795b4fdc998bc94e79ea907db5b3e44cc321c/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f696d6167652d73697a652f6e65787467656e757366732f66756e616e6e6f746174652f6c6174657374\" alt=\"Docker Image Size (tag)\" data-canonical-src=\"https://img.shields.io/docker/image-size/nextgenusfs/funannotate/latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/528f0577834915a1e28ec7423f521cf14aac5496340487bf67cf4abacf31aad3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6e65787467656e757366732f66756e616e6e6f74617465\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/528f0577834915a1e28ec7423f521cf14aac5496340487bf67cf4abacf31aad3/68747470733a2f2f696d672e736869656c64732e696f2f646f636b65722f70756c6c732f6e65787467656e757366732f66756e616e6e6f74617465\" alt=\"Docker Pulls\" data-canonical-src=\"https://img.shields.io/docker/pulls/nextgenusfs/funannotate\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://singularity-hub.org/collections/5068\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a07b23d4880320ac89cdc93bbbb603fa84c215d135e05dd227ba8633a9ff34be/68747470733a2f2f7777772e73696e67756c61726974792d6875622e6f72672f7374617469632f696d672f686f737465642d73696e67756c61726974792d2d6875622d2532336533323932392e737667\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" data-canonical-src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"funannotate-logo.png?raw=true\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"funannotate-logo.png?raw=true\" alt=\"Alt text\" title=\"Funannotate\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003efunannotate is a pipeline for genome annotation (built specifically for fungi, but will also work with higher eukaryotes). Installation, usage, and more information can be found at \u003ca href=\"http://funannotate.readthedocs.io\" rel=\"nofollow\"\u003ehttp://funannotate.readthedocs.io\u003c/a\u003e\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-quickest-start-docker\" class=\"anchor\" href=\"#quickest-start-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickest start Docker:\u003c/h4\u003e\n\u003cp\u003eYou can use docker to run \u003ccode\u003efunannotate\u003c/code\u003e. Caveats are that GeneMark is not included in the docker image (see licensing below and you can complain to the developers for making it difficult to distribute/use). I\u0027ve also written a bash script that can run the docker image and auto-detect/include the proper user/volume bindings.  This docker image is built off of the latest code in master, so it will be ahead of the tagged releases. The image includes the required databases as well, if you want just funannotate without the databases then that is located on docker hub as well \u003ccode\u003enextgenusfs/funannotate-slim\u003c/code\u003e. So this route can be achieved with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# download/pull the image from docker hub\n$ docker pull nextgenusfs/funannotate\n\n# download bash wrapper script (optional)\n$ wget -O funannotate-docker https://raw.githubusercontent.com/nextgenusfs/funannotate/master/funannotate-docker\n\n# might need to make this executable on your system\n$ chmod +x /path/to/funannotate-docker\n\n# assuming it is in your PATH, now you can run this script as if it were the funannotate executable script\n$ funannotate-docker test -t predict --cpus 12\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-quickstart-bioconda-install\" class=\"anchor\" href=\"#quickstart-bioconda-install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuickstart Bioconda install:\u003c/h4\u003e\n\u003cp\u003eThe pipeline can be installed with conda (via \u003ca href=\"https://bioconda.github.io/\" rel=\"nofollow\"\u003ebioconda\u003c/a\u003e):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#add appropriate channels\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\n\n#then create environment\nconda create -n funannotate funannotate\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf \u003ccode\u003econda\u003c/code\u003e is taking forever to solve the environment, I would recommend giving \u003ca href=\"https://github.com/mamba-org/mamba\"\u003emamba\u003c/a\u003e a try:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e#install mamba into base environment\nconda install -n base mamba\n\n#then use mamba as drop in replacmeent\nmamba create -n funannotate funannotate\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you want to use GeneMark-ES/ET you will need to install that manually following developers instructions:\n\u003ca href=\"http://topaz.gatech.edu/GeneMark/license_download.cgi\" rel=\"nofollow\"\u003ehttp://topaz.gatech.edu/GeneMark/license_download.cgi\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eNote that you will need to change the shebang line for all perl scripts in GeneMark to use \u003ccode\u003e/usr/bin/env perl\u003c/code\u003e.\nYou will then also need to add \u003ccode\u003egmes_petap.pl\u003c/code\u003e to the $PATH or set the environmental variable $GENEMARK_PATH to the gmes_petap directory.\u003c/p\u003e\n\u003cp\u003eTo install just the python funannotate package, you can do this with pip:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m pip install funannotate\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eTo install the most updated code in master you can run:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003epython -m pip install git+https://github.com/nextgenusfs/funannotate.git\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 165,
    "subscribers_count": 13,
    "topics": [
      "genome-annotation",
      "gene-models",
      "comparative-genomics",
      "ncbi-submission"
    ],
    "updated_at": 1626963864.0
  },
  {
    "data_format": 2,
    "description": "MRtrix3 provides a set of tools to perform various advanced diffusion MRI analyses, including constrained spherical deconvolution (CSD), probabilistic tractography, track-density imaging, and apparent fibre density",
    "filenames": [
      "Singularity"
    ],
    "full_name": "MRtrix3/mrtrix3",
    "latest_release": "3.0.3",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-mrtrix\" class=\"anchor\" href=\"#mrtrix\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMRtrix\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/MRtrix3/mrtrix3/actions\"\u003e\u003cimg src=\"https://github.com/MRtrix3/mrtrix3/workflows/checks/badge.svg\" alt=\"Build Status\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eMRtrix3\u003c/em\u003e can be installed / run through multiple avenues:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://www.mrtrix.org/download/\" rel=\"nofollow\"\u003eDirect download\u003c/a\u003e through mechanisms tailored for different OS platforms;\u003c/li\u003e\n\u003cli\u003eCompiled from the source code in this repository, for which \u003ca href=\"https://mrtrix.readthedocs.io/en/latest/installation/build_from_source.html\" rel=\"nofollow\"\u003ecomprehensive instructions\u003c/a\u003e are provided in the \u003ca href=\"https://mrtrix.readthedocs.io/en/\" rel=\"nofollow\"\u003eonline documentation\u003c/a\u003e;\u003c/li\u003e\n\u003cli\u003eVia containerisation technology using Docker or Singularity; see \u003ca href=\"https://mrtrix.readthedocs.org/en/latest/installation/using_containers.html\" rel=\"nofollow\"\u003eonline documentation page\u003c/a\u003e for details.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-help\" class=\"anchor\" href=\"#getting-help\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting help\u003c/h2\u003e\n\u003cp\u003eInstructions on software setup and use are provided in the \u003ca href=\"https://mrtrix.readthedocs.org\" rel=\"nofollow\"\u003eonline documentation\u003c/a\u003e.\nSupport and general discussion is hosted on the \u003ca href=\"http://community.mrtrix.org/\" rel=\"nofollow\"\u003e\u003cem\u003eMRtrix3\u003c/em\u003e Community Forum\u003c/a\u003e.\nPlease also look through the Frequently Asked Questions on the \u003ca href=\"http://community.mrtrix.org/c/wiki\" rel=\"nofollow\"\u003ewiki section of the forum\u003c/a\u003e.\nYou can address all \u003cem\u003eMRtrix3\u003c/em\u003e-related queries there, using your GitHub or Google login to post questions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-quick-install\" class=\"anchor\" href=\"#quick-install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuick install\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eInstall dependencies by whichever means your system uses.\nThese include: Python (\u0026gt;=2.6), a C++ compiler with full C++11 support (\u003ccode\u003eg++\u003c/code\u003e 4.9 or later, \u003ccode\u003eclang++\u003c/code\u003e),\nEigen (\u0026gt;=3.2.8), zlib, OpenGL (\u0026gt;=3.3), and Qt (\u0026gt;=4.8, or at least 5.1 on MacOSX).\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eClone Git repository and compile:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ git clone https://github.com/MRtrix3/mrtrix3.git\n $ cd mrtrix3/\n $ ./configure\n $ ./build\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSet the \u003ccode\u003ePATH\u003c/code\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBash shell:\u003c/p\u003e\n\u003cp\u003erun the \u003ccode\u003eset_path\u003c/code\u003e script provided:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  $ ./set_path\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eor edit the startup \u003ccode\u003e~/.bashrc\u003c/code\u003e or \u003ccode\u003e/etc/bash.bashrc\u003c/code\u003e file manually by adding this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  $ export PATH=/\u0026lt;edit as appropriate\u0026gt;/mrtrix3/bin:$PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eC shell:\u003c/p\u003e\n\u003cp\u003eedit the startup \u003ccode\u003e~/.cshrc\u003c/code\u003e or \u003ccode\u003e/etc/csh.cshrc\u003c/code\u003e file manually by adding this line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  $ setenv PATH /\u0026lt;edit as appropriate\u0026gt;/mrtrix3/bin:$PATH\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTest installation:\u003c/p\u003e\n\u003cp\u003eCommand-line:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ mrconvert\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGUI:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e $ mrview\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-keeping-mrtrix3-up-to-date\" class=\"anchor\" href=\"#keeping-mrtrix3-up-to-date\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eKeeping MRtrix3 up to date\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eYou can update your installation at any time by opening a terminal in the mrtrix3 folder, and typing:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e git pull\n ./build\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eIf this doesn\u0027t work immediately, it may be that you need to re-run the configure script:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e ./configure\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eand re-run step 1 again.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building-a-specific-release-of-mrtrix3\" class=\"anchor\" href=\"#building-a-specific-release-of-mrtrix3\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding a specific release of MRtrix3\u003c/h2\u003e\n\u003cp\u003eYou can build a particular release of MRtrix3 by checking out the corresponding \u003cem\u003etag\u003c/em\u003e, and using the same procedure as above to build it:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit checkout 3.0_RC3\n./configure\n./build\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cp\u003eThank you for your interest in contributing to \u003cem\u003eMRtrix3\u003c/em\u003e! Please read on \u003ca href=\"CONTRIBUTING.md\"\u003ehere\u003c/a\u003e to find out how to report issues, request features and make direct contributions.\u003c/p\u003e\n",
    "stargazers_count": 186,
    "subscribers_count": 33,
    "topics": [],
    "updated_at": 1626693746.0
  },
  {
    "data_format": 2,
    "description": "Multiplayer, fast-paced Moba style game",
    "filenames": [
      "docker/Singularity",
      "docker/Singularity.wine"
    ],
    "full_name": "bbodi/rustarok",
    "latest_release": null,
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-table-of-contents\" class=\"anchor\" href=\"#table-of-contents\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTable of contents\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#building\"\u003eBuilding\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-on-windows\"\u003eRunning on Windows\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#running-with-docker\"\u003eRunning with Docker\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#how-to-play\"\u003eHow to play\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#design-decisions\"\u003eDesign decisions\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#blog\"\u003eBlog\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#current-status-and-gallery\"\u003eCurrent Status and Gallery\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#background-story\"\u003eBackground story\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#about-the-used-game-assets\"\u003eAbout the used game assets\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#thanks\"\u003eThanks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\n\u003ca id=\"user-content-rustarok\" class=\"anchor\" href=\"#rustarok\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRustarok\u003c/h1\u003e\n\u003cp\u003eA project whose primary goals are to have fun developing it and experiment with interesting technical problems from the world of game development.\u003c/p\u003e\n\u003cp\u003eIt is intended to be a multiplayer, fast-paced Moba style game. Check \u003ca href=\"#background-story\"\u003eBackground story\u003c/a\u003e for details.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-building\" class=\"anchor\" href=\"#building\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBuilding\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003egit clone https://github.com/bbodi/rustarok.git\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ecargo build\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-on-windows\" class=\"anchor\" href=\"#running-on-windows\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning on Windows\u003c/h2\u003e\n\u003cp\u003eYou will need Ragnarok Online asset files to run the game.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDownload a Ragnarok Online client from \u003ca href=\"https://talonro.com/\" rel=\"nofollow\"\u003esome\u003c/a\u003e \u003ca href=\"http://playdreamerro.com/\" rel=\"nofollow\"\u003epopular\u003c/a\u003e \u003ca href=\"https://topg.org/ragnarok-private-servers/\" rel=\"nofollow\"\u003eprivate server\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstall it\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eCheck the installation directory for any \u003ccode\u003e*.grf\u003c/code\u003e files, and put their paths into \u003ccode\u003econfig.toml\u003c/code\u003e, e.g.:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egrf_paths = [\n  \"d:\\\\Games\\\\TalonRO\\\\rdata.grf\",\n  \"d:\\\\Games\\\\TalonRO\\\\sdata.grf\",\n  \"d:\\\\Games\\\\TalonRO\\\\tdata.grf\"\n]\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRun \u003ccode\u003ecargo run\u003c/code\u003e from rustarok directory.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-running-with-docker\" class=\"anchor\" href=\"#running-with-docker\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRunning with Docker\u003c/h2\u003e\n\u003cp\u003eSee the README.md in the \u003ca href=\"docker\"\u003edocker\u003c/a\u003e folder for complete instructions.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-play\" class=\"anchor\" href=\"#how-to-play\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to play\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eMove your character with the right mouse button\u003c/li\u003e\n\u003cli\u003eCast skills with Q (fire wall), W (lightning), E (heal), R (huge boom) keys\u003c/li\u003e\n\u003cli\u003eSpawn entities with the \"Players\" and \"Monsters\" sliders in the window\u003c/li\u003e\n\u003cli\u003eMove the camera with the cursor keys\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-design-decisions\" class=\"anchor\" href=\"#design-decisions\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDesign decisions\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/bbodi/rustarok/issues/1\"\u003eStatuses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/bbodi/rustarok/issues/4\"\u003eRendering system\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-blog\" class=\"anchor\" href=\"#blog\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBlog\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/bbodi/rustarok/issues/6\"\u003e2019W30\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-current-status-and-gallery\" class=\"anchor\" href=\"#current-status-and-gallery\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCurrent Status and Gallery\u003c/h2\u003e\n\u003cp\u003eCurrently the project is in a very early stage. Nothing is set in stone yet, I mostly prototyping ideas and techniques.\u003c/p\u003e\n\u003cp\u003eList of developed features:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e[x] Asset file loading (grf, gnd, rsm, rsw, spr, act, str)\u003c/li\u003e\n\u003cli\u003e[x] Rendering\n\u003cul\u003e\n\u003cli\u003e[x] Map (ground, static models, lighting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[x] Sprites for UI\n\u003cul\u003e\n\u003cli\u003e[x] Sprites in 3D world (animated sprites and effects as well)\n\u003cul\u003e\n\u003cli\u003e[x] Different actions (idle, sit, walk, attack, cast etc)\u003c/li\u003e\n\u003cli\u003e[x] Directions\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[x] Amount of damages, heals, etc\n\u003cul\u003e\n\u003cli\u003e[x] Health and Mana bar above the characters\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[x] Input handling, Control\n\u003cul\u003e\n\u003cli\u003e[x] Moving around with your character\u003c/li\u003e\n\u003cli\u003e[x] Assigning skills to Q, W, E, R, etc keys\u003c/li\u003e\n\u003cli\u003e[x] Continuous movement towards the mouse if RMB is down\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[x] Skills\n\u003cul\u003e\n\u003cli\u003e[x] Skill target area/entity selection mode\u003c/li\u003e\n\u003cli\u003e[x] Skill casting\u003c/li\u003e\n\u003cli\u003e[x] Skill manifestations (the manifested outcome of using a skill, e.g. a fire wall in the 3D world which can\u0027t be walk through and it damages contacting entities)\u003c/li\u003e\n\u003cli\u003e[x] Quick cast settings (on, on-release, off)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[x] Battle\n\u003cul\u003e\n\u003cli\u003e[x] Attacking an enemy\u003c/li\u003e\n\u003cli\u003e[x] Attack speed\u003c/li\u003e\n\u003cli\u003e[x] Health, dying\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e[x] Collision\n\u003cul\u003e\n\u003cli\u003e[x] Static objects\u003c/li\u003e\n\u003cli\u003e[x] Characters, a.k.a \u003ca href=\"https://www.youtube.com/watch?v=nk2O6YsCWwI\" rel=\"nofollow\"\u003ebody block\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\n\u003ca href=\"readme_assets/body_blocking.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"300\" src=\"readme_assets/body_blocking.gif\" title=\"Body blocking\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"readme_assets/normal_aspd.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"300\" src=\"readme_assets/normal_aspd.gif\" title=\"Normal attack\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"readme_assets/quick_aspd.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"300\" src=\"readme_assets/quick_aspd.gif\" title=\"Quick attack\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"readme_assets/heal.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"300\" src=\"readme_assets/heal.gif\" title=\"Heal\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"readme_assets/aoe.gif\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg width=\"300\" src=\"readme_assets/aoe.gif\" title=\"AoE skill\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-background-story\" class=\"anchor\" href=\"#background-story\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eBackground story\u003c/h2\u003e\n\u003cp\u003eI play computer games rarely, but when I do, I play 1 or 2 sessions of Heroes of The Storm match.\u003c/p\u003e\n\u003cp\u003eBut still, whenever I play, I am constantly thinking about how I would implement some mechanics of the game.\u003c/p\u003e\n\u003cp\u003eSo finally I reached the point where fantasizing is not enough anymore, and wanted to actually try myself in this area as well.\u003c/p\u003e\n\u003cp\u003eDon\u0027t be surprised if the game is heavily inspired by HoTS, most probably the playable character styles and skills will be based on my favourite characters from it, or the ones whose skill mechanics I find challenging or interesting.\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-about-the-used-game-assets\" class=\"anchor\" href=\"#about-the-used-game-assets\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout the used game assets\u003c/h3\u003e\n\u003cp\u003eThe visuals of Rustarok might be familiar to you. It is because the game uses assets from an existing game, an older popular Korean MMORPG, Ragnarok Online. The reasons I used them:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eI am not a graphic designer, I don\u0027t have the skills nor the temptation to create the visuals of a game myself.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eAgain, my primary goal is to experiment, learn and have fun while \u003cstrong\u003edeveloping\u003c/strong\u003e something challenging.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eI am in love with the unique 2D/3D graphic style of the game.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRagnarok Online game asset file structures are known and there are example implementations for processing them.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eRagnarok Online had a huge impact on me when I was younger. I played a lot with it, this might have been the only game I was obsessed with.\u003c/p\u003e\n\u003cp\u003eThanks to it, I know all the skills, sprites, maps, models etc, which is useful when I try to come up with visualities of some new skill.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe server code of Ragnarok Online has been exposed for a very long time. That was the first professional C source code I studied, hacked and even fixed when I was around 14-15, so it had a huge impact on me as a software developer.\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-thanks\" class=\"anchor\" href=\"#thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eThanks\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"https://github.com/vthibault/roBrowser/\"\u003eroBrowser\u003c/a\u003e: Its source code was useful for decoding the game asset files\u003c/li\u003e\n\u003c/ul\u003e\n",
    "stargazers_count": 227,
    "subscribers_count": 13,
    "topics": [
      "rust",
      "opengl",
      "ragnarok",
      "moba",
      "game",
      "multiplayer",
      "2d",
      "3d"
    ],
    "updated_at": 1626620410.0
  },
  {
    "data_format": 2,
    "description": "HiC-Pro: An optimized and flexible pipeline for Hi-C data processing",
    "filenames": [
      "Singularity"
    ],
    "full_name": "nservant/HiC-Pro",
    "latest_release": "v3.0.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-hic-pro\" class=\"anchor\" href=\"#hic-pro\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHiC-Pro\u003c/h1\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-an-optimized-and-flexible-pipeline-for-hi-c-data-processing\" class=\"anchor\" href=\"#an-optimized-and-flexible-pipeline-for-hi-c-data-processing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAn optimized and flexible pipeline for Hi-C data processing\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.com/nservant/HiC-Pro\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/73a86528f082d212ef4ce88c8e2cf0c58768f77ffcca0a20820f724928dd04f1/68747470733a2f2f7472617669732d63692e636f6d2f6e73657276616e742f4869432d50726f2e7376673f6272616e63683d646576656c5f707933\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.com/nservant/HiC-Pro.svg?branch=devel_py3\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/fa5e0167fc052efefc929acb2c1c877388fffa8a486163becac51ff00b7ccadc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e64612d6275696c642d627269676874677265656e2e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fa5e0167fc052efefc929acb2c1c877388fffa8a486163becac51ff00b7ccadc/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e64612d6275696c642d627269676874677265656e2e737667\" alt=\"Conda\" data-canonical-src=\"https://img.shields.io/badge/Conda-build-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://camo.githubusercontent.com/a0a5d2fc20aeae0c34909c3cb3c096a3f2c0592c8e47dc0c0e9c6bb1f0014110/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696e67756c61726974792d6275696c642d627269676874677265656e2e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a0a5d2fc20aeae0c34909c3cb3c096a3f2c0592c8e47dc0c0e9c6bb1f0014110/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f53696e67756c61726974792d6275696c642d627269676874677265656e2e737667\" alt=\"Singularity\" data-canonical-src=\"https://img.shields.io/badge/Singularity-build-brightgreen.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://hub.docker.com/repository/docker/nservant/hicpro\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/a946c73292a5c2f469ea28505b0533d98efa33c02af4137ce82a41ad0a9d96c3/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f446f636b65722d6d616e75616c2d79656c6c6f772e737667\" alt=\"Docker\" data-canonical-src=\"https://img.shields.io/badge/Docker-manual-yellow.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://camo.githubusercontent.com/5224ef8b916118d25eaf98ae2a4b258283c163a2354a008c7eed94dcba0635e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d756c746951432d312e382d626c75652e737667\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5224ef8b916118d25eaf98ae2a4b258283c163a2354a008c7eed94dcba0635e0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4d756c746951432d312e382d626c75652e737667\" alt=\"MultiQC\" data-canonical-src=\"https://img.shields.io/badge/MultiQC-1.8-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://groups.google.com/forum/#!forum/hic-pro\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/1b9d743d52863763181e70393d3998508fea708ce52d4e2c6cb4c9d8d43f3d50/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f47726f7570732d2532306a6f696e253230636861742532302545322538362539322d3466623939612e7376673f7374796c653d666c61742d737175617265\" alt=\"Forum\" data-canonical-src=\"https://img.shields.io/badge/Groups-%20join%20chat%20%E2%86%92-4fb99a.svg?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://doi.org/10.1186/s13059-015-0831-x\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/27e7387e5cf05b69efa15a349127425e83ef45fd07d906c440f0b14d68df2d4e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f444f492d31302e313138362532467331333035392d2d3031352d2d303833312d2d782d6c69676874677265792e7376673f7374796c653d666c61742d737175617265\" alt=\"DOI\" data-canonical-src=\"https://img.shields.io/badge/DOI-10.1186%2Fs13059--015--0831--x-lightgrey.svg?style=flat-square\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003eFind documentation and examples at \u003ca href=\"http://nservant.github.io/HiC-Pro/\" rel=\"nofollow\"\u003ehttp://nservant.github.io/HiC-Pro/\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eFor any question about HiC-Pro, please contact \u003ca href=\"mailto:nicolas.servant@curie.fr\"\u003enicolas.servant@curie.fr\u003c/a\u003e or use the \u003ca href=\"https://groups.google.com/forum/#!forum/hic-pro\" rel=\"nofollow\"\u003eHiC-Pro forum\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-what-is-hic-pro-\" class=\"anchor\" href=\"#what-is-hic-pro-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWhat is HiC-Pro ?\u003c/h2\u003e\n\u003cp\u003eHiC-Pro was designed to process Hi-C data, from raw fastq files (paired-end Illumina data) to normalized contact maps. It supports the main Hi-C protocols, including digestion protocols as well as protocols that do not require restriction enzymes such as DNase Hi-C. In practice, HiC-Pro was successfully applied to many data-sets including dilution Hi-C, in situ Hi-C, DNase Hi-C, Micro-C, capture-C, capture Hi-C or HiChip data.\u003cbr\u003e\nThe pipeline is flexible, scalable and optimized. It can operate either on a single laptop or on a computational cluster. HiC-Pro is sequential and each step of the workflow can be run independantly.\u003cbr\u003e\nHiC-Pro includes a fast implementatation of the iterative correction method (see the \u003ca href=\"https://github.com/hiclib/iced\"\u003eiced python package\u003c/a\u003e for more information).\nFinally, HiC-Pro can use phasing data to build \u003ca href=\"doc/AS.md\"\u003eallele-specific contact maps\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you use HiC-Pro, please cite :\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eServant N., Varoquaux N., Lajoie BR., Viara E., Chen CJ., Vert JP., Dekker J., Heard E., Barillot E.\u003c/em\u003e HiC-Pro: An optimized and flexible pipeline for Hi-C processing. Genome Biology 2015, 16:259 \u003ca href=\"https://doi.org/10.1186/s13059-015-0831-x\" rel=\"nofollow\"\u003edoi:10.1186/s13059-015-0831-x\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-containers\" class=\"anchor\" href=\"#containers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContainers\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-hic-pro-through-conda\" class=\"anchor\" href=\"#using-hic-pro-through-conda\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing HiC-Pro through \u003ccode\u003econda\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eIn order to ease the installation of HiC-Pro dependancies, we provide a \u003ccode\u003eyml\u003c/code\u003e file for conda with all required tools.\nIn order to build your conda environment, first install \u003ca href=\"https://docs.conda.io/en/latest/miniconda.html\" rel=\"nofollow\"\u003eminiconda\u003c/a\u003e and use :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econda env create -f MY_INSTALL_PATH/HiC-Pro/environment.yml -p WHERE_TO_INSTALL_MY_ENV\nconda activate WHERE_TO_INSTALL_MY_ENV\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-the-hic-pro-docker-image\" class=\"anchor\" href=\"#using-the-hic-pro-docker-image\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing the HiC-Pro \u003ccode\u003eDocker\u003c/code\u003e image\u003c/h3\u003e\n\u003cp\u003eA docker image is automatically build and available on \u003ca href=\"https://hub.docker.com/repository/docker/nservant/hicpro\" rel=\"nofollow\"\u003eDocker Hub\u003c/a\u003e\nTo pull a Docker image, simply use :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker pull nservant/hicpro:latest\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that the \u003ccode\u003etag\u003c/code\u003e may depend on the HiC-Pro version.\u003c/p\u003e\n\u003cp\u003eYou can also build your own image from the root folder using\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003edocker build -t hicpro:3.0.0 .\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-using-hic-pro-through-singularity\" class=\"anchor\" href=\"#using-hic-pro-through-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsing HiC-Pro through \u003ccode\u003eSingularity\u003c/code\u003e\n\u003c/h3\u003e\n\u003cp\u003eHiC-Pro provides a Singularity container to ease its installation process.\nA ready-to-use container is available \u003ca href=\"https://zerkalo.curie.fr/partage/HiC-Pro/singularity_images/hicpro_latest_ubuntu.img\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn order to build you own Singularity image;\u003c/p\u003e\n\u003cp\u003e1- Install singularity\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLinux : \u003ca href=\"http://singularity.lbl.gov/install-linux\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/install-linux\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMAC : \u003ca href=\"http://singularity.lbl.gov/install-mac\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/install-mac\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eWindows : \u003ca href=\"http://singularity.lbl.gov/install-windows\" rel=\"nofollow\"\u003ehttp://singularity.lbl.gov/install-windows\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e2- Build the singularity HiC-Pro image using the \u0027Singularity\u0027 file available in the HiC-Pro root directory.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo singularity build hicpro_latest_ubuntu.img MY_INSTALL_PATH/HiC-Pro/envs/Singularity\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e3- Run HiC-pro\u003c/p\u003e\n\u003cp\u003eYou can then either use HiC-Pro using the \u0027exec\u0027 command ;\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity exec hicpro_latest_ubuntu.img HiC-Pro -h\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr directly use HiC-Pro within the Singularity shell\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esingularity shell hicpro_latest_ubuntu.img\nHiC-Pro -h\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-install-it-\" class=\"anchor\" href=\"#how-to-install-it-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to install it ?\u003c/h2\u003e\n\u003cp\u003eThe HiC-Pro pipeline requires the following dependencies :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ca href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\" rel=\"nofollow\"\u003ebowtie2\u003c/a\u003e mapper\u003c/li\u003e\n\u003cli\u003ePython (\u0026gt;3.7) with \u003cem\u003epysam (\u0026gt;=0.15.4)\u003c/em\u003e, \u003cem\u003ebx-python(\u0026gt;=0.8.8)\u003c/em\u003e, \u003cem\u003enumpy(\u0026gt;=1.18.1)\u003c/em\u003e, and \u003cem\u003escipy(\u0026gt;=1.4.1)\u003c/em\u003e libraries.\u003cbr\u003e\n\u003cstrong\u003eNote that the current version no longer supports python 2\u003c/strong\u003e\n\u003c/li\u003e\n\u003cli\u003eR with the \u003cem\u003eRColorBrewer\u003c/em\u003e and \u003cem\u003eggplot2 (\u0026gt;2.2.1)\u003c/em\u003e packages\u003c/li\u003e\n\u003cli\u003eg++ compiler\u003c/li\u003e\n\u003cli\u003esamtools (\u0026gt;1.9)\u003c/li\u003e\n\u003cli\u003eUnix sort (\u003cstrong\u003ewhich support -V option\u003c/strong\u003e) is required ! For Mac OS user, please install the GNU core utilities !\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eNote that Bowtie \u0026gt;2.2.2 is strongly recommanded for allele specific analysis.\u003c/p\u003e\n\u003cp\u003eTo install HiC-Pro, be sure to have the appropriate rights and run :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003etar -zxvf HiC-Pro-master.tar.gz\ncd HiC-Pro-master\n## Edit config-install.txt file if necessary\nmake configure\nmake install\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNote that if some of these dependencies are not installed (i.e. not detected in the $PATH), HiC-Pro will try to install them.\u003cbr\u003e\nYou can also edit the \u003cem\u003econfig-install.txt\u003c/em\u003e file and manually defined the paths to dependencies.\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003eSYSTEM CONFIGURATION\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003ePREFIX\u003c/td\u003e\n\u003ctd\u003ePath to installation folder\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBOWTIE2_PATH\u003c/td\u003e\n\u003ctd\u003eFull path the bowtie2 installation directory\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSAMTOOLS_PATH\u003c/td\u003e\n\u003ctd\u003eFull path to the samtools installation directory\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eR_PATH\u003c/td\u003e\n\u003ctd\u003eFull path to the R installation directory\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003ePYTHON_PATH\u003c/td\u003e\n\u003ctd\u003eFull path to the python installation directory\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eCLUSTER_SYS\u003c/td\u003e\n\u003ctd\u003eScheduler to use for cluster submission. Must be TORQUE, SGE, SLURM or LSF\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-annotation-files\" class=\"anchor\" href=\"#annotation-files\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAnnotation Files\u003c/h2\u003e\n\u003cp\u003eIn order to process the raw data, HiC-Pro requires three annotation files. Note that the pipeline is provided with some Human and Mouse annotation files.\u003cbr\u003e\n\u003cstrong\u003ePlease be sure that the chromosome names are the same than the ones used in your bowtie indexes !\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eA BED file\u003c/strong\u003e of the restriction fragments after digestion. This file depends both of the restriction enzyme and the reference genome. See the \u003ca href=\"doc/FAQ.md\"\u003eFAQ\u003c/a\u003e and the \u003ca href=\"doc/UTILS.md\"\u003eHiC-Pro utilities\u003c/a\u003e for details about how to generate this file. A few annotation files are provided with the HiC-Pro sources as examples.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e   chr1   0       16007   HIC_chr1_1    0   +\n   chr1   16007   24571   HIC_chr1_2    0   +\n   chr1   24571   27981   HIC_chr1_3    0   +\n   chr1   27981   30429   HIC_chr1_4    0   +\n   chr1   30429   32153   HIC_chr1_5    0   +\n   chr1   32153   32774   HIC_chr1_6    0   +\n   chr1   32774   37752   HIC_chr1_7    0   +\n   chr1   37752   38369   HIC_chr1_8    0   +\n   chr1   38369   38791   HIC_chr1_9    0   +\n   chr1   38791   39255   HIC_chr1_10   0   +\n   (...)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eA table file\u003c/strong\u003e of chromosomes\u0027 size. This file can be easily find on the UCSC genome browser. Of note, pay attention to the contigs or scaffolds, and be aware that HiC-pro will generate a map per chromosomes pair. For model organisms such as Human or Mouse, which are well annotated, we usually recommand to remove all scaffolds.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e   chr1    249250621\n   chr2    243199373\n   chr3    198022430\n   chr4    191154276\n   chr5    180915260\n   chr6    171115067\n   chr7    159138663\n   chr8    146364022\n   chr9    141213431\n   chr10   135534747\n   (...)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cstrong\u003eThe bowtie2 indexes\u003c/strong\u003e. See the \u003ca href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\" rel=\"nofollow\"\u003ebowtie2 manual page\u003c/a\u003e for details about how to create such indexes.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-how-to-use-it-\" class=\"anchor\" href=\"#how-to-use-it-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to use it ?\u003c/h2\u003e\n\u003cp\u003eFirst have a look at the help message !\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  HiC-Pro --help\n  usage : HiC-Pro -i INPUT -o OUTPUT -c CONFIG [-s ANALYSIS_STEP] [-p] [-h] [-v]\n  Use option -h|--help for more information\n\n  HiC-Pro 3.0.0\n  ---------------\n  OPTIONS\n\n   -i|--input INPUT : input data folder; Must contains a folder per sample with input files\n   -o|--output OUTPUT : output folder\n   -c|--conf CONFIG : configuration file for Hi-C processing\n   [-p|--parallel] : if specified run HiC-Pro on a cluster\n   [-s|--step ANALYSIS_STEP] : run only a subset of the HiC-Pro workflow; if not specified the complete workflow is run\n      mapping: perform reads alignment - require fast files\n      proc_hic: perform Hi-C filtering - require BAM files\n      quality_checks: run Hi-C quality control plots\n      merge_persample: merge multiple inputs and remove duplicates if specified - require .validPairs files\n      build_contact_maps: Build raw inter/intrachromosomal contact maps - require .allValidPairs files\n      ice_norm : run ICE normalization on contact maps - require .matrix files\n   [-h|--help]: help\n   [-v|--version]: version\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eCopy and edit the configuration file \u003cem\u003e\u0027config-hicpro.txt\u0027\u003c/em\u003e in your local folder. See the \u003ca href=\"doc/MANUAL.md\"\u003emanual\u003c/a\u003e for details about the configuration file\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003ePut all input files in a rawdata folder. The input files have to be organized with \u003cstrong\u003eone folder per sample\u003c/strong\u003e, such as;\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e   + PATH_TO_MY_DATA\n     + sample1\n       ++ file1_R1.fastq.gz\n       ++ file1_R2.fastq.gz\n       ++ ...\n     + sample2\n       ++ file1_R1.fastq.gz\n       ++ file1_R2.fastq.gz\n     *...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRun HiC-Pro on your laptop in standalone model\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e    MY_INSTALL_PATH/bin/HiC-Pro -i FULL_PATH_TO_DATA_FOLDER -o FULL_PATH_TO_OUTPUTS -c MY_LOCAL_CONFIG_FILE\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003eRun HiC-Pro on a cluster (TORQUE/SGE/SLURM/LSF)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre\u003e\u003ccode\u003e   MY_INSTALL_PATH/bin/HiC-Pro -i FULL_PATH_TO_DATA_FOLDER -o FULL_PATH_TO_OUTPUTS -c MY_LOCAL_CONFIG_FILE -p\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn the latter case, you will have the following message :\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  Please run HiC-Pro in two steps :\n  1- The following command will launch the parallel workflow through 12 torque jobs:\n  qsub HiCPro_step1.sh\n  2- The second command will merge all outputs to generate the contact maps:\n  qsub HiCPro_step2.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eExecute the displayed command from the output folder:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  qsub HiCPro_step1.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOnce executed succesfully (may take several hours), run the step using:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e  qsub HiCPro_step2.sh\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-test-dataset\" class=\"anchor\" href=\"#test-dataset\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eTest Dataset\u003c/h2\u003e\n\u003cp\u003eThe test dataset and associated results are available \u003ca href=\"https://zerkalo.curie.fr/partage/HiC-Pro/\" rel=\"nofollow\"\u003ehere\u003c/a\u003e.\nSmall fastq files (2M reads) extracted from the Dixon et al. 2012 paper are available for test.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e ## Get the data. Will download a test_data folder and a configuration file\n wget https://zerkalo.curie.fr/partage/HiC-Pro/HiCPro_testdata.tar.gz \u0026amp;\u0026amp; tar -zxvf HiCPro_testdata.tar.gz\n\n ## Edit the configuration file and set the path to Human bowtie2 indexes\n\n ## Run HiC-Pro\n time HICPRO_INSTALL_DIR/bin/HiC-Pro -c config_test_latest.txt -i test_data -o hicpro_latest_test\n\nRun HiC-Pro 3.0.0\n--------------------------------------------\nThu Mar 19, 12:18:10 (UTC+0100)\nBowtie2 alignment step1 ...\nLogs: logs/dixon_2M_2/mapping_step1.log\nLogs: logs/dixon_2M/mapping_step1.log\n\n--------------------------------------------\nThu Mar 19, 12:18:57 (UTC+0100)\nBowtie2 alignment step2 ...\nLogs: logs/dixon_2M_2/mapping_step2.log\nLogs: logs/dixon_2M/mapping_step2.log\n\n--------------------------------------------\nThu Mar 19, 12:19:08 (UTC+0100)\nCombine R1/R2 alignment files ...\nLogs: logs/dixon_2M_2/mapping_combine.log\nLogs: logs/dixon_2M/mapping_combine.log\n\n--------------------------------------------\nThu Mar 19, 12:19:13 (UTC+0100)\nMapping statistics for R1 and R2 tags ...\nLogs: logs/dixon_2M_2/mapping_stats.log\nLogs: logs/dixon_2M/mapping_stats.log\n\n--------------------------------------------\nThu Mar 19, 12:19:15 (UTC+0100)\nPairing of R1 and R2 tags ...\nLogs: logs/dixon_2M_2/mergeSAM.log\nLogs: logs/dixon_2M/mergeSAM.log\n\n--------------------------------------------\nThu Mar 19, 12:19:25 (UTC+0100)\nAssign alignments to restriction fragments ...\nLogs: logs/dixon_2M_2/mapped_2hic_fragments.log\nLogs: logs/dixon_2M/mapped_2hic_fragments.log\n\n--------------------------------------------\nThu Mar 19, 12:20:10 (UTC+0100)\nMerge chunks from the same sample ...\nLogs: logs/dixon_2M/merge_valid_interactions.log\nLogs: logs/dixon_2M_2/merge_valid_interactions.log\n\n--------------------------------------------\nThu Mar 19, 12:20:11 (UTC+0100)\nMerge stat files per sample ...\nLogs: logs/dixon_2M/merge_stats.log\nLogs: logs/dixon_2M_2/merge_stats.log\n\n--------------------------------------------\nThu Mar 19, 12:20:11 (UTC+0100)\nRun quality checks for all samples ...\nLogs: logs/dixon_2M/make_Rplots.log\nLogs: logs/dixon_2M_2/make_Rplots.log\n\n--------------------------------------------\nThu Mar 19, 12:20:22 (UTC+0100)\nGenerate binned matrix files ...\nLogs: logs/dixon_2M/build_raw_maps.log\nLogs: logs/dixon_2M_2/build_raw_maps.log\n\n--------------------------------------------\nThu Mar 19, 12:20:22 (UTC+0100)\nRun ICE Normalization ...\nLogs: logs/dixon_2M/ice_500000.log\nLogs: logs/dixon_2M/ice_1000000.log\nLogs: logs/dixon_2M_2/ice_500000.log\nLogs: logs/dixon_2M_2/ice_1000000.log\n\nreal\t2m15,736s\nuser\t4m3,277s\nsys\t0m24,423s\n\n\u003c/code\u003e\u003c/pre\u003e\n",
    "stargazers_count": 247,
    "subscribers_count": 20,
    "topics": [],
    "updated_at": 1626145689.0
  },
  {
    "data_format": 2,
    "description": "Code generation framework for automated finite difference computation",
    "filenames": [
      "docker/Singularity.nvidia.def"
    ],
    "full_name": "devitocodes/devito",
    "latest_release": "v4.5",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-devito-fast-stencil-computation-from-symbolic-specification\" class=\"anchor\" href=\"#devito-fast-stencil-computation-from-symbolic-specification\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevito: Fast Stencil Computation from Symbolic Specification\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/devitocodes/devito/actions?query=workflow%3ACI-core\"\u003e\u003cimg src=\"https://github.com/devitocodes/devito/workflows/CI-core/badge.svg\" alt=\"Build Status for the Core backend\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/devitocodes/devito/actions?query=workflow%3ACI-mpi\"\u003e\u003cimg src=\"https://github.com/devitocodes/devito/workflows/CI-mpi/badge.svg\" alt=\"Build Status with MPI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/devitocodes/devito/actions?query=workflow%3ACI-gpu\"\u003e\u003cimg src=\"https://github.com/devitocodes/devito/workflows/CI-gpu/badge.svg\" alt=\"Build Status on GPU\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://codecov.io/gh/devitocodes/devito\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3371fe5bdd570d040c748fb93a3e18ce00797c85315f2d05364781a1e5b9aa53/68747470733a2f2f636f6465636f762e696f2f67682f64657669746f636f6465732f64657669746f2f6272616e63682f6d61737465722f67726170682f62616467652e737667\" alt=\"Code Coverage\" data-canonical-src=\"https://codecov.io/gh/devitocodes/devito/branch/master/graph/badge.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://join.slack.com/t/devitocodes/shared_invite/zt-gtd2yxj9-Y31YKk_7lr9AwfXeL2iMFg\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/f0d0a8f3b06c0808c75575af15a74159d9d34f2bc02997c0f262dd916e0bf948/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f636861742d6f6e253230736c61636b2d253233333643354630\" alt=\"Slack Status\" data-canonical-src=\"https://img.shields.io/badge/chat-on%20slack-%2336C5F0\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://devitocodes.github.io/devito-performance\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/3015b96f702ce7bd0e41f18aa7d7dfb69af77789127d64634a2223f829dbcee1/687474703a2f2f696d672e736869656c64732e696f2f62616467652f62656e63686d61726b656425323062792d6173762d626c75652e7376673f7374796c653d666c6174\" alt=\"asv\" data-canonical-src=\"http://img.shields.io/badge/benchmarked%20by-asv-blue.svg?style=flat\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://badge.fury.io/py/devito\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/249b65986b967e7268f743fa8e3face99c98762feaa8d1417d07769b1d3385bf/68747470733a2f2f62616467652e667572792e696f2f70792f64657669746f2e737667\" alt=\"PyPI version\" data-canonical-src=\"https://badge.fury.io/py/devito.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://mybinder.org/v2/gh/devitocodes/devito/master\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/581c077bdbc6ca6899c86d0acc6145ae85e9d80e6f805a1071793dbe48917982/68747470733a2f2f6d7962696e6465722e6f72672f62616467655f6c6f676f2e737667\" alt=\"Binder\" data-canonical-src=\"https://mybinder.org/badge_logo.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.devitoproject.org\" rel=\"nofollow\"\u003eDevito\u003c/a\u003e is a Python package to implement\noptimized stencil computation (e.g., finite differences, image processing,\nmachine learning) from high-level symbolic problem definitions.  Devito builds\non \u003ca href=\"http://www.sympy.org/en/index.html\" rel=\"nofollow\"\u003eSymPy\u003c/a\u003e and employs automated code\ngeneration and just-in-time compilation to execute optimized computational\nkernels on several computer platforms, including CPUs, GPUs, and clusters\nthereof.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#about-devito\"\u003eAbout Devito\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#installation\"\u003eInstallation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#resources\"\u003eResources\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#performance\"\u003ePerformance\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#get-in-touch\"\u003eGet in touch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#interactive-jupyter-notebooks\"\u003eInteractive jupyter notebooks\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-about-devito\" class=\"anchor\" href=\"#about-devito\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAbout Devito\u003c/h2\u003e\n\u003cp\u003eDevito provides a functional language to implement sophisticated operators that\ncan be made up of multiple stencil computations, boundary conditions, sparse\noperations (e.g., interpolation), and much more.  A typical use case is\nexplicit finite difference methods for approximating partial differential\nequations. For example, a 2D diffusion operator may be implemented with Devito\nas follows\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003egrid\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eGrid\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eshape\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e(\u003cspan class=\"pl-c1\"\u003e10\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e10\u003c/span\u003e))\n\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eTimeFunction\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ename\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s\"\u003e\u0027f\u0027\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003egrid\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003egrid\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003espace_order\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e2\u003c/span\u003e)\n\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eeqn\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eEq\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003edt\u003c/span\u003e, \u003cspan class=\"pl-c1\"\u003e0.5\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e*\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003elaplace\u003c/span\u003e)\n\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-s1\"\u003eop\u003c/span\u003e \u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e \u003cspan class=\"pl-v\"\u003eOperator\u003c/span\u003e(\u003cspan class=\"pl-v\"\u003eEq\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eforward\u003c/span\u003e, \u003cspan class=\"pl-en\"\u003esolve\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003eeqn\u003c/span\u003e, \u003cspan class=\"pl-s1\"\u003ef\u003c/span\u003e.\u003cspan class=\"pl-s1\"\u003eforward\u003c/span\u003e)))\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eAn \u003ccode\u003eOperator\u003c/code\u003e generates low-level code from an ordered collection of \u003ccode\u003eEq\u003c/code\u003e (the\nexample above being for a single equation). This code may also be compiled and\nexecuted\u003c/p\u003e\n\u003cdiv class=\"highlight highlight-source-python\"\u003e\u003cpre\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"pl-en\"\u003eop\u003c/span\u003e(\u003cspan class=\"pl-s1\"\u003et\u003c/span\u003e\u003cspan class=\"pl-c1\"\u003e=\u003c/span\u003e\u003cspan class=\"pl-s1\"\u003etimesteps\u003c/span\u003e)\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eThere is virtually no limit to the complexity of an \u003ccode\u003eOperator\u003c/code\u003e -- the Devito\ncompiler will automatically analyze the input, detect and apply optimizations\n(including single- and multi-node parallelism), and eventually generate code\nwith suitable loops and expressions.\u003c/p\u003e\n\u003cp\u003eKey features include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA functional language to express finite difference operators.\u003c/li\u003e\n\u003cli\u003eStraightforward mechanisms to adjust the discretization.\u003c/li\u003e\n\u003cli\u003eConstructs to express sparse operators (e.g., interpolation), classic linear\noperators (e.g., convolutions), and tensor contractions.\u003c/li\u003e\n\u003cli\u003eSeamless support for boundary conditions and adjoint operators.\u003c/li\u003e\n\u003cli\u003eA flexible API to define custom stencils, sub-domains, sub-sampling,\nand staggered grids.\u003c/li\u003e\n\u003cli\u003eGeneration of highly optimized parallel code (SIMD vectorization, CPU and GPU\nparallelism via OpenMP, multi-node parallelism via MPI, blocking, aggressive\nsymbolic transformations for FLOP reduction, etc.).\u003c/li\u003e\n\u003cli\u003eDistributed NumPy arrays over multi-node (MPI) domain decompositions.\u003c/li\u003e\n\u003cli\u003eInspection and customization of the generated code.\u003c/li\u003e\n\u003cli\u003eAutotuning framework to ease performance tuning.\u003c/li\u003e\n\u003cli\u003eSmooth integration with popular Python packages such as NumPy, SymPy, Dask,\nand SciPy, as well as machine learning frameworks such as TensorFlow and\nPyTorch.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003cp\u003eThe easiest way to try Devito is through Docker using the following commands:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e# get the code\ngit clone https://github.com/devitocodes/devito.git\ncd devito\n\n# start a jupyter notebook server on port 8888\ndocker-compose up devito\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAfter running the last command above, the terminal will display a URL such as\n\u003ccode\u003ehttps://127.0.0.1:8888/?token=XXX\u003c/code\u003e. Copy-paste this URL into a browser window\nto start a \u003ca href=\"https://jupyter.org/\" rel=\"nofollow\"\u003eJupyter\u003c/a\u003e notebook session where you can go\nthrough the \u003ca href=\"https://github.com/devitocodes/devito/tree/master/examples\"\u003etutorials\u003c/a\u003e\nprovided with Devito or create your own notebooks.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://devitocodes.github.io/devito/download.html\" rel=\"nofollow\"\u003eSee here\u003c/a\u003e for detailed installation\ninstructions and other options. If you encounter a problem during installation, please\nsee the\n\u003ca href=\"https://github.com/devitocodes/devito/wiki/Installation-Issues\"\u003einstallation issues\u003c/a\u003e we\nhave seen in the past.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-resources\" class=\"anchor\" href=\"#resources\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eResources\u003c/h2\u003e\n\u003cp\u003eTo learn how to use Devito,\n\u003ca href=\"https://github.com/devitocodes/devito/blob/master/examples\"\u003ehere\u003c/a\u003e is a good\nplace to start, with lots of examples and tutorials.\u003c/p\u003e\n\u003cp\u003eThe \u003ca href=\"https://www.devitoproject.org/\" rel=\"nofollow\"\u003ewebsite\u003c/a\u003e also provides access to other\ninformation, including documentation and instructions for citing us.\u003c/p\u003e\n\u003cp\u003eSome FAQ are discussed \u003ca href=\"https://github.com/devitocodes/devito/wiki/FAQ\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-performance\" class=\"anchor\" href=\"#performance\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePerformance\u003c/h2\u003e\n\u003cp\u003eIf you are interested in any of the following\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGeneration of parallel code (CPU, GPU, multi-node via MPI);\u003c/li\u003e\n\u003cli\u003ePerformance tuning;\u003c/li\u003e\n\u003cli\u003eBenchmarking operators;\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ethen you should take a look at this\n\u003ca href=\"https://github.com/devitocodes/devito/blob/master/benchmarks/user\"\u003eREADME\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou may also be interested in\n\u003ca href=\"https://github.com/devitocodes/thematrix\"\u003eTheMatrix\u003c/a\u003e -- a cross-architecture\nbenchmarking framework showing the performance of several production-grade\nseismic operators implemented with Devito. This is now our flagship project\ntowards neat, open, and reproducible science.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-get-in-touch\" class=\"anchor\" href=\"#get-in-touch\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGet in touch\u003c/h2\u003e\n\u003cp\u003eIf you\u0027re using Devito, we would like to hear from you. Whether you\nare facing issues or just trying it out, join the\n\u003ca href=\"https://join.slack.com/t/devitocodes/shared_invite/zt-gtd2yxj9-Y31YKk_7lr9AwfXeL2iMFg\" rel=\"nofollow\"\u003econversation\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-interactive-jupyter-notebooks\" class=\"anchor\" href=\"#interactive-jupyter-notebooks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInteractive jupyter notebooks\u003c/h2\u003e\n\u003cp\u003eThe tutorial jupyter notebook are available interactively at the public \u003ca href=\"https://mybinder.org/v2/gh/devitocodes/devito/master\" rel=\"nofollow\"\u003ebinder\u003c/a\u003e jupyterhub.\u003c/p\u003e\n",
    "stargazers_count": 311,
    "subscribers_count": 35,
    "topics": [
      "compilers",
      "stencil-codes",
      "finite-difference",
      "sympy",
      "jit",
      "performance",
      "fwi",
      "rtm"
    ],
    "updated_at": 1627313377.0
  },
  {
    "data_format": 2,
    "description": "Particle-in-Cell Simulations for the Exascale Era :sparkles:",
    "filenames": [
      "share/picongpu/dockerfiles/ubuntu-1604/Singularity"
    ],
    "full_name": "ComputationalRadiationPhysics/picongpu",
    "latest_release": "0.5.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-picongpu---particle-in-cell-simulations-for-the-exascale-era\" class=\"anchor\" href=\"#picongpu---particle-in-cell-simulations-for-the-exascale-era\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003ePIConGPU - Particle-in-Cell Simulations for the Exascale Era\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://travis-ci.org/ComputationalRadiationPhysics/picongpu/branches\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8001013459189259d918b4f4e2993dbc331e8a13903199c8cc3a1cf561247c40/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6d61737465722e7376673f6c6162656c3d6d6173746572\" alt=\"Code Status master\" data-canonical-src=\"https://img.shields.io/travis/ComputationalRadiationPhysics/picongpu/master.svg?label=master\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://travis-ci.org/ComputationalRadiationPhysics/picongpu/branches\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/372e2d54a4f7af960e27fc9a084fc93f5cf2b9accfe4e983febe644963c03f1a/68747470733a2f2f696d672e736869656c64732e696f2f7472617669732f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6465762e7376673f6c6162656c3d646576\" alt=\"Code Status dev\" data-canonical-src=\"https://img.shields.io/travis/ComputationalRadiationPhysics/picongpu/dev.svg?label=dev\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://picongpu.readthedocs.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/dd84c49cf1a8e134ca1a5a87517257a4317eecc4a80ff90195b2d2eebeeb7ced/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f7069636f6e6770752f62616467652f3f76657273696f6e3d6c6174657374\" alt=\"Documentation Status\" data-canonical-src=\"https://readthedocs.org/projects/picongpu/badge/?version=latest\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://computationalradiationphysics.github.io/picongpu\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bea8b749e6bc63f677e6ccfc18ae8a6a4a4e39d55e3aac6c872acc8d1ecdc22b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f4150492d446f787967656e2d626c75652e737667\" alt=\"Doxygen\" data-canonical-src=\"https://img.shields.io/badge/API-Doxygen-blue.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu/compare/master...dev\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b36d60b89d6eb05b7977d0522b2c74389a9182e2a00879fa4a09361bfd3a4912/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f636f6d6d6974732d73696e63652f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f6c61746573742f6465762e737667\" alt=\"GitHub commits since last release\" data-canonical-src=\"https://img.shields.io/github/commits-since/ComputationalRadiationPhysics/picongpu/latest/dev.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://isocpp.org/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b5671d1cd5cb649d92fdacc5e3ddc75dc6f6ea22e81efce926664d09ecf04ff/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c616e67756167652d4325324225324231312d6f72616e67652e737667\" alt=\"Language\" data-canonical-src=\"https://img.shields.io/badge/language-C%2B%2B11-orange.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.gnu.org/licenses/gpl-3.0.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d9724a7d6c0aab25ad100b9c974369e2ea7585e6d0d0b87b66aa5bd34f6c2abe/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c76332d626c75652e7376673f6c6162656c3d5049436f6e475055\" alt=\"License PIConGPU\" data-canonical-src=\"https://img.shields.io/badge/license-GPLv3-blue.svg?label=PIConGPU\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.gnu.org/licenses/lgpl-3.0.html\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/d8685de1336e9f80f82625f4271fbd11d00beaf91013768daef9ac1a62e6fe2b/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d4c47504c76332d626c75652e7376673f6c6162656c3d504d616363\" alt=\"License PMacc\" data-canonical-src=\"https://img.shields.io/badge/license-LGPLv3-blue.svg?label=PMacc\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.youtube.com/watch?v=nwZuG-XtUDE\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/be7eb258510f59135b17c487a2bd50e76f186e1c71e56ab87f5cd1afe4df35d3/687474703a2f2f696d672e796f75747562652e636f6d2f76692f6e775a75472d58745544452f302e6a7067\" alt=\"PIConGPU Presentation Video\" data-canonical-src=\"http://img.youtube.com/vi/nwZuG-XtUDE/0.jpg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"http://www.youtube.com/watch?v=nwZuG-XtUDE\" rel=\"nofollow\"\u003e\u003cimg src=\"docs/logo/pic_logo_vert_158x360.png\" alt=\"PIConGPU Release\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003ePIConGPU is a fully relativistic,\n\u003ca href=\"https://en.wikipedia.org/wiki/Manycore_processor\" rel=\"nofollow\"\u003emanycore\u003c/a\u003e,\n3D3V particle-in-cell (\u003ca href=\"http://en.wikipedia.org/wiki/Particle-in-cell\" rel=\"nofollow\"\u003ePIC\u003c/a\u003e)\ncode. The Particle-in-Cell algorithm is a central tool in plasma physics.\nIt describes the dynamics of a plasma by computing the motion of\nelectrons and ions in the plasma based on\n\u003ca href=\"http://en.wikipedia.org/wiki/Maxwell%27s_equations\" rel=\"nofollow\"\u003eMaxwell\u0027s equations\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePIConGPU implements various numerical schemes to solve the PIC cycle.\nIts features for the electro-magnetic PIC algorithm include:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ea central or Yee-lattice for fields\u003c/li\u003e\n\u003cli\u003eparticle pushers that solve the equation of motion for charged and neutral\nparticles, e.g., the \u003cem\u003eBoris-\u003c/em\u003e and the\n\u003ca href=\"http://dx.doi.org/10.1063/1.2837054\" rel=\"nofollow\"\u003e\u003cem\u003eVay-Pusher\u003c/em\u003e\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eMaxwell field solvers, e.g.\n\u003ca href=\"http://dx.doi.org/10.1109/TAP.1966.1138693\" rel=\"nofollow\"\u003e\u003cem\u003eYee\u0027s\u003c/em\u003e\u003c/a\u003e and\n\u003ca href=\"http://dx.doi.org/10.1103/PhysRevSTAB.16.021301\" rel=\"nofollow\"\u003e\u003cem\u003eLehe\u0027s\u003c/em\u003e\u003c/a\u003e scheme\u003c/li\u003e\n\u003cli\u003erigorously charge conserving current deposition schemes, such as\n\u003ca href=\"http://dx.doi.org/10.1016/0010-4655%2892%2990169-Y\" rel=\"nofollow\"\u003e\u003cem\u003eVillasenor-Buneman\u003c/em\u003e\u003c/a\u003e,\n\u003ca href=\"http://dx.doi.org/10.1016/S0010-4655%2800%2900228-9\" rel=\"nofollow\"\u003e\u003cem\u003eEsirkepov\u003c/em\u003e\u003c/a\u003e\nand \u003cem\u003eZigZag\u003c/em\u003e\n\u003c/li\u003e\n\u003cli\u003emacro-particle form factors ranging from NGP (0th order), CIC (1st),\nTSC (2nd), PSQ (3rd) to P4S (4th)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eand the electro-magnetic PIC algorithm is further self-consistently coupled to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eclassical radiation reaction\n(\u003ca href=\"http://dx.doi.org/10.1016/j.cpc.2016.04.002\" rel=\"nofollow\"\u003eDOI:10.1016/j.cpc.2016.04.002\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eQED synchrotron radiation (photon emission)\n(\u003ca href=\"http://dx.doi.org/10.1103/PhysRevE.92.023305\" rel=\"nofollow\"\u003eDOI:10.1103/PhysRevE.92.023305\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eadvanced field ionization methods\n(\u003ca href=\"http://dx.doi.org/10.1103/PhysRevA.59.569\" rel=\"nofollow\"\u003eDOI:10.1103/PhysRevA.59.569\u003c/a\u003e,\n\u003ca href=\"http://www.jetp.ac.ru/cgi-bin/dn/e_020_05_1307.pdf\" rel=\"nofollow\"\u003eLV Keldysh\u003c/a\u003e, BSI)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBesides the electro-magnetic PIC algorithm and extensions to it, we developed\na wide range of tools and diagnostics, e.g.:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eonline, far-field radiation diagnostics for coherent and incoherent radiation\nemitted by charged particles\u003c/li\u003e\n\u003cli\u003efull restart and output capabilities via \u003ca href=\"http://openPMD.org\" rel=\"nofollow\"\u003eopenPMD\u003c/a\u003e,\nincluding \u003ca href=\"http://hdfgroup.org/\" rel=\"nofollow\"\u003eparallel HDF5\u003c/a\u003e (via\n\u003ca href=\"https://github.com/ComputationalRadiationPhysics/libSplash\"\u003elibSplash\u003c/a\u003e) and\n\u003ca href=\"https://csmd.ornl.gov/adios/\" rel=\"nofollow\"\u003eADIOS\u003c/a\u003e, allowing for\nextreme I/O scalability and massively parallel online-analysis\u003c/li\u003e\n\u003cli\u003e2D and 3D live view and diagnostics tools\u003c/li\u003e\n\u003cli\u003ea large selection of extensible\n\u003ca href=\"http://picongpu.readthedocs.io/en/latest/usage/plugins.html\" rel=\"nofollow\"\u003eonline-plugins\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eAs one of our supported compute platforms, GPUs provide a computational\nperformance of several\n\u003ca href=\"http://en.wikipedia.org/wiki/FLOPS\" rel=\"nofollow\"\u003eTFLOP/s\u003c/a\u003e at considerable lower invest and\nmaintenance costs compared to multi CPU-based compute architectures of similar\nperformance. The latest high-performance systems\n(\u003ca href=\"http://www.top500.org/\" rel=\"nofollow\"\u003eTOP500\u003c/a\u003e) are enhanced by accelerator hardware that\nboost their peak performance up to the multi-PFLOP/s level. With its\noutstanding performance and scalability to more than 18\u0027000 GPUs,\nPIConGPU was one of the \u003cstrong\u003efinalists\u003c/strong\u003e of the 2013\n\u003ca href=\"http://sc13.supercomputing.org/content/acm-gordon-bell-prize\" rel=\"nofollow\"\u003eGordon Bell Prize\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003ePIConGPU is developed and maintained by the\n\u003ca href=\"https://www.hzdr.de/db/Cms?pNid=2097\" rel=\"nofollow\"\u003eComputational Radiation Physics Group\u003c/a\u003e\nat the \u003ca href=\"http://www.hzdr.de/db/Cms?pNid=132\" rel=\"nofollow\"\u003eInstitute for Radiation Physics\u003c/a\u003e\nat \u003ca href=\"http://www.hzdr.de/\" rel=\"nofollow\"\u003eHZDR\u003c/a\u003e in close collaboration with the Center\nfor Information Services and High Performance Computing\n(\u003ca href=\"http://tu-dresden.de/die_tu_dresden/zentrale_einrichtungen/zih\" rel=\"nofollow\"\u003eZIH\u003c/a\u003e) of the\nTechnical University Dresden (\u003ca href=\"http://www.tu-dresden.de\" rel=\"nofollow\"\u003eTUD\u003c/a\u003e). We are a\nmember of the \u003ca href=\"http://ccoe-dresden.de/\" rel=\"nofollow\"\u003eDresden GPU Center of Excellence\u003c/a\u003e that\ncooperates on a broad range of scientific GPU and manycore applications,\nworkshops and teaching efforts.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-attribution\" class=\"anchor\" href=\"#attribution\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAttribution\u003c/h2\u003e\n\u003cp\u003ePIConGPU is a \u003cem\u003escientific project\u003c/em\u003e. If you \u003cstrong\u003epresent and/or publish\u003c/strong\u003e scientific\nresults that used PIConGPU, you should set a \u003cstrong\u003ereference\u003c/strong\u003e to show your support.\u003c/p\u003e\n\u003cp\u003eOur according \u003cstrong\u003eup-to-date publication\u003c/strong\u003e at \u003cstrong\u003ethe time of your publication\u003c/strong\u003e\nshould be inquired from:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://raw.githubusercontent.com/ComputationalRadiationPhysics/picongpu/master/REFERENCE.md\" rel=\"nofollow\"\u003eREFERENCE.md\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePlease also consider adding yourself to our \u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu-communitymap\"\u003ecommunity map\u003c/a\u003e.\nWe would love to hear from you!\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-oral-presentations\" class=\"anchor\" href=\"#oral-presentations\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eOral Presentations\u003c/h2\u003e\n\u003cp\u003eThe following slide should be part of \u003cstrong\u003eoral presentations\u003c/strong\u003e. It is intended to\nacknowledge the team maintaining PIConGPU and to support our community:\u003c/p\u003e\n\u003cp\u003e(\u003cem\u003ecoming soon\u003c/em\u003e) presentation_picongpu.pdf\n(svg version, key note version, png version: 1920x1080 and 1024x768)\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-software-license\" class=\"anchor\" href=\"#software-license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware License\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003ePIConGPU\u003c/em\u003e is licensed under the \u003cstrong\u003eGPLv3+\u003c/strong\u003e. Furthermore, you can develop your\nown particle-mesh algorithms based on our general library \u003cem\u003ePMacc\u003c/em\u003e that is\nshipped alongside PIConGPU. \u003cem\u003ePMacc\u003c/em\u003e is \u003cem\u003edual licensed\u003c/em\u003e under both the\n\u003cstrong\u003eGPLv3+ and LGPLv3+\u003c/strong\u003e.\nFor a detailed description, please refer to \u003ca href=\"LICENSE.md\"\u003eLICENSE.md\u003c/a\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-install\" class=\"anchor\" href=\"#install\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstall\u003c/h2\u003e\n\u003cp\u003eSee our notes in \u003ca href=\"INSTALL.rst\"\u003eINSTALL.rst\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-users\" class=\"anchor\" href=\"#users\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUsers\u003c/h2\u003e\n\u003cp\u003eDear User, please be aware that this is an \u003cstrong\u003eopen beta release\u003c/strong\u003e!\nWe hereby emphasize that we are still actively developing PIConGPU at great\nspeed and do, from time to time, break backwards compatibility.\u003c/p\u003e\n\u003cp\u003eWhen using this software, please stick to the \u003ccode\u003emaster\u003c/code\u003e branch containing the\nlatest \u003cem\u003estable\u003c/em\u003e release. It also contains a file \u003ccode\u003eCHANGELOG.md\u003c/code\u003e with the\nlatest changes (and how to update your simulations). Read it first before\nupdating between two versions! Also, we add a git \u003ccode\u003etag\u003c/code\u003e according to a version\nnumber for each release in \u003ccode\u003emaster\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eFor any questions regarding the usage of PIConGPU please \u003cstrong\u003edo not\u003c/strong\u003e contact the\ndevelopers and maintainers directly.\u003c/p\u003e\n\u003cp\u003eInstead, please sign up to our \u003cstrong\u003ePIConGPU-Users\u003c/strong\u003e mailing list so we can\ndistribute and archive user questions:\n\u003ca href=\"https://cg.hzdr.de/Lists/picongpu-users/List.html\" rel=\"nofollow\"\u003eSubscribe (select \"Feed\" on bottom left)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eBefore you post a question, browse the PIConGPU\n\u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu/search?l=markdown\"\u003edocumentation\u003c/a\u003e,\n\u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu/wiki\"\u003ewiki\u003c/a\u003e,\n\u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu/issues\"\u003eissue tracker\u003c/a\u003e and the\n\u003ca href=\"https://cg.hzdr.de/Lists/picongpu-users/List.html\" rel=\"nofollow\"\u003emailing list history\u003c/a\u003e\nto see if your question has been answered, already.\u003c/p\u003e\n\u003cp\u003ePIConGPU is a collaborative project.\nWe thus encourage users to engage in answering questions of other users and post solutions to problems to the list.\nA problem you have encountered might be the future problem of another user.\u003c/p\u003e\n\u003cp\u003eIn addition, please consider using the collaborative features of GitHub if you have questions or comments on code or documentation.\nThis will allow other users to see the piece of code or documentation you are referring to.\u003c/p\u003e\n\u003cp\u003eMain ressources are in our \u003ca href=\"https://picongpu.readthedocs.io\" rel=\"nofollow\"\u003eonline manual\u003c/a\u003e, the \u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu/wiki\"\u003euser section\u003c/a\u003e of our wiki, documentation files in \u003ca href=\"http://commonmark.org/help/\" rel=\"nofollow\"\u003e\u003ccode\u003e.md\u003c/code\u003e (Markdown)\u003c/a\u003e and \u003ca href=\"http://www.sphinx-doc.org/en/stable/rest.html\" rel=\"nofollow\"\u003e\u003ccode\u003e.rst\u003c/code\u003e (reStructuredText)\u003c/a\u003e format in this repository and a \u003ca href=\"http://www.youtube.com/watch?v=7ybsD8G4Rsk\" rel=\"nofollow\"\u003egetting started video\u003c/a\u003e.\nFeel free to visit \u003ca href=\"http://picongpu.hzdr.de\" rel=\"nofollow\"\u003epicongpu.hzdr.de\u003c/a\u003e to learn more about the PIC algorithm.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-software-upgrades\" class=\"anchor\" href=\"#software-upgrades\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSoftware Upgrades\u003c/h2\u003e\n\u003cp\u003ePIConGPU follows a\n\u003ca href=\"http://nvie.com/posts/a-successful-git-branching-model/\" rel=\"nofollow\"\u003emaster - dev\u003c/a\u003e\ndevelopment model. That means our latest stable release is shipped in a branch\ncalled \u003ccode\u003emaster\u003c/code\u003e while new and frequent changes to the code are incooporated\nin the development branch \u003ccode\u003edev\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eEvery time we update the \u003cem\u003emaster\u003c/em\u003e branch, we publish a new release\nof PIConGPU. Before you pull the changes in, please read our\n\u003ca href=\"CHANGELOG.md\"\u003eChangeLog\u003c/a\u003e!\nYou may have to update some of your simulation \u003ccode\u003e.param\u003c/code\u003e and \u003ccode\u003e.cfg\u003c/code\u003e files by\nhand since PIConGPU is an active project and new features often require changes\nin input files. Additionally, a full description of new features and fixed bugs\nin comparison to the previous release is provided in that file.\u003c/p\u003e\n\u003cp\u003eIn case you decide to use \u003cem\u003enew, potentially buggy and experimental\u003c/em\u003e features\nfrom our \u003ccode\u003edev\u003c/code\u003e branch, be aware that support is very limited and you must\nparticipate or at least follow the development yourself. Syntax changes\nand in-development bugs will \u003cem\u003enot\u003c/em\u003e be announced outside of their according pull\nrequests and issues.\u003c/p\u003e\n\u003cp\u003eBefore drafting a new release, we open a new \u003ccode\u003erelease-*\u003c/code\u003e branch from \u003ccode\u003edev\u003c/code\u003e with\nthe \u003ccode\u003e*\u003c/code\u003e being the version number of the upcoming release. This branch only\nreceives bug fixes (feature freeze) and users are welcome to try it out\n(however, the change log and a detailed announcement might still be missing in\nit).\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-developers\" class=\"anchor\" href=\"#developers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDevelopers\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-how-to-participate\" class=\"anchor\" href=\"#how-to-participate\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eHow to participate\u003c/h3\u003e\n\u003cp\u003eSee \u003ca href=\"CONTRIBUTING.md\"\u003eCONTRIBUTING.md\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eIf you like to jump in right away, see\u003cbr\u003e\n\u003ca href=\"https://github.com/ComputationalRadiationPhysics/picongpu/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/32b730b309ff90c1713e8ae39a73ae2145c4cceb0b7e72bacef523db9fc85d62/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6973737565732d7261772f436f6d7075746174696f6e616c526164696174696f6e506879736963732f7069636f6e6770752f676f6f64253230666972737425323069737375652e7376673f636f6c6f723d353663626566\" alt=\u0027open \"good first issue\" issues\u0027 data-canonical-src=\"https://img.shields.io/github/issues-raw/ComputationalRadiationPhysics/picongpu/good%20first%20issue.svg?color=56cbef\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-active-team\" class=\"anchor\" href=\"#active-team\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eActive Team\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-scientific-supervision\" class=\"anchor\" href=\"#scientific-supervision\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eScientific Supervision\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDr. Michael Bussmann\u003c/li\u003e\n\u003cli\u003eDr. Axel Huebl\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-maintainers-and-core-developers\" class=\"anchor\" href=\"#maintainers-and-core-developers\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eMaintainers* and core developers\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eDr. Sergei Bastrakov*\u003c/li\u003e\n\u003cli\u003eDr. Alexander Debus\u003c/li\u003e\n\u003cli\u003eMarco Garten*\u003c/li\u003e\n\u003cli\u003eDr. Axel Huebl*\u003c/li\u003e\n\u003cli\u003eAlexander Matthes\u003c/li\u003e\n\u003cli\u003eDr. Richard Pausch*\u003c/li\u003e\n\u003cli\u003eSophie Rudat\u003c/li\u003e\n\u003cli\u003eSebastian Starke\u003c/li\u003e\n\u003cli\u003eDr. Klaus Steiniger\u003c/li\u003e\n\u003cli\u003eRene Widera*\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-former-members-contributions-and-thanks\" class=\"anchor\" href=\"#former-members-contributions-and-thanks\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFormer Members, Contributions and Thanks\u003c/h3\u003e\n\u003cp\u003eThe PIConGPU Team expresses its gratitude to:\u003c/p\u003e\n\u003cp\u003eFlorian Berninger, Heiko Burau, Robert Dietrich, Carlchristian Eckert,\nWen Fu, Ph.D., Alexander Grund, Sebastian Hahn, Anton Helm, Wolfgang Hoehnig,\nDr.-Ing. Guido Juckeland, Jeffrey Kelling, Maximilian Knespel, Dr. Remi Lehe,\nFelix Schmitt, Benjamin Schneider, Joseph Schuchart, Conrad Schumann,\nStefan Tietze, Marija Vranic, Ph.D., Benjamin Worpitz, and Erik Zenker.\u003c/p\u003e\n\u003cp\u003eKudos to everyone, mentioned or unmentioned, who contributed further in any\nway!\u003c/p\u003e\n\u003chr\u003e\n\u003cp\u003e\u003ca href=\"docs/images/lwfa_iso.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/lwfa_iso.png\" alt=\"image of an lwfa\" title=\"LWFA\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"docs/images/StrongScalingPIConGPU_log.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"docs/images/StrongScalingPIConGPU_log.png\" alt=\"image of our strong scaling\" title=\"Strong Scaling\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n",
    "stargazers_count": 479,
    "subscribers_count": 47,
    "topics": [
      "laser",
      "plasma",
      "physics",
      "gpu",
      "physics-simulation",
      "gpu-computing",
      "particle-accelerator",
      "particle-in-cell",
      "pic",
      "research"
    ],
    "updated_at": 1626806049.0
  },
  {
    "data_format": 2,
    "description": "OpenHPC Integration, Packaging, and Test Repo",
    "filenames": [
      "containers/Singularity.1.3.3.el7"
    ],
    "full_name": "openhpc/ohpc",
    "latest_release": "v2.3.GA",
    "readme": "\u003ch3\u003e\n\u003ca id=\"\" class=\"anchor\" href=\"#\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003e\u003ca href=\"https://github.com/openhpc/ohpc/blob/master/docs/recipes/install/common/figures/ohpc_logo.png\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/openhpc/ohpc/raw/master/docs/recipes/install/common/figures/ohpc_logo.png\" width=\"170\" valign=\"middle\" hspace=\"5\" alt=\"OpenHPC\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/h3\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-community-building-blocks-for-hpc-systems\" class=\"anchor\" href=\"#community-building-blocks-for-hpc-systems\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCommunity building blocks for HPC systems\u003c/h3\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-introduction\" class=\"anchor\" href=\"#introduction\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eIntroduction\u003c/h4\u003e\n\u003cp\u003eThis stack provides a variety of common, pre-built ingredients required to\ndeploy and manage an HPC Linux cluster including provisioning tools, resource\nmanagement, I/O clients, runtimes, development tools, containers, and a variety of\nscientific libraries.\u003c/p\u003e\n\u003cp\u003eThere are currently two release series:\n\u003ca href=\"https://github.com/openhpc/ohpc/wiki/1.3.X\"\u003e1.3.x\u003c/a\u003e and\n\u003ca href=\"https://github.com/openhpc/ohpc/wiki/2.x\"\u003e2.x\u003c/a\u003e, which target different major\nLinux OS distributions. The 1.3.x series targets CentOS7 and SLES12 while the\n2.x series targets CentOS8 and Leap15.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-getting-started\" class=\"anchor\" href=\"#getting-started\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting started\u003c/h4\u003e\n\u003cp\u003eOpenHPC provides pre-built binaries via repositories for use with standard\nLinux package manager tools (e.g. \u003ccode\u003eyum\u003c/code\u003e or \u003ccode\u003ezypper\u003c/code\u003e). To get started,\nyou can enable an OpenHPC repository locally through installation of an\n\u003ccode\u003eohpc-release\u003c/code\u003e RPM which includes gpg keys for package signing and defines\nthe URL locations for [base] and [update] package repositories. Installation\nguides tailored for each supported provisioning system and resource manager\nwith detailed example instructions for installaing a cluster are also available.\nCopies of the \u003ccode\u003eohpc-release\u003c/code\u003e package and installation guides along with\nmore information is available on the relevant release series pages\n(\u003ca href=\"https://github.com/openhpc/ohpc/wiki/1.3.X\"\u003e1.3.x\u003c/a\u003e or\n\u003ca href=\"https://github.com/openhpc/ohpc/wiki/2.x\"\u003e2.x\u003c/a\u003e).\u003c/p\u003e\n\u003chr\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-questions-comments-or-bug-reports\" class=\"anchor\" href=\"#questions-comments-or-bug-reports\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eQuestions, Comments, or Bug Reports?\u003c/h4\u003e\n\u003cp\u003eSubscribe to the users email list at \u003ca href=\"https://groups.io/g/openhpc-users\" rel=\"nofollow\"\u003ehttps://groups.io/g/openhpc-users\u003c/a\u003e or see\nthe \u003ca href=\"http://openhpc.community\" rel=\"nofollow\"\u003ehttp://openhpc.community\u003c/a\u003e page for more pointers.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-additional-software-requests\" class=\"anchor\" href=\"#additional-software-requests\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eAdditional Software Requests?\u003c/h4\u003e\n\u003cp\u003ePlease see the component submission page at\n\u003ca href=\"https://github.com/openhpc/submissions\"\u003ehttps://github.com/openhpc/submissions\u003c/a\u003e for more information regarding new\nsoftware inclusion requests.\u003c/p\u003e\n\u003ch4\u003e\n\u003ca id=\"user-content-register-your-system\" class=\"anchor\" href=\"#register-your-system\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRegister your system\u003c/h4\u003e\n\u003cp\u003eIf you are using elements of OpenHPC, please consider registering your\nsystem(s) using the \u003ca href=\"https://drive.google.com/open?id=1KvFM5DONJigVhOlmDpafNTDDRNTYVdolaYYzfrHkOWI\" rel=\"nofollow\"\u003eSystem Registration\nForm\u003c/a\u003e.\u003c/p\u003e\n",
    "stargazers_count": 573,
    "subscribers_count": 92,
    "topics": [
      "hpc",
      "scientific-computing",
      "package-repository",
      "clusters-management",
      "devtools",
      "mpi",
      "linuxfoundation"
    ],
    "updated_at": 1626694488.0
  },
  {
    "data_format": 2,
    "description": "Collaborate \u0026 label any type of data, images, text, or documents, in an easy web interface or desktop app.",
    "filenames": [
      "Singularity"
    ],
    "full_name": "UniversalDataTool/universal-data-tool",
    "latest_release": "v0.14.26",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-universal-data-tool\" class=\"anchor\" href=\"#universal-data-tool\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eUniversal Data Tool\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/2eea6e9da40ca1a782274a068b67f3ab1f1208a4a59ccbf4a14b476c0f087a38/68747470733a2f2f62616467652e667572792e696f2f67682f556e6976657273616c44617461546f6f6c253246756e6976657273616c2d646174612d746f6f6c2e737667\" alt=\"GitHub version\" data-canonical-src=\"https://badge.fury.io/gh/UniversalDataTool%2Funiversal-data-tool.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\" target=\"_blank\" rel=\"noopener noreferrer\"\u003e\u003cimg src=\"https://github.com/UniversalDataTool/universal-data-tool/workflows/Test/badge.svg\" alt=\"Master Branch\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://badge.fury.io/js/universal-data-tool\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/92028f7e9832479b26379436370bf619605100a737164a096e0a25b9d03e22ad/68747470733a2f2f62616467652e667572792e696f2f6a732f756e6976657273616c2d646174612d746f6f6c2e737667\" alt=\"npm version\" data-canonical-src=\"https://badge.fury.io/js/universal-data-tool.svg\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/blob/master/LICENSE\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/5185391f359e9731c8034aec54f99194a65ac6578512817c54a4004293f7e785/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f556e6976657273616c44617461546f6f6c2f756e6976657273616c2d646174612d746f6f6c\" alt=\"GitHub license\" data-canonical-src=\"https://img.shields.io/github/license/UniversalDataTool/universal-data-tool\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/8251777825daa5c0552e06169a42b848c94c903ed15187c3963a1273e0cb5e42/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f706c6174666f726d732d57656225323057696e646f77732532304c696e75782532304d61632d626c756576696f6c6574\" alt=\"Platform Support Web/Win/Linux/Mac\" data-canonical-src=\"https://img.shields.io/badge/platforms-Web%20Windows%20Linux%20Mac-blueviolet\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/b4aba1e2ce84f30841c975829eedafa775bf8758ef61f1dfef7376483b37cf52/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f736c61636b2d556e6976657273616c25323044617461253230546f6f6c2d626c75652e7376673f6c6f676f3d736c61636b\" alt=\"Slack Image\" data-canonical-src=\"https://img.shields.io/badge/slack-Universal%20Data%20Tool-blue.svg?logo=slack\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://twitter.com/UniversalDataTl\" rel=\"nofollow\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/6b1ef88e8b5811cfa8ae54c4ca8c30076ee79fa069ef516ef901ba9ff832c2e3/68747470733a2f2f696d672e736869656c64732e696f2f747769747465722f666f6c6c6f772f556e6976657273616c44617461546c3f7374796c653d736f6369616c\" alt=\"Twitter Logo\" data-canonical-src=\"https://img.shields.io/twitter/follow/UniversalDataTl?style=social\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTry it out at \u003ca href=\"https://udt.dev\" rel=\"nofollow\"\u003eudt.dev\u003c/a\u003e, \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003edownload the desktop app\u003c/a\u003e or \u003ca href=\"https://docs.universaldatatool.com/running-on-premise\" rel=\"nofollow\"\u003erun on-premise\u003c/a\u003e.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003ca href=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/91648687-729a3b80-ea38-11ea-92f2-7ce94ae04da6.gif\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n  \u003cb\u003e\n  \u003ca href=\"https://docs.universaldatatool.com\" rel=\"nofollow\"\u003eDocs\u003c/a\u003e \u2022 \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003eWebsite\u003c/a\u003e \u2022 \u003ca href=\"https://udt.dev\" rel=\"nofollow\"\u003ePlayground\u003c/a\u003e \u2022 \u003ca href=\"https://docs.universaldatatool.com/integrate-with-any-web-page/integrate-with-the-javascript-library\" rel=\"nofollow\"\u003eLibrary Usage\u003c/a\u003e \u2022 \u003ca href=\"https://docs.universaldatatool.com/running-on-premise\" rel=\"nofollow\"\u003eOn-Premise\u003c/a\u003e\n  \u003c/b\u003e\n\u003c/p\u003e\n\u003cp\u003eThe Universal Data Tool is a web/desktop app for editing and annotating images, text, audio, documents and to view and edit any data defined in the extensible \u003ca href=\"https://github.com/UniversalDataTool/udt-format\"\u003e.udt.json and .udt.csv standard\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-supported-data\" class=\"anchor\" href=\"#supported-data\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupported Data\u003c/h2\u003e\n\u003cp align=\"center\"\u003e\n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-segmentation\" rel=\"nofollow\"\u003eImage Segmentation\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/image-classification\" rel=\"nofollow\"\u003eImage Classification\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/text-classification\" rel=\"nofollow\"\u003eText Classification\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/named-entity-recognition\" rel=\"nofollow\"\u003eNamed Entity Recognition\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/entity-relations-part-of-speech-tagging\" rel=\"nofollow\"\u003eNamed Entity Relations / Part of Speech Tagging\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/audio-transcription\" rel=\"nofollow\"\u003eAudio Transcription\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/data-entry\" rel=\"nofollow\"\u003eData Entry\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/video-segmentation\" rel=\"nofollow\"\u003eVideo Segmentation\u003c/a\u003e \u2022 \n    \u003ca href=\"https://docs.universaldatatool.com/building-and-labeling-datasets/landmark-annotation\" rel=\"nofollow\"\u003eLandmark / Pose Annotation\u003c/a\u003e\n\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-recent-updates\" class=\"anchor\" href=\"#recent-updates\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eRecent Updates\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://www.youtube.com/channel/UCgFkrRN7CLt7_iTa2WDjf2g\" rel=\"nofollow\"\u003eFollow our development on Youtube!\u003c/a\u003e\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://youtu.be/q20WrCRcG4k\" rel=\"nofollow\"\u003eCommunity Update Video 9\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.youtube.com/watch?v=IBWOaw0jMmM\" rel=\"nofollow\"\u003eCommunity Update Video 8\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"https://youtu.be/glPPFgXibdw\" rel=\"nofollow\"\u003eCommunity Update Video 7\u003c/a\u003e \u003ca href=\"https://universaldatatool.substack.com/p/build-your-dataset-from-coco\" rel=\"nofollow\"\u003e(blog version)\u003c/a\u003e\n\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-features\" class=\"anchor\" href=\"#features\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eFeatures\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eCollaborate with others in real time, no sign up!\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eUsable on \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003eweb\u003c/a\u003e or as \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Installation\"\u003eWindows,Mac or Linux desktop application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eConfigure your project with an easy-to-use GUI\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://universaldatatool.com/courses\" rel=\"nofollow\"\u003eEasily create courses to train your labelers\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eDownload/upload as easy-to-use CSV (\u003ca href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.csv\"\u003esample.udt.csv\u003c/a\u003e) or JSON (\u003ca href=\"https://github.com/UniversalDataTool/udt-format/blob/master/SAMPLE.udt.json\"\u003esample.udt.json\u003c/a\u003e)\u003c/li\u003e\n\u003cli\u003eSupport for Images, Videos, PDFs, Text, Audio Transcription and many other formats\u003c/li\u003e\n\u003cli\u003eCan be \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-React\"\u003eeasily integrated into a React application\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eAnnotate images or videos with classifications, tags, bounding boxes, polygons and points\u003c/li\u003e\n\u003cli\u003eFast Automatic Smart Pixel Segmentation using WebWorkers and WebAssembly\u003c/li\u003e\n\u003cli\u003eImport data from Google Drive, Youtube, CSV, Clipboard and more\u003c/li\u003e\n\u003cli\u003eAnnotate NLP datasets with Named Entity Recognition (NER), classification and Part of Speech (PoS) tagging.\u003c/li\u003e\n\u003cli\u003eEasily \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Pandas\"\u003eload into pandas\u003c/a\u003e or \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Usage-with-Fast.ai\"\u003euse with fast.ai\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003eRuns \u003ca href=\"https://hub.docker.com/r/universaldatatool/universaldatatool\" rel=\"nofollow\"\u003ewith docker\u003c/a\u003e \u003ccode\u003edocker run -p 3000:3000 universaldatatool/universaldatatool\u003c/code\u003e\n\u003c/li\u003e\n\u003cli\u003eRuns \u003ca href=\"https://singularity-hub.org/collections/4792\" rel=\"nofollow\"\u003ewith singularity\u003c/a\u003e \u003ccode\u003esingularity run universaldatatool/universaldatatool\u003c/code\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76154066-06033d00-60a4-11ea-9bbd-69a62780769f.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/91648815-07516900-ea3a-11ea-9355-70dfbf5c8974.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/76157343-9a39c800-60d5-11ea-8dd6-a67c516fcf63.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003ckbd\u003e\u003ca href=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\" target=\"_blank\" rel=\"nofollow\"\u003e\u003cimg width=\"600\" src=\"https://user-images.githubusercontent.com/1910070/93283916-7b607080-f79f-11ea-838d-683829aff1b3.png\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/kbd\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-sponsors\" class=\"anchor\" href=\"#sponsors\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSponsors\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://wao.ai\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107271376-20fbd100-6a1a-11eb-9f82-2d10607591ba.png\" alt=\"wao.ai sponsorship image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://momentum-tech.ca/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107270943-8bf8d800-6a19-11eb-97c2-895b0280aa8a.png\" alt=\"momentum image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\n\u003ca href=\"https://www.enabledintelligence.net/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://user-images.githubusercontent.com/1910070/107271756-aaab9e80-6a1a-11eb-887c-6f5d009f0fd2.png\" alt=\"enabled intelligence image\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-installation\" class=\"anchor\" href=\"#installation\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eInstallation\u003c/h2\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-web-app\" class=\"anchor\" href=\"#web-app\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eWeb App\u003c/h3\u003e\n\u003cp\u003eJust visit \u003ca href=\"https://universaldatatool.com\" rel=\"nofollow\"\u003euniversaldatatool.com\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eTrying to run the web app locally? Run \u003ccode\u003enpm install\u003c/code\u003e then \u003ccode\u003enpm run start\u003c/code\u003e after cloning this repository to start the web server.\u003c/em\u003e\u003c/p\u003e\n\u003ch3\u003e\n\u003ca id=\"user-content-desktop-application\" class=\"anchor\" href=\"#desktop-application\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eDesktop Application\u003c/h3\u003e\n\u003cp\u003eDownload the latest release from the \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/releases\"\u003ereleases page\u003c/a\u003e and run the executable you downloaded.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing\" class=\"anchor\" href=\"#contributing\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e(Optional) Say hi in the \u003ca href=\"https://join.slack.com/t/universaldatatool/shared_invite/zt-d8teykwi-iOSOUfxugKR~M4AJN6VL3g\" rel=\"nofollow\"\u003eSlack channel\u003c/a\u003e!\u003c/li\u003e\n\u003cli\u003eRead \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/wiki/Setup-for-Development\"\u003ethis guide to get started with development\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributors-\" class=\"anchor\" href=\"#contributors-\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributors \u003cg-emoji class=\"g-emoji\" alias=\"sparkles\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/2728.png\"\u003e\u2728\u003c/g-emoji\u003e\n\u003c/h2\u003e\n\u003cp\u003eThanks goes to these wonderful people (\u003ca href=\"https://allcontributors.org/docs/en/emoji-key\" rel=\"nofollow\"\u003eemoji key\u003c/a\u003e):\u003c/p\u003e\n\n\n\n\u003ctable\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://twitter.com/seveibar\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/1910070?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eSeverin Ibarluzea\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=seveibar\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Aseveibar\" title=\"Reviewed Pull Requests\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\"\u003e\ud83d\udc40\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"http://puskuruk.github.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/22892227?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ePuskuruk\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=puskuruk\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/pulls?q=is%3Apr+reviewed-by%3Apuskuruk\" title=\"Reviewed Pull Requests\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"eyes\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f440.png\"\u003e\ud83d\udc40\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/CedricJean\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/63243979?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eCedricJean\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=CedricJean\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"http://berupon.hatenablog.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/1131125?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eberu\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=beru\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/Ownmarc\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/24617457?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMarc\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Ownmarc\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/Wafaa-arbash\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/59834878?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eWafaa-arbash\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=Wafaa-arbash\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/pgrimaud\"\u003e\u003cimg src=\"https://avatars1.githubusercontent.com/u/1866496?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ePierre Grimaud\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=pgrimaud\" title=\"Documentation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"book\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4d6.png\"\u003e\ud83d\udcd6\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/sreevardhanreddi\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/31174432?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003esreevardhanreddi\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=sreevardhanreddi\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/mrdadah\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/11255121?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMohammed Eldadah\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=mrdadah\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://x8795278.blogspot.com/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars3.githubusercontent.com/u/9297254?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ex213212\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=x213212\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/hysios\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/103227?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003ehysios \u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hysios\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://congdv.github.io/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/8192210?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eCong Dao\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=congdv\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://www.linkedin.com/in/renato-gonsalves-499317125/\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/47343193?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eRenato Junior\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-MrJunato\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://gitlab.com/rickstaa\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars0.githubusercontent.com/u/17570430?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eRick\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-rickstaa\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e \u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=rickstaa\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n  \u003ctr\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/anaplian\"\u003e\u003cimg src=\"https://avatars3.githubusercontent.com/u/18647401?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eanaplian\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=anaplian\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://www.behance.net/MiguelCarvalho13\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/6718302?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eMiguel Carvalho\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"#translation-miguelcarvalho13\" title=\"Translation\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"earth_africa\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f30d.png\"\u003e\ud83c\udf0d\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://kyleo.io\" rel=\"nofollow\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/27719893?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eKyle OBrien\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=obrien-k\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/hakkiyagiz\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/12295562?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eHakk\u0131 Ya\u011f\u0131z ERD\u0130N\u00c7\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=hakkiyagiz\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n    \u003ctd align=\"center\"\u003e\n\u003ca href=\"https://github.com/jvdavim\"\u003e\u003cimg src=\"https://avatars2.githubusercontent.com/u/16657663?v=4\" width=\"100px;\" alt=\"\" style=\"max-width:100%;\"\u003e\u003cbr\u003e\u003csub\u003e\u003cb\u003eJo\u00e3o Victor Davim\u003c/b\u003e\u003c/sub\u003e\u003c/a\u003e\u003cbr\u003e\u003ca href=\"https://github.com/UniversalDataTool/universal-data-tool/commits?author=jvdavim\" title=\"Code\"\u003e\u003cg-emoji class=\"g-emoji\" alias=\"computer\" fallback-src=\"https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png\"\u003e\ud83d\udcbb\u003c/g-emoji\u003e\u003c/a\u003e\n\u003c/td\u003e\n  \u003c/tr\u003e\n\u003c/table\u003e\n\n\n\n\u003cp\u003eThis project follows the \u003ca href=\"https://github.com/all-contributors/all-contributors\"\u003eall-contributors\u003c/a\u003e specification. Contributions of any kind welcome!\u003c/p\u003e\n",
    "stargazers_count": 1456,
    "subscribers_count": 33,
    "topics": [
      "computer-vision",
      "annotate-images",
      "entity-recognition",
      "desktop",
      "classification",
      "dataset",
      "annotation-tool",
      "deep-learning",
      "text-annotation",
      "named-entity-recognition",
      "text-labeling",
      "semantic-segmentation",
      "image-segmentation",
      "image-labeling-tool",
      "machine-learning",
      "image-annotation",
      "csv",
      "labeling",
      "labeling-tool",
      "hacktoberfest"
    ],
    "updated_at": 1626868391.0
  },
  {
    "data_format": 2,
    "description": "Singularity: Application containers for Linux",
    "filenames": [
      "e2e/testdata/Singularity",
      "examples/arch/Singularity",
      "examples/asciinema/Singularity",
      "examples/busybox/Singularity",
      "examples/shub/Singularity",
      "examples/apps/Singularity",
      "examples/apps/Singularity.cowsay",
      "examples/ubuntu/Singularity",
      "examples/sle/Singularity",
      "examples/instances/Singularity",
      "examples/docker/Singularity",
      "examples/raspbian/Singularity",
      "examples/centos/Singularity",
      "examples/scratch/Singularity.busybox",
      "examples/scratch/Singularity.alpine",
      "examples/opensuse/Singularity",
      "examples/multistage/Singularity",
      "examples/scientific/Singularity",
      "examples/debian/Singularity",
      "examples/library/Singularity",
      "examples/self/Singularity"
    ],
    "full_name": "hpcng/singularity",
    "latest_release": "v3.8.0",
    "readme": "\u003ch1\u003e\n\u003ca id=\"user-content-singularity\" class=\"anchor\" href=\"#singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSingularity\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/hpcng/singularity/actions/workflows/ci.yml\"\u003e\u003cimg src=\"https://github.com/hpcng/singularity/actions/workflows/ci.yml/badge.svg\" alt=\"CI\" style=\"max-width:100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"CONTRIBUTING.md\"\u003eGuidelines for Contributing\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\".github/PULL_REQUEST_TEMPLATE.md\"\u003ePull Request Template\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"LICENSE.md\"\u003eProject License\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://singularity.hpcng.org/docs/\" rel=\"nofollow\"\u003eDocumentation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#support\"\u003eSupport\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#citing-singularity\"\u003eCitation\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSingularity is an open source container platform designed to be simple, fast,\nand secure. Singularity is optimized for compute focused enterprise and HPC\nworkloads, allowing untrusted users to run untrusted containers in a trusted\nway.\u003c/p\u003e\n\u003cp\u003eCheck out \u003ca href=\"https://singularity.hpcng.org/videos\" rel=\"nofollow\"\u003etalks about Singularity\u003c/a\u003e\nand some \u003ca href=\"https://singularity.hpcng.org/usecases\" rel=\"nofollow\"\u003euse cases of Singularity\u003c/a\u003e\non our website.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-getting-started-with-singularity\" class=\"anchor\" href=\"#getting-started-with-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eGetting Started with Singularity\u003c/h2\u003e\n\u003cp\u003eTo install Singularity from source, see the \u003ca href=\"INSTALL.md\"\u003einstallation\ninstructions\u003c/a\u003e. For other installation options, see \u003ca href=\"https://singularity.hpcng.org/admin-docs/master/installation.html\" rel=\"nofollow\"\u003eour\nguide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eSystem administrators can learn how to configure Singularity, and get an\noverview of its architecture and security features in the \u003ca href=\"https://singularity.hpcng.org/admin-docs/master/\" rel=\"nofollow\"\u003eadministrator\nguide\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor users, see the \u003ca href=\"https://singularity.hpcng.org/user-docs/master/\" rel=\"nofollow\"\u003euser\nguide\u003c/a\u003e for details on how to use\nand build Singularity containers.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-contributing-to-singularity\" class=\"anchor\" href=\"#contributing-to-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eContributing to Singularity\u003c/h2\u003e\n\u003cp\u003eCommunity contributions are always greatly appreciated. To start developing\nSingularity, check out the \u003ca href=\"CONTRIBUTING.md\"\u003eguidelines for contributing\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe also welcome contributions to our \u003ca href=\"https://github.com/hpcng/singularity-userdocs\"\u003euser\nguide\u003c/a\u003e and \u003ca href=\"https://github.com/hpcng/singularity-admindocs\"\u003eadmin\nguide\u003c/a\u003e.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-support\" class=\"anchor\" href=\"#support\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eSupport\u003c/h2\u003e\n\u003cp\u003eTo get help with Singularity, check out the \u003ca href=\"https://singularity.hpcng.org/help\" rel=\"nofollow\"\u003eSingularity\nHelp\u003c/a\u003e web page.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-citing-singularity\" class=\"anchor\" href=\"#citing-singularity\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eCiting Singularity\u003c/h2\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer GM, Sochat V, Bauer MW (2017) Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. https://doi.org/10.1371/journal.pone.0177459\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe also have a Zenodo citation:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eKurtzer, Gregory M. et. al. Singularity - Linux application and environment\ncontainers for science. 10.5281/zenodo.1310023\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ca href=\"https://doi.org/10.5281/zenodo.1310023\" rel=\"nofollow\"\u003ehttps://doi.org/10.5281/zenodo.1310023\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eThis is an \u0027all versions\u0027 DOI. Follow the link to Zenodo to obtain a DOI specific\nto a particular version of Singularity.\u003c/p\u003e\n\u003ch2\u003e\n\u003ca id=\"user-content-license\" class=\"anchor\" href=\"#license\" aria-hidden=\"true\"\u003e\u003cspan aria-hidden=\"true\" class=\"octicon octicon-link\"\u003e\u003c/span\u003e\u003c/a\u003eLicense\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eUnless otherwise noted, this project is licensed under a 3-clause BSD license\nfound in the \u003ca href=\"LICENSE.md\"\u003elicense file\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n",
    "stargazers_count": 2145,
    "subscribers_count": 92,
    "topics": [
      "containers",
      "container",
      "singularity-container",
      "portable",
      "linux",
      "hpc",
      "parallel",
      "singularity",
      "science",
      "reproducible",
      "reproducible-science",
      "portability",
      "rootless-containers",
      "cloud-native"
    ],
    "updated_at": 1627261759.0
  }
]
